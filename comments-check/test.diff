diff --git a/.gitignore b/.gitignore
index 5eda2c31e..5b55ccada 100644
--- a/.gitignore
+++ b/.gitignore
@@ -3,6 +3,9 @@
 
 # ignore generated documentation tables
 doc/guides/nics/overview_table.txt
+.cproject
+.project
+.settings/
 doc/guides/cryptodevs/overview_feature_table.txt
 doc/guides/cryptodevs/overview_cipher_table.txt
 doc/guides/cryptodevs/overview_auth_table.txt
@@ -20,6 +23,9 @@ GPATH
 GRTAGS
 tags
 TAGS
+.pydevproject
+build/
+x86_64-native-linuxapp-gcc/
 
 # ignore python bytecode files
 *.pyc
diff --git a/.upstream b/.upstream
new file mode 100644
index 000000000..824e0b887
--- /dev/null
+++ b/.upstream
@@ -0,0 +1 @@
+git://dpdk.org/dpdk
diff --git a/GNUmakefile b/GNUmakefile
index e8de422df..8df2ad232 100644
--- a/GNUmakefile
+++ b/GNUmakefile
@@ -5,6 +5,7 @@
 # Head Makefile for compiling rte SDK
 #
 
+#导出RTE_SDK
 RTE_SDK := $(CURDIR)
 export RTE_SDK
 
diff --git a/app/test-pmd/cmdline.c b/app/test-pmd/cmdline.c
index 74e02d681..50618483a 100644
--- a/app/test-pmd/cmdline.c
+++ b/app/test-pmd/cmdline.c
@@ -18918,6 +18918,7 @@ cmdline_parse_inst_t cmd_show_port_supported_ptypes = {
 /* ******************************************************************************** */
 
 /* list of instructions */
+//命令行数组
 cmdline_parse_ctx_t main_ctx[] = {
 	(cmdline_parse_inst_t *)&cmd_help_brief,
 	(cmdline_parse_inst_t *)&cmd_help_long,
@@ -19211,6 +19212,7 @@ cmdline_parse_ctx_t main_ctx[] = {
 };
 
 /* read cmdline commands from file */
+//自文件中读取cmd
 void
 cmdline_read_from_file(const char *filename)
 {
diff --git a/app/test-pmd/config.c b/app/test-pmd/config.c
index efe2812a8..77b6856c4 100644
--- a/app/test-pmd/config.c
+++ b/app/test-pmd/config.c
@@ -2029,9 +2029,11 @@ setup_fwd_config_of_each_lcore(struct fwd_config *cfg)
 	nb_fs = cfg->nb_fwd_streams;
 	nb_fc = cfg->nb_fwd_lcores;
 	if (nb_fs <= nb_fc) {
+		//转发流数比core数少时，每个core上一个转发流
 		nb_fs_per_lcore = 1;
 		nb_extra = 0;
 	} else {
+		//转发流数/转发core数
 		nb_fs_per_lcore = (streamid_t) (nb_fs / nb_fc);
 		nb_extra = (lcoreid_t) (nb_fs % nb_fc);
 	}
@@ -2040,6 +2042,7 @@ setup_fwd_config_of_each_lcore(struct fwd_config *cfg)
 	sm_id = 0;
 	for (lc_id = 0; lc_id < nb_lc; lc_id++) {
 		fwd_lcores[lc_id]->stream_idx = sm_id;
+		//指字转发stream数目
 		fwd_lcores[lc_id]->stream_nb = nb_fs_per_lcore;
 		sm_id = (streamid_t) (sm_id + nb_fs_per_lcore);
 	}
@@ -2817,6 +2820,7 @@ setup_gso(const char *mode, portid_t port_id)
 	}
 }
 
+//列出所有forward modules
 char*
 list_pkt_forwarding_modes(void)
 {
@@ -2838,6 +2842,7 @@ list_pkt_forwarding_modes(void)
 	return fwd_modes;
 }
 
+//列出转发模式名称（跳过rx_only_engine)
 char*
 list_pkt_forwarding_retry_modes(void)
 {
@@ -2849,7 +2854,7 @@ list_pkt_forwarding_retry_modes(void)
 	if (strlen(fwd_modes) == 0) {
 		while ((fwd_eng = fwd_engines[i++]) != NULL) {
 			if (fwd_eng == &rx_only_engine)
-				continue;
+				continue;//跳过rx_only_engine
 			strncat(fwd_modes, fwd_eng->fwd_mode_name,
 					sizeof(fwd_modes) -
 					strlen(fwd_modes) - 1);
@@ -2863,6 +2868,7 @@ list_pkt_forwarding_retry_modes(void)
 	return fwd_modes;
 }
 
+//设置转发引擎
 void
 set_pkt_forwarding_mode(const char *fwd_mode_name)
 {
diff --git a/app/test-pmd/csumonly.c b/app/test-pmd/csumonly.c
index 5738128e6..56edafd69 100644
--- a/app/test-pmd/csumonly.c
+++ b/app/test-pmd/csumonly.c
@@ -1064,6 +1064,7 @@ pkt_burst_checksum_forward(struct fwd_stream *fs)
 		nb_rx = nb_segments;
 	}
 
+	//发包前准备函数
 	nb_prep = rte_eth_tx_prepare(fs->tx_port, fs->tx_queue,
 			tx_pkts_burst, nb_rx);
 	if (nb_prep != nb_rx)
diff --git a/app/test-pmd/icmpecho.c b/app/test-pmd/icmpecho.c
index 2d359c943..be00bdad3 100644
--- a/app/test-pmd/icmpecho.c
+++ b/app/test-pmd/icmpecho.c
@@ -330,6 +330,7 @@ reply_to_icmp_echo_rqsts(struct fwd_stream *fs)
 			ether_addr_dump("  ETH:  src=", &eth_h->s_addr);
 			ether_addr_dump(" dst=", &eth_h->d_addr);
 		}
+		//vlan处理
 		if (eth_type == RTE_ETHER_TYPE_VLAN) {
 			vlan_h = (struct rte_vlan_hdr *)
 				((char *)eth_h + sizeof(struct rte_ether_hdr));
@@ -383,6 +384,7 @@ reply_to_icmp_echo_rqsts(struct fwd_stream *fs)
 				ipv4_addr_dump(" tip=", ip_addr);
 				printf("\n");
 			}
+			//仅处理arp request,进入arp响应
 			if (arp_op != RTE_ARP_OP_REQUEST) {
 				rte_pktmbuf_free(pkt);
 				continue;
@@ -414,6 +416,7 @@ reply_to_icmp_echo_rqsts(struct fwd_stream *fs)
 			continue;
 		}
 
+		//仅处理ipv4报文
 		if (eth_type != RTE_ETHER_TYPE_IPV4) {
 			rte_pktmbuf_free(pkt);
 			continue;
@@ -427,6 +430,8 @@ reply_to_icmp_echo_rqsts(struct fwd_stream *fs)
 			       ip_proto_name(ip_h->next_proto_id));
 		}
 
+		//仅处理icmp echo request报文，如果不是reqeust，则丢包
+		//否则进行icmp reply报文构造并发送
 		/*
 		 * Check if packet is a ICMP echo request.
 		 */
diff --git a/app/test-pmd/iofwd.c b/app/test-pmd/iofwd.c
index 9dce76efe..79d14f9d1 100644
--- a/app/test-pmd/iofwd.c
+++ b/app/test-pmd/iofwd.c
@@ -64,21 +64,24 @@ pkt_burst_io_forward(struct fwd_stream *fs)
 	/*
 	 * Receive a burst of packets and forward them.
 	 */
+	//自rx_port读取报文
 	nb_rx = rte_eth_rx_burst(fs->rx_port, fs->rx_queue,
 			pkts_burst, nb_pkt_per_burst);
 	if (unlikely(nb_rx == 0))
 		return;
-	fs->rx_packets += nb_rx;
+	fs->rx_packets += nb_rx;//增加收包统计
 
 #ifdef RTE_TEST_PMD_RECORD_BURST_STATS
 	fs->rx_burst_stats.pkt_burst_spread[nb_rx]++;
 #endif
+	//自tx_port写入报文
 	nb_tx = rte_eth_tx_burst(fs->tx_port, fs->tx_queue,
 			pkts_burst, nb_rx);
 	/*
 	 * Retry if necessary
 	 */
 	if (unlikely(nb_tx < nb_rx) && fs->retry_enabled) {
+		//如果未全部发出，且开启了发送重试，则尝试再次发送
 		retry = 0;
 		while (nb_tx < nb_rx && retry++ < burst_tx_retry_num) {
 			rte_delay_us(burst_tx_delay_time);
@@ -86,10 +89,12 @@ pkt_burst_io_forward(struct fwd_stream *fs)
 					&pkts_burst[nb_tx], nb_rx - nb_tx);
 		}
 	}
+	//报文发送
 	fs->tx_packets += nb_tx;
 #ifdef RTE_TEST_PMD_RECORD_BURST_STATS
 	fs->tx_burst_stats.pkt_burst_spread[nb_tx]++;
 #endif
+	//增加丢包数
 	if (unlikely(nb_tx < nb_rx)) {
 		fs->fwd_dropped += (nb_rx - nb_tx);
 		do {
diff --git a/app/test-pmd/parameters.c b/app/test-pmd/parameters.c
index 9ea87c147..da0bdaa81 100644
--- a/app/test-pmd/parameters.c
+++ b/app/test-pmd/parameters.c
@@ -568,6 +568,7 @@ launch_args_parse(int argc, char** argv)
 	int ret;
 
 	static struct option lgopts[] = {
+		//1.选项名；2。参数控制（1有参数，0无参数，2可选参数）;3.保存value的指针;4.val值
 		{ "help",			0, 0, 0 },
 #ifdef RTE_LIBRTE_CMDLINE
 		{ "interactive",		0, 0, 0 },
@@ -676,17 +677,20 @@ launch_args_parse(int argc, char** argv)
 		switch (opt) {
 #ifdef RTE_LIBRTE_CMDLINE
 		case 'i':
+			//交互模式
 			printf("Interactive-mode selected\n");
 			interactive = 1;
 			break;
 #endif
 		case 'a':
 			printf("Auto-start selected\n");
+			//自动开始
 			auto_start = 1;
 			break;
 
 		case 0: /*long options */
 			if (!strcmp(lgopts[opt_idx].name, "help")) {
+				//help显示
 				usage(argv[0]);
 				rte_exit(EXIT_SUCCESS, "Displayed help\n");
 			}
@@ -1022,6 +1026,7 @@ launch_args_parse(int argc, char** argv)
 						 optarg);
 			}
 			if (!strcmp(lgopts[opt_idx].name, "forward-mode"))
+				//设置转发引擎
 				set_pkt_forwarding_mode(optarg);
 			if (!strcmp(lgopts[opt_idx].name, "rss-ip"))
 				rss_hf = ETH_RSS_IP;
diff --git a/app/test-pmd/testpmd.c b/app/test-pmd/testpmd.c
index 38acbc58a..5c0dbf481 100644
--- a/app/test-pmd/testpmd.c
+++ b/app/test-pmd/testpmd.c
@@ -145,6 +145,7 @@ portid_t nb_peer_eth_addrs = 0;
 struct rte_port *ports;	       /**< For all probed ethernet ports. */
 portid_t nb_ports;             /**< Number of probed ethernet ports. */
 struct fwd_lcore **fwd_lcores; /**< For all probed logical cores. */
+//检测出的可用于转发的core数目
 lcoreid_t nb_lcores;           /**< Number of probed logical cores. */
 
 portid_t ports_ids[RTE_MAX_ETHPORTS]; /**< Store all port ids. */
@@ -157,8 +158,10 @@ portid_t ports_ids[RTE_MAX_ETHPORTS]; /**< Store all port ids. */
 lcoreid_t nb_cfg_lcores; /**< Number of configured logical cores. */
 lcoreid_t nb_fwd_lcores; /**< Number of forwarding logical cores. */
 portid_t  nb_cfg_ports;  /**< Number of configured ports. */
+//设置要转发的接口数
 portid_t  nb_fwd_ports;  /**< Number of forwarding ports. */
 
+//可用于转发的core
 unsigned int fwd_lcores_cpuids[RTE_MAX_LCORE]; /**< CPU ids configuration. */
 portid_t fwd_ports_ids[RTE_MAX_ETHPORTS];      /**< Port ids configuration. */
 
@@ -167,6 +170,7 @@ streamid_t nb_fwd_streams;       /**< Is equal to (nb_ports * nb_rxq). */
 
 /*
  * Forwarding engines.
+ * 转发engines
  */
 struct fwd_engine * fwd_engines[] = {
 	&io_fwd_engine,
@@ -190,12 +194,15 @@ struct fwd_engine * fwd_engines[] = {
 struct rte_mempool *mempools[RTE_MAX_NUMA_NODES];
 uint16_t mempool_flags;
 
+//配置的转发配置
 struct fwd_config cur_fwd_config;
+//配置的转发引擎,默认是io_fwd_engine
 struct fwd_engine *cur_fwd_eng = &io_fwd_engine; /**< IO mode by default. */
 uint32_t retry_enabled;
 uint32_t burst_tx_delay_time = BURST_TX_WAIT_US;
 uint32_t burst_tx_retry_num = BURST_TX_RETRIES;
 
+//mbuf缓冲区大小
 uint16_t mbuf_data_size = DEFAULT_MBUF_DATA_SIZE; /**< Mbuf data space size. */
 uint32_t param_total_num_mbufs = 0;  /**< number of mbufs in all pools - if
                                       * specified on command-line. */
@@ -465,6 +472,7 @@ uint16_t nb_rx_queue_stats_mappings = 0;
 uint8_t xstats_hide_zero;
 
 unsigned int num_sockets = 0;
+//记录系统出现的socket id
 unsigned int socket_ids[RTE_MAX_NUMA_NODES];
 
 #ifdef RTE_LIBRTE_BITRATE
@@ -502,6 +510,7 @@ uint16_t gso_max_segment_size = RTE_ETHER_MAX_LEN - RTE_ETHER_CRC_LEN;
  * Helper function to check if socket is already discovered.
  * If yes, return positive value. If not, return zero.
  */
+//检查socket_id是否为一个新出现的socket id
 int
 new_socket_id(unsigned int socket_id)
 {
@@ -521,7 +530,7 @@ static void
 set_default_fwd_lcores_config(void)
 {
 	unsigned int i;
-	unsigned int nb_lc;
+	unsigned int nb_lc;//原文：number logic core
 	unsigned int sock_num;
 
 	nb_lc = 0;
@@ -530,6 +539,7 @@ set_default_fwd_lcores_config(void)
 			continue;
 		sock_num = rte_lcore_to_socket_id(i);
 		if (new_socket_id(sock_num)) {
+			//识别出新的socket,检查sockets是否超限
 			if (num_sockets >= RTE_MAX_NUMA_NODES) {
 				rte_exit(EXIT_FAILURE,
 					 "Total sockets greater than %u\n",
@@ -537,8 +547,10 @@ set_default_fwd_lcores_config(void)
 			}
 			socket_ids[num_sockets++] = sock_num;
 		}
+		//跳过master core
 		if (i == rte_get_master_lcore())
 			continue;
+		//记录可用于转发的core
 		fwd_lcores_cpuids[nb_lc++] = i;
 	}
 	nb_lcores = (lcoreid_t) nb_lc;
@@ -546,6 +558,7 @@ set_default_fwd_lcores_config(void)
 	nb_fwd_lcores = 1;
 }
 
+//设置对端默认以太网地址'02:00:00:00:00:$port_id'
 static void
 set_def_peer_eth_addrs(void)
 {
@@ -563,6 +576,7 @@ set_default_fwd_ports_config(void)
 	portid_t pt_id;
 	int i = 0;
 
+	//遍历所有未用port
 	RTE_ETH_FOREACH_DEV(pt_id) {
 		fwd_ports_ids[i++] = pt_id;
 
@@ -585,6 +599,7 @@ set_default_fwd_ports_config(void)
 void
 set_def_fwd_config(void)
 {
+	//默认使用除master以外所有enable的core
 	set_default_fwd_lcores_config();
 	set_def_peer_eth_addrs();
 	set_default_fwd_ports_config();
@@ -864,11 +879,13 @@ dma_map_cb(struct rte_mempool *mp __rte_unused, void *opaque __rte_unused,
 /*
  * Configuration initialisation done once at init time.
  */
+//创建mbuf pool,每个mbuf大小@mbuf_seg_size,mbuf的数量@nb_mbuf
+//在哪个numa上创建pool @socket_id
 static struct rte_mempool *
 mbuf_pool_create(uint16_t mbuf_seg_size, unsigned nb_mbuf,
 		 unsigned int socket_id)
 {
-	char pool_name[RTE_MEMPOOL_NAMESIZE];
+	char pool_name[RTE_MEMPOOL_NAMESIZE];//pool 名称
 	struct rte_mempool *rte_mp = NULL;
 	uint32_t mb_size;
 
@@ -1110,6 +1127,7 @@ init_config(void)
 		port->dev_conf.txmode = tx_mode;
 		port->dev_conf.rxmode = rx_mode;
 
+		//取设备信息
 		ret = eth_dev_info_get_print_err(pid, &port->dev_info);
 		if (ret != 0)
 			rte_exit(EXIT_FAILURE,
@@ -1186,6 +1204,7 @@ init_config(void)
 	if (param_total_num_mbufs)
 		nb_mbuf_per_pool = param_total_num_mbufs;
 	else {
+		//如果未指定mbuf数量，则自动计算mbuf数量
 		nb_mbuf_per_pool = RTE_TEST_RX_DESC_MAX +
 			(nb_lcores * mb_mempool_cache) +
 			RTE_TEST_TX_DESC_MAX + MAX_PKT_BURST;
@@ -1196,10 +1215,12 @@ init_config(void)
 		uint8_t i;
 
 		for (i = 0; i < num_sockets; i++)
+			//创建mbuf pool
 			mempools[i] = mbuf_pool_create(mbuf_data_size,
 						       nb_mbuf_per_pool,
 						       socket_ids[i]);
 	} else {
+		//非numa情况，只创建一个mbuf pool
 		if (socket_num == UMA_NO_CONFIG)
 			mempools[0] = mbuf_pool_create(mbuf_data_size,
 						       nb_mbuf_per_pool, 0);
@@ -1218,6 +1239,7 @@ init_config(void)
 	 * Records which Mbuf pool to use by each logical core, if needed.
 	 */
 	for (lc_id = 0; lc_id < nb_lcores; lc_id++) {
+		//取lc_id对应socket的mbuf
 		mbp = mbuf_pool_find(
 			rte_lcore_to_socket_id(fwd_lcores_cpuids[lc_id]));
 
@@ -1778,6 +1800,7 @@ run_pkt_fwd_on_lcore(struct fwd_lcore *fc, packet_fwd_t pkt_fwd)
 	} while (! fc->stopped);
 }
 
+//每个core上运行报文发送
 static int
 start_pkt_forward_on_core(void *fwd_arg)
 {
@@ -1816,15 +1839,18 @@ launch_packet_forwarding(lcore_function_t *pkt_fwd_on_lcore)
 	unsigned int lc_id;
 	int diag;
 
+	//如果在forward_begin回调，则在执行forward前执行begin回调
 	port_fwd_begin = cur_fwd_config.fwd_eng->port_fwd_begin;
 	if (port_fwd_begin != NULL) {
 		for (i = 0; i < cur_fwd_config.nb_fwd_ports; i++)
 			(*port_fwd_begin)(fwd_ports_ids[i]);
 	}
+	//执行forward回调
 	for (i = 0; i < cur_fwd_config.nb_fwd_lcores; i++) {
 		lc_id = fwd_lcores_cpuids[i];
 		if ((interactive == 0) || (lc_id != rte_lcore_id())) {
 			fwd_lcores[i]->stopped = 0;
+			//执行core $lc_id运行pkt_fwd_on_lcore函数（fwd_lcores是它的参数）
 			diag = rte_eal_remote_launch(pkt_fwd_on_lcore,
 						     fwd_lcores[i], lc_id);
 			if (diag != 0)
@@ -1859,6 +1885,7 @@ start_packet_forwarding(int with_tx_first)
 			"Either rxq or txq are 0, cannot use %s fwd mode\n",
 			cur_fwd_eng->fwd_mode_name);
 
+	//确认是否所有接口均已start
 	if (all_ports_started() == 0) {
 		printf("Not all ports were started\n");
 		return;
@@ -2173,6 +2200,7 @@ start_port(portid_t pid)
 			RTE_PORT_HANDLING, RTE_PORT_STARTED) == 0)
 			printf("Port %d can not be set into started\n", pi);
 
+		//显示设备网卡
 		if (eth_macaddr_get_print_err(pi, &mac_addr) == 0)
 			printf("Port %d: %02X:%02X:%02X:%02X:%02X:%02X\n", pi,
 				mac_addr.addr_bytes[0], mac_addr.addr_bytes[1],
@@ -2737,6 +2765,7 @@ register_eth_event_callback(void)
 }
 
 /* This function is used by the interrupt thread */
+//仅用于对外告警
 static void
 dev_event_callback(const char *device_name, enum rte_dev_event_type type,
 			     __rte_unused void *arg)
@@ -3170,6 +3199,7 @@ init_port_dcb_config(portid_t pid,
 	return 0;
 }
 
+//申请ports数组
 static void
 init_port(void)
 {
@@ -3193,7 +3223,7 @@ static void
 force_quit(void)
 {
 	pmd_test_exit();
-	prompt_exit();
+	prompt_exit();//退出命令行
 }
 
 static void
@@ -3231,11 +3261,12 @@ signal_handler(int signum)
 		/* Set flag to indicate the force termination. */
 		f_quit = 1;
 		/* exit with the expected status */
-		signal(signum, SIG_DFL);
+		signal(signum, SIG_DFL);//改为默认函数，并发信号杀死自已
 		kill(getpid(), signum);
 	}
 }
 
+//test-pmd测试程序入口
 int
 main(int argc, char** argv)
 {
@@ -3244,12 +3275,14 @@ main(int argc, char** argv)
 	uint16_t count;
 	int ret;
 
+	//收到SIGINT,SIGTERM信号处，执行进程退出
 	signal(SIGINT, signal_handler);
 	signal(SIGTERM, signal_handler);
 
 	testpmd_logtype = rte_log_register("testpmd");
 	if (testpmd_logtype < 0)
 		rte_exit(EXIT_FAILURE, "Cannot register log type");
+	//设置此模块的debug level
 	rte_log_set_level(testpmd_logtype, RTE_LOG_DEBUG);
 
 	diag = rte_eal_init(argc, argv);
@@ -3277,10 +3310,11 @@ main(int argc, char** argv)
 	}
 	nb_ports = (portid_t) count;
 	if (nb_ports == 0)
+		//未识别出任何port,报错
 		TESTPMD_LOG(WARNING, "No probed ethernet devices\n");
 
 	/* allocate port structures, and init them */
-	init_port();
+	init_port();//申请port数组
 
 	set_def_fwd_config();
 	if (nb_lcores == 0)
@@ -3305,13 +3339,16 @@ main(int argc, char** argv)
 	argc -= diag;
 	argv += diag;
 	if (argc > 1)
+		//存在传入给test-pmd的参数，这里解析这些参数
 		launch_args_parse(argc, argv);
 
+	//防止内存换出
 	if (do_mlockall && mlockall(MCL_CURRENT | MCL_FUTURE)) {
 		TESTPMD_LOG(NOTICE, "mlockall() failed with error \"%s\"\n",
 			strerror(errno));
 	}
 
+	//参数互斥检查
 	if (tx_first && interactive)
 		rte_exit(EXIT_FAILURE, "--tx-first cannot be used on "
 				"interactive mode.\n");
@@ -3322,6 +3359,7 @@ main(int argc, char** argv)
 		lsc_interrupt = 0;
 	}
 
+	//收发队列数不能为0
 	if (!nb_rxq && !nb_txq)
 		printf("Warning: Either rx or tx queues should be non-zero\n");
 
@@ -3356,10 +3394,12 @@ main(int argc, char** argv)
 		}
 	}
 
+	//启动所有port
 	if (!no_device_start && start_port(RTE_PORT_ALL) != 0)
 		rte_exit(EXIT_FAILURE, "Start ports failed\n");
 
 	/* set all ports to promiscuous mode by default */
+	//所有接口开启混杂模式
 	RTE_ETH_FOREACH_DEV(port_id) {
 		ret = rte_eth_promiscuous_enable(port_id);
 		if (ret != 0)
diff --git a/app/test-pmd/testpmd.h b/app/test-pmd/testpmd.h
index ec10a1a35..5f6612204 100644
--- a/app/test-pmd/testpmd.h
+++ b/app/test-pmd/testpmd.h
@@ -209,6 +209,7 @@ struct fwd_lcore {
 	struct rte_mempool *mbp; /**< The mbuf pool to use by this core */
 	void *gro_ctx;		/**< GRO context */
 	streamid_t stream_idx;   /**< index of 1st stream in "fwd_streams" */
+	//多少转发流
 	streamid_t stream_nb;    /**< number of streams in "fwd_streams" */
 	lcoreid_t  cpuid_idx;    /**< index of logical core in CPU id table */
 	queueid_t  tx_queue;     /**< TX queue to send forwarded packets */
@@ -608,6 +609,7 @@ current_fwd_lcore(void)
 	return fwd_lcores[lcore_num()];
 }
 
+//构造mbuf pool名称
 /* Mbuf Pools */
 static inline void
 mbuf_poolname_build(unsigned int sock_id, char* mp_name, int name_size)
diff --git a/buildtools/gen-config-h.sh b/buildtools/gen-config-h.sh
index a8c200633..eb4256b53 100755
--- a/buildtools/gen-config-h.sh
+++ b/buildtools/gen-config-h.sh
@@ -4,6 +4,8 @@
 
 echo "#ifndef __RTE_CONFIG_H"
 echo "#define __RTE_CONFIG_H"
+# 采用一连中的sed替换，将CONFIG_XX=y,n,FF格式
+# 替换为 c 语言可识别的宏定义
 grep CONFIG_ $1 |
 grep -v '^[ \t]*#' |
 sed 's,CONFIG_\(.*\)=y.*$,#undef \1\
diff --git a/do.sh b/do.sh
new file mode 100755
index 000000000..6344edff8
--- /dev/null
+++ b/do.sh
@@ -0,0 +1,49 @@
+#! /bin/bash
+function ubuntu_install()
+{
+	echo "ubuntu install"
+	sudo apt install -y gcc libnuma-devel
+}
+
+function centos_install()
+{
+	echo "centos install"
+	sudo yum install -y gcc numactl-devel 
+}
+
+function UNKOWN_install()
+{
+	echo "os unkown,quit"
+	exit 1
+}
+
+function detect_os()
+{
+	name=`cat /etc/os-release | grep "\<NAME\>" | cut -d':' -f 2 | tr [:upper:] [:lower:]`
+	if  `echo "$name" | grep -q "centos"`  ;
+	then
+		echo "centos"
+
+	elif `echo "$name" | grep -q "ubuntu"` ;
+	then
+		echo "ubuntu"
+
+	else
+		echo "UNKOWN"
+	fi;
+}
+
+function install()
+{
+	os=`detect_os`
+	${os}_install
+	
+}
+
+install
+export RTE_TARGET=x86_64-native-linuxapp-gcc
+export EXTRA_CFLAGS="-g -O0"
+make config T=$RTE_TARGET
+make V=1 -j20
+#make install -j20 T=$RTE_TARGET
+make V=1 install -j20 T=$RTE_TARGET DESTDIR=$RTE_TARGET
diff --git a/drivers/bus/fslmc/fslmc_bus.c b/drivers/bus/fslmc/fslmc_bus.c
index b3e964aa9..bf2f2e074 100644
--- a/drivers/bus/fslmc/fslmc_bus.c
+++ b/drivers/bus/fslmc/fslmc_bus.c
@@ -653,6 +653,7 @@ struct rte_fslmc_bus rte_fslmc_bus = {
 	.device_count = {0},
 };
 
+//加载时注册rte_fslmc_bus.bus
 RTE_REGISTER_BUS(FSLMC_BUS_NAME, rte_fslmc_bus.bus);
 
 RTE_INIT(fslmc_init_log)
diff --git a/drivers/bus/ifpga/ifpga_bus.c b/drivers/bus/ifpga/ifpga_bus.c
index dfd6b1fba..123cc9204 100644
--- a/drivers/bus/ifpga/ifpga_bus.c
+++ b/drivers/bus/ifpga/ifpga_bus.c
@@ -472,6 +472,7 @@ static struct rte_bus rte_ifpga_bus = {
 	.parse       = ifpga_parse,
 };
 
+//注册fpga bus
 RTE_REGISTER_BUS(IFPGA_BUS_NAME, rte_ifpga_bus);
 
 RTE_INIT(ifpga_init_log)
diff --git a/drivers/bus/pci/bsd/pci.c b/drivers/bus/pci/bsd/pci.c
index ebbfeb13a..b14e36b81 100644
--- a/drivers/bus/pci/bsd/pci.c
+++ b/drivers/bus/pci/bsd/pci.c
@@ -109,6 +109,7 @@ pci_uio_free_resource(struct rte_pci_device *dev,
 	}
 }
 
+//申请uio_res
 int
 pci_uio_alloc_resource(struct rte_pci_device *dev,
 		struct mapped_pci_resource **uio_res)
@@ -121,6 +122,7 @@ pci_uio_alloc_resource(struct rte_pci_device *dev,
 	snprintf(devname, sizeof(devname), "/dev/uio@pci:%u:%u:%u",
 			dev->addr.bus, dev->addr.devid, dev->addr.function);
 
+	//尝试读写/dev/uio@pci:，检查文件是否存在
 	if (access(devname, O_RDWR) < 0) {
 		RTE_LOG(WARNING, EAL, "  "PCI_PRI_FMT" not managed by UIO driver, "
 				"skipping\n", loc->domain, loc->bus, loc->devid, loc->function);
@@ -137,6 +139,7 @@ pci_uio_alloc_resource(struct rte_pci_device *dev,
 	dev->intr_handle.type = RTE_INTR_HANDLE_UIO;
 
 	/* allocate the mapping details for secondary processes*/
+	//申请内存
 	*uio_res = rte_zmalloc("UIO_RES", sizeof(**uio_res), 0);
 	if (*uio_res == NULL) {
 		RTE_LOG(ERR, EAL,
@@ -180,6 +183,7 @@ pci_uio_map_resource_by_index(struct rte_pci_device *dev, int res_idx,
 	/*
 	 * open resource file, to mmap it
 	 */
+	//打开设备
 	fd = open(devname, O_RDWR);
 	if (fd < 0) {
 		RTE_LOG(ERR, EAL, "Cannot open %s: %s\n",
@@ -188,6 +192,7 @@ pci_uio_map_resource_by_index(struct rte_pci_device *dev, int res_idx,
 	}
 
 	/* if matching map is found, then use it */
+	//通过mmap来映射设备的内存
 	offset = res_idx * pagesz;
 	mapaddr = pci_map_resource(NULL, fd, (off_t)offset,
 			(size_t)dev->mem_resource[res_idx].len, 0);
diff --git a/drivers/bus/pci/linux/pci.c b/drivers/bus/pci/linux/pci.c
index 740a2cdad..1e1de4695 100644
--- a/drivers/bus/pci/linux/pci.c
+++ b/drivers/bus/pci/linux/pci.c
@@ -30,6 +30,7 @@
 
 extern struct rte_pci_bus rte_pci_bus;
 
+//获取pci设备用哪个kernel驱动
 static int
 pci_get_kernel_driver_by_path(const char *filename, char *dri_name,
 			      size_t len)
@@ -41,13 +42,14 @@ pci_get_kernel_driver_by_path(const char *filename, char *dri_name,
 	if (!filename || !dri_name)
 		return -1;
 
+	//读取link
 	count = readlink(filename, path, PATH_MAX);
 	if (count >= PATH_MAX)
 		return -1;
 
 	/* For device does not have a driver */
 	if (count < 0)
-		return 1;
+		return 1;//还没有driver
 
 	path[count] = '\0';
 
@@ -68,15 +70,17 @@ rte_pci_map_device(struct rte_pci_device *dev)
 
 	/* try mapping the NIC resources using VFIO if it exists */
 	switch (dev->kdrv) {
-	case RTE_KDRV_VFIO:
+	case RTE_KDRV_VFIO://采用vfio驱动
 #ifdef VFIO_PRESENT
 		if (pci_vfio_is_enabled())
 			ret = pci_vfio_map_resource(dev);
 #endif
 		break;
+		//采用igb_uio驱动方式时，映射资源
 	case RTE_KDRV_IGB_UIO:
 	case RTE_KDRV_UIO_GENERIC:
 		if (rte_eal_using_phys_addrs()) {
+			//可以使用物理地址，为uio来映射资源
 			/* map resources for devices that use uio */
 			ret = pci_uio_map_resource(dev);
 		}
@@ -115,6 +119,7 @@ rte_pci_unmap_device(struct rte_pci_device *dev)
 	}
 }
 
+//返回虚拟地址尾部
 static int
 find_max_end_va(const struct rte_memseg_list *msl, void *arg)
 {
@@ -140,6 +145,7 @@ pci_find_max_end_va(void)
 /* parse one line of the "resource" sysfs file (note that the 'line'
  * string is modified)
  */
+//解析pci资源行
 int
 pci_parse_one_sysfs_resource(char *line, size_t len, uint64_t *phys_addr,
 	uint64_t *end_addr, uint64_t *flags)
@@ -153,6 +159,7 @@ pci_parse_one_sysfs_resource(char *line, size_t len, uint64_t *phys_addr,
 		char *ptrs[PCI_RESOURCE_FMT_NVAL];
 	} res_info;
 
+	//将buf按空格分为三部分
 	if (rte_strsplit(line, len, res_info.ptrs, 3, ' ') != 3) {
 		RTE_LOG(ERR, EAL,
 			"%s(): bad resource format\n", __func__);
@@ -172,6 +179,7 @@ pci_parse_one_sysfs_resource(char *line, size_t len, uint64_t *phys_addr,
 }
 
 /* parse the "resource" sysfs file */
+//解析资源文件，填充dev->mem_resource
 static int
 pci_parse_sysfs_resource(const char *filename, struct rte_pci_device *dev)
 {
@@ -186,6 +194,7 @@ pci_parse_sysfs_resource(const char *filename, struct rte_pci_device *dev)
 		return -1;
 	}
 
+	//最多提取6块资源
 	for (i = 0; i<PCI_MAX_RESOURCE; i++) {
 
 		if (fgets(buf, sizeof(buf), f) == NULL) {
@@ -193,10 +202,13 @@ pci_parse_sysfs_resource(const char *filename, struct rte_pci_device *dev)
 				"%s(): cannot read resource\n", __func__);
 			goto error;
 		}
+
+		//每行数据由三部分组成: 物理地址,结束地址，标记位
 		if (pci_parse_one_sysfs_resource(buf, sizeof(buf), &phys_addr,
 				&end_addr, &flags) < 0)
 			goto error;
 
+		//收集io resource memory
 		if (flags & IORESOURCE_MEM) {
 			dev->mem_resource[i].phys_addr = phys_addr;
 			dev->mem_resource[i].len = end_addr - phys_addr + 1;
@@ -213,6 +225,7 @@ pci_parse_sysfs_resource(const char *filename, struct rte_pci_device *dev)
 }
 
 /* Scan one pci sysfs entry, and fill the devices list from it. */
+//扫描一个pci文件夹，并将此设备挂在devices链上
 static int
 pci_scan_one(const char *dirname, const struct rte_pci_addr *addr)
 {
@@ -222,6 +235,7 @@ pci_scan_one(const char *dirname, const struct rte_pci_addr *addr)
 	char driver[PATH_MAX];
 	int ret;
 
+	//申请一个空的pci设备
 	dev = malloc(sizeof(*dev));
 	if (dev == NULL)
 		return -1;
@@ -231,6 +245,7 @@ pci_scan_one(const char *dirname, const struct rte_pci_addr *addr)
 	dev->addr = *addr;
 
 	/* get vendor id */
+	//取设备的vendor_id
 	snprintf(filename, sizeof(filename), "%s/vendor", dirname);
 	if (eal_parse_sysfs_value(filename, &tmp) < 0) {
 		free(dev);
@@ -275,12 +290,14 @@ pci_scan_one(const char *dirname, const struct rte_pci_addr *addr)
 	dev->id.class_id = (uint32_t)tmp & RTE_CLASS_ANY_ID;
 
 	/* get max_vfs */
+	//如果有max_vfs，则读取max_vfs
 	dev->max_vfs = 0;
 	snprintf(filename, sizeof(filename), "%s/max_vfs", dirname);
 	if (!access(filename, F_OK) &&
 	    eal_parse_sysfs_value(filename, &tmp) == 0)
 		dev->max_vfs = (uint16_t)tmp;
 	else {
+		//如果有sriov_numvfs,则读取max_vfs
 		/* for non igb_uio driver, need kernel version >= 3.8 */
 		snprintf(filename, sizeof(filename),
 			 "%s/sriov_numvfs", dirname);
@@ -290,6 +307,7 @@ pci_scan_one(const char *dirname, const struct rte_pci_addr *addr)
 	}
 
 	/* get numa node, default to 0 if not present */
+	//获取numa节点
 	snprintf(filename, sizeof(filename), "%s/numa_node",
 		 dirname);
 
@@ -305,6 +323,7 @@ pci_scan_one(const char *dirname, const struct rte_pci_addr *addr)
 	pci_name_set(dev);
 
 	/* parse resources */
+	//取设备的resource
 	snprintf(filename, sizeof(filename), "%s/resource", dirname);
 	if (pci_parse_sysfs_resource(filename, dev) < 0) {
 		RTE_LOG(ERR, EAL, "%s(): cannot parse resource\n", __func__);
@@ -314,6 +333,7 @@ pci_scan_one(const char *dirname, const struct rte_pci_addr *addr)
 
 	/* parse driver */
 	snprintf(filename, sizeof(filename), "%s/driver", dirname);
+	//读取filename链接的地址或者其本身，如果返回值为0，则通过链接地址得知驱动名，如果为1，则为无驱动
 	ret = pci_get_kernel_driver_by_path(filename, driver, sizeof(driver));
 	if (ret < 0) {
 		RTE_LOG(ERR, EAL, "Fail to get kernel driver\n");
@@ -322,6 +342,7 @@ pci_scan_one(const char *dirname, const struct rte_pci_addr *addr)
 	}
 
 	if (!ret) {
+		//有驱动，检查驱动，设置驱动类型
 		if (!strcmp(driver, "vfio-pci"))
 			dev->kdrv = RTE_KDRV_VFIO;
 		else if (!strcmp(driver, "igb_uio"))
@@ -335,7 +356,7 @@ pci_scan_one(const char *dirname, const struct rte_pci_addr *addr)
 
 	/* device is valid, add in list (sorted) */
 	if (TAILQ_EMPTY(&rte_pci_bus.device_list)) {
-		rte_pci_add_device(dev);
+		rte_pci_add_device(dev);//链为空，直接加入
 	} else {
 		struct rte_pci_device *dev2;
 		int ret;
@@ -343,12 +364,13 @@ pci_scan_one(const char *dirname, const struct rte_pci_addr *addr)
 		TAILQ_FOREACH(dev2, &rte_pci_bus.device_list, next) {
 			ret = rte_pci_addr_cmp(&dev->addr, &dev2->addr);
 			if (ret > 0)
-				continue;
+				continue;//需要继续比对
 
 			if (ret < 0) {
-				rte_pci_insert_device(dev2, dev);
+				rte_pci_insert_device(dev2, dev);//加入
 			} else { /* already registered */
 				if (!rte_dev_is_probed(&dev2->device)) {
+					//已存在了（更新）
 					dev2->kdrv = dev->kdrv;
 					dev2->max_vfs = dev->max_vfs;
 					pci_name_set(dev2);
@@ -383,12 +405,14 @@ pci_scan_one(const char *dirname, const struct rte_pci_addr *addr)
 			return 0;
 		}
 
+		//遍历完整个链，没有遇到相同的及比自已大的，加入
 		rte_pci_add_device(dev);
 	}
 
 	return 0;
 }
 
+//扫描给定pci地址的设备，更新其内容
 int
 pci_update_device(const struct rte_pci_addr *addr)
 {
@@ -404,6 +428,7 @@ pci_update_device(const struct rte_pci_addr *addr)
 /*
  * split up a pci address into its constituent parts.
  */
+//将buf解析为pci地址
 static int
 parse_pci_addr_format(const char *buf, int bufsize, struct rte_pci_addr *addr)
 {
@@ -422,6 +447,7 @@ parse_pci_addr_format(const char *buf, int bufsize, struct rte_pci_addr *addr)
 	if (buf_copy == NULL)
 		return -1;
 
+	//传入str,采用union将domain,bus一并赋值
 	if (rte_strsplit(buf_copy, bufsize, splitaddr.str, PCI_FMT_NVAL, ':')
 			!= PCI_FMT_NVAL - 1)
 		goto error;
@@ -432,6 +458,7 @@ parse_pci_addr_format(const char *buf, int bufsize, struct rte_pci_addr *addr)
 	*splitaddr.function++ = '\0';
 
 	/* now convert to int values */
+	//转换为数字
 	errno = 0;
 	addr->domain = strtoul(splitaddr.domain, NULL, 16);
 	addr->bus = strtoul(splitaddr.bus, NULL, 16);
@@ -451,6 +478,8 @@ parse_pci_addr_format(const char *buf, int bufsize, struct rte_pci_addr *addr)
  * Scan the content of the PCI bus, and the devices in the devices
  * list
  */
+//pci扫描，识别所有pci设备
+//通过遍历sys目录中的pci/devices完成此功能
 int
 rte_pci_scan(void)
 {
@@ -468,6 +497,7 @@ rte_pci_scan(void)
 		RTE_LOG(DEBUG, EAL, "VFIO PCI modules not loaded\n");
 #endif
 
+	//通过sysfs的pci进行扫描
 	dir = opendir(rte_pci_get_sysfs_path());
 	if (dir == NULL) {
 		RTE_LOG(ERR, EAL, "%s(): opendir failed: %s\n",
@@ -479,12 +509,18 @@ rte_pci_scan(void)
 		if (e->d_name[0] == '.')
 			continue;
 
+		//anlang@anlang:~/workspace/anlaneg_dpdk$ ls  /sys/bus/pci/devices/
+		//0000:00:00.0  0000:00:16.0  0000:00:1f.0  0000:00:1f.4  0000:01:00.1
+		//0000:00:01.0  0000:00:16.3  0000:00:1f.2  0000:00:1f.6
+		//0000:00:14.0  0000:00:17.0  0000:00:1f.3  0000:01:00.0
+		//解析pci地址,domain,bus,driver-id,function　得到一个具体的pci设备
 		if (parse_pci_addr_format(e->d_name, sizeof(e->d_name), &addr) != 0)
 			continue;
 
 		snprintf(dirname, sizeof(dirname), "%s/%s",
 				rte_pci_get_sysfs_path(), e->d_name);
 
+		//扫描具体的一个pci设备，并将其加入到rte_pci_bus.device_list链上
 		if (pci_scan_one(dirname, &addr) < 0)
 			goto error;
 	}
@@ -598,6 +634,7 @@ pci_device_iova_mode(const struct rte_pci_driver *pdrv,
 }
 
 /* Read PCI config space. */
+//pci配置空间读取
 int rte_pci_read_config(const struct rte_pci_device *device,
 		void *buf, size_t len, off_t offset)
 {
@@ -607,9 +644,11 @@ int rte_pci_read_config(const struct rte_pci_device *device,
 	switch (device->kdrv) {
 	case RTE_KDRV_IGB_UIO:
 	case RTE_KDRV_UIO_GENERIC:
+		//uio方式
 		return pci_uio_read_config(intr_handle, buf, len, offset);
 #ifdef VFIO_PRESENT
 	case RTE_KDRV_VFIO:
+		//vfio方式
 		return pci_vfio_read_config(intr_handle, buf, len, offset);
 #endif
 	default:
diff --git a/drivers/bus/pci/linux/pci_uio.c b/drivers/bus/pci/linux/pci_uio.c
index 6dca05a98..64662e143 100644
--- a/drivers/bus/pci/linux/pci_uio.c
+++ b/drivers/bus/pci/linux/pci_uio.c
@@ -73,6 +73,7 @@ pci_uio_set_bus_master(int dev_fd)
 	return 0;
 }
 
+//采用mknod创建uio 设备
 static int
 pci_mknod_uio_dev(const char *sysfs_uio_path, unsigned uio_num)
 {
@@ -84,6 +85,7 @@ pci_mknod_uio_dev(const char *sysfs_uio_path, unsigned uio_num)
 
 	/* get the name of the sysfs file that contains the major and minor
 	 * of the uio device and read its content */
+	//标例：/sys/bus/pci/devices/0000:00:02.0/uio/uio0/dev
 	snprintf(filename, sizeof(filename), "%s/dev", sysfs_uio_path);
 
 	f = fopen(filename, "r");
@@ -93,6 +95,7 @@ pci_mknod_uio_dev(const char *sysfs_uio_path, unsigned uio_num)
 		return -1;
 	}
 
+	//取设备号
 	ret = fscanf(f, "%u:%u", &major, &minor);
 	if (ret != 2) {
 		RTE_LOG(ERR, EAL, "%s(): cannot parse sysfs to get major:minor\n",
@@ -102,6 +105,7 @@ pci_mknod_uio_dev(const char *sysfs_uio_path, unsigned uio_num)
 	}
 	fclose(f);
 
+	//创建设备/dev/uio%u
 	/* create the char device "mknod /dev/uioX c major minor" */
 	snprintf(filename, sizeof(filename), "/dev/uio%u", uio_num);
 	dev = makedev(major, minor);
@@ -125,7 +129,7 @@ static int
 pci_get_uio_dev(struct rte_pci_device *dev, char *dstbuf,
 			   unsigned int buflen, int create)
 {
-	struct rte_pci_addr *loc = &dev->addr;
+	struct rte_pci_addr *loc = &dev->addr;//设备的pci地址
 	int uio_num = -1;
 	struct dirent *e;
 	DIR *dir;
@@ -134,12 +138,14 @@ pci_get_uio_dev(struct rte_pci_device *dev, char *dstbuf,
 	/* depending on kernel version, uio can be located in uio/uioX
 	 * or uio:uioX */
 
+	//示例：/sys/bus/pci/devices/0000\:00\:02.0/uio/
 	snprintf(dirname, sizeof(dirname),
 			"%s/" PCI_PRI_FMT "/uio", rte_pci_get_sysfs_path(),
 			loc->domain, loc->bus, loc->devid, loc->function);
 
 	dir = opendir(dirname);
 	if (dir == NULL) {
+		//不存在此目录，去除掉/uio后再尝试
 		/* retry with the parent directory */
 		snprintf(dirname, sizeof(dirname),
 				"%s/" PCI_PRI_FMT, rte_pci_get_sysfs_path(),
@@ -147,6 +153,7 @@ pci_get_uio_dev(struct rte_pci_device *dev, char *dstbuf,
 		dir = opendir(dirname);
 
 		if (dir == NULL) {
+			//再尝试失败,pci设备不存在，报错
 			RTE_LOG(ERR, EAL, "Cannot opendir %s\n", dirname);
 			return -1;
 		}
@@ -161,20 +168,23 @@ pci_get_uio_dev(struct rte_pci_device *dev, char *dstbuf,
 		char *endptr;
 
 		if (strncmp(e->d_name, "uio", 3) != 0)
-			continue;
+			continue;//非uio开头的文件名，直接跳过
 
+		//我们找到了名称为"uio%d"目录
 		/* first try uio%d */
 		errno = 0;
-		uio_num = strtoull(e->d_name + shortprefix_len, &endptr, 10);
+		uio_num = strtoull(e->d_name + shortprefix_len, &endptr, 10);//取uio对应的%d
 		if (errno == 0 && endptr != (e->d_name + shortprefix_len)) {
+			//格式错误
 			snprintf(dstbuf, buflen, "%s/uio%u", dirname, uio_num);
 			break;
 		}
 
 		/* then try uio:uio%d */
 		errno = 0;
-		uio_num = strtoull(e->d_name + longprefix_len, &endptr, 10);
+		uio_num = strtoull(e->d_name + longprefix_len, &endptr, 10);//尝试接uio:uio对应的%d
 		if (errno == 0 && endptr != (e->d_name + longprefix_len)) {
+			//格式错误
 			snprintf(dstbuf, buflen, "%s/uio:uio%u", dirname, uio_num);
 			break;
 		}
@@ -183,9 +193,11 @@ pci_get_uio_dev(struct rte_pci_device *dev, char *dstbuf,
 
 	/* No uio resource found */
 	if (e == NULL)
+		//没有找到uio模块
 		return -1;
 
 	/* create uio device if we've been asked to */
+	//参数指定了创建，通过pci_mknod_uio_dev进行创建
 	if (rte_eal_create_uio_dev() && create &&
 			pci_mknod_uio_dev(dstbuf, uio_num) < 0)
 		RTE_LOG(WARNING, EAL, "Cannot create /dev/uio%u\n", uio_num);
@@ -210,6 +222,7 @@ pci_uio_free_resource(struct rte_pci_device *dev,
 	}
 }
 
+//为pci设备申请资源（通过uio)
 int
 pci_uio_alloc_resource(struct rte_pci_device *dev,
 		struct mapped_pci_resource **uio_res)
@@ -223,6 +236,7 @@ pci_uio_alloc_resource(struct rte_pci_device *dev,
 	loc = &dev->addr;
 
 	/* find uio resource */
+	//获取此dev的uio设备编号
 	uio_num = pci_get_uio_dev(dev, dirname, sizeof(dirname), 1);
 	if (uio_num < 0) {
 		RTE_LOG(WARNING, EAL, "  "PCI_PRI_FMT" not managed by UIO driver, "
@@ -232,6 +246,7 @@ pci_uio_alloc_resource(struct rte_pci_device *dev,
 	snprintf(devname, sizeof(devname), "/dev/uio%u", uio_num);
 
 	/* save fd if in primary process */
+	//自此设备读取中断（打开此文件，用于中断处理）
 	dev->intr_handle.fd = open(devname, O_RDWR);
 	if (dev->intr_handle.fd < 0) {
 		RTE_LOG(ERR, EAL, "Cannot open %s: %s\n",
@@ -239,6 +254,7 @@ pci_uio_alloc_resource(struct rte_pci_device *dev,
 		goto error;
 	}
 
+	//打开配置文件
 	snprintf(cfgname, sizeof(cfgname),
 			"/sys/class/uio/uio%u/device/config", uio_num);
 	dev->intr_handle.uio_cfg_fd = open(cfgname, O_RDWR);
@@ -251,6 +267,7 @@ pci_uio_alloc_resource(struct rte_pci_device *dev,
 	if (dev->kdrv == RTE_KDRV_IGB_UIO)
 		dev->intr_handle.type = RTE_INTR_HANDLE_UIO;
 	else {
+		//非igb_uio方式
 		dev->intr_handle.type = RTE_INTR_HANDLE_UIO_INTX;
 
 		/* set bus master that is not done by uio_pci_generic */
@@ -268,6 +285,7 @@ pci_uio_alloc_resource(struct rte_pci_device *dev,
 		goto error;
 	}
 
+	//填充uio_res的path及pci_addr
 	strlcpy((*uio_res)->path, devname, sizeof((*uio_res)->path));
 	memcpy(&(*uio_res)->pci_addr, &dev->addr, sizeof((*uio_res)->pci_addr));
 
@@ -323,6 +341,8 @@ pci_uio_map_resource_by_index(struct rte_pci_device *dev, int res_idx,
 	}
 
 	if (!wc_activate || fd < 0) {
+		//未开启wc_activate或者开启了，但打开文件失败
+		//例：/sys/bus/pci/devices/0000\:00\:01.0/resource
 		snprintf(devname, sizeof(devname),
 			"%s/" PCI_PRI_FMT "/resource%d",
 			rte_pci_get_sysfs_path(),
@@ -330,6 +350,7 @@ pci_uio_map_resource_by_index(struct rte_pci_device *dev, int res_idx,
 			loc->function, res_idx);
 
 		/* then try to map resource file */
+		//打开资源文件
 		fd = open(devname, O_RDWR);
 		if (fd < 0) {
 			RTE_LOG(ERR, EAL, "Cannot open %s: %s\n",
@@ -342,20 +363,24 @@ pci_uio_map_resource_by_index(struct rte_pci_device *dev, int res_idx,
 	if (pci_map_addr == NULL)
 		pci_map_addr = pci_find_max_end_va();
 
+	//对这块地址进行映射（设备资源）
 	mapaddr = pci_map_resource(pci_map_addr, fd, 0,
 			(size_t)dev->mem_resource[res_idx].len, 0);
 	close(fd);
 	if (mapaddr == MAP_FAILED)
 		goto error;
 
+	//取映射的内存地址（虚地址的结束位置，用于映射下一段）
 	pci_map_addr = RTE_PTR_ADD(mapaddr,
 			(size_t)dev->mem_resource[res_idx].len);
 
-	maps[map_idx].phaddr = dev->mem_resource[res_idx].phys_addr;
-	maps[map_idx].size = dev->mem_resource[res_idx].len;
-	maps[map_idx].addr = mapaddr;
+	//记录映射的地址
+	maps[map_idx].phaddr = dev->mem_resource[res_idx].phys_addr;//物理地址
+	maps[map_idx].size = dev->mem_resource[res_idx].len;//这段地址的长度
+	maps[map_idx].addr = mapaddr;//对应的虚拟地址
 	maps[map_idx].offset = 0;
 	strcpy(maps[map_idx].path, devname);
+	//设备资源对应的虚拟地址（有了些地址，就可以实现设备的控制）
 	dev->mem_resource[res_idx].addr = mapaddr;
 
 	return 0;
diff --git a/drivers/bus/pci/pci_common.c b/drivers/bus/pci/pci_common.c
index 6b46b4f07..ef1117a85 100644
--- a/drivers/bus/pci/pci_common.c
+++ b/drivers/bus/pci/pci_common.c
@@ -31,6 +31,7 @@
 
 #define SYSFS_PCI_DEVICES "/sys/bus/pci/devices"
 
+//pci扫描路径（例如/sys/bus/pci/devices)
 const char *rte_pci_get_sysfs_path(void)
 {
 	const char *path = NULL;
@@ -82,11 +83,13 @@ pci_name_set(struct rte_pci_device *dev)
  * Match the PCI Driver and Device using the ID Table
  */
 int
+//检查pci-drv是否可匹配此设备
 rte_pci_match(const struct rte_pci_driver *pci_drv,
 	      const struct rte_pci_device *pci_dev)
 {
 	const struct rte_pci_id *id_table;
 
+	//检查驱动支持的设备列表中是否包含本设备
 	for (id_table = pci_drv->id_table; id_table->vendor_id != 0;
 	     id_table++) {
 		/* check if device's identifiers match the driver's ones */
@@ -108,7 +111,7 @@ rte_pci_match(const struct rte_pci_driver *pci_drv,
 				id_table->class_id != RTE_CLASS_ANY_ID)
 			continue;
 
-		return 1;
+		return 1;//驱动匹配此设备
 	}
 
 	return 0;
@@ -134,13 +137,16 @@ rte_pci_probe_one_driver(struct rte_pci_driver *dr,
 	/* The device is not blacklisted; Check if driver supports it */
 	if (!rte_pci_match(dr, dev))
 		/* Match of device and driver failed */
+		//此driver不匹配此device
 		return 1;
 
+	//设备可以被此驱动使用
 	RTE_LOG(INFO, EAL, "PCI device "PCI_PRI_FMT" on NUMA socket %i\n",
 			loc->domain, loc->bus, loc->devid, loc->function,
 			dev->device.numa_node);
 
 	/* no initialization when blacklisted, return without error */
+	//设备被列在黑名单里，不初始化
 	if (dev->device.devargs != NULL &&
 		dev->device.devargs->policy ==
 			RTE_DEV_BLACKLISTED) {
@@ -149,6 +155,7 @@ rte_pci_probe_one_driver(struct rte_pci_driver *dr,
 		return 1;
 	}
 
+	//设备所属numa不合法，更新为0
 	if (dev->device.numa_node < 0) {
 		RTE_LOG(WARNING, EAL, "  Invalid NUMA socket, default to 0\n");
 		dev->device.numa_node = 0;
@@ -161,6 +168,7 @@ rte_pci_probe_one_driver(struct rte_pci_driver *dr,
 		return -EEXIST;
 	}
 
+	//指名驱动适配成功
 	RTE_LOG(INFO, EAL, "  probe driver: %x:%x %s\n", dev->id.vendor_id,
 		dev->id.device_id, dr->driver.name);
 
@@ -169,6 +177,7 @@ rte_pci_probe_one_driver(struct rte_pci_driver *dr,
 	 * This needs to be before rte_pci_map_device(), as it enables to use
 	 * driver flags for adjusting configuration.
 	 */
+	//为设备暂时赋上此驱动，进行probe(这段代码与kernel极其相似）
 	if (!already_probed) {
 		enum rte_iova_mode dev_iova_mode;
 		enum rte_iova_mode iova_mode;
@@ -186,6 +195,8 @@ rte_pci_probe_one_driver(struct rte_pci_driver *dr,
 		dev->driver = dr;
 	}
 
+	//如果driver要求mapping，则进行资源map(例如bnxt驱动就要求进行mapping)
+	//按理解，所有采用igb_uio方式绑定的均要走此函数
 	if (!already_probed && (dr->drv_flags & RTE_PCI_DRV_NEED_MAPPING)) {
 		/* map resources for devices that use igb_uio */
 		ret = rte_pci_map_device(dev);
@@ -196,6 +207,7 @@ rte_pci_probe_one_driver(struct rte_pci_driver *dr,
 	}
 
 	/* call the driver probe() function */
+	//用此driver探测此设备
 	ret = dr->probe(dr, dev);
 	if (already_probed)
 		return ret; /* no rollback if already succeeded earlier */
@@ -260,6 +272,7 @@ rte_pci_detach_dev(struct rte_pci_device *dev)
  * registered driver for the given device. Return < 0 if initialization
  * failed, return 1 if no driver is found for this device.
  */
+//针对当前device，检查其是否可以匹配已知的driver
 static int
 pci_probe_all_drivers(struct rte_pci_device *dev)
 {
@@ -269,6 +282,7 @@ pci_probe_all_drivers(struct rte_pci_device *dev)
 	if (dev == NULL)
 		return -EINVAL;
 
+	//遍历已注册的所有pci驱动，检查是否有驱动可适配dev
 	FOREACH_DRIVER_ON_PCIBUS(dr) {
 		rc = rte_pci_probe_one_driver(dr, dev);
 		if (rc < 0)
@@ -299,6 +313,7 @@ rte_pci_probe(void)
 	if (rte_pci_bus.bus.conf.scan_mode != RTE_BUS_SCAN_WHITELIST)
 		probe_all = 1;
 
+	//遍历扫描出来的所有设备
 	FOREACH_DEVICE_ON_PCIBUS(dev) {
 		probed++;
 
@@ -309,8 +324,11 @@ rte_pci_probe(void)
 		else if (devargs != NULL &&
 			devargs->policy == RTE_DEV_WHITELISTED)
 			ret = pci_probe_all_drivers(dev);
+
+		//黑名单或者虚拟设备时，将被跳过
 		if (ret < 0) {
 			if (ret != -EEXIST) {
+			        //设备无法被使用，报错，继续尝试
 				RTE_LOG(ERR, EAL, "Requested device "
 					PCI_PRI_FMT " cannot be used\n",
 					dev->addr.domain, dev->addr.bus,
@@ -322,6 +340,7 @@ rte_pci_probe(void)
 		}
 	}
 
+	//探测的与失败的一样多时，返回-1
 	return (probed && probed == failed) ? -1 : 0;
 }
 
@@ -370,6 +389,7 @@ pci_parse(const char *name, void *addr)
 }
 
 /* register a driver */
+//pci驱动注册
 void
 rte_pci_register(struct rte_pci_driver *driver)
 {
@@ -673,8 +693,8 @@ rte_pci_get_iommu_class(void)
 
 struct rte_pci_bus rte_pci_bus = {
 	.bus = {
-		.scan = rte_pci_scan,
-		.probe = rte_pci_probe,
+		.scan = rte_pci_scan,//扫描函数,找到系统中所有pci设备
+		.probe = rte_pci_probe,//探测
 		.find_device = pci_find_device,
 		.plug = pci_plug,
 		.unplug = pci_unplug,
@@ -686,8 +706,10 @@ struct rte_pci_bus rte_pci_bus = {
 		.hot_unplug_handler = pci_hot_unplug_handler,
 		.sigbus_handler = pci_sigbus_handler,
 	},
+	//保存读到的pci设备
 	.device_list = TAILQ_HEAD_INITIALIZER(rte_pci_bus.device_list),
 	.driver_list = TAILQ_HEAD_INITIALIZER(rte_pci_bus.driver_list),
 };
 
+//加载pci bus
 RTE_REGISTER_BUS(pci, rte_pci_bus.bus);
diff --git a/drivers/bus/pci/pci_common_uio.c b/drivers/bus/pci/pci_common_uio.c
index 7ea73dbc5..096839ad2 100644
--- a/drivers/bus/pci/pci_common_uio.c
+++ b/drivers/bus/pci/pci_common_uio.c
@@ -79,6 +79,7 @@ pci_uio_map_secondary(struct rte_pci_device *dev)
 }
 
 /* map the PCI resource of a PCI device in virtual memory */
+//映射设备的内存
 int
 pci_uio_map_resource(struct rte_pci_device *dev)
 {
@@ -93,9 +94,11 @@ pci_uio_map_resource(struct rte_pci_device *dev)
 
 	/* secondary processes - use already recorded details */
 	if (rte_eal_process_type() != RTE_PROC_PRIMARY)
+		//如果是从进程，则使用已记录的映射
 		return pci_uio_map_secondary(dev);
 
 	/* allocate uio resource */
+	//申请uio_res
 	ret = pci_uio_alloc_resource(dev, &uio_res);
 	if (ret)
 		return ret;
@@ -105,8 +108,9 @@ pci_uio_map_resource(struct rte_pci_device *dev)
 		/* skip empty BAR */
 		phaddr = dev->mem_resource[i].phys_addr;
 		if (phaddr == 0)
-			continue;
+			continue;//跳过空的资源
 
+		//映射第i块内存资源（在mem_resource中的编号为i,但映射的段为map_idx)
 		ret = pci_uio_map_resource_by_index(dev, i,
 				uio_res, map_idx);
 		if (ret)
@@ -117,6 +121,7 @@ pci_uio_map_resource(struct rte_pci_device *dev)
 
 	uio_res->nb_maps = map_idx;
 
+	//将映射的内存串在uio_res_list链上
 	TAILQ_INSERT_TAIL(uio_res_list, uio_res, next);
 
 	return 0;
diff --git a/drivers/bus/pci/rte_bus_pci.h b/drivers/bus/pci/rte_bus_pci.h
index 29bea6d70..487058f3e 100644
--- a/drivers/bus/pci/rte_bus_pci.h
+++ b/drivers/bus/pci/rte_bus_pci.h
@@ -42,10 +42,12 @@ TAILQ_HEAD(rte_pci_device_list, rte_pci_device);
 /** List of PCI drivers */
 TAILQ_HEAD(rte_pci_driver_list, rte_pci_driver);
 
+//遍历所有device
 /* PCI Bus iterators */
 #define FOREACH_DEVICE_ON_PCIBUS(p)	\
 		TAILQ_FOREACH(p, &(rte_pci_bus.device_list), next)
 
+//遍历所有driver
 #define FOREACH_DRIVER_ON_PCIBUS(p)	\
 		TAILQ_FOREACH(p, &(rte_pci_bus.driver_list), next)
 
diff --git a/drivers/bus/vdev/rte_bus_vdev.h b/drivers/bus/vdev/rte_bus_vdev.h
index 2bc46530c..1336a5e03 100644
--- a/drivers/bus/vdev/rte_bus_vdev.h
+++ b/drivers/bus/vdev/rte_bus_vdev.h
@@ -34,6 +34,7 @@ struct rte_vdev_device {
 #define RTE_DEV_TO_VDEV_CONST(ptr) \
 	container_of(ptr, const struct rte_vdev_device, device)
 
+//获取虚设备名称
 static inline const char *
 rte_vdev_device_name(const struct rte_vdev_device *dev)
 {
@@ -42,6 +43,7 @@ rte_vdev_device_name(const struct rte_vdev_device *dev)
 	return NULL;
 }
 
+//取vdev设备参数
 static inline const char *
 rte_vdev_device_args(const struct rte_vdev_device *dev)
 {
@@ -70,6 +72,7 @@ struct rte_vdev_driver {
 	TAILQ_ENTRY(rte_vdev_driver) next; /**< Next in list. */
 	struct rte_driver driver;      /**< Inherited general driver. */
 	rte_vdev_probe_t *probe;       /**< Virtual device probe function. */
+	//设备被移除时调用
 	rte_vdev_remove_t *remove;     /**< Virtual device remove function. */
 };
 
@@ -91,7 +94,10 @@ void rte_vdev_register(struct rte_vdev_driver *driver);
  */
 void rte_vdev_unregister(struct rte_vdev_driver *driver);
 
+//vdev注册，指明设备驱动的名称为nm,指明设备驱动的别名
 #define RTE_PMD_REGISTER_VDEV(nm, vdrv)\
+RTE_INIT(vdrvinitfn_ ##vdrv);\
+/*申明驱动别名变量*/\
 static const char *vdrvinit_ ## nm ## _alias;\
 RTE_INIT(vdrvinitfn_ ##vdrv)\
 {\
@@ -101,6 +107,7 @@ RTE_INIT(vdrvinitfn_ ##vdrv)\
 } \
 RTE_PMD_EXPORT_NAME(nm, __COUNTER__)
 
+//设置驱动别名
 #define RTE_PMD_REGISTER_ALIAS(nm, alias)\
 static const char *vdrvinit_ ## nm ## _alias = RTE_STR(alias)
 
diff --git a/drivers/bus/vdev/vdev.c b/drivers/bus/vdev/vdev.c
index a89ea2353..6eadad1ff 100644
--- a/drivers/bus/vdev/vdev.c
+++ b/drivers/bus/vdev/vdev.c
@@ -35,12 +35,14 @@ static struct rte_bus rte_vdev_bus;
 /** Double linked list of virtual device drivers. */
 TAILQ_HEAD(vdev_device_list, rte_vdev_device);
 
+//所有已识别的vdev挂载在此链上
 static struct vdev_device_list vdev_device_list =
 	TAILQ_HEAD_INITIALIZER(vdev_device_list);
 /* The lock needs to be recursive because a vdev can manage another vdev. */
 static rte_spinlock_recursive_t vdev_device_list_lock =
 	RTE_SPINLOCK_RECURSIVE_INITIALIZER;
 
+//所有注册的vdev挂载在此链上
 static struct vdev_driver_list vdev_driver_list =
 	TAILQ_HEAD_INITIALIZER(vdev_driver_list);
 
@@ -55,13 +57,16 @@ static struct vdev_custom_scans vdev_custom_scans =
 static rte_spinlock_t vdev_custom_scan_lock = RTE_SPINLOCK_INITIALIZER;
 
 /* register a driver */
+//注册vdev驱动
 void
 rte_vdev_register(struct rte_vdev_driver *driver)
 {
+	//注册vdev驱动
 	TAILQ_INSERT_TAIL(&vdev_driver_list, driver, next);
 }
 
 /* unregister a driver */
+//驱动解注册
 void
 rte_vdev_unregister(struct rte_vdev_driver *driver)
 {
@@ -115,12 +120,14 @@ rte_vdev_remove_custom_scan(rte_vdev_scan_callback callback, void *user_arg)
 	return 0;
 }
 
+//通过名称匹配来完成驱动匹配，返回可匹配的驱动名称
 static int
 vdev_parse(const char *name, void *addr)
 {
 	struct rte_vdev_driver **out = addr;
 	struct rte_vdev_driver *driver = NULL;
 
+	//如果名称相同或者别名的前缀相同，则驱动会命中
 	TAILQ_FOREACH(driver, &vdev_driver_list, next) {
 		if (strncmp(driver->driver.name, name,
 			    strlen(driver->driver.name)) == 0)
@@ -136,6 +143,7 @@ vdev_parse(const char *name, void *addr)
 	return driver == NULL;
 }
 
+//为dev查找合适的驱动（采用设备名称前缀或者设备参数中的driver=参数)
 static int
 vdev_probe_all_drivers(struct rte_vdev_device *dev)
 {
@@ -146,11 +154,14 @@ vdev_probe_all_drivers(struct rte_vdev_device *dev)
 	if (rte_dev_is_probed(&dev->device))
 		return -EEXIST;
 
+	//取出device名称
 	name = rte_vdev_device_name(dev);
 	VDEV_LOG(DEBUG, "Search driver to probe device %s", name);
 
+	//查找此设备对应的驱动，如果失配，返回-1
 	if (vdev_parse(name, &driver))
 		return -1;
+	//设备找到对应的驱动，执行驱动probe
 	ret = driver->probe(dev);
 	if (ret == 0)
 		dev->device.driver = &driver->driver;
@@ -158,6 +169,7 @@ vdev_probe_all_drivers(struct rte_vdev_device *dev)
 }
 
 /* The caller shall be responsible for thread-safe */
+//给定名称，获取对应的vdev设备
 static struct rte_vdev_device *
 find_vdev(const char *name)
 {
@@ -166,6 +178,7 @@ find_vdev(const char *name)
 	if (!name)
 		return NULL;
 
+	//遍历已识别的设备，找到名称为devname的设备
 	TAILQ_FOREACH(dev, &vdev_device_list, next) {
 		const char *devname = rte_vdev_device_name(dev);
 
@@ -176,6 +189,7 @@ find_vdev(const char *name)
 	return NULL;
 }
 
+//申请devargs,填充bus,args,name
 static struct rte_devargs *
 alloc_devargs(const char *name, const char *args)
 {
@@ -202,6 +216,7 @@ alloc_devargs(const char *name, const char *args)
 	return devargs;
 }
 
+//创建并加入dev
 static int
 insert_vdev(const char *name, const char *args,
 		struct rte_vdev_device **p_dev,
@@ -234,13 +249,14 @@ insert_vdev(const char *name, const char *args,
 		 * So there is no reason to try probing again,
 		 * even with new arguments.
 		 */
-		ret = -EEXIST;
+		ret = -EEXIST;//重复添加，报错
 		goto fail;
 	}
 
 	if (init)
 		rte_devargs_insert(&devargs);
 	dev->device.devargs = devargs;
+	//加入
 	TAILQ_INSERT_TAIL(&vdev_device_list, dev, next);
 
 	if (p_dev)
@@ -261,8 +277,9 @@ rte_vdev_init(const char *name, const char *args)
 	int ret;
 
 	rte_spinlock_recursive_lock(&vdev_device_list_lock);
-	ret = insert_vdev(name, args, &dev, true);
+	ret = insert_vdev(name, args, &dev, true);//创建dev
 	if (ret == 0) {
+		//查驱动
 		ret = vdev_probe_all_drivers(dev);
 		if (ret) {
 			if (ret > 0)
@@ -283,11 +300,13 @@ vdev_remove_driver(struct rte_vdev_device *dev)
 	const char *name = rte_vdev_device_name(dev);
 	const struct rte_vdev_driver *driver;
 
+	//此设备还未绑定驱动，报错
 	if (!dev->device.driver) {
 		VDEV_LOG(DEBUG, "no driver attach to device %s", name);
 		return 1;
 	}
 
+	//调用驱动的remove函数完成移除
 	driver = container_of(dev->device.driver, const struct rte_vdev_driver,
 		driver);
 	return driver->remove(dev);
@@ -304,17 +323,21 @@ rte_vdev_uninit(const char *name)
 
 	rte_spinlock_recursive_lock(&vdev_device_list_lock);
 
+	//通过名称找到dev
 	dev = find_vdev(name);
 	if (!dev) {
 		ret = -ENOENT;
 		goto unlock;
 	}
 
+	//处理具体设备移除
 	ret = vdev_remove_driver(dev);
 	if (ret)
 		goto unlock;
 
+	//自device_list中移除设备
 	TAILQ_REMOVE(&vdev_device_list, dev, next);
+	//移除设备参数
 	rte_devargs_remove(dev->device.devargs);
 	free(dev);
 
@@ -400,6 +423,7 @@ vdev_action(const struct rte_mp_msg *mp_msg, const void *peer)
 	return 0;
 }
 
+//vdev扫描,用于发现devargs_list中指明的属于vdev bus的设备
 static int
 vdev_scan(void)
 {
@@ -459,6 +483,7 @@ vdev_scan(void)
 	/* for virtual devices we scan the devargs_list populated via cmdline */
 	RTE_EAL_DEVARGS_FOREACH("vdev", devargs) {
 
+		//创建vdev,完成vdev bus扫描
 		dev = calloc(1, sizeof(*dev));
 		if (!dev)
 			return -1;
@@ -466,6 +491,7 @@ vdev_scan(void)
 		rte_spinlock_recursive_lock(&vdev_device_list_lock);
 
 		if (find_vdev(devargs->name)) {
+			//已存在同名的vdev，跳过此device
 			rte_spinlock_recursive_unlock(&vdev_device_list_lock);
 			free(dev);
 			continue;
@@ -476,6 +502,7 @@ vdev_scan(void)
 		dev->device.numa_node = SOCKET_ID_ANY;
 		dev->device.name = devargs->name;
 
+		//挂接vdev设备到vdev_device_list
 		TAILQ_INSERT_TAIL(&vdev_device_list, dev, next);
 
 		rte_spinlock_recursive_unlock(&vdev_device_list_lock);
@@ -484,6 +511,7 @@ vdev_scan(void)
 	return 0;
 }
 
+//为所有的vdev设备探测驱动
 static int
 vdev_probe(void)
 {
@@ -491,12 +519,14 @@ vdev_probe(void)
 	int r, ret = 0;
 
 	/* call the init function for each virtual device */
+	//遍历所有的vdev设备
 	TAILQ_FOREACH(dev, &vdev_device_list, next) {
 		/* we don't use the vdev lock here, as it's only used in DPDK
 		 * initialization; and we don't want to hold such a lock when
 		 * we call each driver probe.
 		 */
 
+		//为dev查找合适的驱动
 		r = vdev_probe_all_drivers(dev);
 		if (r != 0) {
 			if (r == -EEXIST)
@@ -540,22 +570,25 @@ vdev_plug(struct rte_device *dev)
 	return vdev_probe_all_drivers(RTE_DEV_TO_VDEV(dev));
 }
 
+//移除设备
 static int
 vdev_unplug(struct rte_device *dev)
 {
+	//移除设备dev->name
 	return rte_vdev_uninit(dev->name);
 }
 
 static struct rte_bus rte_vdev_bus = {
-	.scan = vdev_scan,
-	.probe = vdev_probe,
+	.scan = vdev_scan,//vdev设备发现
+	.probe = vdev_probe,//为所有vdev探测驱动，如果发现可匹配的驱动，则调用driver probe
 	.find_device = rte_vdev_find_device,
 	.plug = vdev_plug,
-	.unplug = vdev_unplug,
-	.parse = vdev_parse,
+	.unplug = vdev_unplug,//设备移除
+	.parse = vdev_parse,//查找设备对应的驱动
 	.dev_iterate = rte_vdev_dev_iterate,
 };
 
+//注册rte_vdev_bus
 RTE_REGISTER_BUS(vdev, rte_vdev_bus);
 
 RTE_INIT(vdev_init_log)
diff --git a/drivers/crypto/virtio/virtio_ring.h b/drivers/crypto/virtio/virtio_ring.h
index ee3067456..3e2e6680a 100644
--- a/drivers/crypto/virtio/virtio_ring.h
+++ b/drivers/crypto/virtio/virtio_ring.h
@@ -39,16 +39,16 @@ struct vring_desc {
 
 struct vring_avail {
 	uint16_t flags;
-	uint16_t idx;
+	uint16_t idx;//队列中目前有效位置（即可存放或可读取的极限位置）
 	uint16_t ring[0];
 };
 
 /* id is a 16bit index. uint32_t is used here for ids for padding reasons. */
 struct vring_used_elem {
 	/* Index of start of used descriptor chain. */
-	uint32_t id;
+	uint32_t id;//记录描述符索引
 	/* Total length of the descriptor chain which was written to. */
-	uint32_t len;
+	uint32_t len;//buffer的长度
 };
 
 struct vring_used {
diff --git a/drivers/mempool/ring/rte_mempool_ring.c b/drivers/mempool/ring/rte_mempool_ring.c
index bc123fc52..a326bef13 100644
--- a/drivers/mempool/ring/rte_mempool_ring.c
+++ b/drivers/mempool/ring/rte_mempool_ring.c
@@ -72,6 +72,7 @@ common_ring_alloc(struct rte_mempool *mp)
 	 * running as a secondary process etc., so no checks made
 	 * in this function for that condition.
 	 */
+	//创建ring,ring大小大于mp->size一致
 	r = rte_ring_create(rg_name, rte_align32pow2(mp->size + 1),
 		mp->socket_id, rg_flags);
 	if (r == NULL)
@@ -123,6 +124,7 @@ static const struct rte_mempool_ops ops_mp_sc = {
 
 static const struct rte_mempool_ops ops_sp_mc = {
 	.name = "ring_sp_mc",
+	//创建ring
 	.alloc = common_ring_alloc,
 	.free = common_ring_free,
 	.enqueue = common_ring_sp_enqueue,
@@ -130,6 +132,7 @@ static const struct rte_mempool_ops ops_sp_mc = {
 	.get_count = common_ring_get_count,
 };
 
+//注册ring实现的mempool
 MEMPOOL_REGISTER_OPS(ops_mp_mc);
 MEMPOOL_REGISTER_OPS(ops_sp_sc);
 MEMPOOL_REGISTER_OPS(ops_mp_sc);
diff --git a/drivers/net/bnxt/bnxt_vnic.h b/drivers/net/bnxt/bnxt_vnic.h
index 4f760e0b0..6c081a292 100644
--- a/drivers/net/bnxt/bnxt_vnic.h
+++ b/drivers/net/bnxt/bnxt_vnic.h
@@ -54,7 +54,7 @@ struct bnxt_vnic_info {
 	bool		rss_dflt_cr;
 
 	STAILQ_HEAD(, bnxt_filter_info)	filter;
-	STAILQ_HEAD(, rte_flow)	flow_list;
+	STAILQ_HEAD(, rte_flow)	flow_list;//保存创建的flow
 };
 
 struct bnxt;
diff --git a/drivers/net/bonding/eth_bond_private.h b/drivers/net/bonding/eth_bond_private.h
index c9b2d0fe4..8dd95bb02 100644
--- a/drivers/net/bonding/eth_bond_private.h
+++ b/drivers/net/bonding/eth_bond_private.h
@@ -136,6 +136,7 @@ struct bond_dev_private {
 	uint16_t active_slave_count;		/**< Number of active slaves */
 	uint16_t active_slaves[RTE_MAX_ETHPORTS];    /**< Active slave list */
 
+	//bond口的成员数目
 	uint16_t slave_count;			/**< Number of bonded slaves */
 	struct bond_slave_details slaves[RTE_MAX_ETHPORTS];
 	/**< Arary of bonded slaves details */
diff --git a/drivers/net/bonding/rte_eth_bond_8023ad.c b/drivers/net/bonding/rte_eth_bond_8023ad.c
index badcd109a..41bf27b13 100644
--- a/drivers/net/bonding/rte_eth_bond_8023ad.c
+++ b/drivers/net/bonding/rte_eth_bond_8023ad.c
@@ -804,6 +804,7 @@ rx_machine_update(struct bond_dev_private *internals, uint16_t slave_id,
 			/* This LACP frame is sending to the bonding port
 			 * so pass it to rx_machine.
 			 */
+			//处理lacp报文
 			rx_machine(internals, slave_id, &lacp->lacpdu);
 		}
 		rte_pktmbuf_free(lacp_pkt);
@@ -1297,6 +1298,7 @@ bond_mode_8023ad_handle_slow_pkt(struct bond_dev_private *internals,
 			struct slow_protocol_frame *)->slow_protocol.subtype;
 
 	if (subtype == SLOW_SUBTYPE_MARKER) {
+		//见上面注释
 		m_hdr = rte_pktmbuf_mtod(pkt, struct marker_header *);
 
 		if (likely(m_hdr->marker.tlv_type_marker != MARKER_TLV_TYPE_INFO)) {
@@ -1341,6 +1343,7 @@ bond_mode_8023ad_handle_slow_pkt(struct bond_dev_private *internals,
 			}
 		}
 	} else if (likely(subtype == SLOW_SUBTYPE_LACP)) {
+		//处理lacp报文（两种模式，一种直接入队，一种直接处理）
 		if (internals->mode4.dedicated_queues.enabled == 0) {
 			int retval = rte_ring_enqueue(port->rx_ring, pkt);
 			if (retval != 0) {
@@ -1349,6 +1352,7 @@ bond_mode_8023ad_handle_slow_pkt(struct bond_dev_private *internals,
 				goto free_out;
 			}
 		} else
+			//直接处理
 			rx_machine_update(internals, slave_id, pkt);
 	} else {
 		wrn = WRN_UNKNOWN_SLOW_TYPE;
diff --git a/drivers/net/bonding/rte_eth_bond_8023ad.h b/drivers/net/bonding/rte_eth_bond_8023ad.h
index 62265f449..4b7486692 100644
--- a/drivers/net/bonding/rte_eth_bond_8023ad.h
+++ b/drivers/net/bonding/rte_eth_bond_8023ad.h
@@ -102,6 +102,7 @@ struct lacpdu {
 } __attribute__((__packed__)) __attribute__((aligned(2)));
 
 /** LACPDU frame: Contains ethernet header and LACPDU. */
+//lacp报文头
 struct lacpdu_header {
 	struct rte_ether_hdr eth_hdr;
 	struct lacpdu lacpdu;
diff --git a/drivers/net/bonding/rte_eth_bond_alb.c b/drivers/net/bonding/rte_eth_bond_alb.c
index 1d36a4a4a..84cd684ed 100644
--- a/drivers/net/bonding/rte_eth_bond_alb.c
+++ b/drivers/net/bonding/rte_eth_bond_alb.c
@@ -86,9 +86,10 @@ void bond_mode_alb_arp_recv(struct rte_ether_hdr *eth_h, uint16_t offset,
 
 	/* ARP Requests are forwarded to the application with no changes */
 	if (arp->arp_opcode != rte_cpu_to_be_16(RTE_ARP_OP_REPLY))
-		return;
+		return;//仅处理arp响应报文
 
 	/* From now on, we analyze only ARP Reply packets */
+	//对源ip进行hash（arp的server方ip)
 	hash_index = simple_hash((uint8_t *) &arp->arp_data.arp_sip,
 			sizeof(arp->arp_data.arp_sip));
 	client_info = &hash_table[hash_index];
@@ -107,6 +108,8 @@ void bond_mode_alb_arp_recv(struct rte_ether_hdr *eth_h, uint16_t offset,
 			client_info->vlan_count != offset / sizeof(struct rte_vlan_hdr) ||
 			memcmp(client_info->vlan, eth_h + 1, offset) != 0
 	) {
+		//接受时通过arp　reply学习client_info表项，学习client_ip,client_mac(client定义为arp relay的发起者）
+		//(app定义为arp replay的接受者）
 		client_info->in_use = 1;
 		client_info->app_ip = arp->arp_data.arp_tip;
 		client_info->cli_ip = arp->arp_data.arp_sip;
@@ -145,11 +148,13 @@ bond_mode_alb_arp_xmit(struct rte_ether_hdr *eth_h, uint16_t offset,
 	 */
 	rte_eth_macaddr_get(internals->port_id, &bonding_mac);
 	if (!rte_is_same_ether_addr(&bonding_mac, &arp->arp_data.arp_sha)) {
+		//如果arp中的源地址不是bonding口的地址，则更新为主port地址，指出自current_primary_port发出
 		rte_eth_macaddr_get(internals->current_primary_port,
 				&arp->arp_data.arp_sha);
 		return internals->current_primary_port;
 	}
 
+	//利用arp的target地址进行hash确定使用那个client_info
 	hash_index = simple_hash((uint8_t *)&arp->arp_data.arp_tip,
 			sizeof(uint32_t));
 	client_info = &hash_table[hash_index];
@@ -157,9 +162,11 @@ bond_mode_alb_arp_xmit(struct rte_ether_hdr *eth_h, uint16_t offset,
 	rte_spinlock_lock(&internals->mode6.lock);
 	if (arp->arp_opcode == rte_cpu_to_be_16(RTE_ARP_OP_REPLY)) {
 		if (client_info->in_use) {
+			//如果已有相应的记录，则与记录进行比对，如果与之相同
 			if (client_info->app_ip == arp->arp_data.arp_sip &&
 				client_info->cli_ip == arp->arp_data.arp_tip) {
 				/* Entry is already assigned to this client */
+				//更新cli_mac
 				if (!rte_is_broadcast_ether_addr(
 						&arp->arp_data.arp_tha)) {
 					rte_ether_addr_copy(
diff --git a/drivers/net/bonding/rte_eth_bond_alb.h b/drivers/net/bonding/rte_eth_bond_alb.h
index 386e70c59..4ae0420ca 100644
--- a/drivers/net/bonding/rte_eth_bond_alb.h
+++ b/drivers/net/bonding/rte_eth_bond_alb.h
@@ -15,16 +15,16 @@ struct client_data {
 	/** ARP data of single client */
 	struct rte_ether_addr app_mac;
 	/**< MAC address of application running DPDK */
-	uint32_t app_ip;
+	uint32_t app_ip;//arp中发送方ip地址
 	/**< IP address of application running DPDK */
-	struct rte_ether_addr cli_mac;
+	struct rte_ether_addr cli_mac;//arp响应中的target mac(即arp请求时的srcmac)
 	/**< Client MAC address */
-	uint32_t cli_ip;
+	uint32_t cli_ip;//arp响应中的target ip
 	/**< Client IP address */
 
-	uint16_t slave_idx;
+	uint16_t slave_idx;//对应的slave端口
 	/**< Index of slave on which we connect with that client */
-	uint8_t in_use;
+	uint8_t in_use;//是否被使用
 	/**< Flag indicating if entry in client table is currently used */
 	uint8_t ntt;
 	/**< Flag indicating if we need to send update to this client on next tx */
diff --git a/drivers/net/bonding/rte_eth_bond_api.c b/drivers/net/bonding/rte_eth_bond_api.c
index f38eb3b47..632815dc1 100644
--- a/drivers/net/bonding/rte_eth_bond_api.c
+++ b/drivers/net/bonding/rte_eth_bond_api.c
@@ -15,6 +15,7 @@
 #include "eth_bond_private.h"
 #include "eth_bond_8023ad_private.h"
 
+//检查是否为bond口
 int
 check_for_bonded_ethdev(const struct rte_eth_dev *eth_dev)
 {
@@ -26,9 +27,11 @@ check_for_bonded_ethdev(const struct rte_eth_dev *eth_dev)
 		return -1;
 
 	/* return 0 if driver name matches */
+	//如果设备驱动采用的是pmd_bond_drv驱动，则认为此接口为bond口
 	return eth_dev->device->driver->name != pmd_bond_drv.driver.name;
 }
 
+//检查port_id是否为bond口
 int
 valid_bonded_port_id(uint16_t port_id)
 {
@@ -147,6 +150,7 @@ deactivate_slave(struct rte_eth_dev *eth_dev, uint16_t port_id)
 	}
 }
 
+//bond接口创建
 int
 rte_eth_bond_create(const char *name, uint8_t mode, uint8_t socket_id)
 {
@@ -160,6 +164,7 @@ rte_eth_bond_create(const char *name, uint8_t mode, uint8_t socket_id)
 		return -EINVAL;
 	}
 
+	//构造设备参数
 	ret = snprintf(devargs, sizeof(devargs),
 		"driver=net_bonding,mode=%d,socket_id=%d", mode, socket_id);
 	if (ret < 0 || ret >= (int)sizeof(devargs))
@@ -169,6 +174,7 @@ rte_eth_bond_create(const char *name, uint8_t mode, uint8_t socket_id)
 	if (ret)
 		return -ENOMEM;
 
+	//生成一个port_id
 	ret = rte_eth_dev_get_port_by_name(name, &port_id);
 	RTE_ASSERT(!ret);
 
@@ -546,7 +552,7 @@ __eth_bond_slave_add_lock_free(uint16_t bonded_port_id, uint16_t slave_port_id)
 		return -1;
 	}
 
-	internals->slave_count++;
+	internals->slave_count++;//存入成功，count+1
 
 	if (bonded_eth_dev->data->dev_started) {
 		if (slave_configure(bonded_eth_dev, slave_eth_dev) != 0) {
@@ -607,6 +613,7 @@ rte_eth_bond_slave_add(uint16_t bonded_port_id, uint16_t slave_port_id)
 	int retval;
 
 	/* Verify that port id's are valid bonded and slave ports */
+	//先确认是否为bond口
 	if (valid_bonded_port_id(bonded_port_id) != 0)
 		return -1;
 
@@ -810,6 +817,8 @@ rte_eth_bond_primary_get(uint16_t bonded_port_id)
 	return internals->current_primary_port;
 }
 
+//返回bonded_port_id对应的bond接口的成员数目，并将这些成员赋值到slaves数组中
+//如果len长度小于成员数目也将返回失败
 int
 rte_eth_bond_slaves_get(uint16_t bonded_port_id, uint16_t slaves[],
 			uint16_t len)
@@ -817,6 +826,7 @@ rte_eth_bond_slaves_get(uint16_t bonded_port_id, uint16_t slaves[],
 	struct bond_dev_private *internals;
 	uint16_t i;
 
+	//如果非bond类型port,则跳出
 	if (valid_bonded_port_id(bonded_port_id) != 0)
 		return -1;
 
@@ -825,9 +835,11 @@ rte_eth_bond_slaves_get(uint16_t bonded_port_id, uint16_t slaves[],
 
 	internals = rte_eth_devices[bonded_port_id].data->dev_private;
 
+	//成员数大于len，则返回失败
 	if (internals->slave_count > len)
 		return -1;
 
+	//填充其slave
 	for (i = 0; i < internals->slave_count; i++)
 		slaves[i] = internals->slaves[i].port_id;
 
@@ -864,9 +876,11 @@ rte_eth_bond_mac_address_set(uint16_t bonded_port_id,
 	struct rte_eth_dev *bonded_eth_dev;
 	struct bond_dev_private *internals;
 
+	//校验必须为bond口
 	if (valid_bonded_port_id(bonded_port_id) != 0)
 		return -1;
 
+	//设置此接口的mac地址（仅设备在结构体上）
 	bonded_eth_dev = &rte_eth_devices[bonded_port_id];
 	internals = bonded_eth_dev->data->dev_private;
 
@@ -924,6 +938,7 @@ rte_eth_bond_mac_address_reset(uint16_t bonded_port_id)
 	return 0;
 }
 
+//设置hash策略函数
 int
 rte_eth_bond_xmit_policy_set(uint16_t bonded_port_id, uint8_t policy)
 {
diff --git a/drivers/net/bonding/rte_eth_bond_args.c b/drivers/net/bonding/rte_eth_bond_args.c
index abdf55261..b3002dbf5 100644
--- a/drivers/net/bonding/rte_eth_bond_args.c
+++ b/drivers/net/bonding/rte_eth_bond_args.c
@@ -116,6 +116,7 @@ parse_port_id(const char *port_str)
 	return port_id;
 }
 
+//由配置串获取到slaves的端口号
 int
 bond_ethdev_parse_slave_port_kvarg(const char *key,
 		const char *value, void *extra_args)
@@ -128,18 +129,21 @@ bond_ethdev_parse_slave_port_kvarg(const char *key,
 	slave_ports = extra_args;
 
 	if (strcmp(key, PMD_BOND_SLAVE_PORT_KVARG) == 0) {
+		//取出对应的port_id
 		int port_id = parse_port_id(value);
 		if (port_id < 0) {
 			RTE_BOND_LOG(ERR, "Invalid slave port value (%s) specified",
 				     value);
 			return -1;
 		} else
+			//填充port_id
 			slave_ports->slaves[slave_ports->slave_count++] =
 					port_id;
 	}
 	return 0;
 }
 
+//解析bond模式参数，是否合法
 int
 bond_ethdev_parse_slave_mode_kvarg(const char *key __rte_unused,
 		const char *value, void *extra_args)
diff --git a/drivers/net/bonding/rte_eth_bond_pmd.c b/drivers/net/bonding/rte_eth_bond_pmd.c
index 707a0f3cd..0a44c5e62 100644
--- a/drivers/net/bonding/rte_eth_bond_pmd.c
+++ b/drivers/net/bonding/rte_eth_bond_pmd.c
@@ -71,6 +71,7 @@ bond_ethdev_rx_burst(void *queue, struct rte_mbuf **bufs, uint16_t nb_pkts)
 	slave_count = internals->active_slave_count;
 	active_slave = internals->active_slave;
 
+	//自slave端口收取报文
 	for (i = 0; i < slave_count && nb_pkts; i++) {
 		uint16_t num_rx_slave;
 
@@ -109,9 +110,11 @@ bond_ethdev_rx_burst_active_backup(void *queue, struct rte_mbuf **bufs,
 static inline uint8_t
 is_lacp_packets(uint16_t ethertype, uint8_t subtype, struct rte_mbuf *mbuf)
 {
+	//lacp的以太网协议号是0x8809
 	const uint16_t ether_type_slow_be =
 		rte_be_to_cpu_16(RTE_ETHER_TYPE_SLOW);
 
+	//报文不能有vlan头，（不明白为什么subtype 可以等于SLOW_SUBTYPE_MARKER）
 	return !((mbuf->ol_flags & PKT_RX_VLAN) ? mbuf->vlan_tci : 0) &&
 		(ethertype == ether_type_slow_be &&
 		(subtype == SLOW_SUBTYPE_MARKER || subtype == SLOW_SUBTYPE_LACP));
@@ -313,7 +316,7 @@ rx_burst_8023ad(void *queue, struct rte_mbuf **bufs, uint16_t nb_pkts,
 		collecting = ACTOR_STATE(&bond_mode_8023ad_ports[slaves[idx]],
 					 COLLECTING);
 
-		/* Read packets from this slave */
+		/* Read packets from this slave */ //收包
 		num_rx_total += rte_eth_rx_burst(slaves[idx], bd_rx_q->queue_id,
 				&bufs[num_rx_total], nb_pkts - num_rx_total);
 
@@ -348,6 +351,7 @@ rx_burst_8023ad(void *queue, struct rte_mbuf **bufs, uint16_t nb_pkts,
 				  (!allmulti &&
 				   rte_is_multicast_ether_addr(&hdr->d_addr)))))) {
 
+				//仅处理lacp报文，其它的丢弃掉
 				if (hdr->ether_type == ether_type_slow_be) {
 					bond_mode_8023ad_handle_slow_pkt(
 					    internals, slaves[idx], bufs[j]);
@@ -541,6 +545,7 @@ bond_ethdev_rx_burst_alb(void *queue, struct rte_mbuf **bufs, uint16_t nb_pkts)
 	uint16_t nb_recv_pkts;
 	int i;
 
+	//自各slave收取报文
 	nb_recv_pkts = bond_ethdev_rx_burst(queue, bufs, nb_pkts);
 
 	for (i = 0; i < nb_recv_pkts; i++) {
@@ -552,6 +557,7 @@ bond_ethdev_rx_burst_alb(void *queue, struct rte_mbuf **bufs, uint16_t nb_pkts)
 #if defined(RTE_LIBRTE_BOND_DEBUG_ALB) || defined(RTE_LIBRTE_BOND_DEBUG_ALB_L1)
 			mode6_debug("RX ARP:", eth_h, bufs[i]->port, &burstnumberRX);
 #endif
+			//对arp报文进行学习client_info,修改arp响应报文
 			bond_mode_alb_arp_recv(eth_h, offset, internals);
 		}
 #if defined(RTE_LIBRTE_BOND_DEBUG_ALB) || defined(RTE_LIBRTE_BOND_DEBUG_ALB_L1)
@@ -578,7 +584,7 @@ bond_ethdev_tx_burst_round_robin(void *queue, struct rte_mbuf **bufs,
 
 	uint16_t num_tx_total = 0, num_tx_slave;
 
-	static int slave_idx = 0;
+	static int slave_idx = 0;//用此变量实现轮循发送
 	int i, cslave_idx = 0, tx_fail_total = 0;
 
 	bd_tx_q = (struct bond_tx_queue *)queue;
@@ -596,6 +602,7 @@ bond_ethdev_tx_burst_round_robin(void *queue, struct rte_mbuf **bufs,
 	/* Populate slaves mbuf with which packets are to be sent on it  */
 	for (i = 0; i < nb_pkts; i++) {
 		cslave_idx = (slave_idx + i) % num_of_slaves;
+		//在cslave_idx位置存入mbuf,计算cslave_idx中有多少个nb_pkts需要发送
 		slave_bufs[cslave_idx][(slave_nb_pkts[cslave_idx])++] = bufs[i];
 	}
 
@@ -604,6 +611,7 @@ bond_ethdev_tx_burst_round_robin(void *queue, struct rte_mbuf **bufs,
 	slave_idx = ++cslave_idx;
 
 	/* Send packet burst on each slave device */
+	//遍历第个slaves，实现发送
 	for (i = 0; i < num_of_slaves; i++) {
 		if (slave_nb_pkts[i] > 0) {
 			num_tx_slave = rte_eth_tx_burst(slaves[i], bd_tx_q->queue_id,
@@ -677,6 +685,7 @@ ipv6_hash(struct rte_ipv6_hdr *ipv6_hdr)
 }
 
 
+//按srcmac,dstmac进行hash
 void
 burst_xmit_l2_hash(struct rte_mbuf **buf, uint16_t nb_pkts,
 		uint16_t slave_count, uint16_t *slaves)
@@ -694,6 +703,7 @@ burst_xmit_l2_hash(struct rte_mbuf **buf, uint16_t nb_pkts,
 	}
 }
 
+//按srcmac,dstmac,srcip,dstip进行hash
 void
 burst_xmit_l23_hash(struct rte_mbuf **buf, uint16_t nb_pkts,
 		uint16_t slave_count, uint16_t *slaves)
@@ -732,6 +742,7 @@ burst_xmit_l23_hash(struct rte_mbuf **buf, uint16_t nb_pkts,
 	}
 }
 
+//按srcip,dstip,srcport,dstport进行hash
 void
 burst_xmit_l34_hash(struct rte_mbuf **buf, uint16_t nb_pkts,
 		uint16_t slave_count, uint16_t *slaves)
@@ -948,6 +959,8 @@ bond_ethdev_tx_burst_tlb(void *queue, struct rte_mbuf **bufs, uint16_t nb_pkts)
 						struct rte_ether_hdr *);
 			if (rte_is_same_ether_addr(&ether_hdr->s_addr,
 							&primary_slave_addr))
+				//由slaves数组的mac来代理primary_addr
+				//将源mac修改为slaves[i]接口对应的mac,使物理交换机只能学习到与之相连口的mac
 				rte_ether_addr_copy(&active_slave_addr,
 						&ether_hdr->s_addr);
 #if defined(RTE_LIBRTE_BOND_DEBUG_ALB) || defined(RTE_LIBRTE_BOND_DEBUG_ALB_L1)
@@ -1018,9 +1031,11 @@ bond_ethdev_tx_burst_alb(void *queue, struct rte_mbuf **bufs, uint16_t nb_pkts)
 		offset = get_vlan_offset(eth_h, &ether_type);
 
 		if (ether_type == rte_cpu_to_be_16(RTE_ETHER_TYPE_ARP)) {
+			//对arp报文在发送时进行特殊处理
 			slave_idx = bond_mode_alb_arp_xmit(eth_h, offset, internals);
 
 			/* Change src mac in eth header */
+			//修改以太头的mac地址为slave接口mac,使物理交换机在遇到arp时能学到slave接口对应的mac
 			rte_eth_macaddr_get(slave_idx, &eth_h->s_addr);
 
 			/* Add packet to slave tx buffer */
@@ -1066,6 +1081,7 @@ bond_ethdev_tx_burst_alb(void *queue, struct rte_mbuf **bufs, uint16_t nb_pkts)
 	}
 
 	/* Send ARP packets on proper slaves */
+	//arp报文发送
 	for (i = 0; i < RTE_MAX_ETHPORTS; i++) {
 		if (slave_bufs_pkts[i] > 0) {
 			num_send = rte_eth_tx_burst(i, bd_tx_q->queue_id,
@@ -1124,6 +1140,7 @@ bond_ethdev_tx_burst_alb(void *queue, struct rte_mbuf **bufs, uint16_t nb_pkts)
 	return num_tx_total;
 }
 
+//按hash结果进行分发
 static inline uint16_t
 tx_burst_balance(void *queue, struct rte_mbuf **bufs, uint16_t nb_bufs,
 		 uint16_t *slave_port_ids, uint16_t slave_count)
@@ -1147,6 +1164,7 @@ tx_burst_balance(void *queue, struct rte_mbuf **bufs, uint16_t nb_bufs,
 	 * Populate slaves mbuf with the packets which are to be sent on it
 	 * selecting output slave using hash based on xmit policy
 	 */
+	//按hash计算出接口
 	internals->burst_xmit_hash(bufs, nb_bufs, slave_count,
 			bufs_slave_port_idxs);
 
@@ -1292,6 +1310,7 @@ bond_ethdev_tx_burst_8023ad_fast_queue(void *queue, struct rte_mbuf **bufs,
 	return tx_burst_8023ad(queue, bufs, nb_bufs, true);
 }
 
+//广播方式发送
 static uint16_t
 bond_ethdev_tx_burst_broadcast(void *queue, struct rte_mbuf **bufs,
 		uint16_t nb_pkts)
@@ -1418,6 +1437,7 @@ mac_address_get(struct rte_eth_dev *eth_dev,
 	return 0;
 }
 
+//设置mac地址，仅设置在结构体上
 int
 mac_address_set(struct rte_eth_dev *eth_dev,
 		struct rte_ether_addr *new_mac_addr)
@@ -1512,6 +1532,7 @@ mac_address_slaves_update(struct rte_eth_dev *bonded_eth_dev)
 	case BONDING_MODE_ROUND_ROBIN:
 	case BONDING_MODE_BALANCE:
 	case BONDING_MODE_BROADCAST:
+		//设置成员接口的mac地址
 		for (i = 0; i < internals->slave_count; i++) {
 			if (rte_eth_dev_default_mac_addr_set(
 					internals->slaves[i].port_id,
@@ -1554,6 +1575,7 @@ mac_address_slaves_update(struct rte_eth_dev *bonded_eth_dev)
 	return 0;
 }
 
+//设置bond的模式
 int
 bond_ethdev_mode_set(struct rte_eth_dev *eth_dev, int mode)
 {
@@ -1562,23 +1584,30 @@ bond_ethdev_mode_set(struct rte_eth_dev *eth_dev, int mode)
 	internals = eth_dev->data->dev_private;
 
 	switch (mode) {
+	//轮循发送
 	case BONDING_MODE_ROUND_ROBIN:
 		eth_dev->tx_pkt_burst = bond_ethdev_tx_burst_round_robin;
 		eth_dev->rx_pkt_burst = bond_ethdev_rx_burst;
 		break;
+		//主备发送
 	case BONDING_MODE_ACTIVE_BACKUP:
 		eth_dev->tx_pkt_burst = bond_ethdev_tx_burst_active_backup;
 		eth_dev->rx_pkt_burst = bond_ethdev_rx_burst_active_backup;
 		break;
+		//按hash结果分发，目前支持l2(srcmac,dstmac)
+		//l2,l3(srcmac,dstmac,srcip,dstip)
+		//l3,l4(srcip,dstip,srcport,dstport) 进行分发
 	case BONDING_MODE_BALANCE:
 		eth_dev->tx_pkt_burst = bond_ethdev_tx_burst_balance;
 		eth_dev->rx_pkt_burst = bond_ethdev_rx_burst;
 		break;
+		//按广播方式发送
 	case BONDING_MODE_BROADCAST:
 		eth_dev->tx_pkt_burst = bond_ethdev_tx_burst_broadcast;
 		eth_dev->rx_pkt_burst = bond_ethdev_rx_burst;
 		break;
 	case BONDING_MODE_8023AD:
+		//采用lacp进行动态配置
 		if (bond_mode_8023ad_enable(eth_dev) != 0)
 			return -1;
 
@@ -3192,6 +3221,7 @@ bond_alloc(struct rte_vdev_device *dev, uint8_t mode)
 	eth_dev->data->nb_tx_queues = (uint16_t)1;
 
 	/* Allocate memory for storing MAC addresses */
+	//mac地址全为０
 	eth_dev->data->mac_addrs = rte_zmalloc_socket(name, RTE_ETHER_ADDR_LEN *
 			BOND_MAX_MAC_ADDRS, 0, socket_id);
 	if (eth_dev->data->mac_addrs == NULL) {
@@ -3286,6 +3316,7 @@ bond_alloc(struct rte_vdev_device *dev, uint8_t mode)
 	return -1;
 }
 
+//bond设备probe,控测dev设备是否可用
 static int
 bond_probe(struct rte_vdev_device *dev)
 {
@@ -3303,6 +3334,7 @@ bond_probe(struct rte_vdev_device *dev)
 	name = rte_vdev_device_name(dev);
 	RTE_BOND_LOG(INFO, "Initializing pmd_bond for %s", name);
 
+	//主备进程模式处理
 	if (rte_eal_process_type() == RTE_PROC_SECONDARY) {
 		eth_dev = rte_eth_dev_attach_secondary(name);
 		if (!eth_dev) {
@@ -3316,6 +3348,7 @@ bond_probe(struct rte_vdev_device *dev)
 		return 0;
 	}
 
+	//将参数解析为list
 	kvlist = rte_kvargs_parse(rte_vdev_device_args(dev),
 		pmd_bond_init_valid_arguments);
 	if (kvlist == NULL)
@@ -3323,6 +3356,7 @@ bond_probe(struct rte_vdev_device *dev)
 
 	/* Parse link bonding mode */
 	if (rte_kvargs_count(kvlist, PMD_BOND_MODE_KVARG) == 1) {
+		//解析bond模式参数，是否合法，将解析值保存在bonding_mode中
 		if (rte_kvargs_process(kvlist, PMD_BOND_MODE_KVARG,
 				&bond_ethdev_parse_slave_mode_kvarg,
 				&bonding_mode) != 0) {
@@ -3339,6 +3373,7 @@ bond_probe(struct rte_vdev_device *dev)
 	/* Parse socket id to create bonding device on */
 	arg_count = rte_kvargs_count(kvlist, PMD_BOND_SOCKET_ID_KVARG);
 	if (arg_count == 1) {
+		//解析socket_id参数，socket_id的值必须大于０
 		if (rte_kvargs_process(kvlist, PMD_BOND_SOCKET_ID_KVARG,
 				&bond_ethdev_parse_socket_id_kvarg, &socket_id)
 				!= 0) {
@@ -3587,6 +3622,7 @@ bond_ethdev_configure(struct rte_eth_dev *dev)
 
 		memset(&slave_ports, 0, sizeof(slave_ports));
 
+		//解析有哪些slave端口
 		if (rte_kvargs_process(kvlist, PMD_BOND_SLAVE_PORT_KVARG,
 				       &bond_ethdev_parse_slave_port_kvarg, &slave_ports) != 0) {
 			RTE_BOND_LOG(ERR,
@@ -3595,6 +3631,7 @@ bond_ethdev_configure(struct rte_eth_dev *dev)
 			return -1;
 		}
 
+		//将slave端口加入到bond端口中
 		for (i = 0; i < slave_ports.slave_count; i++) {
 			if (rte_eth_bond_slave_add(port_id, slave_ports.slaves[i]) != 0) {
 				RTE_BOND_LOG(ERR,
@@ -3735,7 +3772,9 @@ struct rte_vdev_driver pmd_bond_drv = {
 	.remove = bond_remove,
 };
 
+//指明pmd_bond_drv的驱动名称为net_bonding,注册pmd_bond_drv
 RTE_PMD_REGISTER_VDEV(net_bonding, pmd_bond_drv);
+//注册驱动另名
 RTE_PMD_REGISTER_ALIAS(net_bonding, eth_bond);
 
 RTE_PMD_REGISTER_PARAM_STRING(net_bonding,
diff --git a/drivers/net/e1000/em_ethdev.c b/drivers/net/e1000/em_ethdev.c
index 9a88b50b2..63e0aeef0 100644
--- a/drivers/net/e1000/em_ethdev.c
+++ b/drivers/net/e1000/em_ethdev.c
@@ -432,6 +432,7 @@ em_hw_init(struct e1000_hw *hw)
 	return diag;
 }
 
+//添加标记，需要link更新
 static int
 eth_em_configure(struct rte_eth_dev *dev)
 {
@@ -1582,8 +1583,10 @@ eth_em_interrupt_action(struct rte_eth_dev *dev,
 	int ret;
 
 	if (!(intr->flags & E1000_FLAG_NEED_LINK_UPDATE))
+		//无标记，退出
 		return -1;
 
+	//清除标记
 	intr->flags &= ~E1000_FLAG_NEED_LINK_UPDATE;
 	rte_intr_ack(intr_handle);
 
@@ -1623,6 +1626,7 @@ eth_em_interrupt_action(struct rte_eth_dev *dev,
  * @return
  *  void
  */
+//中断处理回调
 static void
 eth_em_interrupt_handler(void *param)
 {
diff --git a/drivers/net/e1000/em_rxtx.c b/drivers/net/e1000/em_rxtx.c
index 49c53712a..b43b309ea 100644
--- a/drivers/net/e1000/em_rxtx.c
+++ b/drivers/net/e1000/em_rxtx.c
@@ -152,6 +152,7 @@ struct em_tx_queue {
 	uint64_t               tx_ring_phys_addr; /**< TX ring DMA address. */
 	struct em_tx_entry    *sw_ring; /**< virtual address of SW ring. */
 	volatile uint32_t      *tdt_reg_addr; /**< Address of TDT register. */
+	//sw_ring中的tx描述符数量
 	uint16_t               nb_tx_desc;    /**< number of TX descriptors. */
 	uint16_t               tx_tail;  /**< Current value of TDT register. */
 	/**< Start freeing TX buffers if there are less free descriptors than
@@ -1192,6 +1193,7 @@ em_get_tx_queue_offloads_capa(struct rte_eth_dev *dev)
 	return tx_queue_offload_capa;
 }
 
+//设置一个tx队列
 int
 eth_em_tx_queue_setup(struct rte_eth_dev *dev,
 			 uint16_t queue_idx,
@@ -1266,6 +1268,7 @@ eth_em_tx_queue_setup(struct rte_eth_dev *dev,
 
 	/* Free memory prior to re-allocation if needed... */
 	if (dev->data->tx_queues[queue_idx] != NULL) {
+		//指定的queue_idx上已有队列，需要释放
 		em_tx_queue_release(dev->data->tx_queues[queue_idx]);
 		dev->data->tx_queues[queue_idx] = NULL;
 	}
@@ -1282,11 +1285,13 @@ eth_em_tx_queue_setup(struct rte_eth_dev *dev,
 		return -ENOMEM;
 
 	/* Allocate the tx queue data structure. */
+	//申请队列结构体
 	if ((txq = rte_zmalloc("ethdev TX queue", sizeof(*txq),
 			RTE_CACHE_LINE_SIZE)) == NULL)
 		return -ENOMEM;
 
 	/* Allocate software ring */
+	//申请交换ring,申请数量为nb_desc
 	if ((txq->sw_ring = rte_zmalloc("txq->sw_ring",
 			sizeof(txq->sw_ring[0]) * nb_desc,
 			RTE_CACHE_LINE_SIZE)) == NULL) {
@@ -1312,6 +1317,7 @@ eth_em_tx_queue_setup(struct rte_eth_dev *dev,
 
 	em_reset_tx_queue(txq);
 
+	//设置tx队列
 	dev->data->tx_queues[queue_idx] = txq;
 	txq->offloads = offloads;
 	return 0;
diff --git a/drivers/net/e1000/igb_ethdev.c b/drivers/net/e1000/igb_ethdev.c
index ce7c9e664..d36a5e339 100644
--- a/drivers/net/e1000/igb_ethdev.c
+++ b/drivers/net/e1000/igb_ethdev.c
@@ -5767,6 +5767,7 @@ igb_filter_restore(struct rte_eth_dev *dev)
 	return 0;
 }
 
+//注册名称为net_e1000_igb的pci驱动rte_igb_pmd
 RTE_PMD_REGISTER_PCI(net_e1000_igb, rte_igb_pmd);
 RTE_PMD_REGISTER_PCI_TABLE(net_e1000_igb, pci_id_igb_map);
 RTE_PMD_REGISTER_KMOD_DEP(net_e1000_igb, "* igb_uio | uio_pci_generic | vfio-pci");
diff --git a/drivers/net/e1000/igb_rxtx.c b/drivers/net/e1000/igb_rxtx.c
index c5606de5d..71667ccba 100644
--- a/drivers/net/e1000/igb_rxtx.c
+++ b/drivers/net/e1000/igb_rxtx.c
@@ -374,6 +374,7 @@ tx_desc_vlan_flags_to_cmdtype(uint64_t ol_flags)
 	return cmdtype;
 }
 
+//发包回调
 uint16_t
 eth_igb_xmit_pkts(void *tx_queue, struct rte_mbuf **tx_pkts,
 	       uint16_t nb_pkts)
@@ -800,6 +801,7 @@ rx_desc_error_to_pkt_flags(uint32_t rx_status)
 		E1000_RXD_ERR_CKSUM_BIT) & E1000_RXD_ERR_CKSUM_MSK];
 }
 
+//收包回调
 uint16_t
 eth_igb_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts,
 	       uint16_t nb_pkts)
diff --git a/drivers/net/ifc/base/ifcvf_osdep.h b/drivers/net/ifc/base/ifcvf_osdep.h
index 6aef25ea4..27ffa678e 100644
--- a/drivers/net/ifc/base/ifcvf_osdep.h
+++ b/drivers/net/ifc/base/ifcvf_osdep.h
@@ -28,6 +28,7 @@
 
 typedef struct rte_pci_device PCI_DEV;
 
+//自pci配置空间读取一个字节
 #define PCI_READ_CONFIG_BYTE(dev, val, where) \
 	rte_pci_read_config(dev, val, 1, where)
 
diff --git a/drivers/net/ifc/ifcvf_vdpa.c b/drivers/net/ifc/ifcvf_vdpa.c
index 9c562def0..a2aa80aec 100644
--- a/drivers/net/ifc/ifcvf_vdpa.c
+++ b/drivers/net/ifc/ifcvf_vdpa.c
@@ -1172,6 +1172,7 @@ ifcvf_pci_probe(struct rte_pci_driver *pci_drv __rte_unused,
 		(1ULL << VHOST_USER_F_PROTOCOL_FEATURES) |
 		(1ULL << VHOST_F_LOG_ALL);
 
+	//设置pci地址
 	internal->dev_addr.pci_addr = pci_dev->addr;
 	internal->dev_addr.type = PCI_ADDR;
 	list->internal = internal;
@@ -1184,6 +1185,7 @@ ifcvf_pci_probe(struct rte_pci_driver *pci_drv __rte_unused,
 	}
 	internal->sw_lm = sw_fallback_lm;
 
+	//注册vdpa设备
 	internal->did = rte_vdpa_register_device(&internal->dev_addr,
 				&ifcvf_ops);
 	if (internal->did < 0) {
@@ -1247,7 +1249,7 @@ ifcvf_pci_remove(struct rte_pci_device *pci_dev)
  */
 static const struct rte_pci_id pci_id_ifcvf_map[] = {
 	{ .class_id = RTE_CLASS_ANY_ID,
-	  .vendor_id = IFCVF_VENDOR_ID,
+	  .vendor_id = IFCVF_VENDOR_ID,//virtio的vendor
 	  .device_id = IFCVF_DEVICE_ID,
 	  .subsystem_vendor_id = IFCVF_SUBSYS_VENDOR_ID,
 	  .subsystem_device_id = IFCVF_SUBSYS_DEVICE_ID,
@@ -1257,6 +1259,7 @@ static const struct rte_pci_id pci_id_ifcvf_map[] = {
 	},
 };
 
+//注册ifcvf_vdpa驱动
 static struct rte_pci_driver rte_ifcvf_vdpa = {
 	.id_table = pci_id_ifcvf_map,
 	.drv_flags = 0,
diff --git a/drivers/net/ixgbe/ixgbe_ethdev.c b/drivers/net/ixgbe/ixgbe_ethdev.c
index dbce7a80e..9f15343ba 100644
--- a/drivers/net/ixgbe/ixgbe_ethdev.c
+++ b/drivers/net/ixgbe/ixgbe_ethdev.c
@@ -1731,6 +1731,7 @@ eth_ixgbevf_dev_uninit(struct rte_eth_dev *eth_dev)
 	return 0;
 }
 
+//ixgbe驱动探测设备
 static int
 eth_ixgbe_pci_probe(struct rte_pci_driver *pci_drv __rte_unused,
 		struct rte_pci_device *pci_dev)
@@ -1808,6 +1809,7 @@ static int eth_ixgbe_pci_remove(struct rte_pci_device *pci_dev)
 		return rte_eth_dev_destroy(ethdev, eth_ixgbe_dev_uninit);
 }
 
+//ixgbe驱动
 static struct rte_pci_driver rte_ixgbe_pmd = {
 	.id_table = pci_id_ixgbe_map,
 	.drv_flags = RTE_PCI_DRV_NEED_MAPPING | RTE_PCI_DRV_INTR_LSC,
diff --git a/drivers/net/mlx4/mlx4.c b/drivers/net/mlx4/mlx4.c
index 931e4f4fe..1e5a3c9a8 100644
--- a/drivers/net/mlx4/mlx4.c
+++ b/drivers/net/mlx4/mlx4.c
@@ -660,6 +660,7 @@ mlx4_hw_rss_sup(struct ibv_context *ctx, struct ibv_pd *pd,
 			.rx_hash_conf = {
 				.rx_hash_function = IBV_RX_HASH_FUNC_TOEPLITZ,
 				.rx_hash_key_len = MLX4_RSS_HASH_KEY_SIZE,
+				//传入rss hash所用的key
 				.rx_hash_key = mlx4_rss_hash_key_default,
 				.rx_hash_fields_mask = hw_rss_sup,
 			},
diff --git a/drivers/net/mlx4/mlx4.h b/drivers/net/mlx4/mlx4.h
index c6cb29493..7cea88ffd 100644
--- a/drivers/net/mlx4/mlx4.h
+++ b/drivers/net/mlx4/mlx4.h
@@ -162,6 +162,7 @@ struct mlx4_priv {
 	unsigned int if_index;	/**< Associated network device index */
 	uint16_t mtu; /**< Configured MTU. */
 	uint8_t port; /**< Physical port number. */
+	//指明设备已被调用了start
 	uint32_t started:1; /**< Device started, flows enabled. */
 	uint32_t vf:1; /**< This is a VF device. */
 	uint32_t intr_alarm:1; /**< An interrupt alarm is scheduled. */
@@ -185,7 +186,9 @@ struct mlx4_priv {
 		struct mlx4_mr_list mr_list; /* Registered MR list. */
 		struct mlx4_mr_list mr_free_list; /* Freed MR list. */
 	} mr;
+	//用来串链rss配置信息
 	LIST_HEAD(, mlx4_rss) rss; /**< Shared targets for Rx flow rules. */
+	//流规则处理
 	LIST_HEAD(, rte_flow) flows; /**< Configured flow rule handles. */
 	struct rte_ether_addr mac[MLX4_MAX_MAC_ADDRESSES];
 	/**< Configured MAC addresses. Unused entries are zeroed. */
diff --git a/drivers/net/mlx4/mlx4_flow.c b/drivers/net/mlx4/mlx4_flow.c
index 96479b83d..3092221a6 100644
--- a/drivers/net/mlx4/mlx4_flow.c
+++ b/drivers/net/mlx4/mlx4_flow.c
@@ -50,11 +50,11 @@
 /** Processor structure associated with a flow item. */
 struct mlx4_flow_proc_item {
 	/** Bit-mask for fields supported by this PMD. */
-	const void *mask_support;
+	const void *mask_support;//用户提供的mask值
 	/** Bit-mask to use when @p item->mask is not provided. */
-	const void *mask_default;
+	const void *mask_default;//默认的mask值
 	/** Size in bytes for @p mask_support and @p mask_default. */
-	const unsigned int mask_sz;
+	const unsigned int mask_sz;//mask的大小
 	/** Merge a pattern item into a flow rule handle. */
 	int (*merge)(struct rte_flow *flow,
 		     const struct rte_flow_item *item,
@@ -63,7 +63,7 @@ struct mlx4_flow_proc_item {
 	/** Size in bytes of the destination structure. */
 	const unsigned int dst_sz;
 	/** List of possible subsequent items. */
-	const enum rte_flow_item_type *const next_item;
+	const enum rte_flow_item_type *const next_item;//指出本项后续可以存在的flow项
 };
 
 /** Shared resources for drop flow rules. */
@@ -93,6 +93,7 @@ struct mlx4_drop {
  *   Converted RSS hash fields on success, (uint64_t)-1 otherwise and
  *   rte_errno is set.
  */
+//将dpdk的rss_type转换为mlx4对应的rss_type或者相反的转换
 uint64_t
 mlx4_conv_rss_types(struct mlx4_priv *priv, uint64_t types, int verbs_to_dpdk)
 {
@@ -103,8 +104,11 @@ mlx4_conv_rss_types(struct mlx4_priv *priv, uint64_t types, int verbs_to_dpdk)
 		IPV4_TCP, IPV4_UDP, IPV6_TCP, IPV6_TCP_1, IPV6_UDP, IPV6_UDP_1,
 	};
 	enum {
+		//ipv4 src地址，dst地址
 		VERBS_IPV4 = IBV_RX_HASH_SRC_IPV4 | IBV_RX_HASH_DST_IPV4,
+		//ipv6 src地址，dst地址
 		VERBS_IPV6 = IBV_RX_HASH_SRC_IPV6 | IBV_RX_HASH_DST_IPV6,
+		//srcport,dstport
 		VERBS_TCP = IBV_RX_HASH_SRC_PORT_TCP | IBV_RX_HASH_DST_PORT_TCP,
 		VERBS_UDP = IBV_RX_HASH_SRC_PORT_UDP | IBV_RX_HASH_DST_PORT_UDP,
 	};
@@ -128,38 +132,42 @@ mlx4_conv_rss_types(struct mlx4_priv *priv, uint64_t types, int verbs_to_dpdk)
 	};
 	static const uint64_t verbs[RTE_DIM(dpdk)] = {
 		[INNER] = IBV_RX_HASH_INNER,
-		[IPV4] = VERBS_IPV4,
+		[IPV4] = VERBS_IPV4,//统一转为ipv4(srcaddr,dstaddr)
 		[IPV4_1] = VERBS_IPV4,
 		[IPV4_2] = VERBS_IPV4,
 		[IPV6] = VERBS_IPV6,
 		[IPV6_1] = VERBS_IPV6,
 		[IPV6_2] = VERBS_IPV6,
 		[IPV6_3] = VERBS_IPV6,
-		[TCP] = VERBS_TCP,
+		[TCP] = VERBS_TCP,//仅srcport,dstport
 		[UDP] = VERBS_UDP,
-		[IPV4_TCP] = VERBS_IPV4 | VERBS_TCP,
+		[IPV4_TCP] = VERBS_IPV4 | VERBS_TCP,//仅ipv4(srcaddr,dstadd,srcport,dstport)
 		[IPV4_UDP] = VERBS_IPV4 | VERBS_UDP,
 		[IPV6_TCP] = VERBS_IPV6 | VERBS_TCP,
 		[IPV6_TCP_1] = VERBS_IPV6 | VERBS_TCP,
 		[IPV6_UDP] = VERBS_IPV6 | VERBS_UDP,
 		[IPV6_UDP_1] = VERBS_IPV6 | VERBS_UDP,
 	};
+	//确定是由哪个集合向哪个集合转换
 	const uint64_t *in = verbs_to_dpdk ? verbs : dpdk;
 	const uint64_t *out = verbs_to_dpdk ? dpdk : verbs;
-	uint64_t seen = 0;
-	uint64_t conv = 0;
+	uint64_t seen = 0;//我们看到的flag
+	uint64_t conv = 0;//我们转换后的flag
 	unsigned int i;
 
 	if (!types) {
+		//未指定type时，使用硬件初始时配置的types
 		if (!verbs_to_dpdk)
 			return priv->hw_rss_sup;
 		types = priv->hw_rss_sup;
 	}
 	for (i = 0; i != RTE_DIM(dpdk); ++i)
 		if (in[i] && (types & in[i]) == in[i]) {
+			//如果in[i]配置了，则更新seen,更新conv
 			seen |= types & in[i];
 			conv |= out[i];
 		}
+	//types与seen一致时，认为转换有效
 	if ((verbs_to_dpdk || (conv & priv->hw_rss_sup) == conv) &&
 	    !(types & ~seen))
 		return conv;
@@ -208,16 +216,20 @@ mlx4_flow_merge_eth(struct rte_flow *flow,
 		uint32_t sum_dst = 0;
 		uint32_t sum_src = 0;
 
+		//当前支持“单播匹配","组播匹配","广播匹配”，“全匹配”
 		for (i = 0; i != sizeof(mask->dst.addr_bytes); ++i) {
 			sum_dst += mask->dst.addr_bytes[i];
 			sum_src += mask->src.addr_bytes[i];
 		}
 		if (sum_src) {
+			//sum_src必须为0
 			msg = "mlx4 does not support source MAC matching";
 			goto error;
 		} else if (!sum_dst) {
+			//目的mac为0
 			flow->promisc = 1;
 		} else if (sum_dst == 1 && mask->dst.addr_bytes[0] == 1) {
+			//组播mask
 			if (!(spec->dst.addr_bytes[0] & 1)) {
 				msg = "mlx4 does not support the explicit"
 					" exclusion of all multicast traffic";
@@ -225,6 +237,7 @@ mlx4_flow_merge_eth(struct rte_flow *flow,
 			}
 			flow->allmulti = 1;
 		} else if (sum_dst != (UINT8_C(0xff) * RTE_ETHER_ADDR_LEN)) {
+			//不支持匹配部分的mac地址（即掩码形式）
 			msg = "mlx4 does not support matching partial"
 				" Ethernet fields";
 			goto error;
@@ -578,7 +591,7 @@ static const struct mlx4_flow_proc_item mlx4_flow_proc_item_list[] = {
 	[RTE_FLOW_ITEM_TYPE_END] = {
 		.next_item = NEXT_ITEM(RTE_FLOW_ITEM_TYPE_ETH),
 	},
-	[RTE_FLOW_ITEM_TYPE_ETH] = {
+	[RTE_FLOW_ITEM_TYPE_ETH] = {//以太头
 		.next_item = NEXT_ITEM(RTE_FLOW_ITEM_TYPE_VLAN,
 				       RTE_FLOW_ITEM_TYPE_IPV4),
 		.mask_support = &(const struct rte_flow_item_eth){
@@ -590,7 +603,7 @@ static const struct mlx4_flow_proc_item mlx4_flow_proc_item_list[] = {
 		.merge = mlx4_flow_merge_eth,
 		.dst_sz = sizeof(struct ibv_flow_spec_eth),
 	},
-	[RTE_FLOW_ITEM_TYPE_VLAN] = {
+	[RTE_FLOW_ITEM_TYPE_VLAN] = {//vlan
 		.next_item = NEXT_ITEM(RTE_FLOW_ITEM_TYPE_IPV4),
 		.mask_support = &(const struct rte_flow_item_vlan){
 			/* Only TCI VID matching is supported. */
@@ -601,7 +614,7 @@ static const struct mlx4_flow_proc_item mlx4_flow_proc_item_list[] = {
 		.merge = mlx4_flow_merge_vlan,
 		.dst_sz = 0,
 	},
-	[RTE_FLOW_ITEM_TYPE_IPV4] = {
+	[RTE_FLOW_ITEM_TYPE_IPV4] = {//ipv4
 		.next_item = NEXT_ITEM(RTE_FLOW_ITEM_TYPE_UDP,
 				       RTE_FLOW_ITEM_TYPE_TCP),
 		.mask_support = &(const struct rte_flow_item_ipv4){
@@ -615,7 +628,7 @@ static const struct mlx4_flow_proc_item mlx4_flow_proc_item_list[] = {
 		.merge = mlx4_flow_merge_ipv4,
 		.dst_sz = sizeof(struct ibv_flow_spec_ipv4),
 	},
-	[RTE_FLOW_ITEM_TYPE_UDP] = {
+	[RTE_FLOW_ITEM_TYPE_UDP] = {//udp匹配
 		.mask_support = &(const struct rte_flow_item_udp){
 			.hdr = {
 				.src_port = RTE_BE16(0xffff),
@@ -627,7 +640,7 @@ static const struct mlx4_flow_proc_item mlx4_flow_proc_item_list[] = {
 		.merge = mlx4_flow_merge_udp,
 		.dst_sz = sizeof(struct ibv_flow_spec_tcp_udp),
 	},
-	[RTE_FLOW_ITEM_TYPE_TCP] = {
+	[RTE_FLOW_ITEM_TYPE_TCP] = {//tcp匹配
 		.mask_support = &(const struct rte_flow_item_tcp){
 			.hdr = {
 				.src_port = RTE_BE16(0xffff),
@@ -663,9 +676,9 @@ static const struct mlx4_flow_proc_item mlx4_flow_proc_item_list[] = {
  */
 static int
 mlx4_flow_prepare(struct mlx4_priv *priv,
-		  const struct rte_flow_attr *attr,
-		  const struct rte_flow_item pattern[],
-		  const struct rte_flow_action actions[],
+		  const struct rte_flow_attr *attr,//flow属性
+		  const struct rte_flow_item pattern[],//flow匹配项
+		  const struct rte_flow_action actions[],//flow 动作项
 		  struct rte_flow_error *error,
 		  struct rte_flow **addr)
 {
@@ -678,23 +691,28 @@ mlx4_flow_prepare(struct mlx4_priv *priv,
 	int overlap;
 
 	if (attr->group)
+		//当前不支持组
 		return rte_flow_error_set
 			(error, ENOTSUP, RTE_FLOW_ERROR_TYPE_ATTR_GROUP,
 			 NULL, "groups are not supported");
 	if (attr->priority > MLX4_FLOW_PRIORITY_LAST)
+		//不支持优先级超过MLX4_FLOW_PRIORITY_LAST
 		return rte_flow_error_set
 			(error, ENOTSUP, RTE_FLOW_ERROR_TYPE_ATTR_PRIORITY,
 			 NULL, "maximum priority level is "
 			 MLX4_STR_EXPAND(MLX4_FLOW_PRIORITY_LAST));
 	if (attr->egress)
+		//不支持对egress设置filter
 		return rte_flow_error_set
 			(error, ENOTSUP, RTE_FLOW_ERROR_TYPE_ATTR_EGRESS,
 			 NULL, "egress is not supported");
 	if (attr->transfer)
+		//不支持transfter规则
 		return rte_flow_error_set
 			(error, ENOTSUP, RTE_FLOW_ERROR_TYPE_ATTR_TRANSFER,
 			 NULL, "transfer is not supported");
 	if (!attr->ingress)
+		//仅支持配置ingress，但ingress未被设置，报错
 		return rte_flow_error_set
 			(error, ENOTSUP, RTE_FLOW_ERROR_TYPE_ATTR_INGRESS,
 			 NULL, "only ingress is supported");
@@ -702,6 +720,7 @@ mlx4_flow_prepare(struct mlx4_priv *priv,
 	overlap = 0;
 	proc = mlx4_flow_proc_item_list;
 	flow->priority = attr->priority;
+	//遍历匹配模式数组
 	/* Go over pattern. */
 	for (item = pattern; item->type; ++item) {
 		const struct mlx4_flow_proc_item *next = NULL;
@@ -709,7 +728,7 @@ mlx4_flow_prepare(struct mlx4_priv *priv,
 		int err;
 
 		if (item->type == RTE_FLOW_ITEM_TYPE_VOID)
-			continue;
+			continue;//跳过空实现
 		if (item->type == MLX4_FLOW_ITEM_TYPE_INTERNAL) {
 			flow->internal = 1;
 			continue;
@@ -720,13 +739,16 @@ mlx4_flow_prepare(struct mlx4_priv *priv,
 				" matching on Ethernet headers";
 			goto exit_item_not_supported;
 		}
+
+		//检查匹配项对应的type
 		for (i = 0; proc->next_item && proc->next_item[i]; ++i) {
 			if (proc->next_item[i] == item->type) {
 				next = &mlx4_flow_proc_item_list[item->type];
 				break;
 			}
 		}
-		if (!next)
+		if (!next)//没有找到对应的type,报错：当前不知支持
+			//这里实际上也说明mlx4支持的几种匹配type
 			goto exit_item_not_supported;
 		proc = next;
 		/*
@@ -746,12 +768,14 @@ mlx4_flow_prepare(struct mlx4_priv *priv,
 		flow->ibv_attr_size += proc->dst_sz;
 	}
 	/* Go over actions list. */
+	//遍历所有action列表
 	for (action = actions; action->type; ++action) {
 		/* This one may appear anywhere multiple times. */
 		if (action->type == RTE_FLOW_ACTION_TYPE_VOID)
-			continue;
+			continue;//忽略掉空的action
 		/* Fate-deciding actions may appear exactly once. */
 		if (overlap) {
+			//不支持合并多个action
 			msg = "cannot combine several fate-deciding actions,"
 				" choose between DROP, QUEUE or RSS";
 			goto exit_action_not_supported;
@@ -765,36 +789,41 @@ mlx4_flow_prepare(struct mlx4_priv *priv,
 			uint64_t fields;
 			unsigned int i;
 
+		//丢包action
 		case RTE_FLOW_ACTION_TYPE_DROP:
 			flow->drop = 1;
 			break;
-		case RTE_FLOW_ACTION_TYPE_QUEUE:
+		case RTE_FLOW_ACTION_TYPE_QUEUE://入队action,处理为rss方式，仅入一个队
 			if (flow->rss)
-				break;
+				break;//flow中指定有rss,则跳出
+			//检查队列配置
 			queue = action->conf;
 			if (queue->index >= ETH_DEV(priv)->data->nb_rx_queues) {
 				msg = "queue target index beyond number of"
 					" configured Rx queues";
-				goto exit_action_not_supported;
+				goto exit_action_not_supported;//配置的队列比报文实现的收队列还大，报错
 			}
+			//创建rss,fields=0,队列索引数组长度为1
 			flow->rss = mlx4_rss_get
 				(priv, 0, mlx4_rss_hash_key_default, 1,
 				 &queue->index);
-			if (!flow->rss) {
+			if (!flow->rss) {//申请内存失败
 				msg = "not enough resources for additional"
 					" single-queue RSS context";
 				goto exit_action_not_supported;
 			}
 			break;
-		case RTE_FLOW_ACTION_TYPE_RSS:
+		case RTE_FLOW_ACTION_TYPE_RSS://由rssh计算后进行入队出理
 			if (flow->rss)
 				break;
 			rss = action->conf;
 			/* Default RSS configuration if none is provided. */
 			if (rss->key_len) {
+				//用户指定了key时
 				rss_key = rss->key;
 				rss_key_len = rss->key_len;
 			} else {
+				//用户未指定key时，采用默认的key
 				rss_key = mlx4_rss_hash_key_default;
 				rss_key_len = MLX4_RSS_HASH_KEY_SIZE;
 			}
@@ -802,23 +831,28 @@ mlx4_flow_prepare(struct mlx4_priv *priv,
 			for (i = 0; i < rss->queue_num; ++i)
 				if (rss->queue[i] >=
 				    ETH_DEV(priv)->data->nb_rx_queues)
-					break;
+					break;//配置的队列号不得大于设备的实际收队列数
 			if (i != rss->queue_num) {
+				//提前即出，即队列配置有误，队列号过大。
 				msg = "queue index target beyond number of"
 					" configured Rx queues";
 				goto exit_action_not_supported;
 			}
+			//队列数不是2的N次方
 			if (!rte_is_power_of_2(rss->queue_num)) {
 				msg = "for RSS, mlx4 requires the number of"
 					" queues to be a power of two";
 				goto exit_action_not_supported;
 			}
+			//只支持长度为40的key
 			if (rss_key_len != sizeof(flow->rss->key)) {
 				msg = "mlx4 supports exactly one RSS hash key"
 					" length: "
 					MLX4_STR_EXPAND(MLX4_RSS_HASH_KEY_SIZE);
 				goto exit_action_not_supported;
 			}
+
+			//配置的rss queue必须是连续的
 			for (i = 1; i < rss->queue_num; ++i)
 				if (rss->queue[i] - rss->queue[i - 1] != 1)
 					break;
@@ -827,29 +861,36 @@ mlx4_flow_prepare(struct mlx4_priv *priv,
 					" consecutive queue indices only";
 				goto exit_action_not_supported;
 			}
+			//首个queue id必须为rss->queue_num的整数倍，例如4，5，6，7 相对于4
 			if (rss->queue[0] % rss->queue_num) {
 				msg = "mlx4 requires the first queue of a RSS"
 					" context to be aligned on a multiple"
 					" of the context size";
 				goto exit_action_not_supported;
 			}
+			//队列函数必须指定为“托普利兹矩阵”
 			if (rss->func &&
 			    rss->func != RTE_ETH_HASH_FUNCTION_TOEPLITZ) {
 				msg = "the only supported RSS hash function"
 					" is Toeplitz";
 				goto exit_action_not_supported;
 			}
+			//level必须指定为0
 			if (rss->level) {
 				msg = "a nonzero RSS encapsulation level is"
 					" not supported";
 				goto exit_action_not_supported;
 			}
 			rte_errno = 0;
+			//转换rss配置（dpdk样式）到mlx4 rss配置样式 (实际上没什么大不了的，
+			//就是看hash计算时采用srcaddr,dstaddr,srcport,dstport等）
 			fields = mlx4_conv_rss_types(priv, rss->types, 0);
 			if (fields == (uint64_t)-1 && rte_errno) {
+				//转换失败，报错
 				msg = "unsupported RSS hash type requested";
 				goto exit_action_not_supported;
 			}
+			//创建rss,队列索引数组长度为rss->queue_num
 			flow->rss = mlx4_rss_get
 				(priv, fields, rss_key, rss->queue_num,
 				 rss->queue);
@@ -861,19 +902,23 @@ mlx4_flow_prepare(struct mlx4_priv *priv,
 			}
 			break;
 		default:
+			//其它的action不支持
 			goto exit_action_not_supported;
 		}
 	}
 	/* When fate is unknown, drop traffic. */
 	if (!overlap)
+		//默认处理为drop action {适用于未明确action的配置）
 		flow->drop = 1;
 	/* Validation ends here. */
 	if (!addr) {
+		//无addr,则flow->rss无用
 		if (flow->rss)
 			mlx4_rss_put(flow->rss);
 		return 0;
 	}
 	if (flow == &temp) {
+		//为flow申请空间
 		/* Allocate proper handle based on collected data. */
 		const struct mlx4_malloc_vec vec[] = {
 			{
@@ -888,7 +933,9 @@ mlx4_flow_prepare(struct mlx4_priv *priv,
 			},
 		};
 
+		//为vec的addr申请空间，并重新设置addr（vec在其后没有被使用？）
 		if (!mlx4_zmallocv(__func__, vec, RTE_DIM(vec))) {
+			//空间申请失败
 			if (temp.rss)
 				mlx4_rss_put(temp.rss);
 			return rte_flow_error_set
@@ -1024,6 +1071,7 @@ mlx4_drop_put(struct mlx4_drop *drop)
  * @return
  *   0 on success, a negative errno value otherwise and rte_errno is set.
  */
+//触发流配置
 static int
 mlx4_flow_toggle(struct mlx4_priv *priv,
 		 struct rte_flow *flow,
@@ -1035,8 +1083,11 @@ mlx4_flow_toggle(struct mlx4_priv *priv,
 	int err;
 
 	if (!enable) {
+		//此条flow没有enable时处理
 		if (!flow->ibv_flow)
 			return 0;
+		//claim_zero用于断言参数为0
+		//销毁flow->ibv_flow
 		claim_zero(mlx4_glue->destroy_flow(flow->ibv_flow));
 		flow->ibv_flow = NULL;
 		if (flow->drop)
@@ -1063,6 +1114,7 @@ mlx4_flow_toggle(struct mlx4_priv *priv,
 		       " is reserved when not in isolated mode");
 		goto error;
 	}
+	//此flow入队或者要求rss分散到多个队列
 	if (flow->rss) {
 		struct mlx4_rss *rss = flow->rss;
 		int missing = 0;
@@ -1074,7 +1126,7 @@ mlx4_flow_toggle(struct mlx4_priv *priv,
 			    ETH_DEV(priv)->data->nb_rx_queues ||
 			    !ETH_DEV(priv)->data->rx_queues[rss->queue_id[i]]) {
 				missing = 1;
-				break;
+				break;//对应的队列不存在时，跳出
 			}
 		if (flow->ibv_flow) {
 			if (missing ^ !flow->drop)
@@ -1088,6 +1140,7 @@ mlx4_flow_toggle(struct mlx4_priv *priv,
 				mlx4_rss_detach(rss);
 		}
 		if (!missing) {
+			//所有队列均存在，没有出现missing的情况
 			err = mlx4_rss_attach(rss);
 			if (err) {
 				err = -err;
@@ -1114,6 +1167,7 @@ mlx4_flow_toggle(struct mlx4_priv *priv,
 	assert(qp);
 	if (flow->ibv_flow)
 		return 0;
+	//创建flow
 	flow->ibv_flow = mlx4_glue->create_flow(qp, flow->ibv_attr);
 	if (flow->ibv_flow)
 		return 0;
@@ -1145,9 +1199,11 @@ mlx4_flow_create(struct rte_eth_dev *dev,
 	struct rte_flow *flow;
 	int err;
 
+	//按pattern,action，attr构造flow
 	err = mlx4_flow_prepare(priv, attr, pattern, actions, error, &flow);
 	if (err)
 		return NULL;
+	//如果当前设备已经started,则此条flow认为enable
 	err = mlx4_flow_toggle(priv, flow, priv->started, error);
 	if (!err) {
 		struct rte_flow *curr = LIST_FIRST(&priv->flows);
@@ -1185,8 +1241,10 @@ mlx4_flow_isolate(struct rte_eth_dev *dev,
 	struct mlx4_priv *priv = dev->data->dev_private;
 
 	if (!!enable == !!priv->isolated)
-		return 0;
-	priv->isolated = !!enable;
+		return 0;//已开启或已关闭，直接返回，用于支持重复调用
+	priv->isolated = !!enable;//置为开启或关闭
+
+	//同步流规则
 	if (mlx4_flow_sync(priv, error)) {
 		priv->isolated = !enable;
 		return -rte_errno;
@@ -1539,6 +1597,7 @@ mlx4_flow_sync(struct mlx4_priv *priv, struct rte_flow_error *error)
 		 * Get rid of them in isolated mode, stop at the first
 		 * non-internal rule found.
 		 */
+		//销毁priv->flows链表头上flow->internal为真的flow
 		for (flow = LIST_FIRST(&priv->flows);
 		     flow && flow->internal;
 		     flow = LIST_FIRST(&priv->flows))
@@ -1551,6 +1610,7 @@ mlx4_flow_sync(struct mlx4_priv *priv, struct rte_flow_error *error)
 			return ret;
 	}
 	/* Toggle the remaining flow rules . */
+	//触发流配置
 	LIST_FOREACH(flow, &priv->flows, next) {
 		ret = mlx4_flow_toggle(priv, flow, priv->started, error);
 		if (ret)
@@ -1582,7 +1642,7 @@ mlx4_flow_clean(struct mlx4_priv *priv)
 
 static const struct rte_flow_ops mlx4_flow_ops = {
 	.validate = mlx4_flow_validate,
-	.create = mlx4_flow_create,
+	.create = mlx4_flow_create,//走mlx4的flow创建流程
 	.destroy = mlx4_flow_destroy,
 	.flush = mlx4_flow_flush,
 	.isolate = mlx4_flow_isolate,
@@ -1613,6 +1673,7 @@ mlx4_filter_ctrl(struct rte_eth_dev *dev,
 	case RTE_ETH_FILTER_GENERIC:
 		if (filter_op != RTE_ETH_FILTER_GET)
 			break;
+		//返回flow的操作函数集
 		*(const void **)arg = &mlx4_flow_ops;
 		return 0;
 	default:
diff --git a/drivers/net/mlx4/mlx4_flow.h b/drivers/net/mlx4/mlx4_flow.h
index 26465c66a..7bfa17ee5 100644
--- a/drivers/net/mlx4/mlx4_flow.h
+++ b/drivers/net/mlx4/mlx4_flow.h
@@ -38,8 +38,11 @@ struct rte_flow {
 	uint32_t select:1; /**< Used by operations on the linked list. */
 	uint32_t internal:1; /**< Internal flow rule outside isolated mode. */
 	uint32_t mac:1; /**< Rule associated with a configured MAC address. */
+	//匹配所有流
 	uint32_t promisc:1; /**< This rule matches everything. */
+	//匹配所有组播流
 	uint32_t allmulti:1; /**< This rule matches all multicast traffic. */
+	//指出action要求drop报文
 	uint32_t drop:1; /**< This rule drops packets. */
 	uint32_t priority; /**< Flow rule priority. */
 	struct mlx4_rss *rss; /**< Rx target. */
diff --git a/drivers/net/mlx4/mlx4_rxq.c b/drivers/net/mlx4/mlx4_rxq.c
index f45c1ff85..d5e88991f 100644
--- a/drivers/net/mlx4/mlx4_rxq.c
+++ b/drivers/net/mlx4/mlx4_rxq.c
@@ -50,6 +50,14 @@
  *
  * Note: this is not const to work around API quirks.
  */
+//82599中datasheet中给出的rss hashcode为
+/**
+ * 0x6d, 0x5a, 0x56, 0xda, 0x25, 0x5b, 0x0e, 0xc2,
+   0x41, 0x67, 0x25, 0x3d, 0x43, 0xa3, 0x8f, 0xb0,
+   0xd0, 0xca, 0x2b, 0xcb, 0xae, 0x7b, 0x30, 0xb4,
+   0x77, 0xcb, 0x2d, 0xa3, 0x80, 0x30, 0xf2, 0x0c,
+   0x6a, 0x42, 0xb7, 0x3b, 0xbe, 0xac, 0x01, 0xfa
+ */
 uint8_t
 mlx4_rss_hash_key_default[MLX4_RSS_HASH_KEY_SIZE] = {
 	0x2c, 0xc6, 0x81, 0xd1,
@@ -86,6 +94,7 @@ mlx4_rss_hash_key_default[MLX4_RSS_HASH_KEY_SIZE] = {
  * @return
  *   Pointer to RSS context on success, NULL otherwise and rte_errno is set.
  */
+//尝试引用rss配置，如果与已有的rss配置相同则直接引用，否则创建之
 struct mlx4_rss *
 mlx4_rss_get(struct mlx4_priv *priv, uint64_t fields,
 	     const uint8_t key[MLX4_RSS_HASH_KEY_SIZE],
@@ -100,12 +109,14 @@ mlx4_rss_get(struct mlx4_priv *priv, uint64_t fields,
 		    !memcmp(key, rss->key, MLX4_RSS_HASH_KEY_SIZE) &&
 		    !memcmp(queue_id, rss->queue_id, queue_id_size)) {
 			++rss->refcnt;
-			return rss;
+			return rss;//复用已有的rss
 		}
+	//新建rss配置，并将其串在priv->ss链表上
 	rss = rte_malloc(__func__, offsetof(struct mlx4_rss, queue_id) +
 			 queue_id_size, 0);
 	if (!rss)
 		goto error;
+	//填充rss
 	*rss = (struct mlx4_rss){
 		.priv = priv,
 		.refcnt = 1,
@@ -137,6 +148,7 @@ mlx4_rss_get(struct mlx4_priv *priv, uint64_t fields,
  * @param rss
  *   RSS context to release.
  */
+//减少引用计数，如果引用计数减为0，则释放
 void
 mlx4_rss_put(struct mlx4_rss *rss)
 {
@@ -164,6 +176,7 @@ mlx4_rss_put(struct mlx4_rss *rss)
  * @return
  *   0 on success, a negative errno value otherwise and rte_errno is set.
  */
+//调用mlx4_glue实现rss分流（具体细节无法观看，调用了相应库）
 int
 mlx4_rss_attach(struct mlx4_rss *rss)
 {
diff --git a/drivers/net/mlx4/mlx4_rxtx.h b/drivers/net/mlx4/mlx4_rxtx.h
index 8baf33fa9..11e08b0fa 100644
--- a/drivers/net/mlx4/mlx4_rxtx.h
+++ b/drivers/net/mlx4/mlx4_rxtx.h
@@ -70,9 +70,13 @@ struct mlx4_rss {
 	uint32_t usecnt; /**< Number of users relying on @p qp and @p ind. */
 	struct ibv_qp *qp; /**< Queue pair. */
 	struct ibv_rwq_ind_table *ind; /**< Indirection table. */
+	//rss处理字段样式
 	uint64_t fields; /**< Fields for RSS processing (Verbs format). */
+	//hash计算时采用的key
 	uint8_t key[MLX4_RSS_HASH_KEY_SIZE]; /**< Hash key to use. */
+	//队列id数组长度
 	uint16_t queues; /**< Number of target queues. */
+	//队列id数组
 	uint16_t queue_id[]; /**< Target queues. */
 };
 
diff --git a/drivers/net/mlx4/mlx4_utils.c b/drivers/net/mlx4/mlx4_utils.c
index a727d7036..d536befe5 100644
--- a/drivers/net/mlx4/mlx4_utils.c
+++ b/drivers/net/mlx4/mlx4_utils.c
@@ -50,6 +50,7 @@ mlx4_fd_set_non_blocking(int fd)
  * C11 code would include stdalign.h and use alignof(max_align_t) however
  * we'll stick with C99 for the time being.
  */
+//为vec申请内存，并赋值（ps.这个函数与其上层函数写的一样屎）
 static inline size_t
 mlx4_mallocv_inline(const char *type, const struct mlx4_malloc_vec *vec,
 		    unsigned int cnt, int zero, int socket)
@@ -67,20 +68,26 @@ mlx4_mallocv_inline(const char *type, const struct mlx4_malloc_vec *vec,
 		size_t align = (uintptr_t)vec[i].align;
 
 		if (!align) {
+			//未指明对齐时，对齐以sizeof(double）定义
 			align = sizeof(double);
 		} else if (!rte_is_power_of_2(align)) {
+			//指定对齐时，校验对齐是否为2的整数次方
 			rte_errno = EINVAL;
 			goto error;
 		}
 		if (least < align)
 			least = align;
+		//使size对齐
 		align = RTE_ALIGN_CEIL(size, align);
 		size = align + vec[i].size;
 		if (fill && vec[i].addr)
+			//当fill为1时，为每个vec[i]填充addr
 			*vec[i].addr = data + align;
 	}
 	if (fill)
+		//申请成功时，自此返回
 		return size;
+	//申请大页内存
 	if (!zero)
 		data = rte_malloc_socket(type, size, least, socket);
 	else
diff --git a/drivers/net/mlx5/mlx5.h b/drivers/net/mlx5/mlx5.h
index b6a51b2b4..48669d6c4 100644
--- a/drivers/net/mlx5/mlx5.h
+++ b/drivers/net/mlx5/mlx5.h
@@ -640,7 +640,7 @@ struct mlx5_priv {
 	unsigned int (*reta_idx)[]; /* RETA index table. */
 	unsigned int reta_idx_n; /* RETA index size. */
 	struct mlx5_drop drop_queue; /* Flow drop queues. */
-	struct mlx5_flows flows; /* RTE Flow rules. */
+	struct mlx5_flows flows; /* RTE Flow rules. *///记录转换后的规则
 	struct mlx5_flows ctrl_flows; /* Control flow rules. */
 	LIST_HEAD(rxq, mlx5_rxq_ctrl) rxqsctrl; /* DPDK Rx queues. */
 	LIST_HEAD(rxqobj, mlx5_rxq_obj) rxqsobj; /* Verbs/DevX Rx queues. */
diff --git a/drivers/net/mlx5/mlx5_ethdev.c b/drivers/net/mlx5/mlx5_ethdev.c
index 2278b24c0..d8bc8128e 100644
--- a/drivers/net/mlx5/mlx5_ethdev.c
+++ b/drivers/net/mlx5/mlx5_ethdev.c
@@ -414,6 +414,7 @@ mlx5_dev_configure(struct rte_eth_dev *dev)
 	       MLX5_RSS_HASH_KEY_LEN);
 	priv->rss_conf.rss_key_len = MLX5_RSS_HASH_KEY_LEN;
 	priv->rss_conf.rss_hf = dev->data->dev_conf.rx_adv_conf.rss_conf.rss_hf;
+	//setup rx/tx队列时，这些数据将被填充
 	priv->rxqs = (void *)dev->data->rx_queues;
 	priv->txqs = (void *)dev->data->tx_queues;
 	if (txqs_n != priv->txqs_n) {
diff --git a/drivers/net/mlx5/mlx5_flow.c b/drivers/net/mlx5/mlx5_flow.c
index d4d956f2e..521f260fe 100644
--- a/drivers/net/mlx5/mlx5_flow.c
+++ b/drivers/net/mlx5/mlx5_flow.c
@@ -231,7 +231,7 @@ static const struct rte_flow_expand_node mlx5_support_expansion[] = {
 
 static const struct rte_flow_ops mlx5_flow_ops = {
 	.validate = mlx5_flow_validate,
-	.create = mlx5_flow_create,
+	.create = mlx5_flow_create,//flow规则创建
 	.destroy = mlx5_flow_destroy,
 	.flush = mlx5_flow_flush,
 	.isolate = mlx5_flow_isolate,
@@ -2723,6 +2723,7 @@ mlx5_flow_stop(struct rte_eth_dev *dev, struct mlx5_flows *list)
  * @return
  *   0 on success, a negative errno value otherwise and rte_errno is set.
  */
+//将规则下发给硬件
 int
 mlx5_flow_start(struct rte_eth_dev *dev, struct mlx5_flows *list)
 {
@@ -3447,6 +3448,7 @@ mlx5_dev_filter_ctrl(struct rte_eth_dev *dev,
 			rte_errno = EINVAL;
 			return -rte_errno;
 		}
+		//返回mlx5的flow_ops
 		*(const void **)arg = &mlx5_flow_ops;
 		return 0;
 	case RTE_ETH_FILTER_FDIR:
diff --git a/drivers/net/mlx5/mlx5_rxtx.h b/drivers/net/mlx5/mlx5_rxtx.h
index 4bb28a485..c6726e21e 100644
--- a/drivers/net/mlx5/mlx5_rxtx.h
+++ b/drivers/net/mlx5/mlx5_rxtx.h
@@ -249,6 +249,7 @@ struct mlx5_txq_local {
 /* TX queue descriptor. */
 __extension__
 struct mlx5_txq_data {
+	//当前可存入报文的指针位置
 	uint16_t elts_head; /* Current counter in (*elts)[]. */
 	uint16_t elts_tail; /* Counter of first element awaiting completion. */
 	uint16_t elts_comp; /* elts index since last completion request. */
diff --git a/drivers/net/mlx5/mlx5_trigger.c b/drivers/net/mlx5/mlx5_trigger.c
index 122f31c55..ee4347e3d 100644
--- a/drivers/net/mlx5/mlx5_trigger.c
+++ b/drivers/net/mlx5/mlx5_trigger.c
@@ -193,6 +193,7 @@ mlx5_dev_start(struct rte_eth_dev *dev)
 			dev->data->port_id);
 		goto error;
 	}
+	//设备启动时设置flow规则
 	ret = mlx5_flow_start(dev, &priv->flows);
 	if (ret) {
 		DRV_LOG(DEBUG, "port %u failed to set flows",
diff --git a/drivers/net/mlx5/mlx5_txq.c b/drivers/net/mlx5/mlx5_txq.c
index 53d45e744..62b24a7ca 100644
--- a/drivers/net/mlx5/mlx5_txq.c
+++ b/drivers/net/mlx5/mlx5_txq.c
@@ -199,6 +199,7 @@ mlx5_tx_queue_setup(struct rte_eth_dev *dev, uint16_t idx, uint16_t desc,
 	}
 	DRV_LOG(DEBUG, "port %u adding Tx queue %u to list",
 		dev->data->port_id, idx);
+	//登记idx号tx队列
 	(*priv->txqs)[idx] = &txq_ctrl->txq;
 	return 0;
 }
@@ -1058,6 +1059,7 @@ struct mlx5_txq_ctrl *
 mlx5_txq_new(struct rte_eth_dev *dev, uint16_t idx, uint16_t desc,
 	     unsigned int socket, const struct rte_eth_txconf *conf)
 {
+	//申请txq队列
 	struct mlx5_priv *priv = dev->data->dev_private;
 	struct mlx5_txq_ctrl *tmpl;
 
diff --git a/drivers/net/nfp/nfp_net.c b/drivers/net/nfp/nfp_net.c
index 22a8b2d19..d6f771b51 100644
--- a/drivers/net/nfp/nfp_net.c
+++ b/drivers/net/nfp/nfp_net.c
@@ -127,6 +127,7 @@ static int nfp_set_mac_addr(struct rte_eth_dev *dev,
 /* Maximum value which can be added to a queue with one transaction */
 #define NFP_QCP_MAX_ADD	0x7f
 
+//从mbuf物理地址向后移动一个headroom,来进行填充
 #define RTE_MBUF_DMA_ADDR_DEFAULT(mb) \
 	(uint64_t)((mb)->buf_iova + RTE_PKTMBUF_HEADROOM)
 
@@ -150,15 +151,17 @@ nfp_qcp_ptr_add(uint8_t *q, enum nfp_qcp_ptr ptr, uint32_t val)
 	uint32_t off;
 
 	if (ptr == NFP_QCP_READ_PTR)
-		off = NFP_QCP_QUEUE_ADD_RPTR;
+		off = NFP_QCP_QUEUE_ADD_RPTR;//读位置q+0
 	else
-		off = NFP_QCP_QUEUE_ADD_WPTR;
+		off = NFP_QCP_QUEUE_ADD_WPTR;//写位置q+1
 
+	//当值>255时，向q+off内一次写一个255
 	while (val > NFP_QCP_MAX_ADD) {
 		nn_writel(rte_cpu_to_le_32(NFP_QCP_MAX_ADD), q + off);
 		val -= NFP_QCP_MAX_ADD;
 	}
 
+	//向q+off位置，写入值val
 	nn_writel(rte_cpu_to_le_32(val), q + off);
 }
 
@@ -1732,6 +1735,7 @@ nfp_net_tx_queue_setup(struct rte_eth_dev *dev, uint16_t queue_idx,
 	 * handle the maximum ring size is allocated in order to allow for
 	 * resizing in later calls to the queue setup function.
 	 */
+	//申请硬件描述符用内存
 	tz = rte_eth_dma_zone_reserve(dev, "tx_ring", queue_idx,
 				   sizeof(struct nfp_net_tx_desc) *
 				   NFP_NET_MAX_TX_DESC, NFP_MEMZONE_ALIGN,
@@ -1751,13 +1755,14 @@ nfp_net_tx_queue_setup(struct rte_eth_dev *dev, uint16_t queue_idx,
 	/* queue mapping based on firmware configuration */
 	txq->qidx = queue_idx;
 	txq->tx_qcidx = queue_idx * hw->stride_tx;
+	//此队列对应的信息
 	txq->qcp_q = hw->tx_bar + NFP_QCP_QUEUE_OFF(txq->tx_qcidx);
 
 	txq->port_id = dev->data->port_id;
 
 	/* Saving physical and virtual addresses for the TX ring */
 	txq->dma = (uint64_t)tz->iova;
-	txq->txds = (struct nfp_net_tx_desc *)tz->addr;
+	txq->txds = (struct nfp_net_tx_desc *)tz->addr;//设置描述符地址
 
 	/* mbuf pointers array for referencing mbufs linked to TX descriptors */
 	txq->txbufs = rte_zmalloc_socket("txq->txbufs",
@@ -1798,14 +1803,16 @@ nfp_net_tx_tso(struct nfp_net_txq *txq, struct nfp_net_tx_desc *txd,
 
 	ol_flags = mb->ol_flags;
 
+	//没有开启tso时
 	if (!(ol_flags & PKT_TX_TCP_SEG))
 		goto clean_txd;
 
 	txd->l3_offset = mb->l2_len;
+	//指明l4层的偏移移
 	txd->l4_offset = mb->l2_len + mb->l3_len;
 	txd->lso_hdrlen = mb->l2_len + mb->l3_len + mb->l4_len;
-	txd->mss = rte_cpu_to_le_16(mb->tso_segsz);
-	txd->flags = PCIE_DESC_TX_LSO;
+	txd->mss = rte_cpu_to_le_16(mb->tso_segsz);//指明段的大小
+	txd->flags = PCIE_DESC_TX_LSO;//指明需要执行tso
 	return;
 
 clean_txd:
@@ -1824,12 +1831,14 @@ nfp_net_tx_cksum(struct nfp_net_txq *txq, struct nfp_net_tx_desc *txd,
 	uint64_t ol_flags;
 	struct nfp_net_hw *hw = txq->hw;
 
+	//硬件不支持checksum offload
 	if (!(hw->cap & NFP_NET_CFG_CTRL_TXCSUM))
 		return;
 
 	ol_flags = mb->ol_flags;
 
 	/* IPv6 does not need checksum */
+	//标记需要计算checksum,标记offload哪种checksum
 	if (ol_flags & PKT_TX_IP_CKSUM)
 		txd->flags |= PCIE_DESC_TX_IP4_CSUM;
 
@@ -1854,7 +1863,7 @@ nfp_net_rx_cksum(struct nfp_net_rxq *rxq, struct nfp_net_rx_desc *rxd,
 	struct nfp_net_hw *hw = rxq->hw;
 
 	if (!(hw->ctrl & NFP_NET_CFG_CTRL_RXCSUM))
-		return;
+		return;//未开启，不处理
 
 	/* If IPv4 and IP checksum error, fail */
 	if (unlikely((rxd->rxd.flags & PCIE_DESC_RX_IP4_CSUM) &&
@@ -1895,6 +1904,7 @@ nfp_net_set_hash(struct nfp_net_rxq *rxq, struct nfp_net_rx_desc *rxd,
 	uint32_t hash = 0;
 	uint32_t hash_type = 0;
 
+	//如果未开启rss，则不处理
 	if (!(hw->ctrl & NFP_NET_CFG_CTRL_RSS))
 		return;
 
@@ -1941,6 +1951,7 @@ nfp_net_set_hash(struct nfp_net_rxq *rxq, struct nfp_net_rx_desc *rxd,
 		hash_type = rte_be_to_cpu_32(*(uint32_t *)NFP_HASH_TYPE_OFFSET);
 	}
 
+	//填充mbuf中关于hash的字段
 	mbuf->hash.rss = hash;
 	mbuf->ol_flags |= PKT_RX_RSS_HASH;
 
@@ -2034,6 +2045,7 @@ nfp_net_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
 	avail = 0;
 	nb_hold = 0;
 
+	//需要接收nb_pkts个，已收取了avail个
 	while (avail < nb_pkts) {
 		rxb = &rxq->rxbufs[rxq->rd_p];
 		if (unlikely(rxb == NULL)) {
@@ -2055,6 +2067,7 @@ nfp_net_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
 		 * We got a packet. Let's alloc a new mbuf for refilling the
 		 * free descriptor ring as soon as possible
 		 */
+		//申请一块新的buf,准备交换给rxbufs
 		new_mb = rte_pktmbuf_alloc(rxq->mem_pool);
 		if (unlikely(new_mb == NULL)) {
 			RTE_LOG_DP(DEBUG, PMD,
@@ -2070,17 +2083,20 @@ nfp_net_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
 		 * Grab the mbuf and refill the descriptor with the
 		 * previously allocated mbuf
 		 */
+		//完成报文交换
 		mb = rxb->mbuf;
 		rxb->mbuf = new_mb;
 
 		PMD_RX_LOG(DEBUG, "Packet len: %u, mbuf_size: %u",
 			   rxds->rxd.data_len, rxq->mbuf_size);
 
+		//netronome当前仅支持收取一个segment,故md->data_len与pkt_len相同
 		/* Size of this segment */
 		mb->data_len = rxds->rxd.data_len - NFP_DESC_META_LEN(rxds);
 		/* Size of the whole packet. We just support 1 segment */
 		mb->pkt_len = rxds->rxd.data_len - NFP_DESC_META_LEN(rxds);
 
+		//防止用户设置mbuf_size过小
 		if (unlikely((mb->data_len + hw->rx_offset) >
 			     rxq->mbuf_size)) {
 			/*
@@ -2101,6 +2117,7 @@ nfp_net_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
 		}
 
 		/* Filling the received mbuf with packet info */
+		//设置data_off,用于提供一些meta信息
 		if (hw->rx_offset)
 			mb->data_off = RTE_PKTMBUF_HEADROOM + hw->rx_offset;
 		else
@@ -2119,6 +2136,7 @@ nfp_net_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
 		/* Checking the checksum flag */
 		nfp_net_rx_cksum(rxq, rxds, mb);
 
+		//剥vlan
 		if ((rxds->rxd.flags & PCIE_DESC_RX_VLAN) &&
 		    (hw->ctrl & NFP_NET_CFG_CTRL_RXVLAN)) {
 			mb->vlan_tci = rte_cpu_to_le_32(rxds->rxd.vlan);
@@ -2126,6 +2144,7 @@ nfp_net_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
 		}
 
 		/* Adding the mbuf to the mbuf array passed by the app */
+		//将收到的报文放在接受缓冲中
 		rx_pkts[avail++] = mb;
 
 		/* Now resetting and updating the descriptor */
@@ -2133,14 +2152,17 @@ nfp_net_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
 		rxds->vals[1] = 0;
 		dma_addr = rte_cpu_to_le_64(RTE_MBUF_DMA_ADDR_DEFAULT(new_mb));
 		rxds->fld.dd = 0;
+		//填写交换的新mbuf的dma地址
 		rxds->fld.dma_addr_hi = (dma_addr >> 32) & 0xff;
 		rxds->fld.dma_addr_lo = dma_addr & 0xffffffff;
 
+		//更新rd_p，使之指向下一个报文
 		rxq->rd_p++;
 		if (unlikely(rxq->rd_p == rxq->rx_count)) /* wrapping?*/
 			rxq->rd_p = 0;
 	}
 
+	//没有收到报文，不需要更新硬件，直接退
 	if (nb_hold == 0)
 		return nb_hold;
 
@@ -2154,6 +2176,7 @@ nfp_net_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
 	 * FL queue WR pointer
 	 */
 	rte_wmb();
+	//如果未通知硬件mbuf已读取过多，则知会硬件，有nb_hold个mbuf已被读取
 	if (nb_hold > rxq->rx_free_thresh) {
 		PMD_RX_LOG(DEBUG, "port=%u queue=%u nb_hold=%u avail=%u",
 			   rxq->port_id, (unsigned int)rxq->qidx,
@@ -2161,7 +2184,7 @@ nfp_net_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
 		nfp_qcp_ptr_add(rxq->qcp_fl, NFP_QCP_WRITE_PTR, nb_hold);
 		nb_hold = 0;
 	}
-	rxq->nb_rx_hold = nb_hold;
+	rxq->nb_rx_hold = nb_hold;//更新当前有多少mbuf已收取，但未知会硬件
 
 	return avail;
 }
@@ -2238,7 +2261,7 @@ nfp_net_xmit_pkts(void *tx_queue, struct rte_mbuf **tx_pkts, uint16_t nb_pkts)
 {
 	struct nfp_net_txq *txq;
 	struct nfp_net_hw *hw;
-	struct nfp_net_tx_desc *txds, txd;
+	struct nfp_net_tx_desc *txds, txd;//发送用的描述符
 	struct rte_mbuf *pkt;
 	uint64_t dma_addr;
 	int pkt_size, dma_size;
@@ -2253,6 +2276,7 @@ nfp_net_xmit_pkts(void *tx_queue, struct rte_mbuf **tx_pkts, uint16_t nb_pkts)
 	PMD_TX_LOG(DEBUG, "working for queue %u at pos %d and %u packets",
 		   txq->qidx, txq->wr_p, nb_pkts);
 
+	//如果txq满或者不足以发送（应是强制发送，未看进去，需要关注）
 	if ((nfp_free_tx_desc(txq) < nb_pkts) || (nfp_net_txq_full(txq)))
 		nfp_net_tx_free_bufs(txq);
 
@@ -2273,6 +2297,7 @@ nfp_net_xmit_pkts(void *tx_queue, struct rte_mbuf **tx_pkts, uint16_t nb_pkts)
 		/* Warming the cache for releasing the mbuf later on */
 		RTE_MBUF_PREFETCH_TO_FREE(*lmbuf);
 
+		//取出需要发送的第i个报文
 		pkt = *(tx_pkts + i);
 
 		if (unlikely((pkt->nb_segs > 1) &&
@@ -2306,6 +2331,8 @@ nfp_net_xmit_pkts(void *tx_queue, struct rte_mbuf **tx_pkts, uint16_t nb_pkts)
 		 */
 		pkt_size = pkt->pkt_len;
 
+		//针对单个报文的发送，由于单个报文可能有多个segment,故采用循环进行处理，一个
+		//报文对应一个txd
 		while (pkt) {
 			/* Copying TSO, VLAN and cksum info */
 			*txds = txd;
@@ -2331,11 +2358,11 @@ nfp_net_xmit_pkts(void *tx_queue, struct rte_mbuf **tx_pkts, uint16_t nb_pkts)
 			txds->dma_addr_hi = (dma_addr >> 32) & 0xff;
 			txds->dma_addr_lo = (dma_addr & 0xffffffff);
 			ASSERT(free_descs > 0);
-			free_descs--;
+			free_descs--;//少了一个描述符
 
 			txq->wr_p++;
 			if (unlikely(txq->wr_p == txq->tx_count)) /* wrapping?*/
-				txq->wr_p = 0;
+				txq->wr_p = 0;//如果绕圈了，则回归到0
 
 			pkt_size -= dma_size;
 
@@ -2344,12 +2371,14 @@ nfp_net_xmit_pkts(void *tx_queue, struct rte_mbuf **tx_pkts, uint16_t nb_pkts)
 			 * the priority
 			 */
 			if (likely(!pkt_size))
+				//一个报文发送完成了
 				txds->offset_eop = PCIE_DESC_TX_EOP;
 			else
 				txds->offset_eop = 0;
 
 			pkt = pkt->next;
 			/* Referencing next free TX descriptor */
+			//取一个新的txds,取一个空闲的空间
 			txds = &txq->txds[txq->wr_p];
 			lmbuf = &txq->txbufs[txq->wr_p].mbuf;
 			issued_descs++;
@@ -2360,7 +2389,7 @@ nfp_net_xmit_pkts(void *tx_queue, struct rte_mbuf **tx_pkts, uint16_t nb_pkts)
 xmit_end:
 	/* Increment write pointers. Force memory write before we let HW know */
 	rte_wmb();
-	nfp_qcp_ptr_add(txq->qcp_q, NFP_QCP_WRITE_PTR, issued_descs);
+	nfp_qcp_ptr_add(txq->qcp_q, NFP_QCP_WRITE_PTR, issued_descs);//知会硬件用了写多少个描述符
 
 	return i;
 }
diff --git a/drivers/net/nfp/nfp_net_ctrl.h b/drivers/net/nfp/nfp_net_ctrl.h
index fc3540a2e..7c8a8e4cb 100644
--- a/drivers/net/nfp/nfp_net_ctrl.h
+++ b/drivers/net/nfp/nfp_net_ctrl.h
@@ -99,21 +99,21 @@
  */
 #define NFP_NET_CFG_CTRL                0x0000
 #define   NFP_NET_CFG_CTRL_ENABLE         (0x1 <<  0) /* Global enable */
-#define   NFP_NET_CFG_CTRL_PROMISC        (0x1 <<  1) /* Enable Promisc mode */
+#define   NFP_NET_CFG_CTRL_PROMISC        (0x1 <<  1) /* Enable Promisc mode */ //开启了混杂模式
 #define   NFP_NET_CFG_CTRL_L2BC           (0x1 <<  2) /* Allow L2 Broadcast */
 #define   NFP_NET_CFG_CTRL_L2MC           (0x1 <<  3) /* Allow L2 Multicast */
-#define   NFP_NET_CFG_CTRL_RXCSUM         (0x1 <<  4) /* Enable RX Checksum */
-#define   NFP_NET_CFG_CTRL_TXCSUM         (0x1 <<  5) /* Enable TX Checksum */
-#define   NFP_NET_CFG_CTRL_RXVLAN         (0x1 <<  6) /* Enable VLAN strip */
+#define   NFP_NET_CFG_CTRL_RXCSUM         (0x1 <<  4) /* Enable RX Checksum */  //开启rx checksum
+#define   NFP_NET_CFG_CTRL_TXCSUM         (0x1 <<  5) /* Enable TX Checksum */  //开启tx checksum
+#define   NFP_NET_CFG_CTRL_RXVLAN         (0x1 <<  6) /* Enable VLAN strip */   //开启vlan剥离
 #define   NFP_NET_CFG_CTRL_TXVLAN         (0x1 <<  7) /* Enable VLAN insert */
 #define   NFP_NET_CFG_CTRL_SCATTER        (0x1 <<  8) /* Scatter DMA */
-#define   NFP_NET_CFG_CTRL_GATHER         (0x1 <<  9) /* Gather DMA */
-#define   NFP_NET_CFG_CTRL_LSO            (0x1 << 10) /* LSO/TSO */
+#define   NFP_NET_CFG_CTRL_GATHER         (0x1 <<  9) /* Gather DMA */          //DMA的模式？
+#define   NFP_NET_CFG_CTRL_LSO            (0x1 << 10) /* LSO/TSO */             //支持lso
 #define   NFP_NET_CFG_CTRL_RINGCFG        (0x1 << 16) /* Ring runtime changes */
-#define   NFP_NET_CFG_CTRL_RSS            (0x1 << 17) /* RSS */
-#define   NFP_NET_CFG_CTRL_IRQMOD         (0x1 << 18) /* Interrupt moderation */
+#define   NFP_NET_CFG_CTRL_RSS            (0x1 << 17) /* RSS */  //支持rss哈希
+#define   NFP_NET_CFG_CTRL_IRQMOD         (0x1 << 18) /* Interrupt moderation */  //中断调整？
 #define   NFP_NET_CFG_CTRL_RINGPRIO       (0x1 << 19) /* Ring priorities */
-#define   NFP_NET_CFG_CTRL_MSIXAUTO       (0x1 << 20) /* MSI-X auto-masking */
+#define   NFP_NET_CFG_CTRL_MSIXAUTO       (0x1 << 20) /* MSI-X auto-masking */    //这个是什么？
 #define   NFP_NET_CFG_CTRL_TXRWB          (0x1 << 21) /* Write-back of TX ring*/
 #define   NFP_NET_CFG_CTRL_L2SWITCH       (0x1 << 22) /* L2 Switch */
 #define   NFP_NET_CFG_CTRL_L2SWITCH_LOCAL (0x1 << 23) /* Switch to local */
diff --git a/drivers/net/nfp/nfp_net_pmd.h b/drivers/net/nfp/nfp_net_pmd.h
index bc288bed6..26f60bb63 100644
--- a/drivers/net/nfp/nfp_net_pmd.h
+++ b/drivers/net/nfp/nfp_net_pmd.h
@@ -185,12 +185,13 @@ static inline void nn_writeq(uint64_t val, volatile void *addr)
 struct nfp_net_tx_desc {
 	union {
 		struct {
-			uint8_t dma_addr_hi; /* High bits of host buf address */
-			__le16 dma_len;     /* Length to DMA for this desc */
+			uint8_t dma_addr_hi; /* High bits of host buf address *///高位8位地址,指出mbuf地址
+			__le16 dma_len;     /* Length to DMA for this desc *///dma的长度
+			//标记一个报文是否已发送完成
 			uint8_t offset_eop; /* Offset in buf where pkt starts +
 					     * highest bit is eop flag.
 					     */
-			__le32 dma_addr_lo; /* Low 32bit of host buf addr */
+			__le32 dma_addr_lo; /* Low 32bit of host buf addr */ //低为32位地址
 
 			__le16 mss;         /* MSS to be used for LSO */
 			uint8_t lso_hdrlen; /* LSO, where the data starts */
@@ -208,7 +209,7 @@ struct nfp_net_tx_desc {
 				__le16 vlan; /* VLAN tag to add if indicated */
 			};
 			__le16 data_len;    /* Length of frame + meta data */
-		} __attribute__((__packed__));
+		} __attribute__((__packed__));//指明结构体不能有空隙（dma_addr_hi与dma_len之间等，均有空隙）
 		__le32 vals[4];
 	};
 };
@@ -224,7 +225,7 @@ struct nfp_net_txq {
 	 * descriptors. @qcp_q is a pointer to the base of the queue
 	 * structure on the NFP
 	 */
-	uint8_t *qcp_q;
+	uint8_t *qcp_q;//指向硬件的数据结构（队列的信息）
 
 	/*
 	 * Read and Write pointers.  @wr_p and @rd_p are host side pointer,
@@ -266,7 +267,7 @@ struct nfp_net_txq {
 	uint32_t tx_hthresh;   /* not used by now. Future? */
 	uint32_t tx_wthresh;   /* not used by now. Future? */
 	uint16_t port_id;
-	int qidx;
+	int qidx;//队列id
 	int tx_qcidx;
 	__le64 dma;
 } __attribute__ ((__aligned__(64)));
@@ -342,7 +343,7 @@ struct nfp_net_rxq {
 	 * freelist descriptors and @rd_p is where the driver start
 	 * reading descriptors for newly arrive packets from.
 	 */
-	uint32_t rd_p;
+	uint32_t rd_p;//收包的指针
 
 	/*
 	 * For each buffer placed on the freelist, record the
@@ -370,7 +371,7 @@ struct nfp_net_rxq {
 	 * to the NFP
 	 */
 	uint16_t rx_free_thresh;
-	uint16_t nb_rx_hold;
+	uint16_t nb_rx_hold;//记录有多少个mbuf已收取，但还没有通知硬件
 
 	 /* the size of the queue in number of descriptors */
 	uint16_t rx_count;
@@ -405,7 +406,7 @@ struct nfp_net_rxq {
 struct nfp_net_hw {
 	/* Info from the firmware */
 	uint32_t ver;
-	uint32_t cap;
+	uint32_t cap;//硬件能力字段
 	uint32_t max_mtu;
 	uint32_t mtu;
 	uint32_t rx_offset;
diff --git a/drivers/net/ring/rte_eth_ring.c b/drivers/net/ring/rte_eth_ring.c
index 41acbc513..2e6af585a 100644
--- a/drivers/net/ring/rte_eth_ring.c
+++ b/drivers/net/ring/rte_eth_ring.c
@@ -67,6 +67,7 @@ static int eth_ring_logtype;
 	rte_log(RTE_LOG_ ## level, eth_ring_logtype, \
 		"%s(): " fmt "\n", __func__, ##args)
 
+//ring设备收包
 static uint16_t
 eth_ring_rx(void *q, struct rte_mbuf **bufs, uint16_t nb_bufs)
 {
@@ -81,6 +82,7 @@ eth_ring_rx(void *q, struct rte_mbuf **bufs, uint16_t nb_bufs)
 	return nb_rx;
 }
 
+//ring设备发包
 static uint16_t
 eth_ring_tx(void *q, struct rte_mbuf **bufs, uint16_t nb_bufs)
 {
diff --git a/drivers/net/vhost/rte_eth_vhost.c b/drivers/net/vhost/rte_eth_vhost.c
index 46f01a7f4..dd18d4a4a 100644
--- a/drivers/net/vhost/rte_eth_vhost.c
+++ b/drivers/net/vhost/rte_eth_vhost.c
@@ -23,6 +23,10 @@ static int vhost_logtype;
 #define VHOST_LOG(level, ...) \
 	rte_log(RTE_LOG_ ## level, vhost_logtype, __VA_ARGS__)
 
+//队列分两种类型（收队列，发队列）故给定一个qid
+//如果它是收，则qid*VIRTIO_QNUM + VIRTIO_RXQ
+//如果它是发，则qid*VIRTIO_QNUM + VIRTIO_TXO
+//这样向上就封装成无论收或者发都有一个0号队列（并各自计数）
 enum {VIRTIO_RXQ, VIRTIO_TXQ, VIRTIO_QNUM};
 
 #define ETH_VHOST_IFACE_ARG		"iface"
@@ -34,6 +38,7 @@ enum {VIRTIO_RXQ, VIRTIO_TXQ, VIRTIO_QNUM};
 #define ETH_VHOST_VIRTIO_NET_F_HOST_TSO "tso"
 #define VHOST_MAX_PKT_BURST 32
 
+//vhost驱动支持的参数
 static const char *valid_arguments[] = {
 	ETH_VHOST_IFACE_ARG,
 	ETH_VHOST_QUEUES_ARG,
@@ -83,24 +88,24 @@ struct vhost_stats {
 };
 
 struct vhost_queue {
-	int vid;
-	rte_atomic32_t allow_queuing;
-	rte_atomic32_t while_queuing;
+	int vid;//virtio_net设备编号
+	rte_atomic32_t allow_queuing;//此队列时否容许收发包
+	rte_atomic32_t while_queuing;//此标记被加上时，表示正在队列操作，不容许更改allow_queuing
 	struct pmd_internal *internal;
-	struct rte_mempool *mb_pool;
+	struct rte_mempool *mb_pool;//vq使用那个pool上的mbuf（用于为ring补充mbuf)
 	uint16_t port;
-	uint16_t virtqueue_id;
+	uint16_t virtqueue_id;//vhost_queue对应的virtqueue编号
 	struct vhost_stats stats;
 };
 
 struct pmd_internal {
 	rte_atomic32_t dev_attached;
 	char *dev_name;
-	char *iface_name;
-	uint16_t max_queues;
+	char *iface_name;//接口名称
+	uint16_t max_queues;//队列数
 	int vid;
 	rte_atomic32_t started;
-	uint8_t vlan_strip;
+	uint8_t vlan_strip;//是否需要做vlan strip
 };
 
 struct internal_list {
@@ -371,6 +376,7 @@ vhost_update_packet_xstats(struct vhost_queue *vq,
 	}
 }
 
+//vhost收包函数
 static uint16_t
 eth_vhost_rx(void *q, struct rte_mbuf **bufs, uint16_t nb_bufs)
 {
@@ -378,20 +384,26 @@ eth_vhost_rx(void *q, struct rte_mbuf **bufs, uint16_t nb_bufs)
 	uint16_t i, nb_rx = 0;
 	uint16_t nb_receive = nb_bufs;
 
+	//此队列是否容许收包
 	if (unlikely(rte_atomic32_read(&r->allow_queuing) == 0))
 		return 0;
 
+	//指明当前正在进行队列操作
 	rte_atomic32_set(&r->while_queuing, 1);
 
+	//这个实现不怎么好，再检查并没什么太大的用处，如果在后面的while中发生呢？
+	//所以不如不加。
 	if (unlikely(rte_atomic32_read(&r->allow_queuing) == 0))
 		goto out;
 
 	/* Dequeue packets from guest TX queue */
+	//尽可能收取nb_receive个报文
 	while (nb_receive) {
 		uint16_t nb_pkts;
 		uint16_t num = (uint16_t)RTE_MIN(nb_receive,
 						 VHOST_MAX_PKT_BURST);
 
+		//实现报文收取
 		nb_pkts = rte_vhost_dequeue_burst(r->vid, r->virtqueue_id,
 						  r->mb_pool, &bufs[nb_rx],
 						  num);
@@ -399,9 +411,10 @@ eth_vhost_rx(void *q, struct rte_mbuf **bufs, uint16_t nb_bufs)
 		nb_rx += nb_pkts;
 		nb_receive -= nb_pkts;
 		if (nb_pkts < num)
-			break;
+			break;//队列中没有多余的包，跳出
 	}
 
+	//收包数计数
 	r->stats.pkts += nb_rx;
 
 	for (i = 0; likely(i < nb_rx); i++) {
@@ -411,17 +424,20 @@ eth_vhost_rx(void *q, struct rte_mbuf **bufs, uint16_t nb_bufs)
 		if (r->internal->vlan_strip)
 			rte_vlan_strip(bufs[i]);
 
+		//收包字节计数
 		r->stats.bytes += bufs[i]->pkt_len;
 	}
 
 	vhost_update_packet_xstats(r, bufs, nb_rx);
 
 out:
+	//指明非队列操作中
 	rte_atomic32_set(&r->while_queuing, 0);
 
 	return nb_rx;
 }
 
+//vhost发包函数
 static uint16_t
 eth_vhost_tx(void *q, struct rte_mbuf **bufs, uint16_t nb_bufs)
 {
@@ -429,22 +445,26 @@ eth_vhost_tx(void *q, struct rte_mbuf **bufs, uint16_t nb_bufs)
 	uint16_t i, nb_tx = 0;
 	uint16_t nb_send = 0;
 
+	//当前不容许对队列操作，返回
 	if (unlikely(rte_atomic32_read(&r->allow_queuing) == 0))
 		return 0;
 
+	//当前正在队列操作，不容许修改allow_queuing
 	rte_atomic32_set(&r->while_queuing, 1);
 
 	if (unlikely(rte_atomic32_read(&r->allow_queuing) == 0))
 		goto out;
 
+	//vlan offload处理
 	for (i = 0; i < nb_bufs; i++) {
 		struct rte_mbuf *m = bufs[i];
 
 		/* Do VLAN tag insertion */
+		//支持tx vlan的offload功能（由于是vhost设备，故需要软件实现）
 		if (m->ol_flags & PKT_TX_VLAN_PKT) {
 			int error = rte_vlan_insert(&m);
 			if (unlikely(error)) {
-				rte_pktmbuf_free(m);
+				rte_pktmbuf_free(m);//无法处理vlan头的插入，丢包
 				continue;
 			}
 		}
@@ -454,6 +474,7 @@ eth_vhost_tx(void *q, struct rte_mbuf **bufs, uint16_t nb_bufs)
 	}
 
 	/* Enqueue packets to guest RX queue */
+	//尽可能多的发送报文
 	while (nb_send) {
 		uint16_t nb_pkts;
 		uint16_t num = (uint16_t)RTE_MIN(nb_send,
@@ -465,11 +486,12 @@ eth_vhost_tx(void *q, struct rte_mbuf **bufs, uint16_t nb_bufs)
 		nb_tx += nb_pkts;
 		nb_send -= nb_pkts;
 		if (nb_pkts < num)
-			break;
+			break;//未完全发送出去，说明底层buffer可能不够，跳出
 	}
 
+	//统计处理
 	r->stats.pkts += nb_tx;
-	r->stats.missed_pkts += nb_bufs - nb_tx;
+	r->stats.missed_pkts += nb_bufs - nb_tx;//未发送出去的报文数
 
 	for (i = 0; likely(i < nb_tx); i++)
 		r->stats.bytes += bufs[i]->pkt_len;
@@ -483,6 +505,7 @@ eth_vhost_tx(void *q, struct rte_mbuf **bufs, uint16_t nb_bufs)
 	for (i = nb_tx; i < nb_bufs; i++)
 		vhost_count_multicast_broadcast(r, bufs[i]);
 
+	//释放未发送成功的报文
 	for (i = 0; likely(i < nb_tx); i++)
 		rte_pktmbuf_free(bufs[i]);
 out:
@@ -660,6 +683,7 @@ eth_vhost_install_intr(struct rte_eth_dev *dev)
 	return 0;
 }
 
+//变更队列状态（vhost是自协商的，而dpdk也有一组自已的start,stop管理方式，故需要这个的融合函数）
 static void
 update_queuing_status(struct rte_eth_dev *dev)
 {
@@ -673,13 +697,14 @@ update_queuing_status(struct rte_eth_dev *dev)
 
 	if (rte_atomic32_read(&internal->started) == 0 ||
 	    rte_atomic32_read(&internal->dev_attached) == 0)
-		allow_queuing = 0;
+		allow_queuing = 0;//设备未启动，不容许收包
 
 	/* Wait until rx/tx_pkt_burst stops accessing vhost device */
 	for (i = 0; i < dev->data->nb_rx_queues; i++) {
 		vq = dev->data->rx_queues[i];
 		if (vq == NULL)
 			continue;
+		//设置收队列的许可标记
 		rte_atomic32_set(&vq->allow_queuing, allow_queuing);
 		while (rte_atomic32_read(&vq->while_queuing))
 			rte_pause();
@@ -689,7 +714,9 @@ update_queuing_status(struct rte_eth_dev *dev)
 		vq = dev->data->tx_queues[i];
 		if (vq == NULL)
 			continue;
+		//设置发队列的许可标记
 		rte_atomic32_set(&vq->allow_queuing, allow_queuing);
+		//正在进行队列操作，等待变更生效
 		while (rte_atomic32_read(&vq->while_queuing))
 			rte_pause();
 	}
@@ -989,6 +1016,7 @@ eth_dev_close(struct rte_eth_dev *dev)
 
 	eth_dev_stop(dev);
 
+	//socket关闭
 	rte_vhost_driver_unregister(internal->iface_name);
 
 	list = find_internal_resource(internal->iface_name);
@@ -1000,10 +1028,12 @@ eth_dev_close(struct rte_eth_dev *dev)
 	pthread_mutex_unlock(&internal_list_lock);
 	rte_free(list);
 
+	//释放rx队列
 	if (dev->data->rx_queues)
 		for (i = 0; i < dev->data->nb_rx_queues; i++)
 			rte_free(dev->data->rx_queues[i]);
 
+	//释放tx队列
 	if (dev->data->tx_queues)
 		for (i = 0; i < dev->data->nb_tx_queues; i++)
 			rte_free(dev->data->tx_queues[i]);
@@ -1018,6 +1048,8 @@ eth_dev_close(struct rte_eth_dev *dev)
 	vring_states[dev->data->port_id] = NULL;
 }
 
+//vhost收队列初始化（在那个socket_id上申请ring)
+//注：队列大小及配置参数将被忽略
 static int
 eth_rx_queue_setup(struct rte_eth_dev *dev, uint16_t rx_queue_id,
 		   uint16_t nb_rx_desc __rte_unused,
@@ -1027,6 +1059,7 @@ eth_rx_queue_setup(struct rte_eth_dev *dev, uint16_t rx_queue_id,
 {
 	struct vhost_queue *vq;
 
+	//申请vq，并初始化
 	vq = rte_zmalloc_socket(NULL, sizeof(struct vhost_queue),
 			RTE_CACHE_LINE_SIZE, socket_id);
 	if (vq == NULL) {
@@ -1041,6 +1074,8 @@ eth_rx_queue_setup(struct rte_eth_dev *dev, uint16_t rx_queue_id,
 	return 0;
 }
 
+//vhost发队列初始化
+//注：队列大小及配置参数将被忽略
 static int
 eth_tx_queue_setup(struct rte_eth_dev *dev, uint16_t tx_queue_id,
 		   uint16_t nb_tx_desc __rte_unused,
@@ -1187,6 +1222,7 @@ eth_rx_queue_count(struct rte_eth_dev *dev, uint16_t rx_queue_id)
 	return rte_vhost_rx_queue_count(vq->vid, vq->virtqueue_id);
 }
 
+//vhost操作集
 static const struct eth_dev_ops ops = {
 	.dev_start = eth_dev_start,
 	.dev_stop = eth_dev_stop,
@@ -1209,6 +1245,7 @@ static const struct eth_dev_ops ops = {
 	.rx_queue_intr_disable = eth_rxq_intr_disable,
 };
 
+//创建vhost设备
 static int
 eth_dev_vhost_create(struct rte_vdev_device *dev, char *iface_name,
 	int16_t queues, const unsigned int numa_node, uint64_t flags,
@@ -1230,11 +1267,13 @@ eth_dev_vhost_create(struct rte_vdev_device *dev, char *iface_name,
 		goto error;
 
 	/* reserve an ethdev entry */
+	//申请eth_dev
 	eth_dev = rte_eth_vdev_allocate(dev, sizeof(*internal));
 	if (eth_dev == NULL)
 		goto error;
 	data = eth_dev->data;
 
+	//设置默认的mac地址
 	eth_addr = rte_zmalloc_socket(name, sizeof(*eth_addr), 0, numa_node);
 	if (eth_addr == NULL)
 		goto error;
@@ -1268,8 +1307,9 @@ eth_dev_vhost_create(struct rte_vdev_device *dev, char *iface_name,
 	rte_spinlock_init(&vring_state->lock);
 	vring_states[eth_dev->data->port_id] = vring_state;
 
-	data->nb_rx_queues = queues;
-	data->nb_tx_queues = queues;
+	//收发队列数保证相同
+	data->nb_rx_queues = queues;//收队列数
+	data->nb_tx_queues = queues;//发队列数
 	internal->max_queues = queues;
 	internal->vid = -1;
 	data->dev_link = pmd_link;
@@ -1277,10 +1317,12 @@ eth_dev_vhost_create(struct rte_vdev_device *dev, char *iface_name,
 
 	eth_dev->dev_ops = &ops;
 
+	//挂载vhost设备的收包，发包函数
 	/* finally assign rx and tx ops */
 	eth_dev->rx_pkt_burst = eth_vhost_rx;
 	eth_dev->tx_pkt_burst = eth_vhost_tx;
 
+	//创建vhost socket
 	if (rte_vhost_driver_register(iface_name, flags))
 		goto error;
 
@@ -1290,11 +1332,13 @@ eth_dev_vhost_create(struct rte_vdev_device *dev, char *iface_name,
 			goto error;
 	}
 
+	//注册vhost socket的注册通知回调
 	if (rte_vhost_driver_callback_register(iface_name, &vhost_ops) < 0) {
 		VHOST_LOG(ERR, "Can't register callbacks\n");
 		goto error;
 	}
 
+	//监听socket（服务器），或者连接到服务器（client端）
 	if (rte_vhost_driver_start(iface_name) < 0) {
 		VHOST_LOG(ERR, "Failed to start driver for %s\n",
 			iface_name);
@@ -1316,6 +1360,7 @@ eth_dev_vhost_create(struct rte_vdev_device *dev, char *iface_name,
 	return -1;
 }
 
+//将value赋给extra-args
 static inline int
 open_iface(const char *key __rte_unused, const char *value, void *extra_args)
 {
@@ -1329,6 +1374,7 @@ open_iface(const char *key __rte_unused, const char *value, void *extra_args)
 	return 0;
 }
 
+//将value转为整数值，赋给extra_args
 static inline int
 open_int(const char *key __rte_unused, const char *value, void *extra_args)
 {
@@ -1344,6 +1390,7 @@ open_int(const char *key __rte_unused, const char *value, void *extra_args)
 	return 0;
 }
 
+//vhost驱动探测设备dev
 static int
 rte_pmd_vhost_probe(struct rte_vdev_device *dev)
 {
@@ -1376,11 +1423,14 @@ rte_pmd_vhost_probe(struct rte_vdev_device *dev)
 		return 0;
 	}
 
+	//vhost目前仅支持valid_arguments参数，如果遇到其它参数，报错
 	kvlist = rte_kvargs_parse(rte_vdev_device_args(dev), valid_arguments);
 	if (kvlist == NULL)
 		return -1;
 
+	//iface仅容许配置一次
 	if (rte_kvargs_count(kvlist, ETH_VHOST_IFACE_ARG) == 1) {
+		//用open_iface函数，设置iface-name为参数iface的取值。
 		ret = rte_kvargs_process(kvlist, ETH_VHOST_IFACE_ARG,
 					 &open_iface, &iface_name);
 		if (ret < 0)
@@ -1390,7 +1440,9 @@ rte_pmd_vhost_probe(struct rte_vdev_device *dev)
 		goto out_free;
 	}
 
+	//queues仅容许配置一次
 	if (rte_kvargs_count(kvlist, ETH_VHOST_QUEUES_ARG) == 1) {
+		//用open-int,将queue的参数转为整数，赋给queues
 		ret = rte_kvargs_process(kvlist, ETH_VHOST_QUEUES_ARG,
 					 &open_int, &queues);
 		if (ret < 0 || queues > RTE_MAX_QUEUES_PER_PORT)
@@ -1399,26 +1451,33 @@ rte_pmd_vhost_probe(struct rte_vdev_device *dev)
 	} else
 		queues = 1;
 
+	//client仅容许配置一次
 	if (rte_kvargs_count(kvlist, ETH_VHOST_CLIENT_ARG) == 1) {
+		//用open-int,将client的参数转为整数，赋给client-mode
 		ret = rte_kvargs_process(kvlist, ETH_VHOST_CLIENT_ARG,
 					 &open_int, &client_mode);
 		if (ret < 0)
 			goto out_free;
 
 		if (client_mode)
+			//标记clinet
 			flags |= RTE_VHOST_USER_CLIENT;
 	}
 
+	//zero-copy参数仅容许配置一次
 	if (rte_kvargs_count(kvlist, ETH_VHOST_DEQUEUE_ZERO_COPY) == 1) {
+		//用open-int,将参数转为整数，存入dequeue_zero_copy
 		ret = rte_kvargs_process(kvlist, ETH_VHOST_DEQUEUE_ZERO_COPY,
 					 &open_int, &dequeue_zero_copy);
 		if (ret < 0)
 			goto out_free;
 
 		if (dequeue_zero_copy)
+			//标记dequeue-zero-copy
 			flags |= RTE_VHOST_USER_DEQUEUE_ZERO_COPY;
 	}
 
+	//iommu-support参数仅容许配置一次
 	if (rte_kvargs_count(kvlist, ETH_VHOST_IOMMU_SUPPORT) == 1) {
 		ret = rte_kvargs_process(kvlist, ETH_VHOST_IOMMU_SUPPORT,
 					 &open_int, &iommu_support);
@@ -1452,9 +1511,11 @@ rte_pmd_vhost_probe(struct rte_vdev_device *dev)
 		}
 	}
 
+	//设置numa_node(如果any,则取当前core对应socket)
 	if (dev->device.numa_node == SOCKET_ID_ANY)
 		dev->device.numa_node = rte_socket_id();
 
+	//创建vhost设备(iface_name为接口名称，queues为队列数，flag为（客户端，出队0copy,iommu支持）
 	eth_dev_vhost_create(dev, iface_name, queues, dev->device.numa_node,
 		flags, disable_flags);
 
@@ -1463,6 +1524,7 @@ rte_pmd_vhost_probe(struct rte_vdev_device *dev)
 	return ret;
 }
 
+//移除设备dev
 static int
 rte_pmd_vhost_remove(struct rte_vdev_device *dev)
 {
@@ -1473,6 +1535,7 @@ rte_pmd_vhost_remove(struct rte_vdev_device *dev)
 	VHOST_LOG(INFO, "Un-Initializing pmd_vhost for %s\n", name);
 
 	/* find an ethdev entry */
+	//找名称为$name的eth_dev
 	eth_dev = rte_eth_dev_allocated(name);
 	if (eth_dev == NULL)
 		return 0;
@@ -1480,6 +1543,7 @@ rte_pmd_vhost_remove(struct rte_vdev_device *dev)
 	if (rte_eal_process_type() != RTE_PROC_PRIMARY)
 		return rte_eth_dev_release_port(eth_dev);
 
+	//停止设备
 	eth_dev_close(eth_dev);
 
 	rte_eth_dev_release_port(eth_dev);
@@ -1487,11 +1551,13 @@ rte_pmd_vhost_remove(struct rte_vdev_device *dev)
 	return 0;
 }
 
+//vhost驱动
 static struct rte_vdev_driver pmd_vhost_drv = {
-	.probe = rte_pmd_vhost_probe,
-	.remove = rte_pmd_vhost_remove,
+	.probe = rte_pmd_vhost_probe,//驱动探测设备时调用
+	.remove = rte_pmd_vhost_remove,//移除设备时调用
 };
 
+//注册vhost驱动
 RTE_PMD_REGISTER_VDEV(net_vhost, pmd_vhost_drv);
 RTE_PMD_REGISTER_ALIAS(net_vhost, eth_vhost);
 RTE_PMD_REGISTER_PARAM_STRING(net_vhost,
@@ -1503,7 +1569,10 @@ RTE_PMD_REGISTER_PARAM_STRING(net_vhost,
 	"postcopy-support=<0|1> "
 	"tso=<0|1>");
 
-RTE_INIT(vhost_init_log)
+//注册vhost log模块
+RTE_INIT(vhost_init_log);
+static void
+vhost_init_log(void)
 {
 	vhost_logtype = rte_log_register("pmd.net.vhost");
 	if (vhost_logtype >= 0)
diff --git a/drivers/net/virtio/virtio_ethdev.c b/drivers/net/virtio/virtio_ethdev.c
index 646de9945..42beb2b46 100644
--- a/drivers/net/virtio/virtio_ethdev.c
+++ b/drivers/net/virtio/virtio_ethdev.c
@@ -360,6 +360,7 @@ virtio_send_command(struct virtnet_ctl *cvq, struct virtio_pmd_ctrl *ctrl,
 	return result->status;
 }
 
+//给对端发送队列队set命令
 static int
 virtio_set_multiple_queues(struct rte_eth_dev *dev, uint16_t nb_queues)
 {
@@ -899,6 +900,7 @@ virtio_dev_rx_queue_intr_disable(struct rte_eth_dev *dev, uint16_t queue_id)
 /*
  * dev_ops for virtio, bare necessities for basic operation
  */
+//virtio操作集
 static const struct eth_dev_ops virtio_eth_dev_ops = {
 	.dev_configure           = virtio_dev_configure,
 	.dev_start               = virtio_dev_start,
@@ -1547,6 +1549,7 @@ set_rxtx_funcs(struct rte_eth_dev *eth_dev)
 			eth_dev->rx_pkt_burst = &virtio_recv_pkts_packed;
 		}
 	} else {
+		//设置virtio设备的发包函数
 		if (hw->use_simple_rx) {
 			PMD_INIT_LOG(INFO, "virtio: using simple Rx path on port %u",
 				eth_dev->data->port_id);
@@ -1721,6 +1724,11 @@ virtio_init_device(struct rte_eth_dev *eth_dev, uint64_t req_features)
 			offsetof(struct virtio_net_config, mac),
 			&config->mac, sizeof(config->mac));
 
+		/**
+		 * If the VIRTIO_NET_F_STATUS feature bit is negotiated, the link status comes from the bottom bit of
+		   status. Otherwise, the driver assumes it’s active.
+		   如果有status,则读取status,否则假设为0
+		 */
 		if (vtpci_with_feature(hw, VIRTIO_NET_F_STATUS)) {
 			vtpci_read_dev_config(hw,
 				offsetof(struct virtio_net_config, status),
@@ -1731,6 +1739,8 @@ virtio_init_device(struct rte_eth_dev *eth_dev, uint64_t req_features)
 			config->status = 0;
 		}
 
+		//如果有virtio_net_f_mq标记，则自配置中读取max_virtqueueu_pairs
+		//否则直接将其设置为1
 		if (vtpci_with_feature(hw, VIRTIO_NET_F_MQ)) {
 			vtpci_read_dev_config(hw,
 				offsetof(struct virtio_net_config, max_virtqueue_pairs),
@@ -1978,11 +1988,12 @@ vdpa_mode_selected(struct rte_devargs *devargs)
 		return 0;
 
 	if (!rte_kvargs_count(kvlist, key))
-		goto exit;
+		goto exit;//无'vdpa'直接返回0
 
 	/* vdpa mode selected when there's a key-value pair: vdpa=1 */
 	if (rte_kvargs_process(kvlist, key,
 				vdpa_check_handler, NULL) < 0) {
+		//无vdpa=1,直接返回0
 		goto exit;
 	}
 	ret = 1;
@@ -2027,6 +2038,7 @@ static struct rte_pci_driver rte_virtio_pmd = {
 RTE_INIT(rte_virtio_pmd_init)
 {
 	rte_eal_iopl_init();
+	//注册virtio pci驱动
 	rte_pci_register(&rte_virtio_pmd);
 }
 
diff --git a/drivers/net/virtio/virtio_pci.h b/drivers/net/virtio/virtio_pci.h
index a38cb45ad..55d01ea8e 100644
--- a/drivers/net/virtio/virtio_pci.h
+++ b/drivers/net/virtio/virtio_pci.h
@@ -77,26 +77,36 @@ struct virtnet_ctl;
 #define VIRTIO_MAX_INDIRECT ((int) (PAGE_SIZE / 16))
 
 /* The feature bitmap for virtio net */
+//Device handles packets with partial checksum. This “checksum offload” is a
+//common feature on modern network cards.(设备支持checksum)
 #define VIRTIO_NET_F_CSUM	0	/* Host handles pkts w/ partial csum */
+//Driver handles packets with partial checksum.(驱动支持checksum)
 #define VIRTIO_NET_F_GUEST_CSUM	1	/* Guest handles pkts w/ partial csum */
 #define VIRTIO_NET_F_MTU	3	/* Initial MTU advice. */
+//设备已设置mac地址（此标记存在时设备mac有效）
 #define VIRTIO_NET_F_MAC	5	/* Host has given MAC address. */
+//驱动处理TSO,EnC,UFO
 #define VIRTIO_NET_F_GUEST_TSO4	7	/* Guest can handle TSOv4 in. */
 #define VIRTIO_NET_F_GUEST_TSO6	8	/* Guest can handle TSOv6 in. */
 #define VIRTIO_NET_F_GUEST_ECN	9	/* Guest can handle TSO[6] w/ ECN in. */
 #define VIRTIO_NET_F_GUEST_UFO	10	/* Guest can handle UFO in. */
+//设备处理TSO，ECN，UFO
 #define VIRTIO_NET_F_HOST_TSO4	11	/* Host can handle TSOv4 in. */
 #define VIRTIO_NET_F_HOST_TSO6	12	/* Host can handle TSOv6 in. */
 #define VIRTIO_NET_F_HOST_ECN	13	/* Host can handle TSO[6] w/ ECN in. */
 #define VIRTIO_NET_F_HOST_UFO	14	/* Host can handle UFO in. */
+
 #define VIRTIO_NET_F_MRG_RXBUF	15	/* Host can merge receive buffers. */
 #define VIRTIO_NET_F_STATUS	16	/* virtio_net_config.status available */
+//是否存在控制虚队列队列
 #define VIRTIO_NET_F_CTRL_VQ	17	/* Control channel available */
 #define VIRTIO_NET_F_CTRL_RX	18	/* Control channel RX mode support */
 #define VIRTIO_NET_F_CTRL_VLAN	19	/* Control channel VLAN filtering */
 #define VIRTIO_NET_F_CTRL_RX_EXTRA 20	/* Extra RX mode control support */
+//驱动可以发送免费报文
 #define VIRTIO_NET_F_GUEST_ANNOUNCE 21	/* Guest can announce device on the
 					 * network */
+//有此标记时，virtio_net支持多队列，否则仅支持单队列
 #define VIRTIO_NET_F_MQ		22	/* Device supports Receive Flow
 					 * Steering */
 #define VIRTIO_NET_F_CTRL_MAC_ADDR 23	/* Set MAC address */
@@ -294,9 +304,27 @@ extern struct virtio_hw_internal virtio_hw_internal[RTE_MAX_ETHPORTS];
  */
 struct virtio_net_config {
 	/* The config defining mac address (if VIRTIO_NET_F_MAC) */
+	/**
+	 * A driver SHOULD negotiate VIRTIO_NET_F_MAC if the device offers it. If the driver negotiates the VIRTIO_-
+NET_F_MAC feature, the driver MUST set the physical address of the NIC to mac. Otherwise, it SHOULD
+use a locally-administered MAC address (see IEEE 802, “9.2 48-bit universal LAN MAC addresses”).
+     mac地址配置
+	 */
 	uint8_t    mac[RTE_ETHER_ADDR_LEN];
 	/* See VIRTIO_NET_F_STATUS and VIRTIO_NET_S_* above */
+	//status only exists if VIRTIO_NET_F_STATUS is
+	//set. Two read-only bits (for the driver) are currently defined for the status field: VIRTIO_NET_S_LINK_UP
+	//and VIRTIO_NET_S_ANNOUNCE.
+	//当前仅两个状态，由virtio_net_f_status来指明有效性
 	uint16_t   status;
+	/*
+	 * max_virtqueue_pairs only exists if VIRTIO_NET_F_MQ is set. This
+field specifies the maximum number of each of transmit and receive virtqueues (receiveq1. . .receiveqN and
+transmitq1. . .transmitqN respectively) that can be configured once VIRTIO_NET_F_MQ is negotiated.
+The device MUST set max_virtqueue_pairs to between 1 and 0x8000 inclusive, if it offers VIRTIO_NET_-
+F_MQ.
+      最大虚队列对
+	 */
 	uint16_t   max_virtqueue_pairs;
 	uint16_t   mtu;
 } __attribute__((packed));
diff --git a/drivers/net/virtio/virtio_ring.h b/drivers/net/virtio/virtio_ring.h
index 7ba34662e..5fc1fccfa 100644
--- a/drivers/net/virtio/virtio_ring.h
+++ b/drivers/net/virtio/virtio_ring.h
@@ -10,11 +10,11 @@
 #include <rte_common.h>
 
 /* This marks a buffer as continuing via the next field. */
-#define VRING_DESC_F_NEXT       1
+#define VRING_DESC_F_NEXT       1 //buffer有下一个描述符
 /* This marks a buffer as write-only (otherwise read-only). */
-#define VRING_DESC_F_WRITE      2
+#define VRING_DESC_F_WRITE      2 //buffer是否为只写的
 /* This means the buffer contains a list of buffer descriptors. */
-#define VRING_DESC_F_INDIRECT   4
+#define VRING_DESC_F_INDIRECT   4 //包含了一组连续的buffer描述符
 
 /* This flag means the descriptor was made available by the driver */
 #define VRING_PACKED_DESC_F_AVAIL	(1 << 7)
@@ -37,24 +37,25 @@
 /* VirtIO ring descriptors: 16 bytes.
  * These can chain together via "next". */
 struct vring_desc {
-	uint64_t addr;  /*  Address (guest-physical). */
-	uint32_t len;   /* Length. */
+	uint64_t addr;  /*  Address (guest-physical). */ //guest的物理地址，指向对应的描述符
+	uint32_t len;   /* Length. */ //数据长度
+	//描述符的标记位，看virtio 1.0 spec 2.4.5节
 	uint16_t flags; /* The flags as indicated above. */
-	uint16_t next;  /* We chain unused descriptors via this. */
+	uint16_t next;  /* We chain unused descriptors via this. */ //split情况下用于实现大包数传输时用
 };
 
 struct vring_avail {
 	uint16_t flags;
-	uint16_t idx;
-	uint16_t ring[0];
+	uint16_t idx;//队列中目前有效位置（即可存放起始位置或可读取的极限位置）
+	uint16_t ring[0];//ring，记录有效的描述符索引号
 };
 
 /* id is a 16bit index. uint32_t is used here for ids for padding reasons. */
 struct vring_used_elem {
 	/* Index of start of used descriptor chain. */
-	uint32_t id;
+	uint32_t id;//指明用哪个描述符
 	/* Total length of the descriptor chain which was written to. */
-	uint32_t len;
+	uint32_t len;//指明要写的数据长度
 };
 
 struct vring_used {
@@ -128,13 +129,14 @@ struct vring {
 #define vring_used_event(vr)  ((vr)->avail->ring[(vr)->num])
 #define vring_avail_event(vr) (*(uint16_t *)&(vr)->used->ring[(vr)->num])
 
+//计算vring内存size
 static inline size_t
 vring_size(struct virtio_hw *hw, unsigned int num, unsigned long align)
 {
 	size_t size;
 
 	if (vtpci_packed_queue(hw)) {
-		size = num * sizeof(struct vring_packed_desc);
+		size = num * sizeof(struct vring_packed_desc);//num个vring_desc
 		size += sizeof(struct vring_packed_desc_event);
 		size = RTE_ALIGN_CEIL(size, align);
 		size += sizeof(struct vring_packed_desc_event);
@@ -145,9 +147,10 @@ vring_size(struct virtio_hw *hw, unsigned int num, unsigned long align)
 	size += sizeof(struct vring_avail) + (num * sizeof(uint16_t));
 	size = RTE_ALIGN_CEIL(size, align);
 	size += sizeof(struct vring_used) +
-		(num * sizeof(struct vring_used_elem));
+		(num * sizeof(struct vring_used_elem));//used+number个数据成员
 	return size;
 }
+//初始化vring
 static inline void
 vring_init_split(struct vring *vr, uint8_t *p, unsigned long align,
 	 unsigned int num)
diff --git a/drivers/net/virtio/virtio_rxtx.c b/drivers/net/virtio/virtio_rxtx.c
index 752faa0f6..852b61bf2 100644
--- a/drivers/net/virtio/virtio_rxtx.c
+++ b/drivers/net/virtio/virtio_rxtx.c
@@ -1159,21 +1159,30 @@ virtio_rx_offload(struct rte_mbuf *m, struct virtio_net_hdr *hdr)
 	int l4_supported = 0;
 
 	/* nothing to do */
+	/*如virtio spec所言：
+	 * The driver can send a completely checksummed packet. In this case,
+	 * flags will be zero, and gso_type will be VIRTIO_NET_HDR_GSO_NONE
+	 * checksum是正确的，无需处理
+	 */
 	if (hdr->flags == 0 && hdr->gso_type == VIRTIO_NET_HDR_GSO_NONE)
-		return 0;
+		return 0;//由于下面的rte_net_get_ptype函数非常差，最好自此处返回
 
 	m->ol_flags |= PKT_RX_IP_CKSUM_UNKNOWN;
 
+	//解析报文协议类型
 	ptype = rte_net_get_ptype(m, &hdr_lens, RTE_PTYPE_ALL_MASK);
 	m->packet_type = ptype;
+	//当前支持对TCP,UDP，SCTP做checksum
 	if ((ptype & RTE_PTYPE_L4_MASK) == RTE_PTYPE_L4_TCP ||
 	    (ptype & RTE_PTYPE_L4_MASK) == RTE_PTYPE_L4_UDP ||
 	    (ptype & RTE_PTYPE_L4_MASK) == RTE_PTYPE_L4_SCTP)
 		l4_supported = 1;
 
 	if (hdr->flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) {
+		//需要做checksum
 		hdrlen = hdr_lens.l2_len + hdr_lens.l3_len + hdr_lens.l4_len;
 		if (hdr->csum_start <= hdrlen && l4_supported) {
+			//底层设备支持做checksum,直接打上offload标记
 			m->ol_flags |= PKT_RX_L4_CKSUM_NONE;
 		} else {
 			/* Unknown proto or tunnel, do sw cksum. We can assume
@@ -1182,6 +1191,7 @@ virtio_rx_offload(struct rte_mbuf *m, struct virtio_net_hdr *hdr)
 			 * In case of SCTP, this will be wrong since it's a CRC
 			 * but there's nothing we can do.
 			 */
+			//采用软件计算并设置checksum
 			uint16_t csum = 0, off;
 
 			rte_raw_cksum_mbuf(m, hdr->csum_start,
@@ -1203,6 +1213,7 @@ virtio_rx_offload(struct rte_mbuf *m, struct virtio_net_hdr *hdr)
 		/* Check unsupported modes */
 		if ((hdr->gso_type & VIRTIO_NET_HDR_GSO_ECN) ||
 		    (hdr->gso_size == 0)) {
+			//当前不支持是示拥塞，且也没有协商，认为参数错误
 			return -EINVAL;
 		}
 
@@ -1211,6 +1222,7 @@ virtio_rx_offload(struct rte_mbuf *m, struct virtio_net_hdr *hdr)
 		switch (hdr->gso_type & ~VIRTIO_NET_HDR_GSO_ECN) {
 			case VIRTIO_NET_HDR_GSO_TCPV4:
 			case VIRTIO_NET_HDR_GSO_TCPV6:
+				//指明tso_segsz有效
 				m->ol_flags |= PKT_RX_LRO | \
 					PKT_RX_L4_CKSUM_NONE;
 				break;
@@ -1253,6 +1265,7 @@ virtio_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
 	if (likely(num > DESC_PER_CACHELINE))
 		num = num - ((vq->vq_used_cons_idx + num) % DESC_PER_CACHELINE);
 
+	//自vq中收队num个报文
 	num = virtqueue_dequeue_burst_rx(vq, rcv_pkts, len, num);
 	PMD_RX_LOG(DEBUG, "used:%d dequeue:%d", nb_used, num);
 
@@ -1280,11 +1293,12 @@ virtio_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
 		rxm->pkt_len = (uint32_t)(len[i] - hdr_size);
 		rxm->data_len = (uint16_t)(len[i] - hdr_size);
 
+		//定位到virtio_net_hdr
 		hdr = (struct virtio_net_hdr *)((char *)rxm->buf_addr +
 			RTE_PKTMBUF_HEADROOM - hdr_size);
 
 		if (hw->vlan_strip)
-			rte_vlan_strip(rxm);
+			rte_vlan_strip(rxm);//剥掉vlan头
 
 		if (hw->has_rx_offload && virtio_rx_offload(rxm, hdr) < 0) {
 			virtio_discard_rxbuf(vq, rxm);
@@ -1294,6 +1308,7 @@ virtio_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
 
 		virtio_rx_stats_updated(rxvq, rxm);
 
+		//设置收到的报文
 		rx_pkts[nb_rx++] = rxm;
 	}
 
@@ -1658,6 +1673,7 @@ virtio_recv_mergeable_pkts(void *rx_queue,
 				DESC_PER_CACHELINE);
 
 
+	//自队列中出一个报文
 	num = virtqueue_dequeue_burst_rx(vq, rcv_pkts, len, num);
 
 	for (i = 0; i < num; i++) {
@@ -1668,6 +1684,7 @@ virtio_recv_mergeable_pkts(void *rx_queue,
 
 		rxm = rcv_pkts[i];
 
+		//报文长度校验
 		if (unlikely(len[i] < hdr_size + RTE_ETHER_HDR_LEN)) {
 			PMD_RX_LOG(ERR, "Packet drop");
 			nb_enqueued++;
@@ -1676,6 +1693,7 @@ virtio_recv_mergeable_pkts(void *rx_queue,
 			continue;
 		}
 
+		//取virtio_net_hdr_mrg_rxbuf结构头
 		header = (struct virtio_net_hdr_mrg_rxbuf *)
 			 ((char *)rxm->buf_addr + RTE_PKTMBUF_HEADROOM
 			 - hdr_size);
@@ -1683,8 +1701,8 @@ virtio_recv_mergeable_pkts(void *rx_queue,
 		if (seg_num == 0)
 			seg_num = 1;
 
-		rxm->data_off = RTE_PKTMBUF_HEADROOM;
-		rxm->nb_segs = seg_num;
+		rxm->data_off = RTE_PKTMBUF_HEADROOM;//指向data
+		rxm->nb_segs = seg_num;//有多少片数
 		rxm->ol_flags = 0;
 		rxm->vlan_tci = 0;
 		rxm->pkt_len = (uint32_t)(len[i] - hdr_size);
@@ -1692,9 +1710,10 @@ virtio_recv_mergeable_pkts(void *rx_queue,
 
 		rxm->port = rxvq->port_id;
 
-		rx_pkts[nb_rx] = rxm;
+		rx_pkts[nb_rx] = rxm;//设置收到的报文
 		prev = rxm;
 
+		//gro处理
 		if (hw->has_rx_offload &&
 				virtio_rx_offload(rxm, &header->hdr) < 0) {
 			virtio_discard_rxbuf(vq, rxm);
@@ -1741,6 +1760,7 @@ virtio_recv_mergeable_pkts(void *rx_queue,
 			uint16_t extra_idx = 0;
 
 			rcv_cnt = num;
+			//填充收取的报文
 			while (extra_idx < rcv_cnt) {
 				rxm = rcv_pkts[extra_idx];
 				rxm->data_off =
@@ -1875,6 +1895,7 @@ virtio_recv_mergeable_pkts_packed(void *rx_queue,
 			continue;
 		}
 
+		//vlan strip处理
 		if (hw->vlan_strip)
 			rte_vlan_strip(rx_pkts[nb_rx]);
 
@@ -1959,6 +1980,7 @@ virtio_recv_mergeable_pkts_packed(void *rx_queue,
 		}
 	}
 
+	//通知对端
 	if (likely(nb_enqueued)) {
 		if (unlikely(virtqueue_kick_prepare_packed(vq))) {
 			virtqueue_notify(vq);
@@ -1969,6 +1991,7 @@ virtio_recv_mergeable_pkts_packed(void *rx_queue,
 	return nb_rx;
 }
 
+//virtio设备发包函数
 uint16_t
 virtio_xmit_pkts_prepare(void *tx_queue __rte_unused, struct rte_mbuf **tx_pkts,
 			uint16_t nb_pkts)
@@ -2115,6 +2138,7 @@ virtio_xmit_pkts(void *tx_queue, struct rte_mbuf **tx_pkts, uint16_t nb_pkts)
 		virtio_xmit_cleanup(vq, nb_used);
 
 	for (nb_tx = 0; nb_tx < nb_pkts; nb_tx++) {
+		//取出要发送的报文txm
 		struct rte_mbuf *txm = tx_pkts[nb_tx];
 		int can_push = 0, use_indirect = 0, slots, need;
 
diff --git a/drivers/net/virtio/virtio_user/vhost.h b/drivers/net/virtio/virtio_user/vhost.h
index 1e784e58e..339f9f8d6 100644
--- a/drivers/net/virtio/virtio_user/vhost.h
+++ b/drivers/net/virtio/virtio_user/vhost.h
@@ -46,11 +46,11 @@ struct vhost_vring_addr {
 
 enum vhost_user_request {
 	VHOST_USER_NONE = 0,
-	VHOST_USER_GET_FEATURES = 1,
-	VHOST_USER_SET_FEATURES = 2,
+	VHOST_USER_GET_FEATURES = 1,//取对端的功能
+	VHOST_USER_SET_FEATURES = 2,//设置对端的功能
 	VHOST_USER_SET_OWNER = 3,
 	VHOST_USER_RESET_OWNER = 4,
-	VHOST_USER_SET_MEM_TABLE = 5,
+	VHOST_USER_SET_MEM_TABLE = 5,//设置对端内存表
 	VHOST_USER_SET_LOG_BASE = 6,
 	VHOST_USER_SET_LOG_FD = 7,
 	VHOST_USER_SET_VRING_NUM = 8,
@@ -80,6 +80,7 @@ struct virtio_user_dev;
 
 struct virtio_user_backend_ops {
 	int (*setup)(struct virtio_user_dev *dev);
+	//向对端发送消息的回调
 	int (*send_request)(struct virtio_user_dev *dev,
 			    enum vhost_user_request req,
 			    void *arg);
diff --git a/drivers/net/virtio/virtio_user/vhost_user.c b/drivers/net/virtio/virtio_user/vhost_user.c
index a4b5c25cd..2743813e7 100644
--- a/drivers/net/virtio/virtio_user/vhost_user.c
+++ b/drivers/net/virtio/virtio_user/vhost_user.c
@@ -243,6 +243,7 @@ const char * const vhost_msg_strings[] = {
 	[VHOST_USER_SET_VRING_ENABLE] = "VHOST_SET_VRING_ENABLE",
 };
 
+//发送请求
 static int
 vhost_user_sock(struct virtio_user_dev *dev,
 		enum vhost_user_request req,
@@ -263,15 +264,18 @@ vhost_user_sock(struct virtio_user_dev *dev,
 	if (dev->is_server && vhostfd < 0)
 		return -1;
 
+	//要发送的请求
 	msg.request = req;
 	msg.flags = VHOST_USER_VERSION;
 	msg.size = 0;
 
 	switch (req) {
+	//取对端功能
 	case VHOST_USER_GET_FEATURES:
 		need_reply = 1;
 		break;
 
+	//设置功能，log base
 	case VHOST_USER_SET_FEATURES:
 	case VHOST_USER_SET_LOG_BASE:
 		msg.payload.u64 = *((__u64 *)arg);
@@ -330,6 +334,7 @@ vhost_user_sock(struct virtio_user_dev *dev,
 		return -1;
 	}
 
+	//向对端发送消息
 	len = VHOST_USER_HDR_SIZE + msg.size;
 	if (vhost_user_write(vhostfd, &msg, len, fds, fd_num) < 0) {
 		PMD_DRV_LOG(ERR, "%s failed: %s",
@@ -338,6 +343,7 @@ vhost_user_sock(struct virtio_user_dev *dev,
 	}
 
 	if (need_reply) {
+		//读取对端的响应
 		if (vhost_user_read(vhostfd, &msg) < 0) {
 			PMD_DRV_LOG(ERR, "Received msg failed: %s",
 				    strerror(errno));
@@ -408,6 +414,7 @@ virtio_user_start_server(struct virtio_user_dev *dev, struct sockaddr_un *un)
  *   - (-1) if fail;
  *   - (0) if succeed.
  */
+//连接到dev->path
 static int
 vhost_user_setup(struct virtio_user_dev *dev)
 {
@@ -415,6 +422,7 @@ vhost_user_setup(struct virtio_user_dev *dev)
 	int flag;
 	struct sockaddr_un un;
 
+	//创建unix socket,并置为exec时close
 	fd = socket(AF_UNIX, SOCK_STREAM, 0);
 	if (fd < 0) {
 		PMD_DRV_LOG(ERR, "socket() error, %s", strerror(errno));
@@ -425,6 +433,7 @@ vhost_user_setup(struct virtio_user_dev *dev)
 	if (fcntl(fd, F_SETFD, flag | FD_CLOEXEC) < 0)
 		PMD_DRV_LOG(WARNING, "fcntl failed, %s", strerror(errno));
 
+	//连接到dev->path
 	memset(&un, 0, sizeof(un));
 	un.sun_family = AF_UNIX;
 	strlcpy(un.sun_path, dev->path, sizeof(un.sun_path));
@@ -469,6 +478,7 @@ vhost_user_enable_queue_pair(struct virtio_user_dev *dev,
 	return 0;
 }
 
+//vhost user后端ops
 struct virtio_user_backend_ops virtio_ops_user = {
 	.setup = vhost_user_setup,
 	.send_request = vhost_user_sock,
diff --git a/drivers/net/virtio/virtio_user/virtio_user_dev.c b/drivers/net/virtio/virtio_user/virtio_user_dev.c
index 1c575d0cd..2185ac9a8 100644
--- a/drivers/net/virtio/virtio_user/virtio_user_dev.c
+++ b/drivers/net/virtio/virtio_user/virtio_user_dev.c
@@ -221,6 +221,7 @@ int virtio_user_stop_device(struct virtio_user_dev *dev)
 	return error;
 }
 
+//由mac串来解析 mac地址，并填充到dev上
 static inline void
 parse_mac(struct virtio_user_dev *dev, const char *mac)
 {
@@ -238,6 +239,7 @@ parse_mac(struct virtio_user_dev *dev, const char *mac)
 	}
 }
 
+//初始化notify fd
 static int
 virtio_user_dev_init_notify(struct virtio_user_dev *dev)
 {
@@ -246,16 +248,18 @@ virtio_user_dev_init_notify(struct virtio_user_dev *dev)
 	int kickfd;
 
 	for (i = 0; i < VIRTIO_MAX_VIRTQUEUES; ++i) {
+		//队列对，初始化为无效值
 		if (i >= dev->max_queue_pairs * 2) {
 			dev->kickfds[i] = -1;
 			dev->callfds[i] = -1;
-			continue;
+			continue;//i过大时，置为-1
 		}
 
 		/* May use invalid flag, but some backend uses kickfd and
 		 * callfd as criteria to judge if dev is alive. so finally we
 		 * use real event_fd.
 		 */
+		//创建事件fd
 		callfd = eventfd(0, EFD_CLOEXEC | EFD_NONBLOCK);
 		if (callfd < 0) {
 			PMD_DRV_LOG(ERR, "callfd error, %s", strerror(errno));
@@ -270,6 +274,7 @@ virtio_user_dev_init_notify(struct virtio_user_dev *dev)
 		dev->kickfds[i] = kickfd;
 	}
 
+	//如果创建失败，则关闭掉已打开的fd
 	if (i < VIRTIO_MAX_VIRTQUEUES) {
 		for (j = 0; j <= i; ++j) {
 			close(dev->callfds[j]);
@@ -358,6 +363,7 @@ virtio_user_dev_setup(struct virtio_user_dev *dev)
 	dev->tapfds = NULL;
 
 	if (dev->is_server) {
+		//server端
 		if (access(dev->path, F_OK) == 0 &&
 		    !is_vhost_user_by_type(dev->path)) {
 			PMD_DRV_LOG(ERR, "Server mode doesn't support vhost-kernel!");
@@ -365,6 +371,7 @@ virtio_user_dev_setup(struct virtio_user_dev *dev)
 		}
 		dev->ops = &virtio_ops_user;
 	} else {
+		//client端
 		if (is_vhost_user_by_type(dev->path)) {
 			dev->ops = &virtio_ops_user;
 		} else {
@@ -386,9 +393,11 @@ virtio_user_dev_setup(struct virtio_user_dev *dev)
 		}
 	}
 
+	//完成连接
 	if (dev->ops->setup(dev) < 0)
 		return -1;
 
+	//生成通知用的eventfd
 	if (virtio_user_dev_init_notify(dev) < 0)
 		return -1;
 
@@ -438,7 +447,7 @@ virtio_user_dev_init(struct virtio_user_dev *dev, char *path, int queues,
 
 	if (*ifname) {
 		dev->ifname = *ifname;
-		*ifname = NULL;
+		*ifname = NULL;//内存传递
 	}
 
 	if (virtio_user_dev_setup(dev) < 0) {
@@ -447,6 +456,7 @@ virtio_user_dev_init(struct virtio_user_dev *dev, char *path, int queues,
 	}
 
 	if (!dev->is_server) {
+		//client模式时，设置owner
 		if (dev->ops->send_request(dev, VHOST_USER_SET_OWNER,
 					   NULL) < 0) {
 			PMD_INIT_LOG(ERR, "set_owner fails: %s",
@@ -454,6 +464,7 @@ virtio_user_dev_init(struct virtio_user_dev *dev, char *path, int queues,
 			return -1;
 		}
 
+		//获取对端功能
 		if (dev->ops->send_request(dev, VHOST_USER_GET_FEATURES,
 					   &dev->device_features) < 0) {
 			PMD_INIT_LOG(ERR, "get_features failed: %s",
@@ -583,6 +594,7 @@ virtio_user_handle_mq(struct virtio_user_dev *dev, uint16_t q_pairs)
 	return ret;
 }
 
+//处理收到的控制消息（控制消息存放在desc的idx_hdr描述符下）
 static uint32_t
 virtio_user_handle_ctrl_msg(struct virtio_user_dev *dev, struct vring *vring,
 			    uint16_t idx_hdr)
@@ -607,6 +619,7 @@ virtio_user_handle_ctrl_msg(struct virtio_user_dev *dev, struct vring *vring,
 	n_descs++;
 
 	hdr = (void *)(uintptr_t)vring->desc[idx_hdr].addr;
+	//收到对端发送过来的队列队set命令
 	if (hdr->class == VIRTIO_NET_CTRL_MQ &&
 	    hdr->cmd == VIRTIO_NET_CTRL_MQ_VQ_PAIRS_SET) {
 		uint16_t queues;
@@ -709,6 +722,7 @@ virtio_user_handle_cq_packed(struct virtio_user_dev *dev, uint16_t queue_idx)
 	}
 }
 
+//处理设备dev的queue_idx号vring上的事件
 void
 virtio_user_handle_cq(struct virtio_user_dev *dev, uint16_t queue_idx)
 {
@@ -718,17 +732,23 @@ virtio_user_handle_cq(struct virtio_user_dev *dev, uint16_t queue_idx)
 	struct vring *vring = &dev->vrings[queue_idx];
 
 	/* Consume avail ring, using used ring idx as first one */
+	//如果used->idx与avail->idx不相等，则说明avail->idx中新增了数据
+	//读端维护used->idx,写端维护avail->idx
 	while (vring->used->idx != vring->avail->idx) {
+		//取出未读取的avail_idx及其对应的描述符索引（读端依赖used->idx)
 		avail_idx = (vring->used->idx) & (vring->num - 1);
 		desc_idx = vring->avail->ring[avail_idx];
 
+		//处理消息（并计算有多个描述符被消费）
 		n_descs = virtio_user_handle_ctrl_msg(dev, vring, desc_idx);
 
+		//更新used ring,标记avail_idx号描述符其包含的n_descs个描述符已被使用
 		/* Update used ring */
 		uep = &vring->used->ring[avail_idx];
 		uep->id = desc_idx;
 		uep->len = n_descs;
 
+		//更新已处理的idx
 		vring->used->idx++;
 	}
 }
diff --git a/drivers/net/virtio/virtio_user/virtio_user_dev.h b/drivers/net/virtio/virtio_user/virtio_user_dev.h
index ad8683771..383e11b16 100644
--- a/drivers/net/virtio/virtio_user/virtio_user_dev.h
+++ b/drivers/net/virtio/virtio_user/virtio_user_dev.h
@@ -18,22 +18,25 @@ struct virtio_user_queue {
 
 struct virtio_user_dev {
 	/* for vhost_user backend */
+	//client端时连接的fd
 	int		vhostfd;
+	//server端时监听的fd
 	int		listenfd;   /* listening fd */
+	//是否server模式
 	bool		is_server;  /* server or client mode */
 
 	/* for vhost_kernel backend */
-	char		*ifname;
+	char		*ifname;//设备名称
 	int		*vhostfds;
 	int		*tapfds;
 
 	/* for both vhost_user and vhost_kernel */
 	int		callfds[VIRTIO_MAX_VIRTQUEUES];
 	int		kickfds[VIRTIO_MAX_VIRTQUEUES];
-	int		mac_specified;
-	uint32_t	max_queue_pairs;
-	uint32_t	queue_pairs;
-	uint32_t	queue_size;
+	int		mac_specified;//是否指定了mac地址
+	uint32_t	max_queue_pairs;//最大队列数
+	uint32_t	queue_pairs;//生效队列数
+	uint32_t	queue_size;//队列大小
 	uint64_t	features; /* the negotiated features with driver,
 				   * and will be sync with device
 				   */
@@ -42,8 +45,8 @@ struct virtio_user_dev {
 	uint64_t	unsupported_features; /* unsupported features mask */
 	uint8_t		status;
 	uint16_t	port_id;
-	uint8_t		mac_addr[RTE_ETHER_ADDR_LEN];
-	char		path[PATH_MAX];
+	uint8_t		mac_addr[RTE_ETHER_ADDR_LEN];//设备的mac地址
+	char		path[PATH_MAX];//unix socet位置
 	union {
 		struct vring		vrings[VIRTIO_MAX_VIRTQUEUES];
 		struct vring_packed	packed_vrings[VIRTIO_MAX_VIRTQUEUES];
diff --git a/drivers/net/virtio/virtio_user_ethdev.c b/drivers/net/virtio/virtio_user_ethdev.c
index 3fc172573..93e3433a1 100644
--- a/drivers/net/virtio/virtio_user_ethdev.c
+++ b/drivers/net/virtio/virtio_user_ethdev.c
@@ -111,6 +111,7 @@ virtio_user_delayed_handler(void *param)
 	}
 }
 
+//virtio user配置读取
 static void
 virtio_user_read_dev_config(struct virtio_hw *hw, size_t offset,
 		     void *dst, int length)
@@ -118,6 +119,7 @@ virtio_user_read_dev_config(struct virtio_hw *hw, size_t offset,
 	int i;
 	struct virtio_user_dev *dev = virtio_user_get_dev(hw);
 
+	//检查是否在读取mac地址
 	if (offset == offsetof(struct virtio_net_config, mac) &&
 	    length == RTE_ETHER_ADDR_LEN) {
 		for (i = 0; i < RTE_ETHER_ADDR_LEN; ++i)
@@ -125,10 +127,12 @@ virtio_user_read_dev_config(struct virtio_hw *hw, size_t offset,
 		return;
 	}
 
+	//检查是否读取status
 	if (offset == offsetof(struct virtio_net_config, status)) {
 		char buf[128];
 
 		if (dev->vhostfd >= 0) {
+			//client端
 			int r;
 			int flags;
 
@@ -140,6 +144,7 @@ virtio_user_read_dev_config(struct virtio_hw *hw, size_t offset,
 			}
 			r = recv(dev->vhostfd, buf, 128, MSG_PEEK);
 			if (r == 0 || (r < 0 && errno != EAGAIN)) {
+				//与对端无法连接，置为down状态
 				dev->status &= (~VIRTIO_NET_S_LINK_UP);
 				PMD_DRV_LOG(ERR, "virtio-user port %u is down",
 					    hw->port_id);
@@ -160,6 +165,7 @@ virtio_user_read_dev_config(struct virtio_hw *hw, size_t offset,
 				return;
 			}
 		} else if (dev->is_server) {
+			//server端
 			dev->status &= (~VIRTIO_NET_S_LINK_UP);
 			if (virtio_user_server_reconnect(dev) >= 0)
 				dev->status |= VIRTIO_NET_S_LINK_UP;
@@ -168,10 +174,12 @@ virtio_user_read_dev_config(struct virtio_hw *hw, size_t offset,
 		*(uint16_t *)dst = dev->status;
 	}
 
+	//读取max_virtqueue_pairs配置
 	if (offset == offsetof(struct virtio_net_config, max_virtqueue_pairs))
 		*(uint16_t *)dst = dev->max_queue_pairs;
 }
 
+//写mac地址
 static void
 virtio_user_write_dev_config(struct virtio_hw *hw, size_t offset,
 		      const void *src, int length)
@@ -179,6 +187,7 @@ virtio_user_write_dev_config(struct virtio_hw *hw, size_t offset,
 	int i;
 	struct virtio_user_dev *dev = virtio_user_get_dev(hw);
 
+	//写配置mac地址
 	if ((offset == offsetof(struct virtio_net_config, mac)) &&
 	    (length == RTE_ETHER_ADDR_LEN))
 		for (i = 0; i < RTE_ETHER_ADDR_LEN; ++i)
@@ -356,6 +365,7 @@ virtio_user_notify_queue(struct virtio_hw *hw, struct virtqueue *vq)
 	uint64_t buf = 1;
 	struct virtio_user_dev *dev = virtio_user_get_dev(hw);
 
+	//收到通知，且被通知的队列为控制队列，处理控制消息
 	if (hw->cvq && (hw->cvq->vq == vq)) {
 		if (vtpci_packed_queue(vq->hw))
 			virtio_user_handle_cq_packed(dev, vq->vq_queue_index);
@@ -364,13 +374,16 @@ virtio_user_notify_queue(struct virtio_hw *hw, struct virtqueue *vq)
 		return;
 	}
 
+	//其它队列，则通过kickfds再次触发事件
 	if (write(dev->kickfds[vq->vq_queue_index], &buf, sizeof(buf)) < 0)
 		PMD_DRV_LOG(ERR, "failed to kick backend: %s",
 			    strerror(errno));
 }
 
 const struct virtio_pci_ops virtio_user_ops = {
+	//读配置
 	.read_dev_cfg	= virtio_user_read_dev_config,
+	//写配置
 	.write_dev_cfg	= virtio_user_write_dev_config,
 	.get_status	= virtio_user_get_status,
 	.set_status	= virtio_user_set_status,
@@ -382,9 +395,11 @@ const struct virtio_pci_ops virtio_user_ops = {
 	.get_queue_num	= virtio_user_get_queue_num,
 	.setup_queue	= virtio_user_setup_queue,
 	.del_queue	= virtio_user_del_queue,
+	//通知指定队列（如果是控制队列，则会触发控制队列处理消息）
 	.notify_queue	= virtio_user_notify_queue,
 };
 
+//valid_支持的参数
 static const char *valid_args[] = {
 #define VIRTIO_USER_ARG_QUEUES_NUM     "queues"
 	VIRTIO_USER_ARG_QUEUES_NUM,
@@ -449,6 +464,7 @@ virtio_user_eth_dev_alloc(struct rte_vdev_device *vdev)
 	struct virtio_hw *hw;
 	struct virtio_user_dev *dev;
 
+	//私有数据为virtio_hw
 	eth_dev = rte_eth_vdev_allocate(vdev, sizeof(*hw));
 	if (!eth_dev) {
 		PMD_INIT_LOG(ERR, "cannot alloc rte_eth_dev");
@@ -495,6 +511,7 @@ virtio_user_eth_dev_free(struct rte_eth_dev *eth_dev)
  * EAL init time, see rte_bus_probe().
  * Returns 0 on success.
  */
+//vritio 探测
 static int
 virtio_user_pmd_probe(struct rte_vdev_device *dev)
 {
@@ -539,6 +556,7 @@ virtio_user_pmd_probe(struct rte_vdev_device *dev)
 		goto end;
 	}
 
+	//取参数并填充path
 	if (rte_kvargs_count(kvlist, VIRTIO_USER_ARG_PATH) == 1) {
 		if (rte_kvargs_process(kvlist, VIRTIO_USER_ARG_PATH,
 				       &get_string_arg, &path) < 0) {
@@ -553,6 +571,7 @@ virtio_user_pmd_probe(struct rte_vdev_device *dev)
 	}
 
 	if (rte_kvargs_count(kvlist, VIRTIO_USER_ARG_INTERFACE_NAME) == 1) {
+		//如果配置了iface,则path不能是socket
 		if (is_vhost_user_by_type(path)) {
 			PMD_INIT_LOG(ERR,
 				"arg %s applies only to vhost-kernel backend",
@@ -560,6 +579,7 @@ virtio_user_pmd_probe(struct rte_vdev_device *dev)
 			goto end;
 		}
 
+		//填充ifname
 		if (rte_kvargs_process(kvlist, VIRTIO_USER_ARG_INTERFACE_NAME,
 				       &get_string_arg, &ifname) < 0) {
 			PMD_INIT_LOG(ERR, "error to parse %s",
@@ -568,6 +588,7 @@ virtio_user_pmd_probe(struct rte_vdev_device *dev)
 		}
 	}
 
+	//取参数配置的mac_addr
 	if (rte_kvargs_count(kvlist, VIRTIO_USER_ARG_MAC) == 1) {
 		if (rte_kvargs_process(kvlist, VIRTIO_USER_ARG_MAC,
 				       &get_string_arg, &mac_addr) < 0) {
@@ -577,6 +598,7 @@ virtio_user_pmd_probe(struct rte_vdev_device *dev)
 		}
 	}
 
+	//填充queue_size
 	if (rte_kvargs_count(kvlist, VIRTIO_USER_ARG_QUEUE_SIZE) == 1) {
 		if (rte_kvargs_process(kvlist, VIRTIO_USER_ARG_QUEUE_SIZE,
 				       &get_integer_arg, &queue_size) < 0) {
@@ -586,6 +608,7 @@ virtio_user_pmd_probe(struct rte_vdev_device *dev)
 		}
 	}
 
+	//填充queues
 	if (rte_kvargs_count(kvlist, VIRTIO_USER_ARG_QUEUES_NUM) == 1) {
 		if (rte_kvargs_process(kvlist, VIRTIO_USER_ARG_QUEUES_NUM,
 				       &get_integer_arg, &queues) < 0) {
@@ -595,6 +618,7 @@ virtio_user_pmd_probe(struct rte_vdev_device *dev)
 		}
 	}
 
+	//填充server mode
 	if (rte_kvargs_count(kvlist, VIRTIO_USER_ARG_SERVER_MODE) == 1) {
 		if (rte_kvargs_process(kvlist, VIRTIO_USER_ARG_SERVER_MODE,
 				       &get_integer_arg, &server_mode) < 0) {
@@ -604,6 +628,7 @@ virtio_user_pmd_probe(struct rte_vdev_device *dev)
 		}
 	}
 
+	//填充cq
 	if (rte_kvargs_count(kvlist, VIRTIO_USER_ARG_CQ_NUM) == 1) {
 		if (rte_kvargs_process(kvlist, VIRTIO_USER_ARG_CQ_NUM,
 				       &get_integer_arg, &cq) < 0) {
@@ -629,6 +654,7 @@ virtio_user_pmd_probe(struct rte_vdev_device *dev)
 		goto end;
 	}
 
+	//配置的队列数量过大
 	if (queues > VIRTIO_MAX_VIRTQUEUE_PAIRS) {
 		PMD_INIT_LOG(ERR, "arg %s %" PRIu64 " exceeds the limit %u",
 			VIRTIO_USER_ARG_QUEUES_NUM, queues,
@@ -636,6 +662,7 @@ virtio_user_pmd_probe(struct rte_vdev_device *dev)
 		goto end;
 	}
 
+	//取参数merge_rxbuf
 	if (rte_kvargs_count(kvlist, VIRTIO_USER_ARG_MRG_RXBUF) == 1) {
 		if (rte_kvargs_process(kvlist, VIRTIO_USER_ARG_MRG_RXBUF,
 				       &get_integer_arg, &mrg_rxbuf) < 0) {
@@ -645,6 +672,7 @@ virtio_user_pmd_probe(struct rte_vdev_device *dev)
 		}
 	}
 
+	//取参数in_order
 	if (rte_kvargs_count(kvlist, VIRTIO_USER_ARG_IN_ORDER) == 1) {
 		if (rte_kvargs_process(kvlist, VIRTIO_USER_ARG_IN_ORDER,
 				       &get_integer_arg, &in_order) < 0) {
@@ -716,11 +744,13 @@ virtio_user_pmd_remove(struct rte_vdev_device *vdev)
 	return 0;
 }
 
+//virtio用户driver
 static struct rte_vdev_driver virtio_user_driver = {
 	.probe = virtio_user_pmd_probe,
 	.remove = virtio_user_pmd_remove,
 };
 
+//注册virtio_user驱动
 RTE_PMD_REGISTER_VDEV(net_virtio_user, virtio_user_driver);
 RTE_PMD_REGISTER_ALIAS(net_virtio_user, virtio_user);
 RTE_PMD_REGISTER_PARAM_STRING(net_virtio_user,
diff --git a/drivers/net/virtio/virtqueue.h b/drivers/net/virtio/virtqueue.h
index 8d7f197b1..bdfed69a6 100644
--- a/drivers/net/virtio/virtqueue.h
+++ b/drivers/net/virtio/virtqueue.h
@@ -298,16 +298,19 @@ struct virtqueue {
 struct virtio_net_hdr {
 #define VIRTIO_NET_HDR_F_NEEDS_CSUM 1    /**< Use csum_start,csum_offset*/
 #define VIRTIO_NET_HDR_F_DATA_VALID 2    /**< Checksum is valid */
-	uint8_t flags;
+	uint8_t flags;//是否需要checksum,checksum是否有效
 #define VIRTIO_NET_HDR_GSO_NONE     0    /**< Not a GSO frame */
 #define VIRTIO_NET_HDR_GSO_TCPV4    1    /**< GSO frame, IPv4 TCP (TSO) */
 #define VIRTIO_NET_HDR_GSO_UDP      3    /**< GSO frame, IPv4 UDP (UFO) */
 #define VIRTIO_NET_HDR_GSO_TCPV6    4    /**< GSO frame, IPv6 TCP */
 #define VIRTIO_NET_HDR_GSO_ECN      0x80 /**< TCP has ECN set */
 	uint8_t gso_type;
+	//指向4层负载
 	uint16_t hdr_len;     /**< Ethernet + IP + tcp/udp hdrs */
-	uint16_t gso_size;    /**< Bytes to append to hdr_len per frame */
+	uint16_t gso_size;    /**< Bytes to append to hdr_len per frame */ //gso的大小（例如tso segment)
+	//用来计算到协议字段的起始位置（csum_start＋csum_offset)即可指向checksum字段
 	uint16_t csum_start;  /**< Position to start checksumming from */
+	//到checksum字段需要跳过的偏移（例如跳过tcp checksum时，我们需要偏移过tcp部分开始16个字节，看tcphdr）
 	uint16_t csum_offset; /**< Offset after that to place checksum */
 };
 
@@ -317,6 +320,7 @@ struct virtio_net_hdr {
  */
 struct virtio_net_hdr_mrg_rxbuf {
 	struct   virtio_net_hdr hdr;
+	//指出有多少个buffer可以合并
 	uint16_t num_buffers; /**< Number of merged rx buffers */
 };
 
diff --git a/examples/ip_pipeline/app.h b/examples/ip_pipeline/app.h
new file mode 100644
index 000000000..cb35defd5
--- /dev/null
+++ b/examples/ip_pipeline/app.h
@@ -0,0 +1,1401 @@
+/* SPDX-License-Identifier: BSD-3-Clause
+ * Copyright(c) 2010-2016 Intel Corporation
+ */
+
+#ifndef __INCLUDE_APP_H__
+#define __INCLUDE_APP_H__
+
+#include <stdint.h>
+#include <string.h>
+
+#include <rte_common.h>
+#include <rte_mempool.h>
+#include <rte_ring.h>
+#include <rte_sched.h>
+#include <cmdline_parse.h>
+
+#include <rte_ethdev.h>
+#ifdef RTE_LIBRTE_KNI
+#include <rte_kni.h>
+#endif
+
+#include "cpu_core_map.h"
+#include "pipeline.h"
+
+#define APP_PARAM_NAME_SIZE                      PIPELINE_NAME_SIZE
+#define APP_LINK_PCI_BDF_SIZE                    16
+
+#ifndef APP_LINK_MAX_HWQ_IN
+#define APP_LINK_MAX_HWQ_IN                      128
+#endif
+
+#ifndef APP_LINK_MAX_HWQ_OUT
+#define APP_LINK_MAX_HWQ_OUT                     128
+#endif
+
+struct app_mempool_params {
+	char *name;
+	uint32_t parsed;
+	uint32_t buffer_size;
+	uint32_t pool_size;
+	uint32_t cache_size;
+	uint32_t cpu_socket_id;
+};
+
+struct app_link_params {
+	char *name;//link%d
+	uint32_t parsed;
+	uint32_t pmd_id; /* Generated based on port mask */ //基于port_mask产生的id
+	uint32_t arp_q; /* 0 = Disabled (packets go to default queue 0) */
+	uint32_t tcp_syn_q; /* 0 = Disabled (pkts go to default queue) */
+	uint32_t ip_local_q; /* 0 = Disabled (pkts go to default queue 0) */
+	uint32_t tcp_local_q; /* 0 = Disabled (pkts go to default queue 0) */
+	uint32_t udp_local_q; /* 0 = Disabled (pkts go to default queue 0) */
+	uint32_t sctp_local_q; /* 0 = Disabled (pkts go to default queue 0) */
+	uint32_t rss_qs[APP_LINK_MAX_HWQ_IN];
+	uint32_t n_rss_qs;
+	uint64_t rss_proto_ipv4;
+	uint64_t rss_proto_ipv6;
+	uint64_t rss_proto_l2;
+	uint32_t promisc;
+	uint32_t state; /* DOWN = 0, UP = 1 */
+	uint32_t ip; /* 0 = Invalid */
+	uint32_t depth; /* Valid only when IP is valid */
+	uint64_t mac_addr; /* Read from HW */
+	char pci_bdf[APP_LINK_PCI_BDF_SIZE];
+
+	struct rte_eth_conf conf;
+};
+
+struct app_pktq_hwq_in_params {
+	char *name;
+	uint32_t parsed;
+	uint32_t mempool_id; /* Position in the app->mempool_params */
+	uint32_t size;
+	uint32_t burst;
+
+	struct rte_eth_rxconf conf;
+};
+
+struct app_pktq_hwq_out_params {
+	char *name;
+	uint32_t parsed;
+	uint32_t size;
+	uint32_t burst;
+	uint32_t dropless;
+	uint64_t n_retries;
+	struct rte_eth_txconf conf;
+};
+
+struct app_pktq_swq_params {
+	char *name;
+	uint32_t parsed;
+	uint32_t size;
+	uint32_t burst_read;
+	uint32_t burst_write;
+	uint32_t dropless;
+	uint64_t n_retries;
+	uint32_t cpu_socket_id;
+	uint32_t ipv4_frag;
+	uint32_t ipv6_frag;
+	uint32_t ipv4_ras;
+	uint32_t ipv6_ras;
+	uint32_t mtu;
+	uint32_t metadata_size;
+	uint32_t mempool_direct_id;
+	uint32_t mempool_indirect_id;
+};
+
+struct app_pktq_kni_params {
+	char *name;
+	uint32_t parsed;
+
+	uint32_t socket_id;
+	uint32_t core_id;
+	uint32_t hyper_th_id;
+	uint32_t force_bind;
+
+	uint32_t mempool_id; /* Position in the app->mempool_params */
+	uint32_t burst_read;
+	uint32_t burst_write;
+	uint32_t dropless;
+	uint64_t n_retries;
+};
+
+#ifndef APP_FILE_NAME_SIZE
+#define APP_FILE_NAME_SIZE                       256
+#endif
+
+#ifndef APP_MAX_SCHED_SUBPORTS
+#define APP_MAX_SCHED_SUBPORTS                   8
+#endif
+
+#ifndef APP_MAX_SCHED_PIPES
+#define APP_MAX_SCHED_PIPES                      4096
+#endif
+
+struct app_pktq_tm_params {
+	char *name;
+	uint32_t parsed;
+	const char *file_name;
+	struct rte_sched_port_params sched_port_params;
+	struct rte_sched_subport_params
+		sched_subport_params[APP_MAX_SCHED_SUBPORTS];
+	struct rte_sched_pipe_params
+		sched_pipe_profiles[RTE_SCHED_PIPE_PROFILES_PER_PORT];
+	int sched_pipe_to_profile[APP_MAX_SCHED_SUBPORTS * APP_MAX_SCHED_PIPES];
+	uint32_t burst_read;
+	uint32_t burst_write;
+};
+
+struct app_pktq_tap_params {
+	char *name;
+	uint32_t parsed;
+	uint32_t burst_read;
+	uint32_t burst_write;
+	uint32_t dropless;
+	uint64_t n_retries;
+	uint32_t mempool_id; /* Position in the app->mempool_params */
+};
+
+struct app_pktq_source_params {
+	char *name;
+	uint32_t parsed;
+	uint32_t mempool_id; /* Position in the app->mempool_params array */
+	uint32_t burst;
+	const char *file_name; /* Full path of PCAP file to be copied to mbufs */
+	uint32_t n_bytes_per_pkt;
+};
+
+struct app_pktq_sink_params {
+	char *name;
+	uint8_t parsed;
+	const char *file_name; /* Full path of PCAP file to be copied to mbufs */
+	uint32_t n_pkts_to_dump;
+};
+
+struct app_msgq_params {
+	char *name;
+	uint32_t parsed;
+	uint32_t size;
+	uint32_t cpu_socket_id;
+};
+
+enum app_pktq_in_type {
+	APP_PKTQ_IN_HWQ,
+	APP_PKTQ_IN_SWQ,
+	APP_PKTQ_IN_TM,
+	APP_PKTQ_IN_TAP,
+	APP_PKTQ_IN_KNI,
+	APP_PKTQ_IN_SOURCE,
+};
+
+struct app_pktq_in_params {
+	enum app_pktq_in_type type;
+	uint32_t id; /* Position in the appropriate app array */
+};
+
+enum app_pktq_out_type {
+	APP_PKTQ_OUT_HWQ,
+	APP_PKTQ_OUT_SWQ,
+	APP_PKTQ_OUT_TM,
+	APP_PKTQ_OUT_TAP,
+	APP_PKTQ_OUT_KNI,
+	APP_PKTQ_OUT_SINK,
+};
+
+struct app_pktq_out_params {
+	enum app_pktq_out_type type;
+	uint32_t id; /* Position in the appropriate app array */
+};
+
+#define APP_PIPELINE_TYPE_SIZE                   PIPELINE_TYPE_SIZE
+
+#define APP_MAX_PIPELINE_PKTQ_IN                 PIPELINE_MAX_PORT_IN
+#define APP_MAX_PIPELINE_PKTQ_OUT                PIPELINE_MAX_PORT_OUT
+#define APP_MAX_PIPELINE_MSGQ_IN                 PIPELINE_MAX_MSGQ_IN
+#define APP_MAX_PIPELINE_MSGQ_OUT                PIPELINE_MAX_MSGQ_OUT
+
+#define APP_MAX_PIPELINE_ARGS                    PIPELINE_MAX_ARGS
+
+struct app_pipeline_params {
+	char *name;
+	uint8_t parsed;
+
+	char type[APP_PIPELINE_TYPE_SIZE];
+
+	uint32_t socket_id;
+	uint32_t core_id;
+	uint32_t hyper_th_id;
+
+	struct app_pktq_in_params pktq_in[APP_MAX_PIPELINE_PKTQ_IN];
+	struct app_pktq_out_params pktq_out[APP_MAX_PIPELINE_PKTQ_OUT];
+	uint32_t msgq_in[APP_MAX_PIPELINE_MSGQ_IN];
+	uint32_t msgq_out[APP_MAX_PIPELINE_MSGQ_OUT];
+
+	uint32_t n_pktq_in;
+	uint32_t n_pktq_out;
+	uint32_t n_msgq_in;
+	uint32_t n_msgq_out;
+
+	uint32_t timer_period;
+
+	char *args_name[APP_MAX_PIPELINE_ARGS];
+	char *args_value[APP_MAX_PIPELINE_ARGS];
+	uint32_t n_args;
+};
+
+struct app_params;
+
+typedef void (*app_link_op)(struct app_params *app,
+	uint32_t link_id,
+	uint32_t up,
+	void *arg);
+
+#ifndef APP_MAX_PIPELINES
+#define APP_MAX_PIPELINES                        64
+#endif
+
+struct app_link_data {
+	app_link_op f_link[APP_MAX_PIPELINES];
+	void *arg[APP_MAX_PIPELINES];
+};
+
+struct app_pipeline_data {
+	void *be;
+	void *fe;
+	struct pipeline_type *ptype;
+	uint64_t timer_period;
+	uint32_t enabled;
+};
+
+struct app_thread_pipeline_data {
+	uint32_t pipeline_id;
+	void *be;
+	pipeline_be_op_run f_run;
+	pipeline_be_op_timer f_timer;
+	uint64_t timer_period;
+	uint64_t deadline;
+};
+
+#ifndef APP_MAX_THREAD_PIPELINES
+#define APP_MAX_THREAD_PIPELINES                 64
+#endif
+
+#ifndef APP_THREAD_TIMER_PERIOD
+#define APP_THREAD_TIMER_PERIOD                  1
+#endif
+
+struct app_thread_data {
+	struct app_thread_pipeline_data regular[APP_MAX_THREAD_PIPELINES];
+	struct app_thread_pipeline_data custom[APP_MAX_THREAD_PIPELINES];
+
+	uint32_t n_regular;
+	uint32_t n_custom;
+
+	uint64_t timer_period;
+	uint64_t thread_req_deadline;
+
+	uint64_t deadline;
+
+	struct rte_ring *msgq_in;
+	struct rte_ring *msgq_out;
+
+	uint64_t headroom_time;
+	uint64_t headroom_cycles;
+	double headroom_ratio;
+} __rte_cache_aligned;
+
+#ifndef APP_MAX_LINKS
+#define APP_MAX_LINKS                            16
+#endif
+
+struct app_eal_params {
+	/* Map lcore set to physical cpu set */
+	char *coremap;
+
+	/* Core ID that is used as master */
+	uint32_t master_lcore_present;
+	uint32_t master_lcore;
+
+	/* Number of memory channels */
+	uint32_t channels_present;
+	uint32_t channels;
+
+	/* Memory to allocate (see also --socket-mem) */
+	uint32_t memory_present;
+	uint32_t memory;
+
+	/* Force number of memory ranks (don't detect) */
+	uint32_t ranks_present;
+	uint32_t ranks;
+
+	/* Add a PCI device in black list. */
+	char *pci_blacklist[APP_MAX_LINKS];
+
+	/* Add a PCI device in white list. */
+	char *pci_whitelist[APP_MAX_LINKS];
+
+	/* Add a virtual device. */
+	char *vdev[APP_MAX_LINKS];
+
+	 /* Use VMware TSC map instead of native RDTSC */
+	uint32_t vmware_tsc_map_present;
+	int vmware_tsc_map;
+
+	 /* Type of this process (primary|secondary|auto) */
+	char *proc_type;
+
+	 /* Set syslog facility */
+	char *syslog;
+
+	/* Set default log level */
+	uint32_t log_level_present;
+	uint32_t log_level;
+
+	/* Display version information on startup */
+	uint32_t version_present;
+	int version;
+
+	/* This help */
+	uint32_t help_present;
+	int help;
+
+	 /* Use malloc instead of hugetlbfs */
+	uint32_t no_huge_present;
+	int no_huge;
+
+	/* Disable PCI */
+	uint32_t no_pci_present;
+	int no_pci;
+
+	/* Disable HPET */
+	uint32_t no_hpet_present;
+	int no_hpet;
+
+	/* No shared config (mmap'd files) */
+	uint32_t no_shconf_present;
+	int no_shconf;
+
+	/* Add driver */
+	char *add_driver;
+
+	/*  Memory to allocate on sockets (comma separated values)*/
+	char *socket_mem;
+
+	/* Directory where hugetlbfs is mounted */
+	char *huge_dir;
+
+	/* Prefix for hugepage filenames */
+	char *file_prefix;
+
+	/* Base virtual address */
+	char *base_virtaddr;
+
+	/* Create /dev/uioX (usually done by hotplug) */
+	uint32_t create_uio_dev_present;
+	int create_uio_dev;
+
+	/* Interrupt mode for VFIO (legacy|msi|msix) */
+	char *vfio_intr;
+
+	uint32_t parsed;
+};
+
+#ifndef APP_APPNAME_SIZE
+#define APP_APPNAME_SIZE                         256
+#endif
+
+#ifndef APP_MAX_MEMPOOLS
+#define APP_MAX_MEMPOOLS                         8
+#endif
+
+#define APP_MAX_HWQ_IN                  (APP_MAX_LINKS * APP_LINK_MAX_HWQ_IN)
+
+#define APP_MAX_HWQ_OUT                 (APP_MAX_LINKS * APP_LINK_MAX_HWQ_OUT)
+
+#ifndef APP_MAX_PKTQ_SWQ
+#define APP_MAX_PKTQ_SWQ                         256
+#endif
+
+#define APP_MAX_PKTQ_TM                          APP_MAX_LINKS
+
+#ifndef APP_MAX_PKTQ_TAP
+#define APP_MAX_PKTQ_TAP                         APP_MAX_LINKS
+#endif
+
+#define APP_MAX_PKTQ_KNI                         APP_MAX_LINKS
+
+#ifndef APP_MAX_PKTQ_SOURCE
+#define APP_MAX_PKTQ_SOURCE                      64
+#endif
+
+#ifndef APP_MAX_PKTQ_SINK
+#define APP_MAX_PKTQ_SINK                        64
+#endif
+
+#ifndef APP_MAX_MSGQ
+#define APP_MAX_MSGQ                             256
+#endif
+
+#ifndef APP_EAL_ARGC
+#define APP_EAL_ARGC                             64
+#endif
+
+#ifndef APP_MAX_PIPELINE_TYPES
+#define APP_MAX_PIPELINE_TYPES                   64
+#endif
+
+#ifndef APP_MAX_THREADS
+#define APP_MAX_THREADS                          RTE_MAX_LCORE
+#endif
+
+#ifndef APP_MAX_CMDS
+#define APP_MAX_CMDS                             64
+#endif
+
+#ifndef APP_THREAD_HEADROOM_STATS_COLLECT
+#define APP_THREAD_HEADROOM_STATS_COLLECT        1
+#endif
+
+#define APP_CORE_MASK_SIZE					\
+	(RTE_MAX_LCORE / 64 + ((RTE_MAX_LCORE % 64) ? 1 : 0))
+
+struct app_params {
+	/* Config */
+	char app_name[APP_APPNAME_SIZE];
+	const char *config_file;
+	const char *script_file;
+	const char *parser_file;
+	const char *output_file;
+	const char *preproc;
+	const char *preproc_args;
+	uint64_t port_mask;
+	uint32_t log_level;
+
+	struct app_eal_params eal_params;
+	struct app_mempool_params mempool_params[APP_MAX_MEMPOOLS];
+	struct app_link_params link_params[APP_MAX_LINKS];
+	struct app_pktq_hwq_in_params hwq_in_params[APP_MAX_HWQ_IN];
+	struct app_pktq_hwq_out_params hwq_out_params[APP_MAX_HWQ_OUT];
+	struct app_pktq_swq_params swq_params[APP_MAX_PKTQ_SWQ];
+	struct app_pktq_tm_params tm_params[APP_MAX_PKTQ_TM];
+	struct app_pktq_tap_params tap_params[APP_MAX_PKTQ_TAP];
+	struct app_pktq_kni_params kni_params[APP_MAX_PKTQ_KNI];
+	struct app_pktq_source_params source_params[APP_MAX_PKTQ_SOURCE];
+	struct app_pktq_sink_params sink_params[APP_MAX_PKTQ_SINK];
+	struct app_msgq_params msgq_params[APP_MAX_MSGQ];
+	struct app_pipeline_params pipeline_params[APP_MAX_PIPELINES];
+
+	uint32_t n_mempools;
+	uint32_t n_links;
+	uint32_t n_pktq_hwq_in;
+	uint32_t n_pktq_hwq_out;
+	uint32_t n_pktq_swq;
+	uint32_t n_pktq_tm;
+	uint32_t n_pktq_tap;
+	uint32_t n_pktq_kni;
+	uint32_t n_pktq_source;
+	uint32_t n_pktq_sink;
+	uint32_t n_msgq;
+	uint32_t n_pipelines;
+
+	/* Init */
+	char *eal_argv[1 + APP_EAL_ARGC];
+	struct cpu_core_map *core_map;
+	uint64_t core_mask[APP_CORE_MASK_SIZE];
+	struct rte_mempool *mempool[APP_MAX_MEMPOOLS];
+	struct app_link_data link_data[APP_MAX_LINKS];
+	struct rte_ring *swq[APP_MAX_PKTQ_SWQ];
+	struct rte_sched_port *tm[APP_MAX_PKTQ_TM];
+	int tap[APP_MAX_PKTQ_TAP];
+#ifdef RTE_LIBRTE_KNI
+	struct rte_kni *kni[APP_MAX_PKTQ_KNI];
+#endif /* RTE_LIBRTE_KNI */
+	struct rte_ring *msgq[APP_MAX_MSGQ];
+	struct pipeline_type pipeline_type[APP_MAX_PIPELINE_TYPES];
+	struct app_pipeline_data pipeline_data[APP_MAX_PIPELINES];
+	struct app_thread_data thread_data[APP_MAX_THREADS];
+	cmdline_parse_ctx_t cmds[APP_MAX_CMDS + 1];
+
+	int eal_argc;
+	uint32_t n_pipeline_types;
+	uint32_t n_cmds;
+};
+
+#define APP_PARAM_VALID(obj) ((obj)->name != NULL)
+
+#define APP_PARAM_COUNT(obj_array, n_objs)				\
+{									\
+	size_t i;							\
+									\
+	n_objs = 0;							\
+	for (i = 0; i < RTE_DIM(obj_array); i++)			\
+		if (APP_PARAM_VALID(&((obj_array)[i])))			\
+			n_objs++;					\
+}
+
+#define APP_PARAM_FIND(obj_array, key)					\
+({									\
+	ssize_t obj_idx;						\
+	const ssize_t obj_count = RTE_DIM(obj_array);			\
+									\
+	for (obj_idx = 0; obj_idx < obj_count; obj_idx++) {		\
+		if (!APP_PARAM_VALID(&((obj_array)[obj_idx])))		\
+			continue;					\
+									\
+		if (strcmp(key, (obj_array)[obj_idx].name) == 0)	\
+			break;						\
+	}								\
+	obj_idx < obj_count ? obj_idx : -ENOENT;			\
+})
+
+#define APP_PARAM_FIND_BY_ID(obj_array, prefix, id, obj)		\
+do {									\
+	char name[APP_PARAM_NAME_SIZE];					\
+	ssize_t pos;							\
+									\
+	sprintf(name, prefix "%" PRIu32, id);				\
+	pos = APP_PARAM_FIND(obj_array, name);				\
+	obj = (pos < 0) ? NULL : &((obj_array)[pos]);			\
+} while (0)
+
+#define APP_PARAM_GET_ID(obj, prefix, id)				\
+do									\
+	sscanf(obj->name, prefix "%" SCNu32, &id);				\
+while (0)								\
+
+#define	APP_CHECK(exp, fmt, ...)					\
+do {									\
+	if (!(exp)) {							\
+		fprintf(stderr, fmt "\n", ## __VA_ARGS__);		\
+		abort();						\
+	}								\
+} while (0)
+
+enum app_log_level {
+	APP_LOG_LEVEL_HIGH = 1,
+	APP_LOG_LEVEL_LOW,
+	APP_LOG_LEVELS
+};
+
+#define APP_LOG(app, level, fmt, ...)					\
+do {									\
+	if (app->log_level >= APP_LOG_LEVEL_ ## level)			\
+		fprintf(stdout, "[APP] " fmt "\n", ## __VA_ARGS__);	\
+} while (0)
+
+static inline uint32_t
+app_link_get_n_rxq(struct app_params *app, struct app_link_params *link)
+{
+	uint32_t n_rxq = 0, link_id, i;
+	uint32_t n_pktq_hwq_in = RTE_MIN(app->n_pktq_hwq_in,
+		RTE_DIM(app->hwq_in_params));
+
+	APP_PARAM_GET_ID(link, "LINK", link_id);
+
+	for (i = 0; i < n_pktq_hwq_in; i++) {
+		struct app_pktq_hwq_in_params *p = &app->hwq_in_params[i];
+		uint32_t rxq_link_id, rxq_queue_id;
+
+		sscanf(p->name, "RXQ%" SCNu32 ".%" SCNu32,
+			&rxq_link_id, &rxq_queue_id);
+		if (rxq_link_id == link_id)
+			n_rxq++;
+	}
+
+	return n_rxq;
+}
+
+static inline uint32_t
+app_link_get_n_txq(struct app_params *app, struct app_link_params *link)
+{
+	uint32_t n_txq = 0, link_id, i;
+	uint32_t n_pktq_hwq_out = RTE_MIN(app->n_pktq_hwq_out,
+		RTE_DIM(app->hwq_out_params));
+
+	APP_PARAM_GET_ID(link, "LINK", link_id);
+
+	for (i = 0; i < n_pktq_hwq_out; i++) {
+		struct app_pktq_hwq_out_params *p = &app->hwq_out_params[i];
+		uint32_t txq_link_id, txq_queue_id;
+
+		sscanf(p->name, "TXQ%" SCNu32 ".%" SCNu32,
+			&txq_link_id, &txq_queue_id);
+		if (txq_link_id == link_id)
+			n_txq++;
+	}
+
+	return n_txq;
+}
+
+static inline uint32_t
+app_rxq_get_readers(struct app_params *app, struct app_pktq_hwq_in_params *rxq)
+{
+	uint32_t pos = rxq - app->hwq_in_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_readers = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_in = RTE_MIN(p->n_pktq_in, RTE_DIM(p->pktq_in));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_in; j++) {
+			struct app_pktq_in_params *pktq = &p->pktq_in[j];
+
+			if ((pktq->type == APP_PKTQ_IN_HWQ) &&
+				(pktq->id == pos))
+				n_readers++;
+		}
+	}
+
+	return n_readers;
+}
+
+static inline uint32_t
+app_swq_get_readers(struct app_params *app, struct app_pktq_swq_params *swq)
+{
+	uint32_t pos = swq - app->swq_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_readers = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_in = RTE_MIN(p->n_pktq_in, RTE_DIM(p->pktq_in));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_in; j++) {
+			struct app_pktq_in_params *pktq = &p->pktq_in[j];
+
+			if ((pktq->type == APP_PKTQ_IN_SWQ) &&
+				(pktq->id == pos))
+				n_readers++;
+		}
+	}
+
+	return n_readers;
+}
+
+static inline struct app_pipeline_params *
+app_swq_get_reader(struct app_params *app,
+	struct app_pktq_swq_params *swq,
+	uint32_t *pktq_in_id)
+{
+	struct app_pipeline_params *reader = NULL;
+	uint32_t pos = swq - app->swq_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_readers = 0, id = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_in = RTE_MIN(p->n_pktq_in, RTE_DIM(p->pktq_in));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_in; j++) {
+			struct app_pktq_in_params *pktq = &p->pktq_in[j];
+
+			if ((pktq->type == APP_PKTQ_IN_SWQ) &&
+				(pktq->id == pos)) {
+				n_readers++;
+				reader = p;
+				id = j;
+			}
+		}
+	}
+
+	if (n_readers != 1)
+		return NULL;
+
+	*pktq_in_id = id;
+	return reader;
+}
+
+static inline uint32_t
+app_tm_get_readers(struct app_params *app, struct app_pktq_tm_params *tm)
+{
+	uint32_t pos = tm - app->tm_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_readers = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_in = RTE_MIN(p->n_pktq_in, RTE_DIM(p->pktq_in));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_in; j++) {
+			struct app_pktq_in_params *pktq = &p->pktq_in[j];
+
+			if ((pktq->type == APP_PKTQ_IN_TM) &&
+				(pktq->id == pos))
+				n_readers++;
+		}
+	}
+
+	return n_readers;
+}
+
+static inline struct app_pipeline_params *
+app_tm_get_reader(struct app_params *app,
+	struct app_pktq_tm_params *tm,
+	uint32_t *pktq_in_id)
+{
+	struct app_pipeline_params *reader = NULL;
+	uint32_t pos = tm - app->tm_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_readers = 0, id = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_in = RTE_MIN(p->n_pktq_in, RTE_DIM(p->pktq_in));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_in; j++) {
+			struct app_pktq_in_params *pktq = &p->pktq_in[j];
+
+			if ((pktq->type == APP_PKTQ_IN_TM) &&
+				(pktq->id == pos)) {
+				n_readers++;
+				reader = p;
+				id = j;
+			}
+		}
+	}
+
+	if (n_readers != 1)
+		return NULL;
+
+	*pktq_in_id = id;
+	return reader;
+}
+
+static inline uint32_t
+app_tap_get_readers(struct app_params *app, struct app_pktq_tap_params *tap)
+{
+	uint32_t pos = tap - app->tap_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_readers = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_in = RTE_MIN(p->n_pktq_in, RTE_DIM(p->pktq_in));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_in; j++) {
+			struct app_pktq_in_params *pktq = &p->pktq_in[j];
+
+			if ((pktq->type == APP_PKTQ_IN_TAP) &&
+				(pktq->id == pos))
+				n_readers++;
+		}
+	}
+
+	return n_readers;
+}
+
+static inline struct app_pipeline_params *
+app_tap_get_reader(struct app_params *app,
+	struct app_pktq_tap_params *tap,
+	uint32_t *pktq_in_id)
+{
+	struct app_pipeline_params *reader = NULL;
+	uint32_t pos = tap - app->tap_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_readers = 0, id = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_in = RTE_MIN(p->n_pktq_in, RTE_DIM(p->pktq_in));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_in; j++) {
+			struct app_pktq_in_params *pktq = &p->pktq_in[j];
+
+			if ((pktq->type == APP_PKTQ_IN_TAP) &&
+				(pktq->id == pos)) {
+				n_readers++;
+				reader = p;
+				id = j;
+			}
+		}
+	}
+
+	if (n_readers != 1)
+		return NULL;
+
+	*pktq_in_id = id;
+	return reader;
+}
+
+static inline uint32_t
+app_kni_get_readers(struct app_params *app, struct app_pktq_kni_params *kni)
+{
+	uint32_t pos = kni - app->kni_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_readers = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_in = RTE_MIN(p->n_pktq_in, RTE_DIM(p->pktq_in));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_in; j++) {
+			struct app_pktq_in_params *pktq = &p->pktq_in[j];
+
+			if ((pktq->type == APP_PKTQ_IN_KNI) &&
+				(pktq->id == pos))
+				n_readers++;
+		}
+	}
+
+	return n_readers;
+}
+
+static inline struct app_pipeline_params *
+app_kni_get_reader(struct app_params *app,
+				  struct app_pktq_kni_params *kni,
+				  uint32_t *pktq_in_id)
+{
+	struct app_pipeline_params *reader = NULL;
+	uint32_t pos = kni - app->kni_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_readers = 0, id = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_in = RTE_MIN(p->n_pktq_in, RTE_DIM(p->pktq_in));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_in; j++) {
+			struct app_pktq_in_params *pktq = &p->pktq_in[j];
+
+			if ((pktq->type == APP_PKTQ_IN_KNI) &&
+				(pktq->id == pos)) {
+				n_readers++;
+				reader = p;
+				id = j;
+			}
+		}
+	}
+
+	if (n_readers != 1)
+		return NULL;
+
+	*pktq_in_id = id;
+	return reader;
+}
+
+static inline uint32_t
+app_source_get_readers(struct app_params *app,
+struct app_pktq_source_params *source)
+{
+	uint32_t pos = source - app->source_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_readers = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_in = RTE_MIN(p->n_pktq_in, RTE_DIM(p->pktq_in));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_in; j++) {
+			struct app_pktq_in_params *pktq = &p->pktq_in[j];
+
+			if ((pktq->type == APP_PKTQ_IN_SOURCE) &&
+				(pktq->id == pos))
+				n_readers++;
+		}
+	}
+
+	return n_readers;
+}
+
+static inline uint32_t
+app_msgq_get_readers(struct app_params *app, struct app_msgq_params *msgq)
+{
+	uint32_t pos = msgq - app->msgq_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_readers = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_msgq_in = RTE_MIN(p->n_msgq_in, RTE_DIM(p->msgq_in));
+		uint32_t j;
+
+		for (j = 0; j < n_msgq_in; j++)
+			if (p->msgq_in[j] == pos)
+				n_readers++;
+	}
+
+	return n_readers;
+}
+
+static inline uint32_t
+app_txq_get_writers(struct app_params *app, struct app_pktq_hwq_out_params *txq)
+{
+	uint32_t pos = txq - app->hwq_out_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_writers = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_out = RTE_MIN(p->n_pktq_out,
+			RTE_DIM(p->pktq_out));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_out; j++) {
+			struct app_pktq_out_params *pktq = &p->pktq_out[j];
+
+			if ((pktq->type == APP_PKTQ_OUT_HWQ) &&
+				(pktq->id == pos))
+				n_writers++;
+		}
+	}
+
+	return n_writers;
+}
+
+static inline uint32_t
+app_swq_get_writers(struct app_params *app, struct app_pktq_swq_params *swq)
+{
+	uint32_t pos = swq - app->swq_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_writers = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_out = RTE_MIN(p->n_pktq_out,
+			RTE_DIM(p->pktq_out));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_out; j++) {
+			struct app_pktq_out_params *pktq = &p->pktq_out[j];
+
+			if ((pktq->type == APP_PKTQ_OUT_SWQ) &&
+				(pktq->id == pos))
+				n_writers++;
+		}
+	}
+
+	return n_writers;
+}
+
+static inline struct app_pipeline_params *
+app_swq_get_writer(struct app_params *app,
+	struct app_pktq_swq_params *swq,
+	uint32_t *pktq_out_id)
+{
+	struct app_pipeline_params *writer = NULL;
+	uint32_t pos = swq - app->swq_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_writers = 0, id = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_out = RTE_MIN(p->n_pktq_out,
+			RTE_DIM(p->pktq_out));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_out; j++) {
+			struct app_pktq_out_params *pktq = &p->pktq_out[j];
+
+			if ((pktq->type == APP_PKTQ_OUT_SWQ) &&
+				(pktq->id == pos)) {
+				n_writers++;
+				writer = p;
+				id = j;
+			}
+		}
+	}
+
+	if (n_writers != 1)
+		return NULL;
+
+	*pktq_out_id = id;
+	return writer;
+}
+
+static inline uint32_t
+app_tm_get_writers(struct app_params *app, struct app_pktq_tm_params *tm)
+{
+	uint32_t pos = tm - app->tm_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_writers = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_out = RTE_MIN(p->n_pktq_out,
+			RTE_DIM(p->pktq_out));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_out; j++) {
+			struct app_pktq_out_params *pktq = &p->pktq_out[j];
+
+			if ((pktq->type == APP_PKTQ_OUT_TM) &&
+				(pktq->id == pos))
+				n_writers++;
+		}
+	}
+
+	return n_writers;
+}
+
+static inline struct app_pipeline_params *
+app_tm_get_writer(struct app_params *app,
+	struct app_pktq_tm_params *tm,
+	uint32_t *pktq_out_id)
+{
+	struct app_pipeline_params *writer = NULL;
+	uint32_t pos = tm - app->tm_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_writers = 0, id = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_out = RTE_MIN(p->n_pktq_out,
+			RTE_DIM(p->pktq_out));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_out; j++) {
+			struct app_pktq_out_params *pktq = &p->pktq_out[j];
+
+			if ((pktq->type == APP_PKTQ_OUT_TM) &&
+				(pktq->id == pos)) {
+				n_writers++;
+				writer = p;
+				id = j;
+			}
+		}
+	}
+
+	if (n_writers != 1)
+		return NULL;
+
+	*pktq_out_id = id;
+	return writer;
+}
+
+static inline uint32_t
+app_tap_get_writers(struct app_params *app, struct app_pktq_tap_params *tap)
+{
+	uint32_t pos = tap - app->tap_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_writers = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_out = RTE_MIN(p->n_pktq_out,
+			RTE_DIM(p->pktq_out));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_out; j++) {
+			struct app_pktq_out_params *pktq = &p->pktq_out[j];
+
+		if ((pktq->type == APP_PKTQ_OUT_TAP) &&
+			(pktq->id == pos))
+			n_writers++;
+		}
+	}
+
+	return n_writers;
+}
+
+static inline struct app_pipeline_params *
+app_tap_get_writer(struct app_params *app,
+	struct app_pktq_tap_params *tap,
+	uint32_t *pktq_out_id)
+{
+	struct app_pipeline_params *writer = NULL;
+	uint32_t pos = tap - app->tap_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_writers = 0, id = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_out = RTE_MIN(p->n_pktq_out,
+			RTE_DIM(p->pktq_out));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_out; j++) {
+			struct app_pktq_out_params *pktq = &p->pktq_out[j];
+
+			if ((pktq->type == APP_PKTQ_OUT_TAP) &&
+				(pktq->id == pos)) {
+				n_writers++;
+				writer = p;
+				id = j;
+			}
+		}
+	}
+
+	if (n_writers != 1)
+		return NULL;
+
+	*pktq_out_id = id;
+	return writer;
+}
+
+static inline uint32_t
+app_kni_get_writers(struct app_params *app, struct app_pktq_kni_params *kni)
+{
+	uint32_t pos = kni - app->kni_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_writers = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_out = RTE_MIN(p->n_pktq_out,
+			RTE_DIM(p->pktq_out));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_out; j++) {
+			struct app_pktq_out_params *pktq = &p->pktq_out[j];
+
+			if ((pktq->type == APP_PKTQ_OUT_KNI) &&
+				(pktq->id == pos))
+				n_writers++;
+		}
+	}
+
+	return n_writers;
+}
+
+static inline struct app_pipeline_params *
+app_kni_get_writer(struct app_params *app,
+				  struct app_pktq_kni_params *kni,
+				  uint32_t *pktq_out_id)
+{
+	struct app_pipeline_params *writer = NULL;
+	uint32_t pos = kni - app->kni_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_writers = 0, id = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_out = RTE_MIN(p->n_pktq_out,
+			RTE_DIM(p->pktq_out));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_out; j++) {
+			struct app_pktq_out_params *pktq = &p->pktq_out[j];
+
+			if ((pktq->type == APP_PKTQ_OUT_KNI) &&
+				(pktq->id == pos)) {
+				n_writers++;
+				writer = p;
+				id = j;
+			}
+		}
+	}
+
+	if (n_writers != 1)
+		return NULL;
+
+	*pktq_out_id = id;
+	return writer;
+}
+
+static inline uint32_t
+app_sink_get_writers(struct app_params *app, struct app_pktq_sink_params *sink)
+{
+	uint32_t pos = sink - app->sink_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_writers = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_pktq_out = RTE_MIN(p->n_pktq_out,
+			RTE_DIM(p->pktq_out));
+		uint32_t j;
+
+		for (j = 0; j < n_pktq_out; j++) {
+			struct app_pktq_out_params *pktq = &p->pktq_out[j];
+
+			if ((pktq->type == APP_PKTQ_OUT_SINK) &&
+				(pktq->id == pos))
+				n_writers++;
+		}
+	}
+
+	return n_writers;
+}
+
+static inline uint32_t
+app_msgq_get_writers(struct app_params *app, struct app_msgq_params *msgq)
+{
+	uint32_t pos = msgq - app->msgq_params;
+	uint32_t n_pipelines = RTE_MIN(app->n_pipelines,
+		RTE_DIM(app->pipeline_params));
+	uint32_t n_writers = 0, i;
+
+	for (i = 0; i < n_pipelines; i++) {
+		struct app_pipeline_params *p = &app->pipeline_params[i];
+		uint32_t n_msgq_out = RTE_MIN(p->n_msgq_out,
+			RTE_DIM(p->msgq_out));
+		uint32_t j;
+
+		for (j = 0; j < n_msgq_out; j++)
+			if (p->msgq_out[j] == pos)
+				n_writers++;
+	}
+
+	return n_writers;
+}
+
+static inline struct app_link_params *
+app_get_link_for_rxq(struct app_params *app, struct app_pktq_hwq_in_params *p)
+{
+	char link_name[APP_PARAM_NAME_SIZE];
+	ssize_t link_param_idx;
+	uint32_t rxq_link_id, rxq_queue_id;
+
+	sscanf(p->name, "RXQ%" SCNu32 ".%" SCNu32,
+		&rxq_link_id, &rxq_queue_id);
+	sprintf(link_name, "LINK%" PRIu32, rxq_link_id);
+	link_param_idx = APP_PARAM_FIND(app->link_params, link_name);
+	APP_CHECK((link_param_idx >= 0),
+		"Cannot find %s for %s", link_name, p->name);
+
+	return &app->link_params[link_param_idx];
+}
+
+static inline struct app_link_params *
+app_get_link_for_txq(struct app_params *app, struct app_pktq_hwq_out_params *p)
+{
+	char link_name[APP_PARAM_NAME_SIZE];
+	ssize_t link_param_idx;
+	uint32_t txq_link_id, txq_queue_id;
+
+	sscanf(p->name, "TXQ%" SCNu32 ".%" SCNu32,
+		&txq_link_id, &txq_queue_id);
+	sprintf(link_name, "LINK%" PRIu32, txq_link_id);
+	link_param_idx = APP_PARAM_FIND(app->link_params, link_name);
+	APP_CHECK((link_param_idx >= 0),
+		"Cannot find %s for %s", link_name, p->name);
+
+	return &app->link_params[link_param_idx];
+}
+
+static inline struct app_link_params *
+app_get_link_for_tm(struct app_params *app, struct app_pktq_tm_params *p_tm)
+{
+	char link_name[APP_PARAM_NAME_SIZE];
+	uint32_t link_id;
+	ssize_t link_param_idx;
+
+	sscanf(p_tm->name, "TM%" PRIu32, &link_id);
+	sprintf(link_name, "LINK%" PRIu32, link_id);
+	link_param_idx = APP_PARAM_FIND(app->link_params, link_name);
+	APP_CHECK((link_param_idx >= 0),
+		"Cannot find %s for %s", link_name, p_tm->name);
+
+	return &app->link_params[link_param_idx];
+}
+
+static inline struct app_link_params *
+app_get_link_for_kni(struct app_params *app, struct app_pktq_kni_params *p_kni)
+{
+	char link_name[APP_PARAM_NAME_SIZE];
+	uint32_t link_id;
+	ssize_t link_param_idx;
+
+	sscanf(p_kni->name, "KNI%" PRIu32, &link_id);
+	sprintf(link_name, "LINK%" PRIu32, link_id);
+	link_param_idx = APP_PARAM_FIND(app->link_params, link_name);
+	APP_CHECK((link_param_idx >= 0),
+			  "Cannot find %s for %s", link_name, p_kni->name);
+
+	return &app->link_params[link_param_idx];
+}
+
+static inline uint32_t
+app_core_is_enabled(struct app_params *app, uint32_t lcore_id)
+{
+	return(app->core_mask[lcore_id / 64] &
+		(1LLU << (lcore_id % 64)));
+}
+
+static inline void
+app_core_enable_in_core_mask(struct app_params *app, int lcore_id)
+{
+	app->core_mask[lcore_id / 64] |= 1LLU << (lcore_id % 64);
+
+}
+
+static inline void
+app_core_build_core_mask_string(struct app_params *app, char *mask_buffer)
+{
+	int i;
+
+	mask_buffer[0] = '\0';
+	for (i = (int)RTE_DIM(app->core_mask); i > 0; i--) {
+		/* For Hex representation of bits in uint64_t */
+		char buffer[(64 / 8) * 2 + 1];
+		memset(buffer, 0, sizeof(buffer));
+		snprintf(buffer, sizeof(buffer), "%016" PRIx64,
+			 app->core_mask[i-1]);
+		strcat(mask_buffer, buffer);
+	}
+}
+
+void app_pipeline_params_get(struct app_params *app,
+	struct app_pipeline_params *p_in,
+	struct pipeline_params *p_out);
+
+int app_config_init(struct app_params *app);
+
+int app_config_args(struct app_params *app,
+	int argc, char **argv);
+
+int app_config_preproc(struct app_params *app);
+
+int app_config_parse(struct app_params *app,
+	const char *file_name);
+
+int app_config_parse_tm(struct app_params *app);
+
+void app_config_save(struct app_params *app,
+	const char *file_name);
+
+int app_config_check(struct app_params *app);
+
+int app_init(struct app_params *app);
+
+int app_post_init(struct app_params *app);
+
+int app_thread(void *arg);
+
+int app_pipeline_type_register(struct app_params *app,
+	struct pipeline_type *ptype);
+
+struct pipeline_type *app_pipeline_type_find(struct app_params *app,
+	char *name);
+
+void app_link_up_internal(struct app_params *app,
+	struct app_link_params *cp);
+
+void app_link_down_internal(struct app_params *app,
+	struct app_link_params *cp);
+
+#endif
diff --git a/examples/ip_pipeline/pipeline/pipeline_common_be.c b/examples/ip_pipeline/pipeline/pipeline_common_be.c
new file mode 100644
index 000000000..03173ea54
--- /dev/null
+++ b/examples/ip_pipeline/pipeline/pipeline_common_be.c
@@ -0,0 +1,177 @@
+/* SPDX-License-Identifier: BSD-3-Clause
+ * Copyright(c) 2010-2015 Intel Corporation
+ */
+
+#include <rte_common.h>
+#include <rte_malloc.h>
+
+#include "pipeline_common_be.h"
+
+void *
+pipeline_msg_req_ping_handler(__rte_unused struct pipeline *p,
+	void *msg)
+{
+	struct pipeline_msg_rsp *rsp = msg;
+
+	rsp->status = 0; /* OK */
+
+	return rsp;
+}
+
+void *
+pipeline_msg_req_stats_port_in_handler(struct pipeline *p,
+	void *msg)
+{
+	struct pipeline_stats_msg_req *req = msg;
+	struct pipeline_stats_port_in_msg_rsp *rsp = msg;
+	uint32_t port_id;
+
+	/* Check request */
+	if (req->id >= p->n_ports_in) {
+		rsp->status = -1;
+		return rsp;
+	}
+	port_id = p->port_in_id[req->id];
+
+	/* Process request */
+	rsp->status = rte_pipeline_port_in_stats_read(p->p,
+		port_id,
+		&rsp->stats,
+		1);
+
+	return rsp;
+}
+
+void *
+pipeline_msg_req_stats_port_out_handler(struct pipeline *p,
+	void *msg)
+{
+	struct pipeline_stats_msg_req *req = msg;
+	struct pipeline_stats_port_out_msg_rsp *rsp = msg;
+	uint32_t port_id;
+
+	/* Check request */
+	if (req->id >= p->n_ports_out) {
+		rsp->status = -1;
+		return rsp;
+	}
+	port_id = p->port_out_id[req->id];
+
+	/* Process request */
+	rsp->status = rte_pipeline_port_out_stats_read(p->p,
+		port_id,
+		&rsp->stats,
+		1);
+
+	return rsp;
+}
+
+void *
+pipeline_msg_req_stats_table_handler(struct pipeline *p,
+	void *msg)
+{
+	struct pipeline_stats_msg_req *req = msg;
+	struct pipeline_stats_table_msg_rsp *rsp = msg;
+	uint32_t table_id;
+
+	/* Check request */
+	if (req->id >= p->n_tables) {
+		rsp->status = -1;
+		return rsp;
+	}
+	table_id = p->table_id[req->id];
+
+	/* Process request */
+	rsp->status = rte_pipeline_table_stats_read(p->p,
+		table_id,
+		&rsp->stats,
+		1);
+
+	return rsp;
+}
+
+void *
+pipeline_msg_req_port_in_enable_handler(struct pipeline *p,
+	void *msg)
+{
+	struct pipeline_port_in_msg_req *req = msg;
+	struct pipeline_msg_rsp *rsp = msg;
+	uint32_t port_id;
+
+	/* Check request */
+	if (req->port_id >= p->n_ports_in) {
+		rsp->status = -1;
+		return rsp;
+	}
+	port_id = p->port_in_id[req->port_id];
+
+	/* Process request */
+	rsp->status = rte_pipeline_port_in_enable(p->p,
+		port_id);
+
+	return rsp;
+}
+
+void *
+pipeline_msg_req_port_in_disable_handler(struct pipeline *p,
+	void *msg)
+{
+	struct pipeline_port_in_msg_req *req = msg;
+	struct pipeline_msg_rsp *rsp = msg;
+	uint32_t port_id;
+
+	/* Check request */
+	if (req->port_id >= p->n_ports_in) {
+		rsp->status = -1;
+		return rsp;
+	}
+	port_id = p->port_in_id[req->port_id];
+
+	/* Process request */
+	rsp->status = rte_pipeline_port_in_disable(p->p,
+		port_id);
+
+	return rsp;
+}
+
+//未知消息响应
+void *
+pipeline_msg_req_invalid_handler(__rte_unused struct pipeline *p,
+	void *msg)
+{
+	struct pipeline_msg_rsp *rsp = msg;
+
+	rsp->status = -1; /* Error */
+
+	return rsp;
+}
+
+int
+pipeline_msg_req_handle(struct pipeline *p)
+{
+	uint32_t msgq_id;
+
+	for (msgq_id = 0; msgq_id < p->n_msgq; msgq_id++) {
+		for ( ; ; ) {
+			struct pipeline_msg_req *req;
+			pipeline_msg_req_handler f_handle;
+
+			req = pipeline_msg_recv(p, msgq_id);
+			if (req == NULL)
+				break;
+
+			f_handle = (req->type < PIPELINE_MSG_REQS) ?
+				p->handlers[req->type] :
+				pipeline_msg_req_invalid_handler;
+
+			if (f_handle == NULL)
+				f_handle = pipeline_msg_req_invalid_handler;
+
+			pipeline_msg_send(p,
+				msgq_id,
+				f_handle(p, (void *) req));
+		}
+	}
+
+	return 0;
+}
diff --git a/examples/ip_pipeline/pipeline/pipeline_master_be.c b/examples/ip_pipeline/pipeline/pipeline_master_be.c
new file mode 100644
index 000000000..44b7fd8f6
--- /dev/null
+++ b/examples/ip_pipeline/pipeline/pipeline_master_be.c
@@ -0,0 +1,145 @@
+/* SPDX-License-Identifier: BSD-3-Clause
+ * Copyright(c) 2010-2015 Intel Corporation
+ */
+
+#include <fcntl.h>
+#include <unistd.h>
+
+#include <rte_common.h>
+#include <rte_malloc.h>
+
+#include <cmdline_parse.h>
+#include <cmdline_parse_string.h>
+#include <cmdline_socket.h>
+#include <cmdline.h>
+
+#include "app.h"
+#include "pipeline_master_be.h"
+
+struct pipeline_master {
+	struct app_params *app;
+	struct cmdline *cl;
+	int post_init_done;
+	int script_file_done;
+} __rte_cache_aligned;
+
+static void*
+pipeline_init(__rte_unused struct pipeline_params *params, void *arg)
+{
+	struct app_params *app = (struct app_params *) arg;
+	struct pipeline_master *p;
+	uint32_t size;
+
+	/* Check input arguments */
+	if (app == NULL)
+		return NULL;
+
+	/* Memory allocation */
+	size = RTE_CACHE_LINE_ROUNDUP(sizeof(struct pipeline_master));
+	p = rte_zmalloc(NULL, size, RTE_CACHE_LINE_SIZE);
+	if (p == NULL)
+		return NULL;
+
+	/* Initialization */
+	p->app = app;
+
+	p->cl = cmdline_stdin_new(app->cmds, "pipeline> ");
+	if (p->cl == NULL) {
+		rte_free(p);
+		return NULL;
+	}
+
+	p->post_init_done = 0;
+	p->script_file_done = 0;
+	if (app->script_file == NULL)
+		p->script_file_done = 1;
+
+	return (void *) p;
+}
+
+static int
+pipeline_free(void *pipeline)
+{
+	struct pipeline_master *p = (struct pipeline_master *) pipeline;
+
+	if (p == NULL)
+		return -EINVAL;
+
+	cmdline_stdin_exit(p->cl);
+	rte_free(p);
+
+	return 0;
+}
+
+static int
+pipeline_run(void *pipeline)
+{
+	struct pipeline_master *p = (struct pipeline_master *) pipeline;
+	struct app_params *app = p->app;
+	int status;
+#ifdef RTE_LIBRTE_KNI
+	uint32_t i;
+#endif /* RTE_LIBRTE_KNI */
+
+	/* Application post-init phase */
+	if (p->post_init_done == 0) {
+		app_post_init(app);
+
+		p->post_init_done = 1;
+	}
+
+	/* Run startup script file */
+	//脚本处理
+	if (p->script_file_done == 0) {
+		struct app_params *app = p->app;
+		int fd = open(app->script_file, O_RDONLY);
+
+		if (fd < 0)
+			printf("Cannot open CLI script file \"%s\"\n",
+				app->script_file);
+		else {
+			struct cmdline *file_cl;
+
+			printf("Running CLI script file \"%s\" ...\n",
+				app->script_file);
+			file_cl = cmdline_new(p->cl->ctx, "", fd, 1);
+			cmdline_interact(file_cl);
+			close(fd);
+		}
+
+		p->script_file_done = 1;
+	}
+
+	/* Command Line Interface (CLI) */
+	//命令行处理
+	status = cmdline_poll(p->cl);
+	if (status < 0)
+		//读标准输入失败，挂掉进程
+		rte_panic("CLI poll error (%" PRId32 ")\n", status);
+	else if (status == RDLINE_EXITED) {
+		//执行进程退出
+		cmdline_stdin_exit(p->cl);
+		rte_exit(0, "Bye!\n");
+	}
+
+#ifdef RTE_LIBRTE_KNI
+	/* Handle KNI requests from Linux kernel */
+	for (i = 0; i < app->n_pktq_kni; i++)
+		rte_kni_handle_request(app->kni[i]);
+#endif /* RTE_LIBRTE_KNI */
+
+	return 0;
+}
+
+static int
+pipeline_timer(__rte_unused void *pipeline)
+{
+	return 0;
+}
+
+struct pipeline_be_ops pipeline_master_be_ops = {
+		.f_init = pipeline_init,
+		.f_free = pipeline_free,
+		.f_run = pipeline_run,
+		.f_timer = pipeline_timer,
+};
diff --git a/examples/l2fwd/main.c b/examples/l2fwd/main.c
index d7bcbfae7..de8071cfd 100644
--- a/examples/l2fwd/main.c
+++ b/examples/l2fwd/main.c
@@ -171,7 +171,7 @@ l2fwd_simple_forward(struct rte_mbuf *m, unsigned portid)
 	int sent;
 	struct rte_eth_dev_tx_buffer *buffer;
 
-	dst_port = l2fwd_dst_ports[portid];
+	dst_port = l2fwd_dst_ports[portid];//portid对应的dst_port
 
 	if (mac_updating)
 		l2fwd_mac_updating(m, dst_port);
@@ -265,6 +265,7 @@ l2fwd_main_loop(void)
 		 */
 		for (i = 0; i < qconf->n_rx_port; i++) {
 
+			//收取portid的报文
 			portid = qconf->rx_port_list[i];
 			nb_rx = rte_eth_rx_burst(portid, 0,
 						 pkts_burst, MAX_PKT_BURST);
@@ -274,7 +275,7 @@ l2fwd_main_loop(void)
 			for (j = 0; j < nb_rx; j++) {
 				m = pkts_burst[j];
 				rte_prefetch0(rte_pktmbuf_mtod(m, void *));
-				l2fwd_simple_forward(m, portid);
+				l2fwd_simple_forward(m, portid);//发送portid的报文
 			}
 		}
 	}
diff --git a/examples/l3fwd/main.c b/examples/l3fwd/main.c
index 4dea12a65..8060dafe0 100644
--- a/examples/l3fwd/main.c
+++ b/examples/l3fwd/main.c
@@ -903,6 +903,7 @@ main(int argc, char **argv)
 				local_port_conf.rx_adv_conf.rss_conf.rss_hf);
 		}
 
+		//执行接口配置
 		ret = rte_eth_dev_configure(portid, nb_rx_queue,
 					(uint16_t)n_tx_queue, &local_port_conf);
 		if (ret < 0)
@@ -948,6 +949,7 @@ main(int argc, char **argv)
 			rte_exit(EXIT_FAILURE, "init_mem failed\n");
 
 		/* init one TX queue per couple (lcore,port) */
+		//初始化发队列
 		queueid = 0;
 		for (lcore_id = 0; lcore_id < RTE_MAX_LCORE; lcore_id++) {
 			if (rte_lcore_is_enabled(lcore_id) == 0)
@@ -981,6 +983,7 @@ main(int argc, char **argv)
 		printf("\n");
 	}
 
+	//初始化收队列
 	for (lcore_id = 0; lcore_id < RTE_MAX_LCORE; lcore_id++) {
 		if (rte_lcore_is_enabled(lcore_id) == 0)
 			continue;
diff --git a/examples/vhost/main.c b/examples/vhost/main.c
index ab649bf14..50133a4c8 100644
--- a/examples/vhost/main.c
+++ b/examples/vhost/main.c
@@ -59,10 +59,10 @@
 #define MAX_LONG_OPT_SZ 64
 
 /* mask of enabled ports */
-static uint32_t enabled_port_mask = 0;
+static uint32_t enabled_port_mask = 0;//哪些port被开启了
 
 /* Promiscuous mode */
-static uint32_t promiscuous;
+static uint32_t promiscuous;//是否开启混杂
 
 /* number of devices/queues to support*/
 static uint32_t num_queues = 0;
@@ -78,7 +78,7 @@ typedef enum {
 	VM2VM_HARDWARE = 2,
 	VM2VM_LAST
 } vm2vm_type;
-static vm2vm_type vm2vm_mode = VM2VM_SOFTWARE;
+static vm2vm_type vm2vm_mode = VM2VM_SOFTWARE;//vm2vm间模式
 
 /* Enable stats. */
 static uint32_t enable_stats = 0;
@@ -102,7 +102,7 @@ static uint32_t burst_rx_delay_time = BURST_RX_WAIT_US;
 static uint32_t burst_rx_retry_num = BURST_RX_RETRIES;
 
 /* Socket file paths. Can be set by user */
-static char *socket_files;
+static char *socket_files;//保存一组socket文件名称
 static int nb_sockets;
 
 /* empty vmdq configuration structure. Filled in programatically */
@@ -142,8 +142,8 @@ static struct rte_eth_conf vmdq_conf_default = {
 };
 
 
-static unsigned lcore_ids[RTE_MAX_LCORE];
-static uint16_t ports[RTE_MAX_ETHPORTS];
+static unsigned lcore_ids[RTE_MAX_LCORE];//那些core被开启了
+static uint16_t ports[RTE_MAX_ETHPORTS];//那些port被开启了
 static unsigned num_ports = 0; /**< The number of ports specified in command line */
 static uint16_t num_pf_queues, num_vmdq_queues;
 static uint16_t vmdq_pool_base, vmdq_queue_base;
@@ -215,6 +215,7 @@ get_eth_conf(struct rte_eth_conf *eth_conf, uint32_t num_devices)
  * Initialises a given port using global settings and with the rx buffers
  * coming from the mbuf_pool passed as parameter
  */
+//物理口初始化
 static inline int
 port_init(uint16_t port)
 {
@@ -228,7 +229,7 @@ port_init(uint16_t port)
 	uint16_t q;
 
 	/* The max pool number from dev_info will be used to validate the pool number specified in cmd line */
-	retval = rte_eth_dev_info_get(port, &dev_info);
+	retval = rte_eth_dev_info_get(port, &dev_info);//获取设备信息
 	if (retval != 0) {
 		RTE_LOG(ERR, VHOST_PORT,
 			"Error during getting device (port %u) info: %s\n",
@@ -237,6 +238,7 @@ port_init(uint16_t port)
 		return retval;
 	}
 
+	//默认的收发配置
 	rxconf = &dev_info.default_rxconf;
 	txconf = &dev_info.default_txconf;
 	rxconf->rx_drop_en = 1;
@@ -281,6 +283,7 @@ port_init(uint16_t port)
 		port_conf.txmode.offloads |=
 			DEV_TX_OFFLOAD_MBUF_FAST_FREE;
 	/* Configure ethernet device. */
+	//配置设备
 	retval = rte_eth_dev_configure(port, rx_rings, tx_rings, &port_conf);
 	if (retval != 0) {
 		RTE_LOG(ERR, VHOST_PORT, "Failed to configure port %u: %s.\n",
@@ -303,6 +306,7 @@ port_init(uint16_t port)
 
 	/* Setup the queues. */
 	rxconf->offloads = port_conf.rxmode.offloads;
+	//配置收队列
 	for (q = 0; q < rx_rings; q ++) {
 		retval = rte_eth_rx_queue_setup(port, q, rx_ring_size,
 						rte_eth_dev_socket_id(port),
@@ -315,7 +319,9 @@ port_init(uint16_t port)
 			return retval;
 		}
 	}
+
 	txconf->offloads = port_conf.txmode.offloads;
+	//配置发队列
 	for (q = 0; q < tx_rings; q ++) {
 		retval = rte_eth_tx_queue_setup(port, q, tx_ring_size,
 						rte_eth_dev_socket_id(port),
@@ -329,6 +335,7 @@ port_init(uint16_t port)
 	}
 
 	/* Start the device. */
+	//设备开启
 	retval  = rte_eth_dev_start(port);
 	if (retval < 0) {
 		RTE_LOG(ERR, VHOST_PORT, "Failed to start port %u: %s\n",
@@ -419,6 +426,7 @@ parse_portmask(const char *portmask)
 /*
  * Parse num options at run time.
  */
+//转换q_arg,要求转换后的值小于等于max_valid_value
 static int
 parse_num_opt(const char *q_arg, uint32_t max_valid_value)
 {
@@ -468,6 +476,7 @@ us_vhost_usage(const char *prgname)
 /*
  * Parse the arguments given in the command line of the application.
  */
+//解析vhost自已的命令行
 static int
 us_vhost_parse_args(int argc, char **argv)
 {
@@ -918,6 +927,7 @@ do_drain_mbuf_table(struct mbuf_table *tx_q)
 	count = rte_eth_tx_burst(ports[0], tx_q->txq_id,
 				 tx_q->m_table, tx_q->len);
 	if (unlikely(count < tx_q->len))
+		//发失败的，直接释放掉
 		free_pkts(&tx_q->m_table[count], tx_q->len - count);
 
 	tx_q->len = 0;
@@ -1135,6 +1145,7 @@ switch_worker(void *arg __rte_unused)
 	tx_q = &lcore_tx_queue[lcore_id];
 	for (i = 0; i < rte_lcore_count(); i++) {
 		if (lcore_ids[i] == lcore_id) {
+			//当前线程绑定的core是lcore_id,自已在lcore_ids数组中的下标为i
 			tx_q->txq_id = i;
 			break;
 		}
@@ -1345,6 +1356,7 @@ unregister_drivers(int socket_num)
 	int i, ret;
 
 	for (i = 0; i < socket_num; i++) {
+		//销毁指定socket
 		ret = rte_vhost_driver_unregister(socket_files + i * PATH_MAX);
 		if (ret != 0)
 			RTE_LOG(ERR, VHOST_CONFIG,
@@ -1448,6 +1460,7 @@ main(int argc, char *argv[])
 			lcore_ids[core_id++] = lcore_id;
 	}
 
+	//core太多
 	if (rte_lcore_count() > RTE_MAX_LCORE)
 		rte_exit(EXIT_FAILURE,"Not enough cores\n");
 
@@ -1490,6 +1503,7 @@ main(int argc, char *argv[])
 				"Skipping disabled port %d\n", portid);
 			continue;
 		}
+		//初始化port
 		if (port_init(portid) != 0)
 			rte_exit(EXIT_FAILURE,
 				"Cannot initialize network ports\n");
@@ -1497,6 +1511,7 @@ main(int argc, char *argv[])
 
 	/* Enable stats if the user option is set. */
 	if (enable_stats) {
+		//显示状态线程
 		ret = rte_ctrl_thread_create(&tid, "print-stats", NULL,
 					print_stats, NULL);
 		if (ret < 0)
@@ -1509,14 +1524,17 @@ main(int argc, char *argv[])
 		rte_eal_remote_launch(switch_worker, NULL, lcore_id);
 
 	if (client_mode)
+		//标记创建为客户端
 		flags |= RTE_VHOST_USER_CLIENT;
 
 	if (dequeue_zero_copy)
+		//标记支持出队zero copy
 		flags |= RTE_VHOST_USER_DEQUEUE_ZERO_COPY;
 
 	/* Register vhost user driver to handle vhost messages. */
 	for (i = 0; i < nb_sockets; i++) {
 		char *file = socket_files + i * PATH_MAX;
+		//注册vsocket
 		ret = rte_vhost_driver_register(file, flags);
 		if (ret != 0) {
 			unregister_drivers(i);
@@ -1525,6 +1543,7 @@ main(int argc, char *argv[])
 		}
 
 		if (builtin_net_driver)
+			//指定支持的功能为0
 			rte_vhost_driver_set_features(file, VIRTIO_NET_FEATURES);
 
 		if (mergeable == 0) {
@@ -1553,6 +1572,7 @@ main(int argc, char *argv[])
 				1ULL << VIRTIO_NET_F_CTRL_RX);
 		}
 
+		//给这个文件注册操作
 		ret = rte_vhost_driver_callback_register(file,
 			&virtio_net_device_ops);
 		if (ret != 0) {
@@ -1560,6 +1580,7 @@ main(int argc, char *argv[])
 				"failed to register vhost driver callbacks.\n");
 		}
 
+		//处理事件
 		if (rte_vhost_driver_start(file) < 0) {
 			rte_exit(EXIT_FAILURE,
 				"failed to start vhost driver.\n");
diff --git a/kernel/linux/igb_uio/igb_uio.c b/kernel/linux/igb_uio/igb_uio.c
index 039f5a5f6..7a048124f 100644
--- a/kernel/linux/igb_uio/igb_uio.c
+++ b/kernel/linux/igb_uio/igb_uio.c
@@ -29,7 +29,7 @@ struct rte_uio_pci_dev {
 	atomic_t refcnt;
 };
 
-static int wc_activate;
+static int wc_activate;//模块参数
 static char *intr_mode;
 static enum rte_intr_mode igbuio_intr_mode_preferred = RTE_INTR_MODE_MSIX;
 /* sriov sysfs */
@@ -312,6 +312,7 @@ igbuio_pci_disable_interrupts(struct rte_uio_pci_dev *udev)
 /**
  * This gets called while opening uio device file.
  */
+//当/dev/uio%d设备被打开时，此回调将被调用
 static int
 igbuio_pci_open(struct uio_info *info, struct inode *inode)
 {
@@ -367,16 +368,18 @@ igbuio_pci_setup_iomem(struct pci_dev *dev, struct uio_info *info,
 	if (addr == 0 || len == 0)
 		return -1;
 	if (wc_activate == 0) {
+		//映射pci资源
 		internal_addr = ioremap(addr, len);
 		if (internal_addr == NULL)
 			return -1;
 	} else {
 		internal_addr = NULL;
 	}
+	//设置内存segment的name,addr,size
 	info->mem[n].name = name;
-	info->mem[n].addr = addr;
-	info->mem[n].internal_addr = internal_addr;
-	info->mem[n].size = len;
+	info->mem[n].addr = addr;//设备物理地址
+	info->mem[n].internal_addr = internal_addr;//映射后的设备资源
+	info->mem[n].size = len;//资源长度
 	info->mem[n].memtype = UIO_MEM_PHYS;
 	return 0;
 }
@@ -438,12 +441,14 @@ igbuio_setup_bars(struct pci_dev *dev, struct uio_info *info)
 				pci_resource_start(dev, i) != 0) {
 			flags = pci_resource_flags(dev, i);
 			if (flags & IORESOURCE_MEM) {
+				//第i块bar是mem space（映射这部分信息，并填充到info中）
 				ret = igbuio_pci_setup_iomem(dev, info, iom,
 							     i, bar_names[i]);
 				if (ret != 0)
 					return ret;
 				iom++;
 			} else if (flags & IORESOURCE_IO) {
+				//第i块bar是io space
 				ret = igbuio_pci_setup_ioport(dev, info, iop,
 							      i, bar_names[i]);
 				if (ret != 0)
@@ -470,6 +475,7 @@ igbuio_pci_probe(struct pci_dev *dev, const struct pci_device_id *id)
 
 #ifdef HAVE_PCI_IS_BRIDGE_API
 	if (pci_is_bridge(dev)) {
+		//pci桥设备，将被忽略
 		dev_warn(&dev->dev, "Ignoring PCI bridge device\n");
 		return -ENODEV;
 	}
@@ -493,6 +499,7 @@ igbuio_pci_probe(struct pci_dev *dev, const struct pci_device_id *id)
 	pci_set_master(dev);
 
 	/* remap IO memory */
+	//内存信息填充到udev->info中
 	err = igbuio_setup_bars(dev, &udev->info);
 	if (err != 0)
 		goto fail_release_iomem;
@@ -520,11 +527,14 @@ igbuio_pci_probe(struct pci_dev *dev, const struct pci_device_id *id)
 	udev->pdev = dev;
 	atomic_set(&udev->refcnt, 0);
 
+	//在sys文件系统中创建dev_attr_grp对应的一组文件
+	//（没有指定文件名称，文件目录来自kobj中对应的kernfs node)
 	err = sysfs_create_group(&dev->dev.kobj, &dev_attr_grp);
 	if (err != 0)
 		goto fail_release_iomem;
 
 	/* register uio driver */
+	//将igb_uio注册到uio设备
 	err = uio_register_device(&dev->dev, &udev->info);
 	if (err != 0)
 		goto fail_remove_group;
@@ -536,6 +546,7 @@ igbuio_pci_probe(struct pci_dev *dev, const struct pci_device_id *id)
 	 * the iommu identity mapping if kernel boots with iommu=pt.
 	 * Note this is not a problem if no IOMMU at all.
 	 */
+	//dma内存映射
 	map_addr = dma_alloc_coherent(&dev->dev, 1024, &map_dma_addr,
 			GFP_KERNEL);
 	if (map_addr)
@@ -580,6 +591,7 @@ igbuio_pci_remove(struct pci_dev *dev)
 	kfree(udev);
 }
 
+//设置中断模式
 static int
 igbuio_config_intr_mode(char *intr_str)
 {
@@ -605,13 +617,16 @@ igbuio_config_intr_mode(char *intr_str)
 	return 0;
 }
 
+//igb_uio是一个pci driver
 static struct pci_driver igbuio_pci_driver = {
 	.name = "igb_uio",
 	.id_table = NULL,
+	//设备与驱动匹配时调用
 	.probe = igbuio_pci_probe,
 	.remove = igbuio_pci_remove,
 };
 
+//igbuio模块初始化
 static int __init
 igbuio_pci_init_module(void)
 {
@@ -629,6 +644,7 @@ igbuio_pci_init_module(void)
 	if (ret < 0)
 		return ret;
 
+	//注册pci driver
 	return pci_register_driver(&igbuio_pci_driver);
 }
 
@@ -641,6 +657,7 @@ igbuio_pci_exit_module(void)
 module_init(igbuio_pci_init_module);
 module_exit(igbuio_pci_exit_module);
 
+//定义模块参数（中断模式）
 module_param(intr_mode, charp, S_IRUGO);
 MODULE_PARM_DESC(intr_mode,
 "igb_uio interrupt mode (default=msix):\n"
diff --git a/kernel/linux/kni/ethtool/igb/igb_main.c b/kernel/linux/kni/ethtool/igb/igb_main.c
new file mode 100644
index 000000000..47d4159ec
--- /dev/null
+++ b/kernel/linux/kni/ethtool/igb/igb_main.c
@@ -0,0 +1,10350 @@
+// SPDX-License-Identifier: GPL-2.0
+/*******************************************************************************
+
+  Intel(R) Gigabit Ethernet Linux driver
+  Copyright(c) 2007-2013 Intel Corporation.
+
+  Contact Information:
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/init.h>
+#include <linux/vmalloc.h>
+#include <linux/pagemap.h>
+#include <linux/netdevice.h>
+#include <linux/tcp.h>
+#ifdef NETIF_F_TSO
+#include <net/checksum.h>
+#ifdef NETIF_F_TSO6
+#include <linux/ipv6.h>
+#include <net/ip6_checksum.h>
+#endif
+#endif
+#ifdef SIOCGMIIPHY
+#include <linux/mii.h>
+#endif
+#ifdef SIOCETHTOOL
+#include <linux/ethtool.h>
+#endif
+#include <linux/if_vlan.h>
+#ifdef CONFIG_PM_RUNTIME
+#include <linux/pm_runtime.h>
+#endif /* CONFIG_PM_RUNTIME */
+
+#include <linux/if_bridge.h>
+#include "igb.h"
+#include "igb_vmdq.h"
+
+#include <linux/uio_driver.h>
+
+#if defined(DEBUG) || defined (DEBUG_DUMP) || defined (DEBUG_ICR) || defined(DEBUG_ITR)
+#define DRV_DEBUG "_debug"
+#else
+#define DRV_DEBUG
+#endif
+#define DRV_HW_PERF
+#define VERSION_SUFFIX
+
+#define MAJ 5
+#define MIN 0
+#define BUILD 6
+#define DRV_VERSION __stringify(MAJ) "." __stringify(MIN) "." __stringify(BUILD) VERSION_SUFFIX DRV_DEBUG DRV_HW_PERF
+
+char igb_driver_name[] = "igb";
+char igb_driver_version[] = DRV_VERSION;
+static const char igb_driver_string[] =
+                                "Intel(R) Gigabit Ethernet Network Driver";
+static const char igb_copyright[] =
+				"Copyright (c) 2007-2013 Intel Corporation.";
+
+const struct pci_device_id igb_pci_tbl[] = {
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_BACKPLANE_1GBPS) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_SGMII) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_BACKPLANE_2_5GBPS) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_COPPER) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_FIBER) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SERDES) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SGMII) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_COPPER_FLASHLESS) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SERDES_FLASHLESS) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I211_COPPER) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I350_COPPER) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I350_FIBER) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I350_SERDES) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I350_SGMII) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82580_COPPER) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82580_FIBER) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82580_QUAD_FIBER) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82580_SERDES) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82580_SGMII) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82580_COPPER_DUAL) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_DH89XXCC_SGMII) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_DH89XXCC_SERDES) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_DH89XXCC_BACKPLANE) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_DH89XXCC_SFP) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82576) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82576_NS) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82576_NS_SERDES) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82576_FIBER) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82576_SERDES) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82576_SERDES_QUAD) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82576_QUAD_COPPER_ET2) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82576_QUAD_COPPER) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82575EB_COPPER) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82575EB_FIBER_SERDES) },
+	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82575GB_QUAD_COPPER) },
+	/* required last entry */
+	{0, }
+};
+
+//MODULE_DEVICE_TABLE(pci, igb_pci_tbl);
+static void igb_set_sriov_capability(struct igb_adapter *adapter) __attribute__((__unused__));
+void igb_reset(struct igb_adapter *);
+static int igb_setup_all_tx_resources(struct igb_adapter *);
+static int igb_setup_all_rx_resources(struct igb_adapter *);
+static void igb_free_all_tx_resources(struct igb_adapter *);
+static void igb_free_all_rx_resources(struct igb_adapter *);
+static void igb_setup_mrqc(struct igb_adapter *);
+void igb_update_stats(struct igb_adapter *);
+static int igb_probe(struct pci_dev *, const struct pci_device_id *);
+static void __devexit igb_remove(struct pci_dev *pdev);
+static int igb_sw_init(struct igb_adapter *);
+static int igb_open(struct net_device *);
+static int igb_close(struct net_device *);
+static void igb_configure(struct igb_adapter *);
+static void igb_configure_tx(struct igb_adapter *);
+static void igb_configure_rx(struct igb_adapter *);
+static void igb_clean_all_tx_rings(struct igb_adapter *);
+static void igb_clean_all_rx_rings(struct igb_adapter *);
+static void igb_clean_tx_ring(struct igb_ring *);
+static void igb_set_rx_mode(struct net_device *);
+#ifdef HAVE_TIMER_SETUP
+static void igb_update_phy_info(struct timer_list *);
+static void igb_watchdog(struct timer_list *);
+#else
+static void igb_update_phy_info(unsigned long);
+static void igb_watchdog(unsigned long);
+#endif
+static void igb_watchdog_task(struct work_struct *);
+static void igb_dma_err_task(struct work_struct *);
+#ifdef HAVE_TIMER_SETUP
+static void igb_dma_err_timer(struct timer_list *);
+#else
+static void igb_dma_err_timer(unsigned long data);
+#endif
+static netdev_tx_t igb_xmit_frame(struct sk_buff *skb, struct net_device *);
+static struct net_device_stats *igb_get_stats(struct net_device *);
+static int igb_change_mtu(struct net_device *, int);
+void igb_full_sync_mac_table(struct igb_adapter *adapter);
+static int igb_set_mac(struct net_device *, void *);
+static void igb_set_uta(struct igb_adapter *adapter);
+static irqreturn_t igb_intr(int irq, void *);
+static irqreturn_t igb_intr_msi(int irq, void *);
+static irqreturn_t igb_msix_other(int irq, void *);
+static irqreturn_t igb_msix_ring(int irq, void *);
+#ifdef IGB_DCA
+static void igb_update_dca(struct igb_q_vector *);
+static void igb_setup_dca(struct igb_adapter *);
+#endif /* IGB_DCA */
+static int igb_poll(struct napi_struct *, int);
+static bool igb_clean_tx_irq(struct igb_q_vector *);
+static bool igb_clean_rx_irq(struct igb_q_vector *, int);
+static int igb_ioctl(struct net_device *, struct ifreq *, int cmd);
+static void igb_tx_timeout(struct net_device *);
+static void igb_reset_task(struct work_struct *);
+#ifdef HAVE_VLAN_RX_REGISTER
+static void igb_vlan_mode(struct net_device *, struct vlan_group *);
+#endif
+#ifdef HAVE_VLAN_PROTOCOL
+static int igb_vlan_rx_add_vid(struct net_device *,
+                               __be16 proto, u16);
+static int igb_vlan_rx_kill_vid(struct net_device *,
+                                __be16 proto, u16);
+#elif defined HAVE_INT_NDO_VLAN_RX_ADD_VID
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
+static int igb_vlan_rx_add_vid(struct net_device *,
+			       __always_unused __be16 proto, u16);
+static int igb_vlan_rx_kill_vid(struct net_device *,
+			        __always_unused __be16 proto, u16);
+#else
+static int igb_vlan_rx_add_vid(struct net_device *, u16);
+static int igb_vlan_rx_kill_vid(struct net_device *, u16);
+#endif
+#else
+static void igb_vlan_rx_add_vid(struct net_device *, u16);
+static void igb_vlan_rx_kill_vid(struct net_device *, u16);
+#endif
+static void igb_restore_vlan(struct igb_adapter *);
+void igb_rar_set(struct igb_adapter *adapter, u32 index);
+static void igb_ping_all_vfs(struct igb_adapter *);
+static void igb_msg_task(struct igb_adapter *);
+static void igb_vmm_control(struct igb_adapter *);
+static int igb_set_vf_mac(struct igb_adapter *, int, unsigned char *);
+static void igb_restore_vf_multicasts(struct igb_adapter *adapter);
+static void igb_process_mdd_event(struct igb_adapter *);
+#ifdef IFLA_VF_MAX
+static int igb_ndo_set_vf_mac( struct net_device *netdev, int vf, u8 *mac);
+static int igb_ndo_set_vf_vlan(struct net_device *netdev,
+#ifdef HAVE_VF_VLAN_PROTO
+				int vf, u16 vlan, u8 qos, __be16 vlan_proto);
+#else
+				int vf, u16 vlan, u8 qos);
+#endif
+#ifdef HAVE_VF_SPOOFCHK_CONFIGURE
+static int igb_ndo_set_vf_spoofchk(struct net_device *netdev, int vf,
+				bool setting);
+#endif
+#ifdef HAVE_VF_MIN_MAX_TXRATE
+static int igb_ndo_set_vf_bw(struct net_device *, int, int, int);
+#else /* HAVE_VF_MIN_MAX_TXRATE */
+static int igb_ndo_set_vf_bw(struct net_device *netdev, int vf, int tx_rate);
+#endif /* HAVE_VF_MIN_MAX_TXRATE */
+static int igb_ndo_get_vf_config(struct net_device *netdev, int vf,
+				 struct ifla_vf_info *ivi);
+static void igb_check_vf_rate_limit(struct igb_adapter *);
+#endif
+static int igb_vf_configure(struct igb_adapter *adapter, int vf);
+#ifdef CONFIG_PM
+#ifdef HAVE_SYSTEM_SLEEP_PM_OPS
+static int igb_suspend(struct device *dev);
+static int igb_resume(struct device *dev);
+#ifdef CONFIG_PM_RUNTIME
+static int igb_runtime_suspend(struct device *dev);
+static int igb_runtime_resume(struct device *dev);
+static int igb_runtime_idle(struct device *dev);
+#endif /* CONFIG_PM_RUNTIME */
+static const struct dev_pm_ops igb_pm_ops = {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,34)
+        .suspend = igb_suspend,
+        .resume = igb_resume,
+        .freeze = igb_suspend,
+        .thaw = igb_resume,
+        .poweroff = igb_suspend,
+        .restore = igb_resume,
+#ifdef CONFIG_PM_RUNTIME
+        .runtime_suspend = igb_runtime_suspend,
+        .runtime_resume = igb_runtime_resume,
+        .runtime_idle = igb_runtime_idle,
+#endif
+#else /* Linux >= 2.6.34 */
+	SET_SYSTEM_SLEEP_PM_OPS(igb_suspend, igb_resume)
+#ifdef CONFIG_PM_RUNTIME
+	SET_RUNTIME_PM_OPS(igb_runtime_suspend, igb_runtime_resume,
+			igb_runtime_idle)
+#endif /* CONFIG_PM_RUNTIME */
+#endif /* Linux version */
+};
+#else
+static int igb_suspend(struct pci_dev *pdev, pm_message_t state);
+static int igb_resume(struct pci_dev *pdev);
+#endif /* HAVE_SYSTEM_SLEEP_PM_OPS */
+#endif /* CONFIG_PM */
+#ifndef USE_REBOOT_NOTIFIER
+static void igb_shutdown(struct pci_dev *);
+#else
+static int igb_notify_reboot(struct notifier_block *, unsigned long, void *);
+static struct notifier_block igb_notifier_reboot = {
+	.notifier_call	= igb_notify_reboot,
+	.next		= NULL,
+	.priority	= 0
+};
+#endif
+#ifdef IGB_DCA
+static int igb_notify_dca(struct notifier_block *, unsigned long, void *);
+static struct notifier_block dca_notifier = {
+	.notifier_call	= igb_notify_dca,
+	.next		= NULL,
+	.priority	= 0
+};
+#endif
+#ifdef CONFIG_NET_POLL_CONTROLLER
+/* for netdump / net console */
+static void igb_netpoll(struct net_device *);
+#endif
+
+#ifdef HAVE_PCI_ERS
+static pci_ers_result_t igb_io_error_detected(struct pci_dev *,
+		     pci_channel_state_t);
+static pci_ers_result_t igb_io_slot_reset(struct pci_dev *);
+static void igb_io_resume(struct pci_dev *);
+
+static struct pci_error_handlers igb_err_handler = {
+	.error_detected = igb_io_error_detected,
+	.slot_reset = igb_io_slot_reset,
+	.resume = igb_io_resume,
+};
+#endif
+
+static void igb_init_fw(struct igb_adapter *adapter);
+static void igb_init_dmac(struct igb_adapter *adapter, u32 pba);
+
+static struct pci_driver igb_driver = {
+	.name     = igb_driver_name,
+	.id_table = igb_pci_tbl,
+	.probe    = igb_probe,
+	.remove   = __devexit_p(igb_remove),
+#ifdef CONFIG_PM
+#ifdef HAVE_SYSTEM_SLEEP_PM_OPS
+	.driver.pm = &igb_pm_ops,
+#else
+	.suspend  = igb_suspend,
+	.resume   = igb_resume,
+#endif /* HAVE_SYSTEM_SLEEP_PM_OPS */
+#endif /* CONFIG_PM */
+#ifndef USE_REBOOT_NOTIFIER
+	.shutdown = igb_shutdown,
+#endif
+#ifdef HAVE_PCI_ERS
+	.err_handler = &igb_err_handler
+#endif
+};
+
+//MODULE_AUTHOR("Intel Corporation, <e1000-devel@lists.sourceforge.net>");
+//MODULE_DESCRIPTION("Intel(R) Gigabit Ethernet Network Driver");
+//MODULE_LICENSE("GPL");
+//MODULE_VERSION(DRV_VERSION);
+
+static void igb_vfta_set(struct igb_adapter *adapter, u32 vid, bool add)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	struct e1000_host_mng_dhcp_cookie *mng_cookie = &hw->mng_cookie;
+	u32 index = (vid >> E1000_VFTA_ENTRY_SHIFT) & E1000_VFTA_ENTRY_MASK;
+	u32 mask = 1 << (vid & E1000_VFTA_ENTRY_BIT_SHIFT_MASK);
+	u32 vfta;
+
+	/*
+	 * if this is the management vlan the only option is to add it in so
+	 * that the management pass through will continue to work
+	 */
+	if ((mng_cookie->status & E1000_MNG_DHCP_COOKIE_STATUS_VLAN) &&
+	    (vid == mng_cookie->vlan_id))
+		add = TRUE;
+
+	vfta = adapter->shadow_vfta[index];
+
+	if (add)
+		vfta |= mask;
+	else
+		vfta &= ~mask;
+
+	e1000_write_vfta(hw, index, vfta);
+	adapter->shadow_vfta[index] = vfta;
+}
+
+static int debug = NETIF_MSG_DRV | NETIF_MSG_PROBE;
+//module_param(debug, int, 0);
+//MODULE_PARM_DESC(debug, "Debug level (0=none, ..., 16=all)");
+
+/**
+ * igb_init_module - Driver Registration Routine
+ *
+ * igb_init_module is the first routine called when the driver is
+ * loaded. All it does is register with the PCI subsystem.
+ **/
+static int __init igb_init_module(void)
+{
+	int ret;
+
+	printk(KERN_INFO "%s - version %s\n",
+	       igb_driver_string, igb_driver_version);
+
+	printk(KERN_INFO "%s\n", igb_copyright);
+#ifdef IGB_HWMON
+/* only use IGB_PROCFS if IGB_HWMON is not defined */
+#else
+#ifdef IGB_PROCFS
+	if (igb_procfs_topdir_init())
+		printk(KERN_INFO "Procfs failed to initialize topdir\n");
+#endif /* IGB_PROCFS */
+#endif /* IGB_HWMON  */
+
+#ifdef IGB_DCA
+	dca_register_notify(&dca_notifier);
+#endif
+	ret = pci_register_driver(&igb_driver);
+#ifdef USE_REBOOT_NOTIFIER
+	if (ret >= 0) {
+		register_reboot_notifier(&igb_notifier_reboot);
+	}
+#endif
+	return ret;
+}
+
+#undef module_init
+#define module_init(x) static int x(void)  __attribute__((__unused__));
+module_init(igb_init_module);
+
+/**
+ * igb_exit_module - Driver Exit Cleanup Routine
+ *
+ * igb_exit_module is called just before the driver is removed
+ * from memory.
+ **/
+static void __exit igb_exit_module(void)
+{
+#ifdef IGB_DCA
+	dca_unregister_notify(&dca_notifier);
+#endif
+#ifdef USE_REBOOT_NOTIFIER
+	unregister_reboot_notifier(&igb_notifier_reboot);
+#endif
+	pci_unregister_driver(&igb_driver);
+
+#ifdef IGB_HWMON
+/* only compile IGB_PROCFS if IGB_HWMON is not defined */
+#else
+#ifdef IGB_PROCFS
+	igb_procfs_topdir_exit();
+#endif /* IGB_PROCFS */
+#endif /* IGB_HWMON */
+}
+
+#undef module_exit
+#define module_exit(x) static void x(void)  __attribute__((__unused__));
+module_exit(igb_exit_module);
+
+#define Q_IDX_82576(i) (((i & 0x1) << 3) + (i >> 1))
+/**
+ * igb_cache_ring_register - Descriptor ring to register mapping
+ * @adapter: board private structure to initialize
+ *
+ * Once we know the feature-set enabled for the device, we'll cache
+ * the register offset the descriptor ring is assigned to.
+ **/
+static void igb_cache_ring_register(struct igb_adapter *adapter)
+{
+	int i = 0, j = 0;
+	u32 rbase_offset = adapter->vfs_allocated_count;
+
+	switch (adapter->hw.mac.type) {
+	case e1000_82576:
+		/* The queues are allocated for virtualization such that VF 0
+		 * is allocated queues 0 and 8, VF 1 queues 1 and 9, etc.
+		 * In order to avoid collision we start at the first free queue
+		 * and continue consuming queues in the same sequence
+		 */
+		if ((adapter->rss_queues > 1) && adapter->vmdq_pools) {
+			for (; i < adapter->rss_queues; i++)
+				adapter->rx_ring[i]->reg_idx = rbase_offset +
+				                               Q_IDX_82576(i);
+		}
+	case e1000_82575:
+	case e1000_82580:
+	case e1000_i350:
+	case e1000_i354:
+	case e1000_i210:
+	case e1000_i211:
+	default:
+		for (; i < adapter->num_rx_queues; i++)
+			adapter->rx_ring[i]->reg_idx = rbase_offset + i;
+		for (; j < adapter->num_tx_queues; j++)
+			adapter->tx_ring[j]->reg_idx = rbase_offset + j;
+		break;
+	}
+}
+
+static void igb_configure_lli(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u16 port;
+
+	/* LLI should only be enabled for MSI-X or MSI interrupts */
+	if (!adapter->msix_entries && !(adapter->flags & IGB_FLAG_HAS_MSI))
+		return;
+
+	if (adapter->lli_port) {
+		/* use filter 0 for port */
+		port = htons((u16)adapter->lli_port);
+		E1000_WRITE_REG(hw, E1000_IMIR(0),
+			(port | E1000_IMIR_PORT_IM_EN));
+		E1000_WRITE_REG(hw, E1000_IMIREXT(0),
+			(E1000_IMIREXT_SIZE_BP | E1000_IMIREXT_CTRL_BP));
+	}
+
+	if (adapter->flags & IGB_FLAG_LLI_PUSH) {
+		/* use filter 1 for push flag */
+		E1000_WRITE_REG(hw, E1000_IMIR(1),
+			(E1000_IMIR_PORT_BP | E1000_IMIR_PORT_IM_EN));
+		E1000_WRITE_REG(hw, E1000_IMIREXT(1),
+			(E1000_IMIREXT_SIZE_BP | E1000_IMIREXT_CTRL_PSH));
+	}
+
+	if (adapter->lli_size) {
+		/* use filter 2 for size */
+		E1000_WRITE_REG(hw, E1000_IMIR(2),
+			(E1000_IMIR_PORT_BP | E1000_IMIR_PORT_IM_EN));
+		E1000_WRITE_REG(hw, E1000_IMIREXT(2),
+			(adapter->lli_size | E1000_IMIREXT_CTRL_BP));
+	}
+
+}
+
+/**
+ *  igb_write_ivar - configure ivar for given MSI-X vector
+ *  @hw: pointer to the HW structure
+ *  @msix_vector: vector number we are allocating to a given ring
+ *  @index: row index of IVAR register to write within IVAR table
+ *  @offset: column offset of in IVAR, should be multiple of 8
+ *
+ *  This function is intended to handle the writing of the IVAR register
+ *  for adapters 82576 and newer.  The IVAR table consists of 2 columns,
+ *  each containing an cause allocation for an Rx and Tx ring, and a
+ *  variable number of rows depending on the number of queues supported.
+ **/
+static void igb_write_ivar(struct e1000_hw *hw, int msix_vector,
+			   int index, int offset)
+{
+	u32 ivar = E1000_READ_REG_ARRAY(hw, E1000_IVAR0, index);
+
+	/* clear any bits that are currently set */
+	ivar &= ~((u32)0xFF << offset);
+
+	/* write vector and valid bit */
+	ivar |= (msix_vector | E1000_IVAR_VALID) << offset;
+
+	E1000_WRITE_REG_ARRAY(hw, E1000_IVAR0, index, ivar);
+}
+
+#define IGB_N0_QUEUE -1
+static void igb_assign_vector(struct igb_q_vector *q_vector, int msix_vector)
+{
+	struct igb_adapter *adapter = q_vector->adapter;
+	struct e1000_hw *hw = &adapter->hw;
+	int rx_queue = IGB_N0_QUEUE;
+	int tx_queue = IGB_N0_QUEUE;
+	u32 msixbm = 0;
+
+	if (q_vector->rx.ring)
+		rx_queue = q_vector->rx.ring->reg_idx;
+	if (q_vector->tx.ring)
+		tx_queue = q_vector->tx.ring->reg_idx;
+
+	switch (hw->mac.type) {
+	case e1000_82575:
+		/* The 82575 assigns vectors using a bitmask, which matches the
+		   bitmask for the EICR/EIMS/EIMC registers.  To assign one
+		   or more queues to a vector, we write the appropriate bits
+		   into the MSIXBM register for that vector. */
+		if (rx_queue > IGB_N0_QUEUE)
+			msixbm = E1000_EICR_RX_QUEUE0 << rx_queue;
+		if (tx_queue > IGB_N0_QUEUE)
+			msixbm |= E1000_EICR_TX_QUEUE0 << tx_queue;
+		if (!adapter->msix_entries && msix_vector == 0)
+			msixbm |= E1000_EIMS_OTHER;
+		E1000_WRITE_REG_ARRAY(hw, E1000_MSIXBM(0), msix_vector, msixbm);
+		q_vector->eims_value = msixbm;
+		break;
+	case e1000_82576:
+		/*
+		 * 82576 uses a table that essentially consists of 2 columns
+		 * with 8 rows.  The ordering is column-major so we use the
+		 * lower 3 bits as the row index, and the 4th bit as the
+		 * column offset.
+		 */
+		if (rx_queue > IGB_N0_QUEUE)
+			igb_write_ivar(hw, msix_vector,
+				       rx_queue & 0x7,
+				       (rx_queue & 0x8) << 1);
+		if (tx_queue > IGB_N0_QUEUE)
+			igb_write_ivar(hw, msix_vector,
+				       tx_queue & 0x7,
+				       ((tx_queue & 0x8) << 1) + 8);
+		q_vector->eims_value = 1 << msix_vector;
+		break;
+	case e1000_82580:
+	case e1000_i350:
+	case e1000_i354:
+	case e1000_i210:
+	case e1000_i211:
+		/*
+		 * On 82580 and newer adapters the scheme is similar to 82576
+		 * however instead of ordering column-major we have things
+		 * ordered row-major.  So we traverse the table by using
+		 * bit 0 as the column offset, and the remaining bits as the
+		 * row index.
+		 */
+		if (rx_queue > IGB_N0_QUEUE)
+			igb_write_ivar(hw, msix_vector,
+				       rx_queue >> 1,
+				       (rx_queue & 0x1) << 4);
+		if (tx_queue > IGB_N0_QUEUE)
+			igb_write_ivar(hw, msix_vector,
+				       tx_queue >> 1,
+				       ((tx_queue & 0x1) << 4) + 8);
+		q_vector->eims_value = 1 << msix_vector;
+		break;
+	default:
+		BUG();
+		break;
+	}
+
+	/* add q_vector eims value to global eims_enable_mask */
+	adapter->eims_enable_mask |= q_vector->eims_value;
+
+	/* configure q_vector to set itr on first interrupt */
+	q_vector->set_itr = 1;
+}
+
+/**
+ * igb_configure_msix - Configure MSI-X hardware
+ *
+ * igb_configure_msix sets up the hardware to properly
+ * generate MSI-X interrupts.
+ **/
+static void igb_configure_msix(struct igb_adapter *adapter)
+{
+	u32 tmp;
+	int i, vector = 0;
+	struct e1000_hw *hw = &adapter->hw;
+
+	adapter->eims_enable_mask = 0;
+
+	/* set vector for other causes, i.e. link changes */
+	switch (hw->mac.type) {
+	case e1000_82575:
+		tmp = E1000_READ_REG(hw, E1000_CTRL_EXT);
+		/* enable MSI-X PBA support*/
+		tmp |= E1000_CTRL_EXT_PBA_CLR;
+
+		/* Auto-Mask interrupts upon ICR read. */
+		tmp |= E1000_CTRL_EXT_EIAME;
+		tmp |= E1000_CTRL_EXT_IRCA;
+
+		E1000_WRITE_REG(hw, E1000_CTRL_EXT, tmp);
+
+		/* enable msix_other interrupt */
+		E1000_WRITE_REG_ARRAY(hw, E1000_MSIXBM(0), vector++,
+		                      E1000_EIMS_OTHER);
+		adapter->eims_other = E1000_EIMS_OTHER;
+
+		break;
+
+	case e1000_82576:
+	case e1000_82580:
+	case e1000_i350:
+	case e1000_i354:
+	case e1000_i210:
+	case e1000_i211:
+		/* Turn on MSI-X capability first, or our settings
+		 * won't stick.  And it will take days to debug. */
+		E1000_WRITE_REG(hw, E1000_GPIE, E1000_GPIE_MSIX_MODE |
+		                E1000_GPIE_PBA | E1000_GPIE_EIAME |
+		                E1000_GPIE_NSICR);
+
+		/* enable msix_other interrupt */
+		adapter->eims_other = 1 << vector;
+		tmp = (vector++ | E1000_IVAR_VALID) << 8;
+
+		E1000_WRITE_REG(hw, E1000_IVAR_MISC, tmp);
+		break;
+	default:
+		/* do nothing, since nothing else supports MSI-X */
+		break;
+	} /* switch (hw->mac.type) */
+
+	adapter->eims_enable_mask |= adapter->eims_other;
+
+	for (i = 0; i < adapter->num_q_vectors; i++)
+		igb_assign_vector(adapter->q_vector[i], vector++);
+
+	E1000_WRITE_FLUSH(hw);
+}
+
+/**
+ * igb_request_msix - Initialize MSI-X interrupts
+ *
+ * igb_request_msix allocates MSI-X vectors and requests interrupts from the
+ * kernel.
+ **/
+static int igb_request_msix(struct igb_adapter *adapter)
+{
+	struct net_device *netdev = adapter->netdev;
+	struct e1000_hw *hw = &adapter->hw;
+	int i, err = 0, vector = 0, free_vector = 0;
+
+	err = request_irq(adapter->msix_entries[vector].vector,
+	                  &igb_msix_other, 0, netdev->name, adapter);
+	if (err)
+		goto err_out;
+
+	for (i = 0; i < adapter->num_q_vectors; i++) {
+		struct igb_q_vector *q_vector = adapter->q_vector[i];
+
+		vector++;
+
+		q_vector->itr_register = hw->hw_addr + E1000_EITR(vector);
+
+		if (q_vector->rx.ring && q_vector->tx.ring)
+			sprintf(q_vector->name, "%s-TxRx-%u", netdev->name,
+			        q_vector->rx.ring->queue_index);
+		else if (q_vector->tx.ring)
+			sprintf(q_vector->name, "%s-tx-%u", netdev->name,
+			        q_vector->tx.ring->queue_index);
+		else if (q_vector->rx.ring)
+			sprintf(q_vector->name, "%s-rx-%u", netdev->name,
+			        q_vector->rx.ring->queue_index);
+		else
+			sprintf(q_vector->name, "%s-unused", netdev->name);
+
+		err = request_irq(adapter->msix_entries[vector].vector,
+		                  igb_msix_ring, 0, q_vector->name,
+		                  q_vector);
+		if (err)
+			goto err_free;
+	}
+
+	igb_configure_msix(adapter);
+	return 0;
+
+err_free:
+	/* free already assigned IRQs */
+	free_irq(adapter->msix_entries[free_vector++].vector, adapter);
+
+	vector--;
+	for (i = 0; i < vector; i++) {
+		free_irq(adapter->msix_entries[free_vector++].vector,
+			 adapter->q_vector[i]);
+	}
+err_out:
+	return err;
+}
+
+static void igb_reset_interrupt_capability(struct igb_adapter *adapter)
+{
+	if (adapter->msix_entries) {
+		pci_disable_msix(adapter->pdev);
+		kfree(adapter->msix_entries);
+		adapter->msix_entries = NULL;
+	} else if (adapter->flags & IGB_FLAG_HAS_MSI) {
+		pci_disable_msi(adapter->pdev);
+	}
+}
+
+/**
+ * igb_free_q_vector - Free memory allocated for specific interrupt vector
+ * @adapter: board private structure to initialize
+ * @v_idx: Index of vector to be freed
+ *
+ * This function frees the memory allocated to the q_vector.  In addition if
+ * NAPI is enabled it will delete any references to the NAPI struct prior
+ * to freeing the q_vector.
+ **/
+static void igb_free_q_vector(struct igb_adapter *adapter, int v_idx)
+{
+	struct igb_q_vector *q_vector = adapter->q_vector[v_idx];
+
+	if (q_vector->tx.ring)
+		adapter->tx_ring[q_vector->tx.ring->queue_index] = NULL;
+
+	if (q_vector->rx.ring)
+		adapter->tx_ring[q_vector->rx.ring->queue_index] = NULL;
+
+	adapter->q_vector[v_idx] = NULL;
+	netif_napi_del(&q_vector->napi);
+#ifndef IGB_NO_LRO
+	__skb_queue_purge(&q_vector->lrolist.active);
+#endif
+	kfree(q_vector);
+}
+
+/**
+ * igb_free_q_vectors - Free memory allocated for interrupt vectors
+ * @adapter: board private structure to initialize
+ *
+ * This function frees the memory allocated to the q_vectors.  In addition if
+ * NAPI is enabled it will delete any references to the NAPI struct prior
+ * to freeing the q_vector.
+ **/
+static void igb_free_q_vectors(struct igb_adapter *adapter)
+{
+	int v_idx = adapter->num_q_vectors;
+
+	adapter->num_tx_queues = 0;
+	adapter->num_rx_queues = 0;
+	adapter->num_q_vectors = 0;
+
+	while (v_idx--)
+		igb_free_q_vector(adapter, v_idx);
+}
+
+/**
+ * igb_clear_interrupt_scheme - reset the device to a state of no interrupts
+ *
+ * This function resets the device so that it has 0 rx queues, tx queues, and
+ * MSI-X interrupts allocated.
+ */
+static void igb_clear_interrupt_scheme(struct igb_adapter *adapter)
+{
+	igb_free_q_vectors(adapter);
+	igb_reset_interrupt_capability(adapter);
+}
+
+/**
+ * igb_process_mdd_event
+ * @adapter - board private structure
+ *
+ * Identify a malicious VF, disable the VF TX/RX queues and log a message.
+ */
+static void igb_process_mdd_event(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 lvmmc, vfte, vfre, mdfb;
+	u8 vf_queue;
+
+	lvmmc = E1000_READ_REG(hw, E1000_LVMMC);
+	vf_queue = lvmmc >> 29;
+
+	/* VF index cannot be bigger or equal to VFs allocated */
+	if (vf_queue >= adapter->vfs_allocated_count)
+		return;
+
+	netdev_info(adapter->netdev,
+	            "VF %d misbehaved. VF queues are disabled. "
+	            "VM misbehavior code is 0x%x\n", vf_queue, lvmmc);
+
+	/* Disable VFTE and VFRE related bits */
+	vfte = E1000_READ_REG(hw, E1000_VFTE);
+	vfte &= ~(1 << vf_queue);
+	E1000_WRITE_REG(hw, E1000_VFTE, vfte);
+
+	vfre = E1000_READ_REG(hw, E1000_VFRE);
+	vfre &= ~(1 << vf_queue);
+	E1000_WRITE_REG(hw, E1000_VFRE, vfre);
+
+	/* Disable MDFB related bit. Clear on write */
+	mdfb = E1000_READ_REG(hw, E1000_MDFB);
+	mdfb |= (1 << vf_queue);
+	E1000_WRITE_REG(hw, E1000_MDFB, mdfb);
+
+	/* Reset the specific VF */
+	E1000_WRITE_REG(hw, E1000_VTCTRL(vf_queue), E1000_VTCTRL_RST);
+}
+
+/**
+ * igb_disable_mdd
+ * @adapter - board private structure
+ *
+ * Disable MDD behavior in the HW
+ **/
+static void igb_disable_mdd(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 reg;
+
+	if ((hw->mac.type != e1000_i350) ||
+	    (hw->mac.type != e1000_i354))
+		return;
+
+	reg = E1000_READ_REG(hw, E1000_DTXCTL);
+	reg &= (~E1000_DTXCTL_MDP_EN);
+	E1000_WRITE_REG(hw, E1000_DTXCTL, reg);
+}
+
+/**
+ * igb_enable_mdd
+ * @adapter - board private structure
+ *
+ * Enable the HW to detect malicious driver and sends an interrupt to
+ * the driver.
+ **/
+static void igb_enable_mdd(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 reg;
+
+	/* Only available on i350 device */
+	if (hw->mac.type != e1000_i350)
+		return;
+
+	reg = E1000_READ_REG(hw, E1000_DTXCTL);
+	reg |= E1000_DTXCTL_MDP_EN;
+	E1000_WRITE_REG(hw, E1000_DTXCTL, reg);
+}
+
+/**
+ * igb_reset_sriov_capability - disable SR-IOV if enabled
+ *
+ * Attempt to disable single root IO virtualization capabilites present in the
+ * kernel.
+ **/
+static void igb_reset_sriov_capability(struct igb_adapter *adapter)
+{
+	struct pci_dev *pdev = adapter->pdev;
+	struct e1000_hw *hw = &adapter->hw;
+
+	/* reclaim resources allocated to VFs */
+	if (adapter->vf_data) {
+		if (!pci_vfs_assigned(pdev)) {
+			/*
+			 * disable iov and allow time for transactions to
+			 * clear
+			 */
+			pci_disable_sriov(pdev);
+			msleep(500);
+
+			dev_info(pci_dev_to_dev(pdev), "IOV Disabled\n");
+		} else {
+			dev_info(pci_dev_to_dev(pdev), "IOV Not Disabled\n "
+					"VF(s) are assigned to guests!\n");
+		}
+		/* Disable Malicious Driver Detection */
+		igb_disable_mdd(adapter);
+
+		/* free vf data storage */
+		kfree(adapter->vf_data);
+		adapter->vf_data = NULL;
+
+		/* switch rings back to PF ownership */
+		E1000_WRITE_REG(hw, E1000_IOVCTL,
+				E1000_IOVCTL_REUSE_VFQ);
+		E1000_WRITE_FLUSH(hw);
+		msleep(100);
+	}
+
+	adapter->vfs_allocated_count = 0;
+}
+
+/**
+ * igb_set_sriov_capability - setup SR-IOV if supported
+ *
+ * Attempt to enable single root IO virtualization capabilites present in the
+ * kernel.
+ **/
+static void igb_set_sriov_capability(struct igb_adapter *adapter)
+{
+	struct pci_dev *pdev = adapter->pdev;
+	int old_vfs = 0;
+	int i;
+
+	old_vfs = pci_num_vf(pdev);
+	if (old_vfs) {
+		dev_info(pci_dev_to_dev(pdev),
+				"%d pre-allocated VFs found - override "
+				"max_vfs setting of %d\n", old_vfs,
+				adapter->vfs_allocated_count);
+		adapter->vfs_allocated_count = old_vfs;
+	}
+	/* no VFs requested, do nothing */
+	if (!adapter->vfs_allocated_count)
+		return;
+
+	/* allocate vf data storage */
+	adapter->vf_data = kcalloc(adapter->vfs_allocated_count,
+	                           sizeof(struct vf_data_storage),
+	                           GFP_KERNEL);
+
+	if (adapter->vf_data) {
+		if (!old_vfs) {
+			if (pci_enable_sriov(pdev,
+					adapter->vfs_allocated_count))
+				goto err_out;
+		}
+		for (i = 0; i < adapter->vfs_allocated_count; i++)
+			igb_vf_configure(adapter, i);
+
+		switch (adapter->hw.mac.type) {
+		case e1000_82576:
+		case e1000_i350:
+			/* Enable VM to VM loopback by default */
+			adapter->flags |= IGB_FLAG_LOOPBACK_ENABLE;
+			break;
+		default:
+			/* Currently no other hardware supports loopback */
+			break;
+		}
+
+		/* DMA Coalescing is not supported in IOV mode. */
+		if (adapter->hw.mac.type >= e1000_i350)
+		adapter->dmac = IGB_DMAC_DISABLE;
+		if (adapter->hw.mac.type < e1000_i350)
+		adapter->flags |= IGB_FLAG_DETECT_BAD_DMA;
+		return;
+
+	}
+
+err_out:
+	kfree(adapter->vf_data);
+	adapter->vf_data = NULL;
+	adapter->vfs_allocated_count = 0;
+	dev_warn(pci_dev_to_dev(pdev),
+			"Failed to initialize SR-IOV virtualization\n");
+}
+
+/**
+ * igb_set_interrupt_capability - set MSI or MSI-X if supported
+ *
+ * Attempt to configure interrupts using the best available
+ * capabilities of the hardware and kernel.
+ **/
+static void igb_set_interrupt_capability(struct igb_adapter *adapter, bool msix)
+{
+	struct pci_dev *pdev = adapter->pdev;
+	int err;
+	int numvecs, i;
+
+	if (!msix)
+		adapter->int_mode = IGB_INT_MODE_MSI;
+
+	/* Number of supported queues. */
+	adapter->num_rx_queues = adapter->rss_queues;
+
+	if (adapter->vmdq_pools > 1)
+		adapter->num_rx_queues += adapter->vmdq_pools - 1;
+
+#ifdef HAVE_TX_MQ
+	if (adapter->vmdq_pools)
+		adapter->num_tx_queues = adapter->vmdq_pools;
+	else
+		adapter->num_tx_queues = adapter->num_rx_queues;
+#else
+	adapter->num_tx_queues = max_t(u32, 1, adapter->vmdq_pools);
+#endif
+
+	switch (adapter->int_mode) {
+	case IGB_INT_MODE_MSIX:
+		/* start with one vector for every rx queue */
+		numvecs = adapter->num_rx_queues;
+
+		/* if tx handler is separate add 1 for every tx queue */
+		if (!(adapter->flags & IGB_FLAG_QUEUE_PAIRS))
+			numvecs += adapter->num_tx_queues;
+
+		/* store the number of vectors reserved for queues */
+		adapter->num_q_vectors = numvecs;
+
+		/* add 1 vector for link status interrupts */
+		numvecs++;
+		adapter->msix_entries = kcalloc(numvecs,
+		                                sizeof(struct msix_entry),
+		                                GFP_KERNEL);
+		if (adapter->msix_entries) {
+			for (i = 0; i < numvecs; i++)
+				adapter->msix_entries[i].entry = i;
+
+#ifdef HAVE_PCI_ENABLE_MSIX
+			err = pci_enable_msix(pdev,
+			                      adapter->msix_entries, numvecs);
+#else
+			err = pci_enable_msix_range(pdev,
+					adapter->msix_entries,
+					numvecs,
+					numvecs);
+#endif
+			if (err == 0)
+				break;
+		}
+		/* MSI-X failed, so fall through and try MSI */
+		dev_warn(pci_dev_to_dev(pdev), "Failed to initialize MSI-X interrupts. "
+		         "Falling back to MSI interrupts.\n");
+		igb_reset_interrupt_capability(adapter);
+	case IGB_INT_MODE_MSI:
+		if (!pci_enable_msi(pdev))
+			adapter->flags |= IGB_FLAG_HAS_MSI;
+		else
+			dev_warn(pci_dev_to_dev(pdev), "Failed to initialize MSI "
+			         "interrupts.  Falling back to legacy "
+			         "interrupts.\n");
+		/* Fall through */
+	case IGB_INT_MODE_LEGACY:
+		/* disable advanced features and set number of queues to 1 */
+		igb_reset_sriov_capability(adapter);
+		adapter->vmdq_pools = 0;
+		adapter->rss_queues = 1;
+		adapter->flags |= IGB_FLAG_QUEUE_PAIRS;
+		adapter->num_rx_queues = 1;
+		adapter->num_tx_queues = 1;
+		adapter->num_q_vectors = 1;
+		/* Don't do anything; this is system default */
+		break;
+	}
+}
+
+static void igb_add_ring(struct igb_ring *ring,
+			 struct igb_ring_container *head)
+{
+	head->ring = ring;
+	head->count++;
+}
+
+/**
+ * igb_alloc_q_vector - Allocate memory for a single interrupt vector
+ * @adapter: board private structure to initialize
+ * @v_count: q_vectors allocated on adapter, used for ring interleaving
+ * @v_idx: index of vector in adapter struct
+ * @txr_count: total number of Tx rings to allocate
+ * @txr_idx: index of first Tx ring to allocate
+ * @rxr_count: total number of Rx rings to allocate
+ * @rxr_idx: index of first Rx ring to allocate
+ *
+ * We allocate one q_vector.  If allocation fails we return -ENOMEM.
+ **/
+static int igb_alloc_q_vector(struct igb_adapter *adapter,
+			      unsigned int v_count, unsigned int v_idx,
+			      unsigned int txr_count, unsigned int txr_idx,
+			      unsigned int rxr_count, unsigned int rxr_idx)
+{
+	struct igb_q_vector *q_vector;
+	struct igb_ring *ring;
+	int ring_count, size;
+
+	/* igb only supports 1 Tx and/or 1 Rx queue per vector */
+	if (txr_count > 1 || rxr_count > 1)
+		return -ENOMEM;
+
+	ring_count = txr_count + rxr_count;
+	size = sizeof(struct igb_q_vector) +
+	       (sizeof(struct igb_ring) * ring_count);
+
+	/* allocate q_vector and rings */
+	q_vector = kzalloc(size, GFP_KERNEL);
+	if (!q_vector)
+		return -ENOMEM;
+
+#ifndef IGB_NO_LRO
+	/* initialize LRO */
+	__skb_queue_head_init(&q_vector->lrolist.active);
+
+#endif
+	/* initialize NAPI */
+	netif_napi_add(adapter->netdev, &q_vector->napi,
+		       igb_poll, 64);
+
+	/* tie q_vector and adapter together */
+	adapter->q_vector[v_idx] = q_vector;
+	q_vector->adapter = adapter;
+
+	/* initialize work limits */
+	q_vector->tx.work_limit = adapter->tx_work_limit;
+
+	/* initialize ITR configuration */
+	q_vector->itr_register = adapter->hw.hw_addr + E1000_EITR(0);
+	q_vector->itr_val = IGB_START_ITR;
+
+	/* initialize pointer to rings */
+	ring = q_vector->ring;
+
+	/* initialize ITR */
+	if (rxr_count) {
+		/* rx or rx/tx vector */
+		if (!adapter->rx_itr_setting || adapter->rx_itr_setting > 3)
+			q_vector->itr_val = adapter->rx_itr_setting;
+	} else {
+		/* tx only vector */
+		if (!adapter->tx_itr_setting || adapter->tx_itr_setting > 3)
+			q_vector->itr_val = adapter->tx_itr_setting;
+	}
+
+	if (txr_count) {
+		/* assign generic ring traits */
+		ring->dev = &adapter->pdev->dev;
+		ring->netdev = adapter->netdev;
+
+		/* configure backlink on ring */
+		ring->q_vector = q_vector;
+
+		/* update q_vector Tx values */
+		igb_add_ring(ring, &q_vector->tx);
+
+		/* For 82575, context index must be unique per ring. */
+		if (adapter->hw.mac.type == e1000_82575)
+			set_bit(IGB_RING_FLAG_TX_CTX_IDX, &ring->flags);
+
+		/* apply Tx specific ring traits */
+		ring->count = adapter->tx_ring_count;
+		ring->queue_index = txr_idx;
+
+		/* assign ring to adapter */
+		adapter->tx_ring[txr_idx] = ring;
+
+		/* push pointer to next ring */
+		ring++;
+	}
+
+	if (rxr_count) {
+		/* assign generic ring traits */
+		ring->dev = &adapter->pdev->dev;
+		ring->netdev = adapter->netdev;
+
+		/* configure backlink on ring */
+		ring->q_vector = q_vector;
+
+		/* update q_vector Rx values */
+		igb_add_ring(ring, &q_vector->rx);
+
+#ifndef HAVE_NDO_SET_FEATURES
+		/* enable rx checksum */
+		set_bit(IGB_RING_FLAG_RX_CSUM, &ring->flags);
+
+#endif
+		/* set flag indicating ring supports SCTP checksum offload */
+		if (adapter->hw.mac.type >= e1000_82576)
+			set_bit(IGB_RING_FLAG_RX_SCTP_CSUM, &ring->flags);
+
+		if ((adapter->hw.mac.type == e1000_i350) ||
+		    (adapter->hw.mac.type == e1000_i354))
+			set_bit(IGB_RING_FLAG_RX_LB_VLAN_BSWAP, &ring->flags);
+
+		/* apply Rx specific ring traits */
+		ring->count = adapter->rx_ring_count;
+		ring->queue_index = rxr_idx;
+
+		/* assign ring to adapter */
+		adapter->rx_ring[rxr_idx] = ring;
+	}
+
+	return 0;
+}
+
+/**
+ * igb_alloc_q_vectors - Allocate memory for interrupt vectors
+ * @adapter: board private structure to initialize
+ *
+ * We allocate one q_vector per queue interrupt.  If allocation fails we
+ * return -ENOMEM.
+ **/
+static int igb_alloc_q_vectors(struct igb_adapter *adapter)
+{
+	int q_vectors = adapter->num_q_vectors;
+	int rxr_remaining = adapter->num_rx_queues;
+	int txr_remaining = adapter->num_tx_queues;
+	int rxr_idx = 0, txr_idx = 0, v_idx = 0;
+	int err;
+
+	if (q_vectors >= (rxr_remaining + txr_remaining)) {
+		for (; rxr_remaining; v_idx++) {
+			err = igb_alloc_q_vector(adapter, q_vectors, v_idx,
+						 0, 0, 1, rxr_idx);
+
+			if (err)
+				goto err_out;
+
+			/* update counts and index */
+			rxr_remaining--;
+			rxr_idx++;
+		}
+	}
+
+	for (; v_idx < q_vectors; v_idx++) {
+		int rqpv = DIV_ROUND_UP(rxr_remaining, q_vectors - v_idx);
+		int tqpv = DIV_ROUND_UP(txr_remaining, q_vectors - v_idx);
+		err = igb_alloc_q_vector(adapter, q_vectors, v_idx,
+					 tqpv, txr_idx, rqpv, rxr_idx);
+
+		if (err)
+			goto err_out;
+
+		/* update counts and index */
+		rxr_remaining -= rqpv;
+		txr_remaining -= tqpv;
+		rxr_idx++;
+		txr_idx++;
+	}
+
+	return 0;
+
+err_out:
+	adapter->num_tx_queues = 0;
+	adapter->num_rx_queues = 0;
+	adapter->num_q_vectors = 0;
+
+	while (v_idx--)
+		igb_free_q_vector(adapter, v_idx);
+
+	return -ENOMEM;
+}
+
+/**
+ * igb_init_interrupt_scheme - initialize interrupts, allocate queues/vectors
+ *
+ * This function initializes the interrupts and allocates all of the queues.
+ **/
+static int igb_init_interrupt_scheme(struct igb_adapter *adapter, bool msix)
+{
+	struct pci_dev *pdev = adapter->pdev;
+	int err;
+
+	igb_set_interrupt_capability(adapter, msix);
+
+	err = igb_alloc_q_vectors(adapter);
+	if (err) {
+		dev_err(pci_dev_to_dev(pdev), "Unable to allocate memory for vectors\n");
+		goto err_alloc_q_vectors;
+	}
+
+	igb_cache_ring_register(adapter);
+
+	return 0;
+
+err_alloc_q_vectors:
+	igb_reset_interrupt_capability(adapter);
+	return err;
+}
+
+/**
+ * igb_request_irq - initialize interrupts
+ *
+ * Attempts to configure interrupts using the best available
+ * capabilities of the hardware and kernel.
+ **/
+static int igb_request_irq(struct igb_adapter *adapter)
+{
+	struct net_device *netdev = adapter->netdev;
+	struct pci_dev *pdev = adapter->pdev;
+	int err = 0;
+
+	if (adapter->msix_entries) {
+		err = igb_request_msix(adapter);
+		if (!err)
+			goto request_done;
+		/* fall back to MSI */
+		igb_free_all_tx_resources(adapter);
+		igb_free_all_rx_resources(adapter);
+
+		igb_clear_interrupt_scheme(adapter);
+		igb_reset_sriov_capability(adapter);
+		err = igb_init_interrupt_scheme(adapter, false);
+		if (err)
+			goto request_done;
+		igb_setup_all_tx_resources(adapter);
+		igb_setup_all_rx_resources(adapter);
+		igb_configure(adapter);
+	}
+
+	igb_assign_vector(adapter->q_vector[0], 0);
+
+	if (adapter->flags & IGB_FLAG_HAS_MSI) {
+		err = request_irq(pdev->irq, &igb_intr_msi, 0,
+				  netdev->name, adapter);
+		if (!err)
+			goto request_done;
+
+		/* fall back to legacy interrupts */
+		igb_reset_interrupt_capability(adapter);
+		adapter->flags &= ~IGB_FLAG_HAS_MSI;
+	}
+
+	err = request_irq(pdev->irq, &igb_intr, IRQF_SHARED,
+			  netdev->name, adapter);
+
+	if (err)
+		dev_err(pci_dev_to_dev(pdev), "Error %d getting interrupt\n",
+			err);
+
+request_done:
+	return err;
+}
+
+static void igb_free_irq(struct igb_adapter *adapter)
+{
+	if (adapter->msix_entries) {
+		int vector = 0, i;
+
+		free_irq(adapter->msix_entries[vector++].vector, adapter);
+
+		for (i = 0; i < adapter->num_q_vectors; i++)
+			free_irq(adapter->msix_entries[vector++].vector,
+			         adapter->q_vector[i]);
+	} else {
+		free_irq(adapter->pdev->irq, adapter);
+	}
+}
+
+/**
+ * igb_irq_disable - Mask off interrupt generation on the NIC
+ * @adapter: board private structure
+ **/
+static void igb_irq_disable(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+
+	/*
+	 * we need to be careful when disabling interrupts.  The VFs are also
+	 * mapped into these registers and so clearing the bits can cause
+	 * issues on the VF drivers so we only need to clear what we set
+	 */
+	if (adapter->msix_entries) {
+		u32 regval = E1000_READ_REG(hw, E1000_EIAM);
+		E1000_WRITE_REG(hw, E1000_EIAM, regval & ~adapter->eims_enable_mask);
+		E1000_WRITE_REG(hw, E1000_EIMC, adapter->eims_enable_mask);
+		regval = E1000_READ_REG(hw, E1000_EIAC);
+		E1000_WRITE_REG(hw, E1000_EIAC, regval & ~adapter->eims_enable_mask);
+	}
+
+	E1000_WRITE_REG(hw, E1000_IAM, 0);
+	E1000_WRITE_REG(hw, E1000_IMC, ~0);
+	E1000_WRITE_FLUSH(hw);
+
+	if (adapter->msix_entries) {
+		int vector = 0, i;
+
+		synchronize_irq(adapter->msix_entries[vector++].vector);
+
+		for (i = 0; i < adapter->num_q_vectors; i++)
+			synchronize_irq(adapter->msix_entries[vector++].vector);
+	} else {
+		synchronize_irq(adapter->pdev->irq);
+	}
+}
+
+/**
+ * igb_irq_enable - Enable default interrupt generation settings
+ * @adapter: board private structure
+ **/
+static void igb_irq_enable(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+
+	if (adapter->msix_entries) {
+		u32 ims = E1000_IMS_LSC | E1000_IMS_DOUTSYNC | E1000_IMS_DRSTA;
+		u32 regval = E1000_READ_REG(hw, E1000_EIAC);
+		E1000_WRITE_REG(hw, E1000_EIAC, regval | adapter->eims_enable_mask);
+		regval = E1000_READ_REG(hw, E1000_EIAM);
+		E1000_WRITE_REG(hw, E1000_EIAM, regval | adapter->eims_enable_mask);
+		E1000_WRITE_REG(hw, E1000_EIMS, adapter->eims_enable_mask);
+		if (adapter->vfs_allocated_count) {
+			E1000_WRITE_REG(hw, E1000_MBVFIMR, 0xFF);
+			ims |= E1000_IMS_VMMB;
+			if (adapter->mdd)
+				if ((adapter->hw.mac.type == e1000_i350) ||
+				    (adapter->hw.mac.type == e1000_i354))
+				ims |= E1000_IMS_MDDET;
+		}
+		E1000_WRITE_REG(hw, E1000_IMS, ims);
+	} else {
+		E1000_WRITE_REG(hw, E1000_IMS, IMS_ENABLE_MASK |
+				E1000_IMS_DRSTA);
+		E1000_WRITE_REG(hw, E1000_IAM, IMS_ENABLE_MASK |
+				E1000_IMS_DRSTA);
+	}
+}
+
+static void igb_update_mng_vlan(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u16 vid = adapter->hw.mng_cookie.vlan_id;
+	u16 old_vid = adapter->mng_vlan_id;
+
+	if (hw->mng_cookie.status & E1000_MNG_DHCP_COOKIE_STATUS_VLAN) {
+		/* add VID to filter table */
+		igb_vfta_set(adapter, vid, TRUE);
+		adapter->mng_vlan_id = vid;
+	} else {
+		adapter->mng_vlan_id = IGB_MNG_VLAN_NONE;
+	}
+
+	if ((old_vid != (u16)IGB_MNG_VLAN_NONE) &&
+	    (vid != old_vid) &&
+#ifdef HAVE_VLAN_RX_REGISTER
+	    !vlan_group_get_device(adapter->vlgrp, old_vid)) {
+#else
+	    !test_bit(old_vid, adapter->active_vlans)) {
+#endif
+		/* remove VID from filter table */
+		igb_vfta_set(adapter, old_vid, FALSE);
+	}
+}
+
+/**
+ * igb_release_hw_control - release control of the h/w to f/w
+ * @adapter: address of board private structure
+ *
+ * igb_release_hw_control resets CTRL_EXT:DRV_LOAD bit.
+ * For ASF and Pass Through versions of f/w this means that the
+ * driver is no longer loaded.
+ *
+ **/
+static void igb_release_hw_control(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 ctrl_ext;
+
+	/* Let firmware take over control of h/w */
+	ctrl_ext = E1000_READ_REG(hw, E1000_CTRL_EXT);
+	E1000_WRITE_REG(hw, E1000_CTRL_EXT,
+			ctrl_ext & ~E1000_CTRL_EXT_DRV_LOAD);
+}
+
+/**
+ * igb_get_hw_control - get control of the h/w from f/w
+ * @adapter: address of board private structure
+ *
+ * igb_get_hw_control sets CTRL_EXT:DRV_LOAD bit.
+ * For ASF and Pass Through versions of f/w this means that
+ * the driver is loaded.
+ *
+ **/
+static void igb_get_hw_control(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 ctrl_ext;
+
+	/* Let firmware know the driver has taken over */
+	ctrl_ext = E1000_READ_REG(hw, E1000_CTRL_EXT);
+	E1000_WRITE_REG(hw, E1000_CTRL_EXT,
+			ctrl_ext | E1000_CTRL_EXT_DRV_LOAD);
+}
+
+/**
+ * igb_configure - configure the hardware for RX and TX
+ * @adapter: private board structure
+ **/
+static void igb_configure(struct igb_adapter *adapter)
+{
+	struct net_device *netdev = adapter->netdev;
+	int i;
+
+	igb_get_hw_control(adapter);
+	igb_set_rx_mode(netdev);
+
+	igb_restore_vlan(adapter);
+
+	igb_setup_tctl(adapter);
+	igb_setup_mrqc(adapter);
+	igb_setup_rctl(adapter);
+
+	igb_configure_tx(adapter);
+	igb_configure_rx(adapter);
+
+	e1000_rx_fifo_flush_82575(&adapter->hw);
+#ifdef CONFIG_NETDEVICES_MULTIQUEUE
+	if (adapter->num_tx_queues > 1)
+		netdev->features |= NETIF_F_MULTI_QUEUE;
+	else
+		netdev->features &= ~NETIF_F_MULTI_QUEUE;
+#endif
+
+	/* call igb_desc_unused which always leaves
+	 * at least 1 descriptor unused to make sure
+	 * next_to_use != next_to_clean */
+	for (i = 0; i < adapter->num_rx_queues; i++) {
+		struct igb_ring *ring = adapter->rx_ring[i];
+		igb_alloc_rx_buffers(ring, igb_desc_unused(ring));
+	}
+}
+
+/**
+ * igb_power_up_link - Power up the phy/serdes link
+ * @adapter: address of board private structure
+ **/
+void igb_power_up_link(struct igb_adapter *adapter)
+{
+	e1000_phy_hw_reset(&adapter->hw);
+
+	if (adapter->hw.phy.media_type == e1000_media_type_copper)
+		e1000_power_up_phy(&adapter->hw);
+	else
+		e1000_power_up_fiber_serdes_link(&adapter->hw);
+}
+
+/**
+ * igb_power_down_link - Power down the phy/serdes link
+ * @adapter: address of board private structure
+ */
+static void igb_power_down_link(struct igb_adapter *adapter)
+{
+	if (adapter->hw.phy.media_type == e1000_media_type_copper)
+		e1000_power_down_phy(&adapter->hw);
+	else
+		e1000_shutdown_fiber_serdes_link(&adapter->hw);
+}
+
+/* Detect and switch function for Media Auto Sense */
+static void igb_check_swap_media(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 ctrl_ext, connsw;
+	bool swap_now = false;
+	bool link;
+
+	ctrl_ext = E1000_READ_REG(hw, E1000_CTRL_EXT);
+	connsw = E1000_READ_REG(hw, E1000_CONNSW);
+	link = igb_has_link(adapter);
+	(void) link;
+
+	/* need to live swap if current media is copper and we have fiber/serdes
+	 * to go to.
+	 */
+
+	if ((hw->phy.media_type == e1000_media_type_copper) &&
+	    (!(connsw & E1000_CONNSW_AUTOSENSE_EN))) {
+		swap_now = true;
+	} else if (!(connsw & E1000_CONNSW_SERDESD)) {
+		/* copper signal takes time to appear */
+		if (adapter->copper_tries < 2) {
+			adapter->copper_tries++;
+			connsw |= E1000_CONNSW_AUTOSENSE_CONF;
+			E1000_WRITE_REG(hw, E1000_CONNSW, connsw);
+			return;
+		} else {
+			adapter->copper_tries = 0;
+			if ((connsw & E1000_CONNSW_PHYSD) &&
+			    (!(connsw & E1000_CONNSW_PHY_PDN))) {
+				swap_now = true;
+				connsw &= ~E1000_CONNSW_AUTOSENSE_CONF;
+				E1000_WRITE_REG(hw, E1000_CONNSW, connsw);
+			}
+		}
+	}
+
+	if (swap_now) {
+		switch (hw->phy.media_type) {
+		case e1000_media_type_copper:
+			dev_info(pci_dev_to_dev(adapter->pdev),
+				 "%s:MAS: changing media to fiber/serdes\n",
+			adapter->netdev->name);
+			ctrl_ext |=
+				E1000_CTRL_EXT_LINK_MODE_PCIE_SERDES;
+			adapter->flags |= IGB_FLAG_MEDIA_RESET;
+			adapter->copper_tries = 0;
+			break;
+		case e1000_media_type_internal_serdes:
+		case e1000_media_type_fiber:
+			dev_info(pci_dev_to_dev(adapter->pdev),
+				 "%s:MAS: changing media to copper\n",
+				 adapter->netdev->name);
+			ctrl_ext &=
+				~E1000_CTRL_EXT_LINK_MODE_PCIE_SERDES;
+			adapter->flags |= IGB_FLAG_MEDIA_RESET;
+			break;
+		default:
+			/* shouldn't get here during regular operation */
+			dev_err(pci_dev_to_dev(adapter->pdev),
+				"%s:AMS: Invalid media type found, returning\n",
+				adapter->netdev->name);
+			break;
+		}
+		E1000_WRITE_REG(hw, E1000_CTRL_EXT, ctrl_ext);
+	}
+}
+
+#ifdef HAVE_I2C_SUPPORT
+/*  igb_get_i2c_data - Reads the I2C SDA data bit
+ *  @hw: pointer to hardware structure
+ *  @i2cctl: Current value of I2CCTL register
+ *
+ *  Returns the I2C data bit value
+ */
+static int igb_get_i2c_data(void *data)
+{
+	struct igb_adapter *adapter = data;
+	struct e1000_hw *hw = &adapter->hw;
+	s32 i2cctl = E1000_READ_REG(hw, E1000_I2CPARAMS);
+
+	return (i2cctl & E1000_I2C_DATA_IN) != 0;
+}
+
+/* igb_set_i2c_data - Sets the I2C data bit
+ *  @data: pointer to hardware structure
+ *  @state: I2C data value (0 or 1) to set
+ *
+ *  Sets the I2C data bit
+ */
+static void igb_set_i2c_data(void *data, int state)
+{
+	struct igb_adapter *adapter = data;
+	struct e1000_hw *hw = &adapter->hw;
+	s32 i2cctl = E1000_READ_REG(hw, E1000_I2CPARAMS);
+
+	if (state)
+		i2cctl |= E1000_I2C_DATA_OUT;
+	else
+		i2cctl &= ~E1000_I2C_DATA_OUT;
+
+	i2cctl &= ~E1000_I2C_DATA_OE_N;
+	i2cctl |= E1000_I2C_CLK_OE_N;
+
+	E1000_WRITE_REG(hw, E1000_I2CPARAMS, i2cctl);
+	E1000_WRITE_FLUSH(hw);
+
+}
+
+/* igb_set_i2c_clk - Sets the I2C SCL clock
+ *  @data: pointer to hardware structure
+ *  @state: state to set clock
+ *
+ *  Sets the I2C clock line to state
+ */
+static void igb_set_i2c_clk(void *data, int state)
+{
+	struct igb_adapter *adapter = data;
+	struct e1000_hw *hw = &adapter->hw;
+	s32 i2cctl = E1000_READ_REG(hw, E1000_I2CPARAMS);
+
+	if (state) {
+		i2cctl |= E1000_I2C_CLK_OUT;
+		i2cctl &= ~E1000_I2C_CLK_OE_N;
+	} else {
+		i2cctl &= ~E1000_I2C_CLK_OUT;
+		i2cctl &= ~E1000_I2C_CLK_OE_N;
+	}
+	E1000_WRITE_REG(hw, E1000_I2CPARAMS, i2cctl);
+	E1000_WRITE_FLUSH(hw);
+}
+
+/* igb_get_i2c_clk - Gets the I2C SCL clock state
+ *  @data: pointer to hardware structure
+ *
+ *  Gets the I2C clock state
+ */
+static int igb_get_i2c_clk(void *data)
+{
+	struct igb_adapter *adapter = data;
+	struct e1000_hw *hw = &adapter->hw;
+	s32 i2cctl = E1000_READ_REG(hw, E1000_I2CPARAMS);
+
+	return (i2cctl & E1000_I2C_CLK_IN) != 0;
+}
+
+static const struct i2c_algo_bit_data igb_i2c_algo = {
+	.setsda		= igb_set_i2c_data,
+	.setscl		= igb_set_i2c_clk,
+	.getsda		= igb_get_i2c_data,
+	.getscl		= igb_get_i2c_clk,
+	.udelay		= 5,
+	.timeout	= 20,
+};
+
+/*  igb_init_i2c - Init I2C interface
+ *  @adapter: pointer to adapter structure
+ *
+ */
+static s32 igb_init_i2c(struct igb_adapter *adapter)
+{
+	s32 status = E1000_SUCCESS;
+
+	/* I2C interface supported on i350 devices */
+	if (adapter->hw.mac.type != e1000_i350)
+		return E1000_SUCCESS;
+
+	/* Initialize the i2c bus which is controlled by the registers.
+	 * This bus will use the i2c_algo_bit structue that implements
+	 * the protocol through toggling of the 4 bits in the register.
+	 */
+	adapter->i2c_adap.owner = THIS_MODULE;
+	adapter->i2c_algo = igb_i2c_algo;
+	adapter->i2c_algo.data = adapter;
+	adapter->i2c_adap.algo_data = &adapter->i2c_algo;
+	adapter->i2c_adap.dev.parent = &adapter->pdev->dev;
+	strlcpy(adapter->i2c_adap.name, "igb BB",
+		sizeof(adapter->i2c_adap.name));
+	status = i2c_bit_add_bus(&adapter->i2c_adap);
+	return status;
+}
+
+#endif /* HAVE_I2C_SUPPORT */
+/**
+ * igb_up - Open the interface and prepare it to handle traffic
+ * @adapter: board private structure
+ **/
+int igb_up(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	int i;
+
+	/* hardware has been reset, we need to reload some things */
+	igb_configure(adapter);
+
+	clear_bit(__IGB_DOWN, &adapter->state);
+
+	for (i = 0; i < adapter->num_q_vectors; i++)
+		napi_enable(&(adapter->q_vector[i]->napi));
+
+	if (adapter->msix_entries)
+		igb_configure_msix(adapter);
+	else
+		igb_assign_vector(adapter->q_vector[0], 0);
+
+	igb_configure_lli(adapter);
+
+	/* Clear any pending interrupts. */
+	E1000_READ_REG(hw, E1000_ICR);
+	igb_irq_enable(adapter);
+
+	/* notify VFs that reset has been completed */
+	if (adapter->vfs_allocated_count) {
+		u32 reg_data = E1000_READ_REG(hw, E1000_CTRL_EXT);
+		reg_data |= E1000_CTRL_EXT_PFRSTD;
+		E1000_WRITE_REG(hw, E1000_CTRL_EXT, reg_data);
+	}
+
+	netif_tx_start_all_queues(adapter->netdev);
+
+	if (adapter->flags & IGB_FLAG_DETECT_BAD_DMA)
+		schedule_work(&adapter->dma_err_task);
+	/* start the watchdog. */
+	hw->mac.get_link_status = 1;
+	schedule_work(&adapter->watchdog_task);
+
+	if ((adapter->flags & IGB_FLAG_EEE) &&
+	    (!hw->dev_spec._82575.eee_disable))
+		adapter->eee_advert = MDIO_EEE_100TX | MDIO_EEE_1000T;
+
+	return 0;
+}
+
+void igb_down(struct igb_adapter *adapter)
+{
+	struct net_device *netdev = adapter->netdev;
+	struct e1000_hw *hw = &adapter->hw;
+	u32 tctl, rctl;
+	int i;
+
+	/* signal that we're down so the interrupt handler does not
+	 * reschedule our watchdog timer */
+	set_bit(__IGB_DOWN, &adapter->state);
+
+	/* disable receives in the hardware */
+	rctl = E1000_READ_REG(hw, E1000_RCTL);
+	E1000_WRITE_REG(hw, E1000_RCTL, rctl & ~E1000_RCTL_EN);
+	/* flush and sleep below */
+
+	netif_tx_stop_all_queues(netdev);
+
+	/* disable transmits in the hardware */
+	tctl = E1000_READ_REG(hw, E1000_TCTL);
+	tctl &= ~E1000_TCTL_EN;
+	E1000_WRITE_REG(hw, E1000_TCTL, tctl);
+	/* flush both disables and wait for them to finish */
+	E1000_WRITE_FLUSH(hw);
+	usleep_range(10000, 20000);
+
+	for (i = 0; i < adapter->num_q_vectors; i++)
+		napi_disable(&(adapter->q_vector[i]->napi));
+
+	igb_irq_disable(adapter);
+
+	adapter->flags &= ~IGB_FLAG_NEED_LINK_UPDATE;
+
+	del_timer_sync(&adapter->watchdog_timer);
+	if (adapter->flags & IGB_FLAG_DETECT_BAD_DMA)
+		del_timer_sync(&adapter->dma_err_timer);
+	del_timer_sync(&adapter->phy_info_timer);
+
+	netif_carrier_off(netdev);
+
+	/* record the stats before reset*/
+	igb_update_stats(adapter);
+
+	adapter->link_speed = 0;
+	adapter->link_duplex = 0;
+
+#ifdef HAVE_PCI_ERS
+	if (!pci_channel_offline(adapter->pdev))
+		igb_reset(adapter);
+#else
+	igb_reset(adapter);
+#endif
+	igb_clean_all_tx_rings(adapter);
+	igb_clean_all_rx_rings(adapter);
+#ifdef IGB_DCA
+	/* since we reset the hardware DCA settings were cleared */
+	igb_setup_dca(adapter);
+#endif
+}
+
+void igb_reinit_locked(struct igb_adapter *adapter)
+{
+	WARN_ON(in_interrupt());
+	while (test_and_set_bit(__IGB_RESETTING, &adapter->state))
+		usleep_range(1000, 2000);
+	igb_down(adapter);
+	igb_up(adapter);
+	clear_bit(__IGB_RESETTING, &adapter->state);
+}
+
+/**
+ * igb_enable_mas - Media Autosense re-enable after swap
+ *
+ * @adapter: adapter struct
+ **/
+static s32  igb_enable_mas(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 connsw;
+	s32 ret_val = E1000_SUCCESS;
+
+	connsw = E1000_READ_REG(hw, E1000_CONNSW);
+	if (hw->phy.media_type == e1000_media_type_copper) {
+		/* configure for SerDes media detect */
+		if (!(connsw & E1000_CONNSW_SERDESD)) {
+			connsw |= E1000_CONNSW_ENRGSRC;
+			connsw |= E1000_CONNSW_AUTOSENSE_EN;
+			E1000_WRITE_REG(hw, E1000_CONNSW, connsw);
+			E1000_WRITE_FLUSH(hw);
+		} else if (connsw & E1000_CONNSW_SERDESD) {
+			/* already SerDes, no need to enable anything */
+			return ret_val;
+		} else {
+			dev_info(pci_dev_to_dev(adapter->pdev),
+			"%s:MAS: Unable to configure feature, disabling..\n",
+			adapter->netdev->name);
+			adapter->flags &= ~IGB_FLAG_MAS_ENABLE;
+		}
+	}
+	return ret_val;
+}
+
+void igb_reset(struct igb_adapter *adapter)
+{
+	struct pci_dev *pdev = adapter->pdev;
+	struct e1000_hw *hw = &adapter->hw;
+	struct e1000_mac_info *mac = &hw->mac;
+	struct e1000_fc_info *fc = &hw->fc;
+	u32 pba = 0, tx_space, min_tx_space, min_rx_space, hwm;
+
+	/* Repartition Pba for greater than 9k mtu
+	 * To take effect CTRL.RST is required.
+	 */
+	switch (mac->type) {
+	case e1000_i350:
+	case e1000_82580:
+	case e1000_i354:
+		pba = E1000_READ_REG(hw, E1000_RXPBS);
+		pba = e1000_rxpbs_adjust_82580(pba);
+		break;
+	case e1000_82576:
+		pba = E1000_READ_REG(hw, E1000_RXPBS);
+		pba &= E1000_RXPBS_SIZE_MASK_82576;
+		break;
+	case e1000_82575:
+	case e1000_i210:
+	case e1000_i211:
+	default:
+		pba = E1000_PBA_34K;
+		break;
+	}
+
+	if ((adapter->max_frame_size > ETH_FRAME_LEN + ETH_FCS_LEN) &&
+	    (mac->type < e1000_82576)) {
+		/* adjust PBA for jumbo frames */
+		E1000_WRITE_REG(hw, E1000_PBA, pba);
+
+		/* To maintain wire speed transmits, the Tx FIFO should be
+		 * large enough to accommodate two full transmit packets,
+		 * rounded up to the next 1KB and expressed in KB.  Likewise,
+		 * the Rx FIFO should be large enough to accommodate at least
+		 * one full receive packet and is similarly rounded up and
+		 * expressed in KB. */
+		pba = E1000_READ_REG(hw, E1000_PBA);
+		/* upper 16 bits has Tx packet buffer allocation size in KB */
+		tx_space = pba >> 16;
+		/* lower 16 bits has Rx packet buffer allocation size in KB */
+		pba &= 0xffff;
+		/* the tx fifo also stores 16 bytes of information about the tx
+		 * but don't include ethernet FCS because hardware appends it */
+		min_tx_space = (adapter->max_frame_size +
+				sizeof(union e1000_adv_tx_desc) -
+				ETH_FCS_LEN) * 2;
+		min_tx_space = ALIGN(min_tx_space, 1024);
+		min_tx_space >>= 10;
+		/* software strips receive CRC, so leave room for it */
+		min_rx_space = adapter->max_frame_size;
+		min_rx_space = ALIGN(min_rx_space, 1024);
+		min_rx_space >>= 10;
+
+		/* If current Tx allocation is less than the min Tx FIFO size,
+		 * and the min Tx FIFO size is less than the current Rx FIFO
+		 * allocation, take space away from current Rx allocation */
+		if (tx_space < min_tx_space &&
+		    ((min_tx_space - tx_space) < pba)) {
+			pba = pba - (min_tx_space - tx_space);
+
+			/* if short on rx space, rx wins and must trump tx
+			 * adjustment */
+			if (pba < min_rx_space)
+				pba = min_rx_space;
+		}
+		E1000_WRITE_REG(hw, E1000_PBA, pba);
+	}
+
+	/* flow control settings */
+	/* The high water mark must be low enough to fit one full frame
+	 * (or the size used for early receive) above it in the Rx FIFO.
+	 * Set it to the lower of:
+	 * - 90% of the Rx FIFO size, or
+	 * - the full Rx FIFO size minus one full frame */
+	hwm = min(((pba << 10) * 9 / 10),
+			((pba << 10) - 2 * adapter->max_frame_size));
+
+	fc->high_water = hwm & 0xFFFFFFF0;	/* 16-byte granularity */
+	fc->low_water = fc->high_water - 16;
+	fc->pause_time = 0xFFFF;
+	fc->send_xon = 1;
+	fc->current_mode = fc->requested_mode;
+
+	/* disable receive for all VFs and wait one second */
+	if (adapter->vfs_allocated_count) {
+		int i;
+		/*
+		 * Clear all flags except indication that the PF has set
+		 * the VF MAC addresses administratively
+		 */
+		for (i = 0 ; i < adapter->vfs_allocated_count; i++)
+			adapter->vf_data[i].flags &= IGB_VF_FLAG_PF_SET_MAC;
+
+		/* ping all the active vfs to let them know we are going down */
+		igb_ping_all_vfs(adapter);
+
+		/* disable transmits and receives */
+		E1000_WRITE_REG(hw, E1000_VFRE, 0);
+		E1000_WRITE_REG(hw, E1000_VFTE, 0);
+	}
+
+	/* Allow time for pending master requests to run */
+	e1000_reset_hw(hw);
+	E1000_WRITE_REG(hw, E1000_WUC, 0);
+
+	if (adapter->flags & IGB_FLAG_MEDIA_RESET) {
+		e1000_setup_init_funcs(hw, TRUE);
+		igb_check_options(adapter);
+		e1000_get_bus_info(hw);
+		adapter->flags &= ~IGB_FLAG_MEDIA_RESET;
+	}
+	if (adapter->flags & IGB_FLAG_MAS_ENABLE) {
+		if (igb_enable_mas(adapter))
+			dev_err(pci_dev_to_dev(pdev),
+				"Error enabling Media Auto Sense\n");
+	}
+	if (e1000_init_hw(hw))
+		dev_err(pci_dev_to_dev(pdev), "Hardware Error\n");
+
+	/*
+	 * Flow control settings reset on hardware reset, so guarantee flow
+	 * control is off when forcing speed.
+	 */
+	if (!hw->mac.autoneg)
+		e1000_force_mac_fc(hw);
+
+	igb_init_dmac(adapter, pba);
+	/* Re-initialize the thermal sensor on i350 devices. */
+	if (mac->type == e1000_i350 && hw->bus.func == 0) {
+		/*
+		 * If present, re-initialize the external thermal sensor
+		 * interface.
+		 */
+		if (adapter->ets)
+			e1000_set_i2c_bb(hw);
+		e1000_init_thermal_sensor_thresh(hw);
+	}
+
+	/*Re-establish EEE setting */
+	if (hw->phy.media_type == e1000_media_type_copper) {
+		switch (mac->type) {
+		case e1000_i350:
+		case e1000_i210:
+		case e1000_i211:
+			e1000_set_eee_i350(hw);
+			break;
+		case e1000_i354:
+			e1000_set_eee_i354(hw);
+			break;
+		default:
+			break;
+		}
+	}
+
+	if (!netif_running(adapter->netdev))
+		igb_power_down_link(adapter);
+
+	igb_update_mng_vlan(adapter);
+
+	/* Enable h/w to recognize an 802.1Q VLAN Ethernet packet */
+	E1000_WRITE_REG(hw, E1000_VET, ETHERNET_IEEE_VLAN_TYPE);
+
+
+#ifdef HAVE_PTP_1588_CLOCK
+	/* Re-enable PTP, where applicable. */
+	igb_ptp_reset(adapter);
+#endif /* HAVE_PTP_1588_CLOCK */
+
+	e1000_get_phy_info(hw);
+
+	adapter->devrc++;
+}
+
+#ifdef HAVE_NDO_SET_FEATURES
+static kni_netdev_features_t igb_fix_features(struct net_device *netdev,
+					      kni_netdev_features_t features)
+{
+	/*
+	 * Since there is no support for separate tx vlan accel
+	 * enabled make sure tx flag is cleared if rx is.
+	 */
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
+	if (!(features & NETIF_F_HW_VLAN_CTAG_RX))
+		features &= ~NETIF_F_HW_VLAN_CTAG_TX;
+#else
+	if (!(features & NETIF_F_HW_VLAN_RX))
+		features &= ~NETIF_F_HW_VLAN_TX;
+#endif
+
+	/* If Rx checksum is disabled, then LRO should also be disabled */
+	if (!(features & NETIF_F_RXCSUM))
+		features &= ~NETIF_F_LRO;
+
+	return features;
+}
+
+static int igb_set_features(struct net_device *netdev,
+			    kni_netdev_features_t features)
+{
+	u32 changed = netdev->features ^ features;
+
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
+	if (changed & NETIF_F_HW_VLAN_CTAG_RX)
+#else
+	if (changed & NETIF_F_HW_VLAN_RX)
+#endif
+		igb_vlan_mode(netdev, features);
+
+	return 0;
+}
+
+#ifdef NTF_SELF
+#ifdef USE_CONST_DEV_UC_CHAR
+static int igb_ndo_fdb_add(struct ndmsg *ndm, struct nlattr *tb[],
+			   struct net_device *dev,
+			   const unsigned char *addr,
+#ifdef HAVE_NDO_FDB_ADD_VID
+			   u16 vid,
+#endif
+			   u16 flags)
+#else
+static int igb_ndo_fdb_add(struct ndmsg *ndm,
+			   struct net_device *dev,
+			   unsigned char *addr,
+			   u16 flags)
+#endif
+{
+	struct igb_adapter *adapter = netdev_priv(dev);
+	struct e1000_hw *hw = &adapter->hw;
+	int err;
+
+	if (!(adapter->vfs_allocated_count))
+		return -EOPNOTSUPP;
+
+	/* Hardware does not support aging addresses so if a
+	 * ndm_state is given only allow permanent addresses
+	 */
+	if (ndm->ndm_state && !(ndm->ndm_state & NUD_PERMANENT)) {
+		pr_info("%s: FDB only supports static addresses\n",
+			igb_driver_name);
+		return -EINVAL;
+	}
+
+	if (is_unicast_ether_addr(addr) || is_link_local_ether_addr(addr)) {
+		u32 rar_uc_entries = hw->mac.rar_entry_count -
+					(adapter->vfs_allocated_count + 1);
+
+		if (netdev_uc_count(dev) < rar_uc_entries)
+			err = dev_uc_add_excl(dev, addr);
+		else
+			err = -ENOMEM;
+	} else if (is_multicast_ether_addr(addr)) {
+		err = dev_mc_add_excl(dev, addr);
+	} else {
+		err = -EINVAL;
+	}
+
+	/* Only return duplicate errors if NLM_F_EXCL is set */
+	if (err == -EEXIST && !(flags & NLM_F_EXCL))
+		err = 0;
+
+	return err;
+}
+
+#ifndef USE_DEFAULT_FDB_DEL_DUMP
+#ifdef USE_CONST_DEV_UC_CHAR
+static int igb_ndo_fdb_del(struct ndmsg *ndm,
+			   struct net_device *dev,
+			   const unsigned char *addr)
+#else
+static int igb_ndo_fdb_del(struct ndmsg *ndm,
+			   struct net_device *dev,
+			   unsigned char *addr)
+#endif
+{
+	struct igb_adapter *adapter = netdev_priv(dev);
+	int err = -EOPNOTSUPP;
+
+	if (ndm->ndm_state & NUD_PERMANENT) {
+		pr_info("%s: FDB only supports static addresses\n",
+			igb_driver_name);
+		return -EINVAL;
+	}
+
+	if (adapter->vfs_allocated_count) {
+		if (is_unicast_ether_addr(addr))
+			err = dev_uc_del(dev, addr);
+		else if (is_multicast_ether_addr(addr))
+			err = dev_mc_del(dev, addr);
+		else
+			err = -EINVAL;
+	}
+
+	return err;
+}
+
+static int igb_ndo_fdb_dump(struct sk_buff *skb,
+			    struct netlink_callback *cb,
+			    struct net_device *dev,
+			    int idx)
+{
+	struct igb_adapter *adapter = netdev_priv(dev);
+
+	if (adapter->vfs_allocated_count)
+		idx = ndo_dflt_fdb_dump(skb, cb, dev, idx);
+
+	return idx;
+}
+#endif /* USE_DEFAULT_FDB_DEL_DUMP */
+
+#ifdef HAVE_BRIDGE_ATTRIBS
+#ifdef HAVE_NDO_BRIDGE_SET_DEL_LINK_FLAGS
+static int igb_ndo_bridge_setlink(struct net_device *dev,
+				  struct nlmsghdr *nlh,
+#ifdef HAVE_NDO_BRIDGE_SETLINK_EXTACK
+				  u16 flags, struct netlink_ext_ack *extack)
+#else
+				  u16 flags)
+#endif
+
+#else
+static int igb_ndo_bridge_setlink(struct net_device *dev,
+				  struct nlmsghdr *nlh)
+#endif /* HAVE_NDO_BRIDGE_SET_DEL_LINK_FLAGS */
+{
+	struct igb_adapter *adapter = netdev_priv(dev);
+	struct e1000_hw *hw = &adapter->hw;
+	struct nlattr *attr, *br_spec;
+	int rem;
+
+	if (!(adapter->vfs_allocated_count))
+		return -EOPNOTSUPP;
+
+	switch (adapter->hw.mac.type) {
+	case e1000_82576:
+	case e1000_i350:
+	case e1000_i354:
+		break;
+	default:
+		return -EOPNOTSUPP;
+	}
+
+	br_spec = nlmsg_find_attr(nlh, sizeof(struct ifinfomsg), IFLA_AF_SPEC);
+
+	nla_for_each_nested(attr, br_spec, rem) {
+		__u16 mode;
+
+		if (nla_type(attr) != IFLA_BRIDGE_MODE)
+			continue;
+
+		mode = nla_get_u16(attr);
+		if (mode == BRIDGE_MODE_VEPA) {
+			e1000_vmdq_set_loopback_pf(hw, 0);
+			adapter->flags &= ~IGB_FLAG_LOOPBACK_ENABLE;
+		} else if (mode == BRIDGE_MODE_VEB) {
+			e1000_vmdq_set_loopback_pf(hw, 1);
+			adapter->flags |= IGB_FLAG_LOOPBACK_ENABLE;
+		} else
+			return -EINVAL;
+
+		netdev_info(adapter->netdev, "enabling bridge mode: %s\n",
+			    mode == BRIDGE_MODE_VEPA ? "VEPA" : "VEB");
+	}
+
+	return 0;
+}
+
+#ifdef HAVE_BRIDGE_FILTER
+#ifdef HAVE_NDO_BRIDGE_GETLINK_NLFLAGS
+static int igb_ndo_bridge_getlink(struct sk_buff *skb, u32 pid, u32 seq,
+				  struct net_device *dev, u32 filter_mask,
+				  int nlflags)
+#else
+static int igb_ndo_bridge_getlink(struct sk_buff *skb, u32 pid, u32 seq,
+				  struct net_device *dev, u32 filter_mask)
+#endif /* HAVE_NDO_BRIDGE_GETLINK_NLFLAGS */
+#else
+static int igb_ndo_bridge_getlink(struct sk_buff *skb, u32 pid, u32 seq,
+				  struct net_device *dev)
+#endif
+{
+	struct igb_adapter *adapter = netdev_priv(dev);
+	u16 mode;
+
+	if (!(adapter->vfs_allocated_count))
+		return -EOPNOTSUPP;
+
+	if (adapter->flags & IGB_FLAG_LOOPBACK_ENABLE)
+		mode = BRIDGE_MODE_VEB;
+	else
+		mode = BRIDGE_MODE_VEPA;
+
+#ifdef HAVE_NDO_DFLT_BRIDGE_ADD_MASK
+#ifdef HAVE_NDO_BRIDGE_GETLINK_NLFLAGS
+#ifdef HAVE_NDO_BRIDGE_GETLINK_FILTER_MASK_VLAN_FILL
+	return ndo_dflt_bridge_getlink(skb, pid, seq, dev, mode, 0, 0,
+				nlflags, filter_mask, NULL);
+#else
+	return ndo_dflt_bridge_getlink(skb, pid, seq, dev, mode, 0, 0, nlflags);
+#endif /* HAVE_NDO_BRIDGE_GETLINK_FILTER_MASK_VLAN_FILL */
+#else
+	return ndo_dflt_bridge_getlink(skb, pid, seq, dev, mode, 0, 0);
+#endif /* HAVE_NDO_BRIDGE_GETLINK_NLFLAGS */
+#else
+	return ndo_dflt_bridge_getlink(skb, pid, seq, dev, mode);
+#endif /* HAVE_NDO_DFLT_BRIDGE_ADD_MASK */
+}
+#endif /* HAVE_BRIDGE_ATTRIBS */
+#endif /* NTF_SELF */
+
+#endif /* HAVE_NDO_SET_FEATURES */
+#ifdef HAVE_NET_DEVICE_OPS
+static const struct net_device_ops igb_netdev_ops = {
+	.ndo_open		= igb_open,
+	.ndo_stop		= igb_close,
+	.ndo_start_xmit		= igb_xmit_frame,
+	.ndo_get_stats		= igb_get_stats,
+	.ndo_set_rx_mode	= igb_set_rx_mode,
+	.ndo_set_mac_address	= igb_set_mac,
+	.ndo_change_mtu		= igb_change_mtu,
+	.ndo_do_ioctl		= igb_ioctl,
+	.ndo_tx_timeout		= igb_tx_timeout,
+	.ndo_validate_addr	= eth_validate_addr,
+	//为设备添加vlan id　过滤时将使用此函数
+	.ndo_vlan_rx_add_vid	= igb_vlan_rx_add_vid,
+	.ndo_vlan_rx_kill_vid	= igb_vlan_rx_kill_vid,
+#ifdef IFLA_VF_MAX
+	.ndo_set_vf_mac		= igb_ndo_set_vf_mac,
+	.ndo_set_vf_vlan	= igb_ndo_set_vf_vlan,
+#ifdef HAVE_VF_MIN_MAX_TXRATE
+	.ndo_set_vf_rate	= igb_ndo_set_vf_bw,
+#else /* HAVE_VF_MIN_MAX_TXRATE */
+	.ndo_set_vf_tx_rate	= igb_ndo_set_vf_bw,
+#endif /* HAVE_VF_MIN_MAX_TXRATE */
+	.ndo_get_vf_config	= igb_ndo_get_vf_config,
+#ifdef HAVE_VF_SPOOFCHK_CONFIGURE
+	.ndo_set_vf_spoofchk	= igb_ndo_set_vf_spoofchk,
+#endif /* HAVE_VF_SPOOFCHK_CONFIGURE */
+#endif /* IFLA_VF_MAX */
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= igb_netpoll,
+#endif
+#ifdef HAVE_NDO_SET_FEATURES
+	.ndo_fix_features	= igb_fix_features,
+	.ndo_set_features	= igb_set_features,
+#endif
+#ifdef HAVE_VLAN_RX_REGISTER
+	.ndo_vlan_rx_register	= igb_vlan_mode,
+#endif
+#ifndef HAVE_RHEL6_NETDEV_OPS_EXT_FDB
+#ifdef NTF_SELF
+	.ndo_fdb_add		= igb_ndo_fdb_add,
+#ifndef USE_DEFAULT_FDB_DEL_DUMP
+	.ndo_fdb_del		= igb_ndo_fdb_del,
+	.ndo_fdb_dump		= igb_ndo_fdb_dump,
+#endif
+#endif /* ! HAVE_RHEL6_NETDEV_OPS_EXT_FDB */
+#ifdef HAVE_BRIDGE_ATTRIBS
+	.ndo_bridge_setlink	= igb_ndo_bridge_setlink,
+	.ndo_bridge_getlink	= igb_ndo_bridge_getlink,
+#endif /* HAVE_BRIDGE_ATTRIBS */
+#endif
+};
+
+#ifdef CONFIG_IGB_VMDQ_NETDEV
+static const struct net_device_ops igb_vmdq_ops = {
+	.ndo_open		= &igb_vmdq_open,
+	.ndo_stop		= &igb_vmdq_close,
+	.ndo_start_xmit		= &igb_vmdq_xmit_frame,
+	.ndo_get_stats		= &igb_vmdq_get_stats,
+	.ndo_set_rx_mode	= &igb_vmdq_set_rx_mode,
+	.ndo_validate_addr	= eth_validate_addr,
+	.ndo_set_mac_address	= &igb_vmdq_set_mac,
+	.ndo_change_mtu		= &igb_vmdq_change_mtu,
+	.ndo_tx_timeout		= &igb_vmdq_tx_timeout,
+	.ndo_vlan_rx_register	= &igb_vmdq_vlan_rx_register,
+	.ndo_vlan_rx_add_vid	= &igb_vmdq_vlan_rx_add_vid,
+	.ndo_vlan_rx_kill_vid	= &igb_vmdq_vlan_rx_kill_vid,
+};
+
+#endif /* CONFIG_IGB_VMDQ_NETDEV */
+#endif /* HAVE_NET_DEVICE_OPS */
+#ifdef CONFIG_IGB_VMDQ_NETDEV
+void igb_assign_vmdq_netdev_ops(struct net_device *vnetdev)
+{
+#ifdef HAVE_NET_DEVICE_OPS
+	vnetdev->netdev_ops = &igb_vmdq_ops;
+#else
+	dev->open = &igb_vmdq_open;
+	dev->stop = &igb_vmdq_close;
+	dev->hard_start_xmit = &igb_vmdq_xmit_frame;
+	dev->get_stats = &igb_vmdq_get_stats;
+#ifdef HAVE_SET_RX_MODE
+	dev->set_rx_mode = &igb_vmdq_set_rx_mode;
+#endif
+	dev->set_multicast_list = &igb_vmdq_set_rx_mode;
+	dev->set_mac_address = &igb_vmdq_set_mac;
+	dev->change_mtu = &igb_vmdq_change_mtu;
+#ifdef HAVE_TX_TIMEOUT
+	dev->tx_timeout = &igb_vmdq_tx_timeout;
+#endif
+#if defined(NETIF_F_HW_VLAN_TX) || defined(NETIF_F_HW_VLAN_CTAG_TX)
+	dev->vlan_rx_register = &igb_vmdq_vlan_rx_register;
+	dev->vlan_rx_add_vid = &igb_vmdq_vlan_rx_add_vid;
+	dev->vlan_rx_kill_vid = &igb_vmdq_vlan_rx_kill_vid;
+#endif
+#endif
+	igb_vmdq_set_ethtool_ops(vnetdev);
+	vnetdev->watchdog_timeo = 5 * HZ;
+
+}
+
+int igb_init_vmdq_netdevs(struct igb_adapter *adapter)
+{
+	int pool, err = 0, base_queue;
+	struct net_device *vnetdev;
+	struct igb_vmdq_adapter *vmdq_adapter;
+
+	for (pool = 1; pool < adapter->vmdq_pools; pool++) {
+		int qpp = (!adapter->rss_queues ? 1 : adapter->rss_queues);
+		base_queue = pool * qpp;
+		vnetdev = alloc_etherdev(sizeof(struct igb_vmdq_adapter));
+		if (!vnetdev) {
+			err = -ENOMEM;
+			break;
+		}
+		vmdq_adapter = netdev_priv(vnetdev);
+		vmdq_adapter->vnetdev = vnetdev;
+		vmdq_adapter->real_adapter = adapter;
+		vmdq_adapter->rx_ring = adapter->rx_ring[base_queue];
+		vmdq_adapter->tx_ring = adapter->tx_ring[base_queue];
+		igb_assign_vmdq_netdev_ops(vnetdev);
+		snprintf(vnetdev->name, IFNAMSIZ, "%sv%d",
+			 adapter->netdev->name, pool);
+		vnetdev->features = adapter->netdev->features;
+#ifdef HAVE_NETDEV_VLAN_FEATURES
+		vnetdev->vlan_features = adapter->netdev->vlan_features;
+#endif
+		adapter->vmdq_netdev[pool-1] = vnetdev;
+		err = register_netdev(vnetdev);
+		if (err)
+			break;
+	}
+	return err;
+}
+
+int igb_remove_vmdq_netdevs(struct igb_adapter *adapter)
+{
+	int pool, err = 0;
+
+	for (pool = 1; pool < adapter->vmdq_pools; pool++) {
+		unregister_netdev(adapter->vmdq_netdev[pool-1]);
+		free_netdev(adapter->vmdq_netdev[pool-1]);
+		adapter->vmdq_netdev[pool-1] = NULL;
+	}
+	return err;
+}
+#endif /* CONFIG_IGB_VMDQ_NETDEV */
+
+/**
+ * igb_set_fw_version - Configure version string for ethtool
+ * @adapter: adapter struct
+ *
+ **/
+static void igb_set_fw_version(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	struct e1000_fw_version fw;
+
+	e1000_get_fw_version(hw, &fw);
+
+	switch (hw->mac.type) {
+	case e1000_i210:
+	case e1000_i211:
+		if (!(e1000_get_flash_presence_i210(hw))) {
+			snprintf(adapter->fw_version,
+			    sizeof(adapter->fw_version),
+			    "%2d.%2d-%d",
+			    fw.invm_major, fw.invm_minor, fw.invm_img_type);
+			break;
+		}
+		/* fall through */
+	default:
+		/* if option rom is valid, display its version too*/
+		if (fw.or_valid) {
+			snprintf(adapter->fw_version,
+			    sizeof(adapter->fw_version),
+			    "%d.%d, 0x%08x, %d.%d.%d",
+			    fw.eep_major, fw.eep_minor, fw.etrack_id,
+			    fw.or_major, fw.or_build, fw.or_patch);
+		/* no option rom */
+		} else {
+			if (fw.etrack_id != 0X0000) {
+			snprintf(adapter->fw_version,
+			    sizeof(adapter->fw_version),
+			    "%d.%d, 0x%08x",
+			    fw.eep_major, fw.eep_minor, fw.etrack_id);
+			} else {
+			snprintf(adapter->fw_version,
+			    sizeof(adapter->fw_version),
+			    "%d.%d.%d",
+			    fw.eep_major, fw.eep_minor, fw.eep_build);
+			}
+		}
+		break;
+	}
+
+	return;
+}
+
+/**
+ * igb_init_mas - init Media Autosense feature if enabled in the NVM
+ *
+ * @adapter: adapter struct
+ **/
+static void igb_init_mas(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u16 eeprom_data;
+
+	e1000_read_nvm(hw, NVM_COMPAT, 1, &eeprom_data);
+	switch (hw->bus.func) {
+	case E1000_FUNC_0:
+		if (eeprom_data & IGB_MAS_ENABLE_0)
+			adapter->flags |= IGB_FLAG_MAS_ENABLE;
+		break;
+	case E1000_FUNC_1:
+		if (eeprom_data & IGB_MAS_ENABLE_1)
+			adapter->flags |= IGB_FLAG_MAS_ENABLE;
+		break;
+	case E1000_FUNC_2:
+		if (eeprom_data & IGB_MAS_ENABLE_2)
+			adapter->flags |= IGB_FLAG_MAS_ENABLE;
+		break;
+	case E1000_FUNC_3:
+		if (eeprom_data & IGB_MAS_ENABLE_3)
+			adapter->flags |= IGB_FLAG_MAS_ENABLE;
+		break;
+	default:
+		/* Shouldn't get here */
+		dev_err(pci_dev_to_dev(adapter->pdev),
+			"%s:AMS: Invalid port configuration, returning\n",
+			adapter->netdev->name);
+		break;
+	}
+}
+
+/**
+ * igb_probe - Device Initialization Routine
+ * @pdev: PCI device information struct
+ * @ent: entry in igb_pci_tbl
+ *
+ * Returns 0 on success, negative on failure
+ *
+ * igb_probe initializes an adapter identified by a pci_dev structure.
+ * The OS initialization, configuring of the adapter private structure,
+ * and a hardware reset occur.
+ **/
+static int __devinit igb_probe(struct pci_dev *pdev,
+			       const struct pci_device_id *ent)
+{
+	struct net_device *netdev;
+	struct igb_adapter *adapter;
+	struct e1000_hw *hw;
+	u16 eeprom_data = 0;
+	u8 pba_str[E1000_PBANUM_LENGTH];
+	s32 ret_val;
+	static int global_quad_port_a; /* global quad port a indication */
+	int i, err, pci_using_dac;
+	static int cards_found;
+
+	err = pci_enable_device_mem(pdev);
+	if (err)
+		return err;
+
+	pci_using_dac = 0;
+	err = dma_set_mask(pci_dev_to_dev(pdev), DMA_BIT_MASK(64));
+	if (!err) {
+		err = dma_set_coherent_mask(pci_dev_to_dev(pdev), DMA_BIT_MASK(64));
+		if (!err)
+			pci_using_dac = 1;
+	} else {
+		err = dma_set_mask(pci_dev_to_dev(pdev), DMA_BIT_MASK(32));
+		if (err) {
+			err = dma_set_coherent_mask(pci_dev_to_dev(pdev), DMA_BIT_MASK(32));
+			if (err) {
+				IGB_ERR("No usable DMA configuration, "
+				        "aborting\n");
+				goto err_dma;
+			}
+		}
+	}
+
+#ifndef HAVE_ASPM_QUIRKS
+	/* 82575 requires that the pci-e link partner disable the L0s state */
+	switch (pdev->device) {
+	case E1000_DEV_ID_82575EB_COPPER:
+	case E1000_DEV_ID_82575EB_FIBER_SERDES:
+	case E1000_DEV_ID_82575GB_QUAD_COPPER:
+		pci_disable_link_state(pdev, PCIE_LINK_STATE_L0S);
+	default:
+		break;
+	}
+
+#endif /* HAVE_ASPM_QUIRKS */
+	err = pci_request_selected_regions(pdev,
+	                                   pci_select_bars(pdev,
+                                                           IORESOURCE_MEM),
+	                                   igb_driver_name);
+	if (err)
+		goto err_pci_reg;
+
+	pci_enable_pcie_error_reporting(pdev);
+
+	pci_set_master(pdev);
+
+	err = -ENOMEM;
+#ifdef HAVE_TX_MQ
+	netdev = alloc_etherdev_mq(sizeof(struct igb_adapter),
+	                           IGB_MAX_TX_QUEUES);
+#else
+	netdev = alloc_etherdev(sizeof(struct igb_adapter));
+#endif /* HAVE_TX_MQ */
+	if (!netdev)
+		goto err_alloc_etherdev;
+
+	SET_MODULE_OWNER(netdev);
+	SET_NETDEV_DEV(netdev, &pdev->dev);
+
+	pci_set_drvdata(pdev, netdev);
+	adapter = netdev_priv(netdev);
+	adapter->netdev = netdev;
+	adapter->pdev = pdev;
+	hw = &adapter->hw;
+	hw->back = adapter;
+	adapter->port_num = hw->bus.func;
+	adapter->msg_enable = (1 << debug) - 1;
+
+#ifdef HAVE_PCI_ERS
+	err = pci_save_state(pdev);
+	if (err)
+		goto err_ioremap;
+#endif
+	err = -EIO;
+	hw->hw_addr = ioremap(pci_resource_start(pdev, 0),
+	                      pci_resource_len(pdev, 0));
+	if (!hw->hw_addr)
+		goto err_ioremap;
+
+#ifdef HAVE_NET_DEVICE_OPS
+	netdev->netdev_ops = &igb_netdev_ops;
+#else /* HAVE_NET_DEVICE_OPS */
+	netdev->open = &igb_open;
+	netdev->stop = &igb_close;
+	netdev->get_stats = &igb_get_stats;
+#ifdef HAVE_SET_RX_MODE
+	netdev->set_rx_mode = &igb_set_rx_mode;
+#endif
+	netdev->set_multicast_list = &igb_set_rx_mode;
+	netdev->set_mac_address = &igb_set_mac;
+	netdev->change_mtu = &igb_change_mtu;
+	netdev->do_ioctl = &igb_ioctl;
+#ifdef HAVE_TX_TIMEOUT
+	netdev->tx_timeout = &igb_tx_timeout;
+#endif
+	netdev->vlan_rx_register = igb_vlan_mode;
+	netdev->vlan_rx_add_vid = igb_vlan_rx_add_vid;
+	netdev->vlan_rx_kill_vid = igb_vlan_rx_kill_vid;
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	netdev->poll_controller = igb_netpoll;
+#endif
+	netdev->hard_start_xmit = &igb_xmit_frame;
+#endif /* HAVE_NET_DEVICE_OPS */
+	igb_set_ethtool_ops(netdev);
+#ifdef HAVE_TX_TIMEOUT
+	netdev->watchdog_timeo = 5 * HZ;
+#endif
+
+	strncpy(netdev->name, pci_name(pdev), sizeof(netdev->name) - 1);
+
+	adapter->bd_number = cards_found;
+
+	/* setup the private structure */
+	err = igb_sw_init(adapter);
+	if (err)
+		goto err_sw_init;
+
+	e1000_get_bus_info(hw);
+
+	hw->phy.autoneg_wait_to_complete = FALSE;
+	hw->mac.adaptive_ifs = FALSE;
+
+	/* Copper options */
+	if (hw->phy.media_type == e1000_media_type_copper) {
+		hw->phy.mdix = AUTO_ALL_MODES;
+		hw->phy.disable_polarity_correction = FALSE;
+		hw->phy.ms_type = e1000_ms_hw_default;
+	}
+
+	if (e1000_check_reset_block(hw))
+		dev_info(pci_dev_to_dev(pdev),
+			"PHY reset is blocked due to SOL/IDER session.\n");
+
+	/*
+	 * features is initialized to 0 in allocation, it might have bits
+	 * set by igb_sw_init so we should use an or instead of an
+	 * assignment.
+	 */
+	netdev->features |= NETIF_F_SG |
+			    NETIF_F_IP_CSUM |
+#ifdef NETIF_F_IPV6_CSUM
+			    NETIF_F_IPV6_CSUM |
+#endif
+#ifdef NETIF_F_TSO
+			    NETIF_F_TSO |
+#ifdef NETIF_F_TSO6
+			    NETIF_F_TSO6 |
+#endif
+#endif /* NETIF_F_TSO */
+#ifdef NETIF_F_RXHASH
+			    NETIF_F_RXHASH |
+#endif
+			    NETIF_F_RXCSUM |
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
+			    NETIF_F_HW_VLAN_CTAG_RX |
+			    NETIF_F_HW_VLAN_CTAG_TX;
+#else
+			    NETIF_F_HW_VLAN_RX |
+			    NETIF_F_HW_VLAN_TX;
+#endif
+
+	if (hw->mac.type >= e1000_82576)
+		netdev->features |= NETIF_F_SCTP_CSUM;
+
+#ifdef HAVE_NDO_SET_FEATURES
+	/* copy netdev features into list of user selectable features */
+	netdev->hw_features |= netdev->features;
+#ifndef IGB_NO_LRO
+
+	/* give us the option of enabling LRO later */
+	netdev->hw_features |= NETIF_F_LRO;
+#endif
+#else
+#ifdef NETIF_F_GRO
+
+	/* this is only needed on kernels prior to 2.6.39 */
+	netdev->features |= NETIF_F_GRO;
+#endif
+#endif
+
+	/* set this bit last since it cannot be part of hw_features */
+#ifdef NETIF_F_HW_VLAN_CTAG_FILTER
+	netdev->features |= NETIF_F_HW_VLAN_CTAG_FILTER;
+#else
+	netdev->features |= NETIF_F_HW_VLAN_FILTER;
+#endif
+
+#ifdef HAVE_NETDEV_VLAN_FEATURES
+	netdev->vlan_features |= NETIF_F_TSO |
+				 NETIF_F_TSO6 |
+				 NETIF_F_IP_CSUM |
+				 NETIF_F_IPV6_CSUM |
+				 NETIF_F_SG;
+
+#endif
+	if (pci_using_dac)
+		netdev->features |= NETIF_F_HIGHDMA;
+
+	adapter->en_mng_pt = e1000_enable_mng_pass_thru(hw);
+#ifdef DEBUG
+	if (adapter->dmac != IGB_DMAC_DISABLE)
+		printk("%s: DMA Coalescing is enabled..\n", netdev->name);
+#endif
+
+	/* before reading the NVM, reset the controller to put the device in a
+	 * known good starting state */
+	e1000_reset_hw(hw);
+
+	/* make sure the NVM is good */
+	if (e1000_validate_nvm_checksum(hw) < 0) {
+		dev_err(pci_dev_to_dev(pdev), "The NVM Checksum Is Not"
+		        " Valid\n");
+		err = -EIO;
+		goto err_eeprom;
+	}
+
+	/* copy the MAC address out of the NVM */
+	if (e1000_read_mac_addr(hw))
+		dev_err(pci_dev_to_dev(pdev), "NVM Read Error\n");
+	memcpy(netdev->dev_addr, hw->mac.addr, netdev->addr_len);
+#ifdef ETHTOOL_GPERMADDR
+	memcpy(netdev->perm_addr, hw->mac.addr, netdev->addr_len);
+
+	if (!is_valid_ether_addr(netdev->perm_addr)) {
+#else
+	if (!is_valid_ether_addr(netdev->dev_addr)) {
+#endif
+		dev_err(pci_dev_to_dev(pdev), "Invalid MAC Address\n");
+		err = -EIO;
+		goto err_eeprom;
+	}
+
+	memcpy(&adapter->mac_table[0].addr, hw->mac.addr, netdev->addr_len);
+	adapter->mac_table[0].queue = adapter->vfs_allocated_count;
+	adapter->mac_table[0].state = (IGB_MAC_STATE_DEFAULT | IGB_MAC_STATE_IN_USE);
+	igb_rar_set(adapter, 0);
+
+	/* get firmware version for ethtool -i */
+	igb_set_fw_version(adapter);
+
+	/* Check if Media Autosense is enabled */
+	if (hw->mac.type == e1000_82580)
+		igb_init_mas(adapter);
+#ifdef HAVE_TIMER_SETUP
+	timer_setup(&adapter->watchdog_timer, &igb_watchdog, 0);
+	if (adapter->flags & IGB_FLAG_DETECT_BAD_DMA)
+		timer_setup(&adapter->dma_err_timer, &igb_dma_err_timer, 0);
+	timer_setup(&adapter->phy_info_timer, &igb_update_phy_info, 0);
+#else
+	setup_timer(&adapter->watchdog_timer, &igb_watchdog,
+	            (unsigned long) adapter);
+	if (adapter->flags & IGB_FLAG_DETECT_BAD_DMA)
+		setup_timer(&adapter->dma_err_timer, &igb_dma_err_timer,
+			    (unsigned long) adapter);
+	setup_timer(&adapter->phy_info_timer, &igb_update_phy_info,
+	            (unsigned long) adapter);
+#endif
+
+	INIT_WORK(&adapter->reset_task, igb_reset_task);
+	INIT_WORK(&adapter->watchdog_task, igb_watchdog_task);
+	if (adapter->flags & IGB_FLAG_DETECT_BAD_DMA)
+		INIT_WORK(&adapter->dma_err_task, igb_dma_err_task);
+
+	/* Initialize link properties that are user-changeable */
+	adapter->fc_autoneg = true;
+	hw->mac.autoneg = true;
+	hw->phy.autoneg_advertised = 0x2f;
+
+	hw->fc.requested_mode = e1000_fc_default;
+	hw->fc.current_mode = e1000_fc_default;
+
+	e1000_validate_mdi_setting(hw);
+
+	/* By default, support wake on port A */
+	if (hw->bus.func == 0)
+		adapter->flags |= IGB_FLAG_WOL_SUPPORTED;
+
+	/* Check the NVM for wake support for non-port A ports */
+	if (hw->mac.type >= e1000_82580)
+		hw->nvm.ops.read(hw, NVM_INIT_CONTROL3_PORT_A +
+		                 NVM_82580_LAN_FUNC_OFFSET(hw->bus.func), 1,
+		                 &eeprom_data);
+	else if (hw->bus.func == 1)
+		e1000_read_nvm(hw, NVM_INIT_CONTROL3_PORT_B, 1, &eeprom_data);
+
+	if (eeprom_data & IGB_EEPROM_APME)
+		adapter->flags |= IGB_FLAG_WOL_SUPPORTED;
+
+	/* now that we have the eeprom settings, apply the special cases where
+	 * the eeprom may be wrong or the board simply won't support wake on
+	 * lan on a particular port */
+	switch (pdev->device) {
+	case E1000_DEV_ID_82575GB_QUAD_COPPER:
+		adapter->flags &= ~IGB_FLAG_WOL_SUPPORTED;
+		break;
+	case E1000_DEV_ID_82575EB_FIBER_SERDES:
+	case E1000_DEV_ID_82576_FIBER:
+	case E1000_DEV_ID_82576_SERDES:
+		/* Wake events only supported on port A for dual fiber
+		 * regardless of eeprom setting */
+		if (E1000_READ_REG(hw, E1000_STATUS) & E1000_STATUS_FUNC_1)
+			adapter->flags &= ~IGB_FLAG_WOL_SUPPORTED;
+		break;
+	case E1000_DEV_ID_82576_QUAD_COPPER:
+	case E1000_DEV_ID_82576_QUAD_COPPER_ET2:
+		/* if quad port adapter, disable WoL on all but port A */
+		if (global_quad_port_a != 0)
+			adapter->flags &= ~IGB_FLAG_WOL_SUPPORTED;
+		else
+			adapter->flags |= IGB_FLAG_QUAD_PORT_A;
+		/* Reset for multiple quad port adapters */
+		if (++global_quad_port_a == 4)
+			global_quad_port_a = 0;
+		break;
+	default:
+		/* If the device can't wake, don't set software support */
+		if (!device_can_wakeup(&adapter->pdev->dev))
+			adapter->flags &= ~IGB_FLAG_WOL_SUPPORTED;
+		break;
+	}
+
+	/* initialize the wol settings based on the eeprom settings */
+	if (adapter->flags & IGB_FLAG_WOL_SUPPORTED)
+		adapter->wol |= E1000_WUFC_MAG;
+
+	/* Some vendors want WoL disabled by default, but still supported */
+	if ((hw->mac.type == e1000_i350) &&
+	    (pdev->subsystem_vendor == PCI_VENDOR_ID_HP)) {
+		adapter->flags |= IGB_FLAG_WOL_SUPPORTED;
+		adapter->wol = 0;
+	}
+
+	device_set_wakeup_enable(pci_dev_to_dev(adapter->pdev),
+				 adapter->flags & IGB_FLAG_WOL_SUPPORTED);
+
+	/* reset the hardware with the new settings */
+	igb_reset(adapter);
+	adapter->devrc = 0;
+
+#ifdef HAVE_I2C_SUPPORT
+	/* Init the I2C interface */
+	err = igb_init_i2c(adapter);
+	if (err) {
+		dev_err(&pdev->dev, "failed to init i2c interface\n");
+		goto err_eeprom;
+	}
+#endif /* HAVE_I2C_SUPPORT */
+
+	/* let the f/w know that the h/w is now under the control of the
+	 * driver. */
+	igb_get_hw_control(adapter);
+
+	strncpy(netdev->name, "eth%d", IFNAMSIZ);
+	err = register_netdev(netdev);
+	if (err)
+		goto err_register;
+
+#ifdef CONFIG_IGB_VMDQ_NETDEV
+	err = igb_init_vmdq_netdevs(adapter);
+	if (err)
+		goto err_register;
+#endif
+	/* carrier off reporting is important to ethtool even BEFORE open */
+	netif_carrier_off(netdev);
+
+#ifdef IGB_DCA
+	if (dca_add_requester(&pdev->dev) == E1000_SUCCESS) {
+		adapter->flags |= IGB_FLAG_DCA_ENABLED;
+		dev_info(pci_dev_to_dev(pdev), "DCA enabled\n");
+		igb_setup_dca(adapter);
+	}
+
+#endif
+#ifdef HAVE_PTP_1588_CLOCK
+	/* do hw tstamp init after resetting */
+	igb_ptp_init(adapter);
+#endif /* HAVE_PTP_1588_CLOCK */
+
+	dev_info(pci_dev_to_dev(pdev), "Intel(R) Gigabit Ethernet Network Connection\n");
+	/* print bus type/speed/width info */
+	dev_info(pci_dev_to_dev(pdev), "%s: (PCIe:%s:%s) ",
+	         netdev->name,
+	         ((hw->bus.speed == e1000_bus_speed_2500) ? "2.5GT/s" :
+	          (hw->bus.speed == e1000_bus_speed_5000) ? "5.0GT/s" :
+		  (hw->mac.type == e1000_i354) ? "integrated" :
+	                                                    "unknown"),
+	         ((hw->bus.width == e1000_bus_width_pcie_x4) ? "Width x4" :
+	          (hw->bus.width == e1000_bus_width_pcie_x2) ? "Width x2" :
+	          (hw->bus.width == e1000_bus_width_pcie_x1) ? "Width x1" :
+		  (hw->mac.type == e1000_i354) ? "integrated" :
+	           "unknown"));
+	dev_info(pci_dev_to_dev(pdev), "%s: MAC: ", netdev->name);
+	for (i = 0; i < 6; i++)
+		printk("%2.2x%c", netdev->dev_addr[i], i == 5 ? '\n' : ':');
+
+	ret_val = e1000_read_pba_string(hw, pba_str, E1000_PBANUM_LENGTH);
+	if (ret_val)
+		strncpy(pba_str, "Unknown", sizeof(pba_str) - 1);
+	dev_info(pci_dev_to_dev(pdev), "%s: PBA No: %s\n", netdev->name,
+		 pba_str);
+
+
+	/* Initialize the thermal sensor on i350 devices. */
+	if (hw->mac.type == e1000_i350) {
+		if (hw->bus.func == 0) {
+			u16 ets_word;
+
+			/*
+			 * Read the NVM to determine if this i350 device
+			 * supports an external thermal sensor.
+			 */
+			e1000_read_nvm(hw, NVM_ETS_CFG, 1, &ets_word);
+			if (ets_word != 0x0000 && ets_word != 0xFFFF)
+				adapter->ets = true;
+			else
+				adapter->ets = false;
+		}
+#ifdef IGB_HWMON
+
+		igb_sysfs_init(adapter);
+#else
+#ifdef IGB_PROCFS
+
+		igb_procfs_init(adapter);
+#endif /* IGB_PROCFS */
+#endif /* IGB_HWMON */
+	} else {
+		adapter->ets = false;
+	}
+
+	if (hw->phy.media_type == e1000_media_type_copper) {
+		switch (hw->mac.type) {
+		case e1000_i350:
+		case e1000_i210:
+		case e1000_i211:
+			/* Enable EEE for internal copper PHY devices */
+			err = e1000_set_eee_i350(hw);
+			if (!err &&
+			    (adapter->flags & IGB_FLAG_EEE))
+				adapter->eee_advert =
+					MDIO_EEE_100TX | MDIO_EEE_1000T;
+			break;
+		case e1000_i354:
+			if ((E1000_READ_REG(hw, E1000_CTRL_EXT)) &
+			    (E1000_CTRL_EXT_LINK_MODE_SGMII)) {
+				err = e1000_set_eee_i354(hw);
+				if ((!err) &&
+				    (adapter->flags & IGB_FLAG_EEE))
+					adapter->eee_advert =
+					   MDIO_EEE_100TX | MDIO_EEE_1000T;
+			}
+			break;
+		default:
+			break;
+		}
+	}
+
+	/* send driver version info to firmware */
+	if (hw->mac.type >= e1000_i350)
+		igb_init_fw(adapter);
+
+#ifndef IGB_NO_LRO
+	if (netdev->features & NETIF_F_LRO)
+		dev_info(pci_dev_to_dev(pdev), "Internal LRO is enabled \n");
+	else
+		dev_info(pci_dev_to_dev(pdev), "LRO is disabled \n");
+#endif
+	dev_info(pci_dev_to_dev(pdev),
+	         "Using %s interrupts. %d rx queue(s), %d tx queue(s)\n",
+	         adapter->msix_entries ? "MSI-X" :
+	         (adapter->flags & IGB_FLAG_HAS_MSI) ? "MSI" : "legacy",
+	         adapter->num_rx_queues, adapter->num_tx_queues);
+
+	cards_found++;
+
+	pm_runtime_put_noidle(&pdev->dev);
+	return 0;
+
+err_register:
+	igb_release_hw_control(adapter);
+#ifdef HAVE_I2C_SUPPORT
+	memset(&adapter->i2c_adap, 0, sizeof(adapter->i2c_adap));
+#endif /* HAVE_I2C_SUPPORT */
+err_eeprom:
+	if (!e1000_check_reset_block(hw))
+		e1000_phy_hw_reset(hw);
+
+	if (hw->flash_address)
+		iounmap(hw->flash_address);
+err_sw_init:
+	igb_clear_interrupt_scheme(adapter);
+	igb_reset_sriov_capability(adapter);
+	iounmap(hw->hw_addr);
+err_ioremap:
+	free_netdev(netdev);
+err_alloc_etherdev:
+	pci_release_selected_regions(pdev,
+	                             pci_select_bars(pdev, IORESOURCE_MEM));
+err_pci_reg:
+err_dma:
+	pci_disable_device(pdev);
+	return err;
+}
+#ifdef HAVE_I2C_SUPPORT
+/*
+ *  igb_remove_i2c - Cleanup  I2C interface
+ *  @adapter: pointer to adapter structure
+ *
+ */
+static void igb_remove_i2c(struct igb_adapter *adapter)
+{
+
+	/* free the adapter bus structure */
+	i2c_del_adapter(&adapter->i2c_adap);
+}
+#endif /* HAVE_I2C_SUPPORT */
+
+/**
+ * igb_remove - Device Removal Routine
+ * @pdev: PCI device information struct
+ *
+ * igb_remove is called by the PCI subsystem to alert the driver
+ * that it should release a PCI device.  The could be caused by a
+ * Hot-Plug event, or because the driver is going to be removed from
+ * memory.
+ **/
+static void __devexit igb_remove(struct pci_dev *pdev)
+{
+	struct net_device *netdev = pci_get_drvdata(pdev);
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	struct e1000_hw *hw = &adapter->hw;
+
+	pm_runtime_get_noresume(&pdev->dev);
+#ifdef HAVE_I2C_SUPPORT
+	igb_remove_i2c(adapter);
+#endif /* HAVE_I2C_SUPPORT */
+#ifdef HAVE_PTP_1588_CLOCK
+	igb_ptp_stop(adapter);
+#endif /* HAVE_PTP_1588_CLOCK */
+
+	/* flush_scheduled work may reschedule our watchdog task, so
+	 * explicitly disable watchdog tasks from being rescheduled  */
+	set_bit(__IGB_DOWN, &adapter->state);
+	del_timer_sync(&adapter->watchdog_timer);
+	if (adapter->flags & IGB_FLAG_DETECT_BAD_DMA)
+		del_timer_sync(&adapter->dma_err_timer);
+	del_timer_sync(&adapter->phy_info_timer);
+
+	flush_scheduled_work();
+
+#ifdef IGB_DCA
+	if (adapter->flags & IGB_FLAG_DCA_ENABLED) {
+		dev_info(pci_dev_to_dev(pdev), "DCA disabled\n");
+		dca_remove_requester(&pdev->dev);
+		adapter->flags &= ~IGB_FLAG_DCA_ENABLED;
+		E1000_WRITE_REG(hw, E1000_DCA_CTRL, E1000_DCA_CTRL_DCA_DISABLE);
+	}
+#endif
+
+	/* Release control of h/w to f/w.  If f/w is AMT enabled, this
+	 * would have already happened in close and is redundant. */
+	igb_release_hw_control(adapter);
+
+	unregister_netdev(netdev);
+#ifdef CONFIG_IGB_VMDQ_NETDEV
+	igb_remove_vmdq_netdevs(adapter);
+#endif
+
+	igb_clear_interrupt_scheme(adapter);
+	igb_reset_sriov_capability(adapter);
+
+	iounmap(hw->hw_addr);
+	if (hw->flash_address)
+		iounmap(hw->flash_address);
+	pci_release_selected_regions(pdev,
+	                             pci_select_bars(pdev, IORESOURCE_MEM));
+
+#ifdef IGB_HWMON
+	igb_sysfs_exit(adapter);
+#else
+#ifdef IGB_PROCFS
+	igb_procfs_exit(adapter);
+#endif /* IGB_PROCFS */
+#endif /* IGB_HWMON */
+	kfree(adapter->mac_table);
+	kfree(adapter->shadow_vfta);
+	free_netdev(netdev);
+
+	pci_disable_pcie_error_reporting(pdev);
+
+	pci_disable_device(pdev);
+}
+
+/**
+ * igb_sw_init - Initialize general software structures (struct igb_adapter)
+ * @adapter: board private structure to initialize
+ *
+ * igb_sw_init initializes the Adapter private data structure.
+ * Fields are initialized based on PCI device information and
+ * OS network device settings (MTU size).
+ **/
+static int igb_sw_init(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	struct net_device *netdev = adapter->netdev;
+	struct pci_dev *pdev = adapter->pdev;
+
+	/* PCI config space info */
+
+	hw->vendor_id = pdev->vendor;
+	hw->device_id = pdev->device;
+	hw->subsystem_vendor_id = pdev->subsystem_vendor;
+	hw->subsystem_device_id = pdev->subsystem_device;
+
+	pci_read_config_byte(pdev, PCI_REVISION_ID, &hw->revision_id);
+
+	pci_read_config_word(pdev, PCI_COMMAND, &hw->bus.pci_cmd_word);
+
+	/* set default ring sizes */
+	adapter->tx_ring_count = IGB_DEFAULT_TXD;
+	adapter->rx_ring_count = IGB_DEFAULT_RXD;
+
+	/* set default work limits */
+	adapter->tx_work_limit = IGB_DEFAULT_TX_WORK;
+
+	adapter->max_frame_size = netdev->mtu + ETH_HLEN + ETH_FCS_LEN +
+					      VLAN_HLEN;
+
+	/* Initialize the hardware-specific values */
+	if (e1000_setup_init_funcs(hw, TRUE)) {
+		dev_err(pci_dev_to_dev(pdev), "Hardware Initialization Failure\n");
+		return -EIO;
+	}
+
+	adapter->mac_table = kzalloc(sizeof(struct igb_mac_addr) *
+				     hw->mac.rar_entry_count,
+				     GFP_ATOMIC);
+
+	/* Setup and initialize a copy of the hw vlan table array */
+	adapter->shadow_vfta = kzalloc(sizeof(u32) * E1000_VFTA_ENTRIES,
+				       GFP_ATOMIC);
+#ifdef NO_KNI
+	/* These calls may decrease the number of queues */
+	if (hw->mac.type < e1000_i210) {
+		igb_set_sriov_capability(adapter);
+	}
+
+	if (igb_init_interrupt_scheme(adapter, true)) {
+		dev_err(pci_dev_to_dev(pdev), "Unable to allocate memory for queues\n");
+		return -ENOMEM;
+	}
+
+	/* Explicitly disable IRQ since the NIC can be in any state. */
+	igb_irq_disable(adapter);
+
+	set_bit(__IGB_DOWN, &adapter->state);
+#endif
+	return 0;
+}
+
+/**
+ * igb_open - Called when a network interface is made active
+ * @netdev: network interface device structure
+ *
+ * Returns 0 on success, negative value on failure
+ *
+ * The open entry point is called when a network interface is made
+ * active by the system (IFF_UP).  At this point all resources needed
+ * for transmit and receive operations are allocated, the interrupt
+ * handler is registered with the OS, the watchdog timer is started,
+ * and the stack is notified that the interface is ready.
+ **/
+static int __igb_open(struct net_device *netdev, bool resuming)
+{
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	struct e1000_hw *hw = &adapter->hw;
+#ifdef CONFIG_PM_RUNTIME
+	struct pci_dev *pdev = adapter->pdev;
+#endif /* CONFIG_PM_RUNTIME */
+	int err;
+	int i;
+
+	/* disallow open during test */
+	if (test_bit(__IGB_TESTING, &adapter->state)) {
+		WARN_ON(resuming);
+		return -EBUSY;
+	}
+
+#ifdef CONFIG_PM_RUNTIME
+	if (!resuming)
+		pm_runtime_get_sync(&pdev->dev);
+#endif /* CONFIG_PM_RUNTIME */
+
+	netif_carrier_off(netdev);
+
+	/* allocate transmit descriptors */
+	err = igb_setup_all_tx_resources(adapter);
+	if (err)
+		goto err_setup_tx;
+
+	/* allocate receive descriptors */
+	err = igb_setup_all_rx_resources(adapter);
+	if (err)
+		goto err_setup_rx;
+
+	igb_power_up_link(adapter);
+
+	/* before we allocate an interrupt, we must be ready to handle it.
+	 * Setting DEBUG_SHIRQ in the kernel makes it fire an interrupt
+	 * as soon as we call pci_request_irq, so we have to setup our
+	 * clean_rx handler before we do so.  */
+	igb_configure(adapter);
+
+	err = igb_request_irq(adapter);
+	if (err)
+		goto err_req_irq;
+
+	/* Notify the stack of the actual queue counts. */
+	netif_set_real_num_tx_queues(netdev,
+				     adapter->vmdq_pools ? 1 :
+				     adapter->num_tx_queues);
+
+	err = netif_set_real_num_rx_queues(netdev,
+					   adapter->vmdq_pools ? 1 :
+					   adapter->num_rx_queues);
+	if (err)
+		goto err_set_queues;
+
+	/* From here on the code is the same as igb_up() */
+	clear_bit(__IGB_DOWN, &adapter->state);
+
+	for (i = 0; i < adapter->num_q_vectors; i++)
+		napi_enable(&(adapter->q_vector[i]->napi));
+	igb_configure_lli(adapter);
+
+	/* Clear any pending interrupts. */
+	E1000_READ_REG(hw, E1000_ICR);
+
+	igb_irq_enable(adapter);
+
+	/* notify VFs that reset has been completed */
+	if (adapter->vfs_allocated_count) {
+		u32 reg_data = E1000_READ_REG(hw, E1000_CTRL_EXT);
+		reg_data |= E1000_CTRL_EXT_PFRSTD;
+		E1000_WRITE_REG(hw, E1000_CTRL_EXT, reg_data);
+	}
+
+	netif_tx_start_all_queues(netdev);
+
+	if (adapter->flags & IGB_FLAG_DETECT_BAD_DMA)
+		schedule_work(&adapter->dma_err_task);
+
+	/* start the watchdog. */
+	hw->mac.get_link_status = 1;
+	schedule_work(&adapter->watchdog_task);
+
+	return E1000_SUCCESS;
+
+err_set_queues:
+	igb_free_irq(adapter);
+err_req_irq:
+	igb_release_hw_control(adapter);
+	igb_power_down_link(adapter);
+	igb_free_all_rx_resources(adapter);
+err_setup_rx:
+	igb_free_all_tx_resources(adapter);
+err_setup_tx:
+	igb_reset(adapter);
+
+#ifdef CONFIG_PM_RUNTIME
+	if (!resuming)
+		pm_runtime_put(&pdev->dev);
+#endif /* CONFIG_PM_RUNTIME */
+
+	return err;
+}
+
+static int igb_open(struct net_device *netdev)
+{
+	return __igb_open(netdev, false);
+}
+
+/**
+ * igb_close - Disables a network interface
+ * @netdev: network interface device structure
+ *
+ * Returns 0, this is not allowed to fail
+ *
+ * The close entry point is called when an interface is de-activated
+ * by the OS.  The hardware is still under the driver's control, but
+ * needs to be disabled.  A global MAC reset is issued to stop the
+ * hardware, and all transmit and receive resources are freed.
+ **/
+static int __igb_close(struct net_device *netdev, bool suspending)
+{
+	struct igb_adapter *adapter = netdev_priv(netdev);
+#ifdef CONFIG_PM_RUNTIME
+	struct pci_dev *pdev = adapter->pdev;
+#endif /* CONFIG_PM_RUNTIME */
+
+	WARN_ON(test_bit(__IGB_RESETTING, &adapter->state));
+
+#ifdef CONFIG_PM_RUNTIME
+	if (!suspending)
+		pm_runtime_get_sync(&pdev->dev);
+#endif /* CONFIG_PM_RUNTIME */
+
+	igb_down(adapter);
+
+	igb_release_hw_control(adapter);
+
+	igb_free_irq(adapter);
+
+	igb_free_all_tx_resources(adapter);
+	igb_free_all_rx_resources(adapter);
+
+#ifdef CONFIG_PM_RUNTIME
+	if (!suspending)
+		pm_runtime_put_sync(&pdev->dev);
+#endif /* CONFIG_PM_RUNTIME */
+
+	return 0;
+}
+
+static int igb_close(struct net_device *netdev)
+{
+	return __igb_close(netdev, false);
+}
+
+/**
+ * igb_setup_tx_resources - allocate Tx resources (Descriptors)
+ * @tx_ring: tx descriptor ring (for a specific queue) to setup
+ *
+ * Return 0 on success, negative on failure
+ **/
+int igb_setup_tx_resources(struct igb_ring *tx_ring)
+{
+	struct device *dev = tx_ring->dev;
+	int size;
+
+	size = sizeof(struct igb_tx_buffer) * tx_ring->count;
+	tx_ring->tx_buffer_info = vzalloc(size);
+	if (!tx_ring->tx_buffer_info)
+		goto err;
+
+	/* round up to nearest 4K */
+	tx_ring->size = tx_ring->count * sizeof(union e1000_adv_tx_desc);
+	tx_ring->size = ALIGN(tx_ring->size, 4096);
+
+	tx_ring->desc = dma_alloc_coherent(dev, tx_ring->size,
+					   &tx_ring->dma, GFP_KERNEL);
+
+	if (!tx_ring->desc)
+		goto err;
+
+	tx_ring->next_to_use = 0;
+	tx_ring->next_to_clean = 0;
+
+	return 0;
+
+err:
+	vfree(tx_ring->tx_buffer_info);
+	dev_err(dev,
+		"Unable to allocate memory for the transmit descriptor ring\n");
+	return -ENOMEM;
+}
+
+/**
+ * igb_setup_all_tx_resources - wrapper to allocate Tx resources
+ *				  (Descriptors) for all queues
+ * @adapter: board private structure
+ *
+ * Return 0 on success, negative on failure
+ **/
+static int igb_setup_all_tx_resources(struct igb_adapter *adapter)
+{
+	struct pci_dev *pdev = adapter->pdev;
+	int i, err = 0;
+
+	for (i = 0; i < adapter->num_tx_queues; i++) {
+		err = igb_setup_tx_resources(adapter->tx_ring[i]);
+		if (err) {
+			dev_err(pci_dev_to_dev(pdev),
+				"Allocation for Tx Queue %u failed\n", i);
+			for (i--; i >= 0; i--)
+				igb_free_tx_resources(adapter->tx_ring[i]);
+			break;
+		}
+	}
+
+	return err;
+}
+
+/**
+ * igb_setup_tctl - configure the transmit control registers
+ * @adapter: Board private structure
+ **/
+void igb_setup_tctl(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 tctl;
+
+	/* disable queue 0 which is enabled by default on 82575 and 82576 */
+	E1000_WRITE_REG(hw, E1000_TXDCTL(0), 0);
+
+	/* Program the Transmit Control Register */
+	tctl = E1000_READ_REG(hw, E1000_TCTL);
+	tctl &= ~E1000_TCTL_CT;
+	tctl |= E1000_TCTL_PSP | E1000_TCTL_RTLC |
+		(E1000_COLLISION_THRESHOLD << E1000_CT_SHIFT);
+
+	e1000_config_collision_dist(hw);
+
+	/* Enable transmits */
+	tctl |= E1000_TCTL_EN;
+
+	E1000_WRITE_REG(hw, E1000_TCTL, tctl);
+}
+
+static u32 igb_tx_wthresh(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	switch (hw->mac.type) {
+	case e1000_i354:
+		return 4;
+	case e1000_82576:
+		if (adapter->msix_entries)
+			return 1;
+	default:
+		break;
+	}
+
+	return 16;
+}
+
+/**
+ * igb_configure_tx_ring - Configure transmit ring after Reset
+ * @adapter: board private structure
+ * @ring: tx ring to configure
+ *
+ * Configure a transmit ring after a reset.
+ **/
+void igb_configure_tx_ring(struct igb_adapter *adapter,
+                           struct igb_ring *ring)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 txdctl = 0;
+	u64 tdba = ring->dma;
+	int reg_idx = ring->reg_idx;
+
+	/* disable the queue */
+	E1000_WRITE_REG(hw, E1000_TXDCTL(reg_idx), 0);
+	E1000_WRITE_FLUSH(hw);
+	mdelay(10);
+
+	E1000_WRITE_REG(hw, E1000_TDLEN(reg_idx),
+	                ring->count * sizeof(union e1000_adv_tx_desc));
+	E1000_WRITE_REG(hw, E1000_TDBAL(reg_idx),
+	                tdba & 0x00000000ffffffffULL);
+	E1000_WRITE_REG(hw, E1000_TDBAH(reg_idx), tdba >> 32);
+
+	ring->tail = hw->hw_addr + E1000_TDT(reg_idx);
+	E1000_WRITE_REG(hw, E1000_TDH(reg_idx), 0);
+	writel(0, ring->tail);
+
+	txdctl |= IGB_TX_PTHRESH;
+	txdctl |= IGB_TX_HTHRESH << 8;
+	txdctl |= igb_tx_wthresh(adapter) << 16;
+
+	txdctl |= E1000_TXDCTL_QUEUE_ENABLE;
+	E1000_WRITE_REG(hw, E1000_TXDCTL(reg_idx), txdctl);
+}
+
+/**
+ * igb_configure_tx - Configure transmit Unit after Reset
+ * @adapter: board private structure
+ *
+ * Configure the Tx unit of the MAC after a reset.
+ **/
+static void igb_configure_tx(struct igb_adapter *adapter)
+{
+	int i;
+
+	for (i = 0; i < adapter->num_tx_queues; i++)
+		igb_configure_tx_ring(adapter, adapter->tx_ring[i]);
+}
+
+/**
+ * igb_setup_rx_resources - allocate Rx resources (Descriptors)
+ * @rx_ring:    rx descriptor ring (for a specific queue) to setup
+ *
+ * Returns 0 on success, negative on failure
+ **/
+int igb_setup_rx_resources(struct igb_ring *rx_ring)
+{
+	struct device *dev = rx_ring->dev;
+	int size, desc_len;
+
+	size = sizeof(struct igb_rx_buffer) * rx_ring->count;
+	rx_ring->rx_buffer_info = vzalloc(size);
+	if (!rx_ring->rx_buffer_info)
+		goto err;
+
+	desc_len = sizeof(union e1000_adv_rx_desc);
+
+	/* Round up to nearest 4K */
+	rx_ring->size = rx_ring->count * desc_len;
+	rx_ring->size = ALIGN(rx_ring->size, 4096);
+
+	rx_ring->desc = dma_alloc_coherent(dev, rx_ring->size,
+					   &rx_ring->dma, GFP_KERNEL);
+
+	if (!rx_ring->desc)
+		goto err;
+
+	rx_ring->next_to_alloc = 0;
+	rx_ring->next_to_clean = 0;
+	rx_ring->next_to_use = 0;
+
+	return 0;
+
+err:
+	vfree(rx_ring->rx_buffer_info);
+	rx_ring->rx_buffer_info = NULL;
+	dev_err(dev, "Unable to allocate memory for the receive descriptor"
+		" ring\n");
+	return -ENOMEM;
+}
+
+/**
+ * igb_setup_all_rx_resources - wrapper to allocate Rx resources
+ *				  (Descriptors) for all queues
+ * @adapter: board private structure
+ *
+ * Return 0 on success, negative on failure
+ **/
+static int igb_setup_all_rx_resources(struct igb_adapter *adapter)
+{
+	struct pci_dev *pdev = adapter->pdev;
+	int i, err = 0;
+
+	for (i = 0; i < adapter->num_rx_queues; i++) {
+		err = igb_setup_rx_resources(adapter->rx_ring[i]);
+		if (err) {
+			dev_err(pci_dev_to_dev(pdev),
+				"Allocation for Rx Queue %u failed\n", i);
+			for (i--; i >= 0; i--)
+				igb_free_rx_resources(adapter->rx_ring[i]);
+			break;
+		}
+	}
+
+	return err;
+}
+
+/**
+ * igb_setup_mrqc - configure the multiple receive queue control registers
+ * @adapter: Board private structure
+ **/
+static void igb_setup_mrqc(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 mrqc, rxcsum;
+	u32 j, num_rx_queues, shift = 0, shift2 = 0;
+	static const u32 rsskey[10] = { 0xDA565A6D, 0xC20E5B25, 0x3D256741,
+					0xB08FA343, 0xCB2BCAD0, 0xB4307BAE,
+					0xA32DCB77, 0x0CF23080, 0x3BB7426A,
+					0xFA01ACBE };
+
+	/* Fill out hash function seeds */
+	for (j = 0; j < 10; j++)
+		E1000_WRITE_REG(hw, E1000_RSSRK(j), rsskey[j]);
+
+	num_rx_queues = adapter->rss_queues;
+
+	/* 82575 and 82576 supports 2 RSS queues for VMDq */
+	switch (hw->mac.type) {
+	case e1000_82575:
+		if (adapter->vmdq_pools) {
+			shift = 2;
+			shift2 = 6;
+			break;
+		}
+		shift = 6;
+		break;
+	case e1000_82576:
+		/* 82576 supports 2 RSS queues for SR-IOV */
+		if (adapter->vfs_allocated_count || adapter->vmdq_pools) {
+			shift = 3;
+			num_rx_queues = 2;
+		}
+		break;
+	default:
+		break;
+	}
+
+	/*
+	 * Populate the redirection table 4 entries at a time.  To do this
+	 * we are generating the results for n and n+2 and then interleaving
+	 * those with the results with n+1 and n+3.
+	 */
+	for (j = 0; j < 32; j++) {
+		/* first pass generates n and n+2 */
+		u32 base = ((j * 0x00040004) + 0x00020000) * num_rx_queues;
+		u32 reta = (base & 0x07800780) >> (7 - shift);
+
+		/* second pass generates n+1 and n+3 */
+		base += 0x00010001 * num_rx_queues;
+		reta |= (base & 0x07800780) << (1 + shift);
+
+		/* generate 2nd table for 82575 based parts */
+		if (shift2)
+			reta |= (0x01010101 * num_rx_queues) << shift2;
+
+		E1000_WRITE_REG(hw, E1000_RETA(j), reta);
+	}
+
+	/*
+	 * Disable raw packet checksumming so that RSS hash is placed in
+	 * descriptor on writeback.  No need to enable TCP/UDP/IP checksum
+	 * offloads as they are enabled by default
+	 */
+	rxcsum = E1000_READ_REG(hw, E1000_RXCSUM);
+	rxcsum |= E1000_RXCSUM_PCSD;
+
+	if (adapter->hw.mac.type >= e1000_82576)
+		/* Enable Receive Checksum Offload for SCTP */
+		rxcsum |= E1000_RXCSUM_CRCOFL;
+
+	/* Don't need to set TUOFL or IPOFL, they default to 1 */
+	E1000_WRITE_REG(hw, E1000_RXCSUM, rxcsum);
+
+	/* Generate RSS hash based on packet types, TCP/UDP
+	 * port numbers and/or IPv4/v6 src and dst addresses
+	 */
+	mrqc = E1000_MRQC_RSS_FIELD_IPV4 |
+	       E1000_MRQC_RSS_FIELD_IPV4_TCP |
+	       E1000_MRQC_RSS_FIELD_IPV6 |
+	       E1000_MRQC_RSS_FIELD_IPV6_TCP |
+	       E1000_MRQC_RSS_FIELD_IPV6_TCP_EX;
+
+	if (adapter->flags & IGB_FLAG_RSS_FIELD_IPV4_UDP)
+		mrqc |= E1000_MRQC_RSS_FIELD_IPV4_UDP;
+	if (adapter->flags & IGB_FLAG_RSS_FIELD_IPV6_UDP)
+		mrqc |= E1000_MRQC_RSS_FIELD_IPV6_UDP;
+
+	/* If VMDq is enabled then we set the appropriate mode for that, else
+	 * we default to RSS so that an RSS hash is calculated per packet even
+	 * if we are only using one queue */
+	if (adapter->vfs_allocated_count || adapter->vmdq_pools) {
+		if (hw->mac.type > e1000_82575) {
+			/* Set the default pool for the PF's first queue */
+			u32 vtctl = E1000_READ_REG(hw, E1000_VT_CTL);
+			vtctl &= ~(E1000_VT_CTL_DEFAULT_POOL_MASK |
+				   E1000_VT_CTL_DISABLE_DEF_POOL);
+			vtctl |= adapter->vfs_allocated_count <<
+				E1000_VT_CTL_DEFAULT_POOL_SHIFT;
+			E1000_WRITE_REG(hw, E1000_VT_CTL, vtctl);
+		} else if (adapter->rss_queues > 1) {
+			/* set default queue for pool 1 to queue 2 */
+			E1000_WRITE_REG(hw, E1000_VT_CTL,
+				        adapter->rss_queues << 7);
+		}
+		if (adapter->rss_queues > 1)
+			mrqc |= E1000_MRQC_ENABLE_VMDQ_RSS_2Q;
+		else
+			mrqc |= E1000_MRQC_ENABLE_VMDQ;
+	} else {
+		mrqc |= E1000_MRQC_ENABLE_RSS_4Q;
+	}
+	igb_vmm_control(adapter);
+
+	E1000_WRITE_REG(hw, E1000_MRQC, mrqc);
+}
+
+/**
+ * igb_setup_rctl - configure the receive control registers
+ * @adapter: Board private structure
+ **/
+void igb_setup_rctl(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 rctl;
+
+	rctl = E1000_READ_REG(hw, E1000_RCTL);
+
+	rctl &= ~(3 << E1000_RCTL_MO_SHIFT);
+	rctl &= ~(E1000_RCTL_LBM_TCVR | E1000_RCTL_LBM_MAC);
+
+	rctl |= E1000_RCTL_EN | E1000_RCTL_BAM | E1000_RCTL_RDMTS_HALF |
+		(hw->mac.mc_filter_type << E1000_RCTL_MO_SHIFT);
+
+	/*
+	 * enable stripping of CRC. It's unlikely this will break BMC
+	 * redirection as it did with e1000. Newer features require
+	 * that the HW strips the CRC.
+	 */
+	rctl |= E1000_RCTL_SECRC;
+
+	/* disable store bad packets and clear size bits. */
+	rctl &= ~(E1000_RCTL_SBP | E1000_RCTL_SZ_256);
+
+	/* enable LPE to prevent packets larger than max_frame_size */
+	rctl |= E1000_RCTL_LPE;
+
+	/* disable queue 0 to prevent tail write w/o re-config */
+	E1000_WRITE_REG(hw, E1000_RXDCTL(0), 0);
+
+	/* Attention!!!  For SR-IOV PF driver operations you must enable
+	 * queue drop for all VF and PF queues to prevent head of line blocking
+	 * if an un-trusted VF does not provide descriptors to hardware.
+	 */
+	if (adapter->vfs_allocated_count) {
+		/* set all queue drop enable bits */
+		E1000_WRITE_REG(hw, E1000_QDE, ALL_QUEUES);
+	}
+
+	E1000_WRITE_REG(hw, E1000_RCTL, rctl);
+}
+
+static inline int igb_set_vf_rlpml(struct igb_adapter *adapter, int size,
+                                   int vfn)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 vmolr;
+
+	/* if it isn't the PF check to see if VFs are enabled and
+	 * increase the size to support vlan tags */
+	if (vfn < adapter->vfs_allocated_count &&
+	    adapter->vf_data[vfn].vlans_enabled)
+		size += VLAN_HLEN;
+
+#ifdef CONFIG_IGB_VMDQ_NETDEV
+	if (vfn >= adapter->vfs_allocated_count) {
+		int queue = vfn - adapter->vfs_allocated_count;
+		struct igb_vmdq_adapter *vadapter;
+
+		vadapter = netdev_priv(adapter->vmdq_netdev[queue-1]);
+		if (vadapter->vlgrp)
+			size += VLAN_HLEN;
+	}
+#endif
+	vmolr = E1000_READ_REG(hw, E1000_VMOLR(vfn));
+	vmolr &= ~E1000_VMOLR_RLPML_MASK;
+	vmolr |= size | E1000_VMOLR_LPE;
+	E1000_WRITE_REG(hw, E1000_VMOLR(vfn), vmolr);
+
+	return 0;
+}
+
+/**
+ * igb_rlpml_set - set maximum receive packet size
+ * @adapter: board private structure
+ *
+ * Configure maximum receivable packet size.
+ **/
+static void igb_rlpml_set(struct igb_adapter *adapter)
+{
+	u32 max_frame_size = adapter->max_frame_size;
+	struct e1000_hw *hw = &adapter->hw;
+	u16 pf_id = adapter->vfs_allocated_count;
+
+	if (adapter->vmdq_pools && hw->mac.type != e1000_82575) {
+		int i;
+		for (i = 0; i < adapter->vmdq_pools; i++)
+			igb_set_vf_rlpml(adapter, max_frame_size, pf_id + i);
+		/*
+		 * If we're in VMDQ or SR-IOV mode, then set global RLPML
+		 * to our max jumbo frame size, in case we need to enable
+		 * jumbo frames on one of the rings later.
+		 * This will not pass over-length frames into the default
+		 * queue because it's gated by the VMOLR.RLPML.
+		 */
+		max_frame_size = MAX_JUMBO_FRAME_SIZE;
+	}
+	/* Set VF RLPML for the PF device. */
+	if (adapter->vfs_allocated_count)
+		igb_set_vf_rlpml(adapter, max_frame_size, pf_id);
+
+	E1000_WRITE_REG(hw, E1000_RLPML, max_frame_size);
+}
+
+static inline void igb_set_vf_vlan_strip(struct igb_adapter *adapter,
+					int vfn, bool enable)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 val;
+	void __iomem *reg;
+
+	if (hw->mac.type < e1000_82576)
+		return;
+
+	if (hw->mac.type == e1000_i350)
+		reg = hw->hw_addr + E1000_DVMOLR(vfn);
+	else
+		reg = hw->hw_addr + E1000_VMOLR(vfn);
+
+	val = readl(reg);
+	if (enable)
+		val |= E1000_VMOLR_STRVLAN;
+	else
+		val &= ~(E1000_VMOLR_STRVLAN);
+	writel(val, reg);
+}
+static inline void igb_set_vmolr(struct igb_adapter *adapter,
+				 int vfn, bool aupe)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 vmolr;
+
+	/*
+	 * This register exists only on 82576 and newer so if we are older then
+	 * we should exit and do nothing
+	 */
+	if (hw->mac.type < e1000_82576)
+		return;
+
+	vmolr = E1000_READ_REG(hw, E1000_VMOLR(vfn));
+
+	if (aupe)
+		vmolr |= E1000_VMOLR_AUPE;        /* Accept untagged packets */
+	else
+		vmolr &= ~(E1000_VMOLR_AUPE); /* Tagged packets ONLY */
+
+	/* clear all bits that might not be set */
+	vmolr &= ~E1000_VMOLR_RSSE;
+
+	if (adapter->rss_queues > 1 && vfn == adapter->vfs_allocated_count)
+		vmolr |= E1000_VMOLR_RSSE; /* enable RSS */
+
+	vmolr |= E1000_VMOLR_BAM;	   /* Accept broadcast */
+	vmolr |= E1000_VMOLR_LPE;	   /* Accept long packets */
+
+	E1000_WRITE_REG(hw, E1000_VMOLR(vfn), vmolr);
+}
+
+/**
+ * igb_configure_rx_ring - Configure a receive ring after Reset
+ * @adapter: board private structure
+ * @ring: receive ring to be configured
+ *
+ * Configure the Rx unit of the MAC after a reset.
+ **/
+void igb_configure_rx_ring(struct igb_adapter *adapter,
+                           struct igb_ring *ring)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u64 rdba = ring->dma;
+	int reg_idx = ring->reg_idx;
+	u32 srrctl = 0, rxdctl = 0;
+
+#ifdef CONFIG_IGB_DISABLE_PACKET_SPLIT
+	/*
+	 * RLPML prevents us from receiving a frame larger than max_frame so
+	 * it is safe to just set the rx_buffer_len to max_frame without the
+	 * risk of an skb over panic.
+	 */
+	ring->rx_buffer_len = max_t(u32, adapter->max_frame_size,
+				    MAXIMUM_ETHERNET_VLAN_SIZE);
+
+#endif
+	/* disable the queue */
+	E1000_WRITE_REG(hw, E1000_RXDCTL(reg_idx), 0);
+
+	/* Set DMA base address registers */
+	E1000_WRITE_REG(hw, E1000_RDBAL(reg_idx),
+	                rdba & 0x00000000ffffffffULL);
+	E1000_WRITE_REG(hw, E1000_RDBAH(reg_idx), rdba >> 32);
+	E1000_WRITE_REG(hw, E1000_RDLEN(reg_idx),
+	               ring->count * sizeof(union e1000_adv_rx_desc));
+
+	/* initialize head and tail */
+	ring->tail = hw->hw_addr + E1000_RDT(reg_idx);
+	E1000_WRITE_REG(hw, E1000_RDH(reg_idx), 0);
+	writel(0, ring->tail);
+
+	/* reset next-to- use/clean to place SW in sync with hardwdare */
+	ring->next_to_clean = 0;
+	ring->next_to_use = 0;
+#ifndef CONFIG_IGB_DISABLE_PACKET_SPLIT
+	ring->next_to_alloc = 0;
+
+#endif
+	/* set descriptor configuration */
+#ifndef CONFIG_IGB_DISABLE_PACKET_SPLIT
+	srrctl = IGB_RX_HDR_LEN << E1000_SRRCTL_BSIZEHDRSIZE_SHIFT;
+	srrctl |= IGB_RX_BUFSZ >> E1000_SRRCTL_BSIZEPKT_SHIFT;
+#else /* CONFIG_IGB_DISABLE_PACKET_SPLIT */
+	srrctl = ALIGN(ring->rx_buffer_len, 1024) >>
+	         E1000_SRRCTL_BSIZEPKT_SHIFT;
+#endif /* CONFIG_IGB_DISABLE_PACKET_SPLIT */
+	srrctl |= E1000_SRRCTL_DESCTYPE_ADV_ONEBUF;
+#ifdef HAVE_PTP_1588_CLOCK
+	if (hw->mac.type >= e1000_82580)
+		srrctl |= E1000_SRRCTL_TIMESTAMP;
+#endif /* HAVE_PTP_1588_CLOCK */
+	/*
+	 * We should set the drop enable bit if:
+	 *  SR-IOV is enabled
+	 *   or
+	 *  Flow Control is disabled and number of RX queues > 1
+	 *
+	 *  This allows us to avoid head of line blocking for security
+	 *  and performance reasons.
+	 */
+	if (adapter->vfs_allocated_count ||
+	    (adapter->num_rx_queues > 1 &&
+	     (hw->fc.requested_mode == e1000_fc_none ||
+	      hw->fc.requested_mode == e1000_fc_rx_pause)))
+		srrctl |= E1000_SRRCTL_DROP_EN;
+
+	E1000_WRITE_REG(hw, E1000_SRRCTL(reg_idx), srrctl);
+
+	/* set filtering for VMDQ pools */
+	igb_set_vmolr(adapter, reg_idx & 0x7, true);
+
+	rxdctl |= IGB_RX_PTHRESH;
+	rxdctl |= IGB_RX_HTHRESH << 8;
+	rxdctl |= IGB_RX_WTHRESH << 16;
+
+	/* enable receive descriptor fetching */
+	rxdctl |= E1000_RXDCTL_QUEUE_ENABLE;
+	E1000_WRITE_REG(hw, E1000_RXDCTL(reg_idx), rxdctl);
+}
+
+/**
+ * igb_configure_rx - Configure receive Unit after Reset
+ * @adapter: board private structure
+ *
+ * Configure the Rx unit of the MAC after a reset.
+ **/
+static void igb_configure_rx(struct igb_adapter *adapter)
+{
+	int i;
+
+	/* set UTA to appropriate mode */
+	igb_set_uta(adapter);
+
+	igb_full_sync_mac_table(adapter);
+	/* Setup the HW Rx Head and Tail Descriptor Pointers and
+	 * the Base and Length of the Rx Descriptor Ring */
+	for (i = 0; i < adapter->num_rx_queues; i++)
+		igb_configure_rx_ring(adapter, adapter->rx_ring[i]);
+}
+
+/**
+ * igb_free_tx_resources - Free Tx Resources per Queue
+ * @tx_ring: Tx descriptor ring for a specific queue
+ *
+ * Free all transmit software resources
+ **/
+void igb_free_tx_resources(struct igb_ring *tx_ring)
+{
+	igb_clean_tx_ring(tx_ring);
+
+	vfree(tx_ring->tx_buffer_info);
+	tx_ring->tx_buffer_info = NULL;
+
+	/* if not set, then don't free */
+	if (!tx_ring->desc)
+		return;
+
+	dma_free_coherent(tx_ring->dev, tx_ring->size,
+			  tx_ring->desc, tx_ring->dma);
+
+	tx_ring->desc = NULL;
+}
+
+/**
+ * igb_free_all_tx_resources - Free Tx Resources for All Queues
+ * @adapter: board private structure
+ *
+ * Free all transmit software resources
+ **/
+static void igb_free_all_tx_resources(struct igb_adapter *adapter)
+{
+	int i;
+
+	for (i = 0; i < adapter->num_tx_queues; i++)
+		igb_free_tx_resources(adapter->tx_ring[i]);
+}
+
+void igb_unmap_and_free_tx_resource(struct igb_ring *ring,
+				    struct igb_tx_buffer *tx_buffer)
+{
+	if (tx_buffer->skb) {
+		dev_kfree_skb_any(tx_buffer->skb);
+		if (dma_unmap_len(tx_buffer, len))
+			dma_unmap_single(ring->dev,
+			                 dma_unmap_addr(tx_buffer, dma),
+			                 dma_unmap_len(tx_buffer, len),
+			                 DMA_TO_DEVICE);
+	} else if (dma_unmap_len(tx_buffer, len)) {
+		dma_unmap_page(ring->dev,
+		               dma_unmap_addr(tx_buffer, dma),
+		               dma_unmap_len(tx_buffer, len),
+		               DMA_TO_DEVICE);
+	}
+	tx_buffer->next_to_watch = NULL;
+	tx_buffer->skb = NULL;
+	dma_unmap_len_set(tx_buffer, len, 0);
+	/* buffer_info must be completely set up in the transmit path */
+}
+
+/**
+ * igb_clean_tx_ring - Free Tx Buffers
+ * @tx_ring: ring to be cleaned
+ **/
+static void igb_clean_tx_ring(struct igb_ring *tx_ring)
+{
+	struct igb_tx_buffer *buffer_info;
+	unsigned long size;
+	u16 i;
+
+	if (!tx_ring->tx_buffer_info)
+		return;
+	/* Free all the Tx ring sk_buffs */
+
+	for (i = 0; i < tx_ring->count; i++) {
+		buffer_info = &tx_ring->tx_buffer_info[i];
+		igb_unmap_and_free_tx_resource(tx_ring, buffer_info);
+	}
+
+	netdev_tx_reset_queue(txring_txq(tx_ring));
+
+	size = sizeof(struct igb_tx_buffer) * tx_ring->count;
+	memset(tx_ring->tx_buffer_info, 0, size);
+
+	/* Zero out the descriptor ring */
+	memset(tx_ring->desc, 0, tx_ring->size);
+
+	tx_ring->next_to_use = 0;
+	tx_ring->next_to_clean = 0;
+}
+
+/**
+ * igb_clean_all_tx_rings - Free Tx Buffers for all queues
+ * @adapter: board private structure
+ **/
+static void igb_clean_all_tx_rings(struct igb_adapter *adapter)
+{
+	int i;
+
+	for (i = 0; i < adapter->num_tx_queues; i++)
+		igb_clean_tx_ring(adapter->tx_ring[i]);
+}
+
+/**
+ * igb_free_rx_resources - Free Rx Resources
+ * @rx_ring: ring to clean the resources from
+ *
+ * Free all receive software resources
+ **/
+void igb_free_rx_resources(struct igb_ring *rx_ring)
+{
+	igb_clean_rx_ring(rx_ring);
+
+	vfree(rx_ring->rx_buffer_info);
+	rx_ring->rx_buffer_info = NULL;
+
+	/* if not set, then don't free */
+	if (!rx_ring->desc)
+		return;
+
+	dma_free_coherent(rx_ring->dev, rx_ring->size,
+			  rx_ring->desc, rx_ring->dma);
+
+	rx_ring->desc = NULL;
+}
+
+/**
+ * igb_free_all_rx_resources - Free Rx Resources for All Queues
+ * @adapter: board private structure
+ *
+ * Free all receive software resources
+ **/
+static void igb_free_all_rx_resources(struct igb_adapter *adapter)
+{
+	int i;
+
+	for (i = 0; i < adapter->num_rx_queues; i++)
+		igb_free_rx_resources(adapter->rx_ring[i]);
+}
+
+/**
+ * igb_clean_rx_ring - Free Rx Buffers per Queue
+ * @rx_ring: ring to free buffers from
+ **/
+void igb_clean_rx_ring(struct igb_ring *rx_ring)
+{
+	unsigned long size;
+	u16 i;
+
+	if (!rx_ring->rx_buffer_info)
+		return;
+
+#ifndef CONFIG_IGB_DISABLE_PACKET_SPLIT
+	if (rx_ring->skb)
+		dev_kfree_skb(rx_ring->skb);
+	rx_ring->skb = NULL;
+
+#endif
+	/* Free all the Rx ring sk_buffs */
+	for (i = 0; i < rx_ring->count; i++) {
+		struct igb_rx_buffer *buffer_info = &rx_ring->rx_buffer_info[i];
+#ifdef CONFIG_IGB_DISABLE_PACKET_SPLIT
+		if (buffer_info->dma) {
+			dma_unmap_single(rx_ring->dev,
+			                 buffer_info->dma,
+					 rx_ring->rx_buffer_len,
+					 DMA_FROM_DEVICE);
+			buffer_info->dma = 0;
+		}
+
+		if (buffer_info->skb) {
+			dev_kfree_skb(buffer_info->skb);
+			buffer_info->skb = NULL;
+		}
+#else
+		if (!buffer_info->page)
+			continue;
+
+		dma_unmap_page(rx_ring->dev,
+			       buffer_info->dma,
+			       PAGE_SIZE,
+			       DMA_FROM_DEVICE);
+		__free_page(buffer_info->page);
+
+		buffer_info->page = NULL;
+#endif
+	}
+
+	size = sizeof(struct igb_rx_buffer) * rx_ring->count;
+	memset(rx_ring->rx_buffer_info, 0, size);
+
+	/* Zero out the descriptor ring */
+	memset(rx_ring->desc, 0, rx_ring->size);
+
+	rx_ring->next_to_alloc = 0;
+	rx_ring->next_to_clean = 0;
+	rx_ring->next_to_use = 0;
+}
+
+/**
+ * igb_clean_all_rx_rings - Free Rx Buffers for all queues
+ * @adapter: board private structure
+ **/
+static void igb_clean_all_rx_rings(struct igb_adapter *adapter)
+{
+	int i;
+
+	for (i = 0; i < adapter->num_rx_queues; i++)
+		igb_clean_rx_ring(adapter->rx_ring[i]);
+}
+
+/**
+ * igb_set_mac - Change the Ethernet Address of the NIC
+ * @netdev: network interface device structure
+ * @p: pointer to an address structure
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int igb_set_mac(struct net_device *netdev, void *p)
+{
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	struct e1000_hw *hw = &adapter->hw;
+	struct sockaddr *addr = p;
+
+	if (!is_valid_ether_addr(addr->sa_data))
+		return -EADDRNOTAVAIL;
+
+	igb_del_mac_filter(adapter, hw->mac.addr,
+			   adapter->vfs_allocated_count);
+	memcpy(netdev->dev_addr, addr->sa_data, netdev->addr_len);
+	memcpy(hw->mac.addr, addr->sa_data, netdev->addr_len);
+
+	/* set the correct pool for the new PF MAC address in entry 0 */
+	return igb_add_mac_filter(adapter, hw->mac.addr,
+	                   adapter->vfs_allocated_count);
+}
+
+/**
+ * igb_write_mc_addr_list - write multicast addresses to MTA
+ * @netdev: network interface device structure
+ *
+ * Writes multicast address list to the MTA hash table.
+ * Returns: -ENOMEM on failure
+ *                0 on no addresses written
+ *                X on writing X addresses to MTA
+ **/
+int igb_write_mc_addr_list(struct net_device *netdev)
+{
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	struct e1000_hw *hw = &adapter->hw;
+#ifdef NETDEV_HW_ADDR_T_MULTICAST
+	struct netdev_hw_addr *ha;
+#else
+	struct dev_mc_list *ha;
+#endif
+	u8  *mta_list;
+	int i, count;
+#ifdef CONFIG_IGB_VMDQ_NETDEV
+	int vm;
+#endif
+	count = netdev_mc_count(netdev);
+#ifdef CONFIG_IGB_VMDQ_NETDEV
+	for (vm = 1; vm < adapter->vmdq_pools; vm++) {
+		if (!adapter->vmdq_netdev[vm])
+			break;
+		if (!netif_running(adapter->vmdq_netdev[vm]))
+			continue;
+		count += netdev_mc_count(adapter->vmdq_netdev[vm]);
+	}
+#endif
+
+	if (!count) {
+		e1000_update_mc_addr_list(hw, NULL, 0);
+		return 0;
+	}
+	mta_list = kzalloc(count * 6, GFP_ATOMIC);
+	if (!mta_list)
+		return -ENOMEM;
+
+	/* The shared function expects a packed array of only addresses. */
+	i = 0;
+	netdev_for_each_mc_addr(ha, netdev)
+#ifdef NETDEV_HW_ADDR_T_MULTICAST
+		memcpy(mta_list + (i++ * ETH_ALEN), ha->addr, ETH_ALEN);
+#else
+		memcpy(mta_list + (i++ * ETH_ALEN), ha->dmi_addr, ETH_ALEN);
+#endif
+#ifdef CONFIG_IGB_VMDQ_NETDEV
+	for (vm = 1; vm < adapter->vmdq_pools; vm++) {
+		if (!adapter->vmdq_netdev[vm])
+			break;
+		if (!netif_running(adapter->vmdq_netdev[vm]) ||
+		    !netdev_mc_count(adapter->vmdq_netdev[vm]))
+			continue;
+		netdev_for_each_mc_addr(ha, adapter->vmdq_netdev[vm])
+#ifdef NETDEV_HW_ADDR_T_MULTICAST
+			memcpy(mta_list + (i++ * ETH_ALEN),
+			       ha->addr, ETH_ALEN);
+#else
+			memcpy(mta_list + (i++ * ETH_ALEN),
+			       ha->dmi_addr, ETH_ALEN);
+#endif
+	}
+#endif
+	e1000_update_mc_addr_list(hw, mta_list, i);
+	kfree(mta_list);
+
+	return count;
+}
+
+void igb_rar_set(struct igb_adapter *adapter, u32 index)
+{
+	u32 rar_low, rar_high;
+	struct e1000_hw *hw = &adapter->hw;
+	u8 *addr = adapter->mac_table[index].addr;
+	/* HW expects these in little endian so we reverse the byte order
+	 * from network order (big endian) to little endian
+	 */
+	rar_low = ((u32) addr[0] | ((u32) addr[1] << 8) |
+	          ((u32) addr[2] << 16) | ((u32) addr[3] << 24));
+	rar_high = ((u32) addr[4] | ((u32) addr[5] << 8));
+
+	/* Indicate to hardware the Address is Valid. */
+	if (adapter->mac_table[index].state & IGB_MAC_STATE_IN_USE)
+		rar_high |= E1000_RAH_AV;
+
+	if (hw->mac.type == e1000_82575)
+		rar_high |= E1000_RAH_POOL_1 * adapter->mac_table[index].queue;
+	else
+		rar_high |= E1000_RAH_POOL_1 << adapter->mac_table[index].queue;
+
+	E1000_WRITE_REG(hw, E1000_RAL(index), rar_low);
+	E1000_WRITE_FLUSH(hw);
+	E1000_WRITE_REG(hw, E1000_RAH(index), rar_high);
+	E1000_WRITE_FLUSH(hw);
+}
+
+void igb_full_sync_mac_table(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	int i;
+	for (i = 0; i < hw->mac.rar_entry_count; i++) {
+			igb_rar_set(adapter, i);
+	}
+}
+
+void igb_sync_mac_table(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	int i;
+	for (i = 0; i < hw->mac.rar_entry_count; i++) {
+		if (adapter->mac_table[i].state & IGB_MAC_STATE_MODIFIED)
+			igb_rar_set(adapter, i);
+		adapter->mac_table[i].state &= ~(IGB_MAC_STATE_MODIFIED);
+	}
+}
+
+int igb_available_rars(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	int i, count = 0;
+
+	for (i = 0; i < hw->mac.rar_entry_count; i++) {
+		if (adapter->mac_table[i].state == 0)
+			count++;
+	}
+	return count;
+}
+
+#ifdef HAVE_SET_RX_MODE
+/**
+ * igb_write_uc_addr_list - write unicast addresses to RAR table
+ * @netdev: network interface device structure
+ *
+ * Writes unicast address list to the RAR table.
+ * Returns: -ENOMEM on failure/insufficient address space
+ *                0 on no addresses written
+ *                X on writing X addresses to the RAR table
+ **/
+static int igb_write_uc_addr_list(struct net_device *netdev)
+{
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	unsigned int vfn = adapter->vfs_allocated_count;
+	int count = 0;
+
+	/* return ENOMEM indicating insufficient memory for addresses */
+	if (netdev_uc_count(netdev) > igb_available_rars(adapter))
+		return -ENOMEM;
+	if (!netdev_uc_empty(netdev)) {
+#ifdef NETDEV_HW_ADDR_T_UNICAST
+		struct netdev_hw_addr *ha;
+#else
+		struct dev_mc_list *ha;
+#endif
+		netdev_for_each_uc_addr(ha, netdev) {
+#ifdef NETDEV_HW_ADDR_T_UNICAST
+			igb_del_mac_filter(adapter, ha->addr, vfn);
+			igb_add_mac_filter(adapter, ha->addr, vfn);
+#else
+			igb_del_mac_filter(adapter, ha->da_addr, vfn);
+			igb_add_mac_filter(adapter, ha->da_addr, vfn);
+#endif
+			count++;
+		}
+	}
+	return count;
+}
+
+#endif /* HAVE_SET_RX_MODE */
+/**
+ * igb_set_rx_mode - Secondary Unicast, Multicast and Promiscuous mode set
+ * @netdev: network interface device structure
+ *
+ * The set_rx_mode entry point is called whenever the unicast or multicast
+ * address lists or the network interface flags are updated.  This routine is
+ * responsible for configuring the hardware for proper unicast, multicast,
+ * promiscuous mode, and all-multi behavior.
+ **/
+static void igb_set_rx_mode(struct net_device *netdev)
+{
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	struct e1000_hw *hw = &adapter->hw;
+	unsigned int vfn = adapter->vfs_allocated_count;
+	u32 rctl, vmolr = 0;
+	int count;
+
+	/* Check for Promiscuous and All Multicast modes */
+	rctl = E1000_READ_REG(hw, E1000_RCTL);
+
+	/* clear the effected bits */
+	rctl &= ~(E1000_RCTL_UPE | E1000_RCTL_MPE | E1000_RCTL_VFE);
+
+	if (netdev->flags & IFF_PROMISC) {
+		rctl |= (E1000_RCTL_UPE | E1000_RCTL_MPE);
+		vmolr |= (E1000_VMOLR_ROPE | E1000_VMOLR_MPME);
+		/* retain VLAN HW filtering if in VT mode */
+		if (adapter->vfs_allocated_count || adapter->vmdq_pools)
+			rctl |= E1000_RCTL_VFE;
+	} else {
+		if (netdev->flags & IFF_ALLMULTI) {
+			rctl |= E1000_RCTL_MPE;
+			vmolr |= E1000_VMOLR_MPME;
+		} else {
+			/*
+			 * Write addresses to the MTA, if the attempt fails
+			 * then we should just turn on promiscuous mode so
+			 * that we can at least receive multicast traffic
+			 */
+			count = igb_write_mc_addr_list(netdev);
+			if (count < 0) {
+				rctl |= E1000_RCTL_MPE;
+				vmolr |= E1000_VMOLR_MPME;
+			} else if (count) {
+				vmolr |= E1000_VMOLR_ROMPE;
+			}
+		}
+#ifdef HAVE_SET_RX_MODE
+		/*
+		 * Write addresses to available RAR registers, if there is not
+		 * sufficient space to store all the addresses then enable
+		 * unicast promiscuous mode
+		 */
+		count = igb_write_uc_addr_list(netdev);
+		if (count < 0) {
+			rctl |= E1000_RCTL_UPE;
+			vmolr |= E1000_VMOLR_ROPE;
+		}
+#endif /* HAVE_SET_RX_MODE */
+		rctl |= E1000_RCTL_VFE;
+	}
+	E1000_WRITE_REG(hw, E1000_RCTL, rctl);
+
+	/*
+	 * In order to support SR-IOV and eventually VMDq it is necessary to set
+	 * the VMOLR to enable the appropriate modes.  Without this workaround
+	 * we will have issues with VLAN tag stripping not being done for frames
+	 * that are only arriving because we are the default pool
+	 */
+	if (hw->mac.type < e1000_82576)
+		return;
+
+	vmolr |= E1000_READ_REG(hw, E1000_VMOLR(vfn)) &
+	         ~(E1000_VMOLR_ROPE | E1000_VMOLR_MPME | E1000_VMOLR_ROMPE);
+	E1000_WRITE_REG(hw, E1000_VMOLR(vfn), vmolr);
+	igb_restore_vf_multicasts(adapter);
+}
+
+static void igb_check_wvbr(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 wvbr = 0;
+
+	switch (hw->mac.type) {
+	case e1000_82576:
+	case e1000_i350:
+		if (!(wvbr = E1000_READ_REG(hw, E1000_WVBR)))
+			return;
+		break;
+	default:
+		break;
+	}
+
+	adapter->wvbr |= wvbr;
+}
+
+#define IGB_STAGGERED_QUEUE_OFFSET 8
+
+static void igb_spoof_check(struct igb_adapter *adapter)
+{
+	int j;
+
+	if (!adapter->wvbr)
+		return;
+
+	switch (adapter->hw.mac.type) {
+	case e1000_82576:
+		for (j = 0; j < adapter->vfs_allocated_count; j++) {
+			if (adapter->wvbr & (1 << j) ||
+			    adapter->wvbr & (1 << (j + IGB_STAGGERED_QUEUE_OFFSET))) {
+				DPRINTK(DRV, WARNING,
+					"Spoof event(s) detected on VF %d\n", j);
+				adapter->wvbr &=
+					~((1 << j) |
+					  (1 << (j + IGB_STAGGERED_QUEUE_OFFSET)));
+			}
+		}
+		break;
+	case e1000_i350:
+		for (j = 0; j < adapter->vfs_allocated_count; j++) {
+			if (adapter->wvbr & (1 << j)) {
+				DPRINTK(DRV, WARNING,
+					"Spoof event(s) detected on VF %d\n", j);
+				adapter->wvbr &= ~(1 << j);
+			}
+		}
+		break;
+	default:
+		break;
+	}
+}
+
+/* Need to wait a few seconds after link up to get diagnostic information from
+ * the phy */
+#ifdef HAVE_TIMER_SETUP
+static void igb_update_phy_info(struct timer_list *t)
+{
+	struct igb_adapter *adapter = from_timer(adapter, t, phy_info_timer);
+#else
+static void igb_update_phy_info(unsigned long data)
+{
+	struct igb_adapter *adapter = (struct igb_adapter *) data;
+#endif
+	e1000_get_phy_info(&adapter->hw);
+}
+
+/**
+ * igb_has_link - check shared code for link and determine up/down
+ * @adapter: pointer to driver private info
+ **/
+bool igb_has_link(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	bool link_active = FALSE;
+
+	/* get_link_status is set on LSC (link status) interrupt or
+	 * rx sequence error interrupt.  get_link_status will stay
+	 * false until the e1000_check_for_link establishes link
+	 * for copper adapters ONLY
+	 */
+	switch (hw->phy.media_type) {
+	case e1000_media_type_copper:
+		if (!hw->mac.get_link_status)
+			return true;
+	case e1000_media_type_internal_serdes:
+		e1000_check_for_link(hw);
+		link_active = !hw->mac.get_link_status;
+		break;
+	case e1000_media_type_unknown:
+	default:
+		break;
+	}
+
+	if (((hw->mac.type == e1000_i210) ||
+	     (hw->mac.type == e1000_i211)) &&
+	     (hw->phy.id == I210_I_PHY_ID)) {
+		if (!netif_carrier_ok(adapter->netdev)) {
+			adapter->flags &= ~IGB_FLAG_NEED_LINK_UPDATE;
+		} else if (!(adapter->flags & IGB_FLAG_NEED_LINK_UPDATE)) {
+			adapter->flags |= IGB_FLAG_NEED_LINK_UPDATE;
+			adapter->link_check_timeout = jiffies;
+		}
+	}
+
+	return link_active;
+}
+
+/**
+ * igb_watchdog - Timer Call-back
+ * @data: pointer to adapter cast into an unsigned long
+ **/
+#ifdef HAVE_TIMER_SETUP
+static void igb_watchdog(struct timer_list *t)
+{
+	struct igb_adapter *adapter = from_timer(adapter, t, watchdog_timer);
+#else
+static void igb_watchdog(unsigned long data)
+{
+	struct igb_adapter *adapter = (struct igb_adapter *)data;
+#endif
+	/* Do the rest outside of interrupt context */
+	schedule_work(&adapter->watchdog_task);
+}
+
+static void igb_watchdog_task(struct work_struct *work)
+{
+	struct igb_adapter *adapter = container_of(work,
+	                                           struct igb_adapter,
+                                                   watchdog_task);
+	struct e1000_hw *hw = &adapter->hw;
+	struct net_device *netdev = adapter->netdev;
+	u32 link;
+	int i;
+	u32 thstat, ctrl_ext;
+	u32 connsw;
+
+	link = igb_has_link(adapter);
+	/* Force link down if we have fiber to swap to */
+	if (adapter->flags & IGB_FLAG_MAS_ENABLE) {
+		if (hw->phy.media_type == e1000_media_type_copper) {
+			connsw = E1000_READ_REG(hw, E1000_CONNSW);
+			if (!(connsw & E1000_CONNSW_AUTOSENSE_EN))
+				link = 0;
+		}
+	}
+
+	if (adapter->flags & IGB_FLAG_NEED_LINK_UPDATE) {
+		if (time_after(jiffies, (adapter->link_check_timeout + HZ)))
+			adapter->flags &= ~IGB_FLAG_NEED_LINK_UPDATE;
+		else
+			link = FALSE;
+	}
+
+	if (link) {
+		/* Perform a reset if the media type changed. */
+		if (hw->dev_spec._82575.media_changed) {
+			hw->dev_spec._82575.media_changed = false;
+			adapter->flags |= IGB_FLAG_MEDIA_RESET;
+			igb_reset(adapter);
+		}
+
+		/* Cancel scheduled suspend requests. */
+		pm_runtime_resume(netdev->dev.parent);
+
+		if (!netif_carrier_ok(netdev)) {
+			u32 ctrl;
+			e1000_get_speed_and_duplex(hw,
+			                           &adapter->link_speed,
+			                           &adapter->link_duplex);
+
+			ctrl = E1000_READ_REG(hw, E1000_CTRL);
+			/* Links status message must follow this format */
+			printk(KERN_INFO "igb: %s NIC Link is Up %d Mbps %s, "
+				 "Flow Control: %s\n",
+			       netdev->name,
+			       adapter->link_speed,
+			       adapter->link_duplex == FULL_DUPLEX ?
+				 "Full Duplex" : "Half Duplex",
+			       ((ctrl & E1000_CTRL_TFCE) &&
+			        (ctrl & E1000_CTRL_RFCE)) ? "RX/TX":
+			       ((ctrl & E1000_CTRL_RFCE) ?  "RX" :
+			       ((ctrl & E1000_CTRL_TFCE) ?  "TX" : "None")));
+			/* adjust timeout factor according to speed/duplex */
+			adapter->tx_timeout_factor = 1;
+			switch (adapter->link_speed) {
+			case SPEED_10:
+				adapter->tx_timeout_factor = 14;
+				break;
+			case SPEED_100:
+				/* maybe add some timeout factor ? */
+				break;
+			default:
+				break;
+			}
+
+			netif_carrier_on(netdev);
+			netif_tx_wake_all_queues(netdev);
+
+			igb_ping_all_vfs(adapter);
+#ifdef IFLA_VF_MAX
+			igb_check_vf_rate_limit(adapter);
+#endif /* IFLA_VF_MAX */
+
+			/* link state has changed, schedule phy info update */
+			if (!test_bit(__IGB_DOWN, &adapter->state))
+				mod_timer(&adapter->phy_info_timer,
+					  round_jiffies(jiffies + 2 * HZ));
+		}
+	} else {
+		if (netif_carrier_ok(netdev)) {
+			adapter->link_speed = 0;
+			adapter->link_duplex = 0;
+			/* check for thermal sensor event on i350 */
+			if (hw->mac.type == e1000_i350) {
+				thstat = E1000_READ_REG(hw, E1000_THSTAT);
+				ctrl_ext = E1000_READ_REG(hw, E1000_CTRL_EXT);
+				if ((hw->phy.media_type ==
+					e1000_media_type_copper) &&
+					!(ctrl_ext &
+					E1000_CTRL_EXT_LINK_MODE_SGMII)) {
+					if (thstat & E1000_THSTAT_PWR_DOWN) {
+						printk(KERN_ERR "igb: %s The "
+						"network adapter was stopped "
+						"because it overheated.\n",
+						netdev->name);
+					}
+					if (thstat & E1000_THSTAT_LINK_THROTTLE) {
+						printk(KERN_INFO
+							"igb: %s The network "
+							"adapter supported "
+							"link speed "
+							"was downshifted "
+							"because it "
+							"overheated.\n",
+							netdev->name);
+					}
+				}
+			}
+
+			/* Links status message must follow this format */
+			printk(KERN_INFO "igb: %s NIC Link is Down\n",
+			       netdev->name);
+			netif_carrier_off(netdev);
+			netif_tx_stop_all_queues(netdev);
+
+			igb_ping_all_vfs(adapter);
+
+			/* link state has changed, schedule phy info update */
+			if (!test_bit(__IGB_DOWN, &adapter->state))
+				mod_timer(&adapter->phy_info_timer,
+					  round_jiffies(jiffies + 2 * HZ));
+			/* link is down, time to check for alternate media */
+			if (adapter->flags & IGB_FLAG_MAS_ENABLE) {
+				igb_check_swap_media(adapter);
+				if (adapter->flags & IGB_FLAG_MEDIA_RESET) {
+					schedule_work(&adapter->reset_task);
+					/* return immediately */
+					return;
+				}
+			}
+			pm_schedule_suspend(netdev->dev.parent,
+					    MSEC_PER_SEC * 5);
+
+		/* also check for alternate media here */
+		} else if (!netif_carrier_ok(netdev) &&
+			   (adapter->flags & IGB_FLAG_MAS_ENABLE)) {
+			hw->mac.ops.power_up_serdes(hw);
+			igb_check_swap_media(adapter);
+			if (adapter->flags & IGB_FLAG_MEDIA_RESET) {
+				schedule_work(&adapter->reset_task);
+				/* return immediately */
+				return;
+			}
+		}
+	}
+
+	igb_update_stats(adapter);
+
+	for (i = 0; i < adapter->num_tx_queues; i++) {
+		struct igb_ring *tx_ring = adapter->tx_ring[i];
+		if (!netif_carrier_ok(netdev)) {
+			/* We've lost link, so the controller stops DMA,
+			 * but we've got queued Tx work that's never going
+			 * to get done, so reset controller to flush Tx.
+			 * (Do the reset outside of interrupt context). */
+			if (igb_desc_unused(tx_ring) + 1 < tx_ring->count) {
+				adapter->tx_timeout_count++;
+				schedule_work(&adapter->reset_task);
+				/* return immediately since reset is imminent */
+				return;
+			}
+		}
+
+		/* Force detection of hung controller every watchdog period */
+		set_bit(IGB_RING_FLAG_TX_DETECT_HANG, &tx_ring->flags);
+	}
+
+	/* Cause software interrupt to ensure rx ring is cleaned */
+	if (adapter->msix_entries) {
+		u32 eics = 0;
+		for (i = 0; i < adapter->num_q_vectors; i++)
+			eics |= adapter->q_vector[i]->eims_value;
+		E1000_WRITE_REG(hw, E1000_EICS, eics);
+	} else {
+		E1000_WRITE_REG(hw, E1000_ICS, E1000_ICS_RXDMT0);
+	}
+
+	igb_spoof_check(adapter);
+
+	/* Reset the timer */
+	if (!test_bit(__IGB_DOWN, &adapter->state)) {
+		if (adapter->flags & IGB_FLAG_NEED_LINK_UPDATE)
+			mod_timer(&adapter->watchdog_timer,
+				  round_jiffies(jiffies +  HZ));
+		else
+			mod_timer(&adapter->watchdog_timer,
+				  round_jiffies(jiffies + 2 * HZ));
+	}
+}
+
+static void igb_dma_err_task(struct work_struct *work)
+{
+	struct igb_adapter *adapter = container_of(work,
+	                                           struct igb_adapter,
+                                                   dma_err_task);
+	int vf;
+	struct e1000_hw *hw = &adapter->hw;
+	struct net_device *netdev = adapter->netdev;
+	u32 hgptc;
+	u32 ciaa, ciad;
+
+	hgptc = E1000_READ_REG(hw, E1000_HGPTC);
+	if (hgptc) /* If incrementing then no need for the check below */
+		goto dma_timer_reset;
+	/*
+	 * Check to see if a bad DMA write target from an errant or
+	 * malicious VF has caused a PCIe error.  If so then we can
+	 * issue a VFLR to the offending VF(s) and then resume without
+	 * requesting a full slot reset.
+	 */
+
+	for (vf = 0; vf < adapter->vfs_allocated_count; vf++) {
+		ciaa = (vf << 16) | 0x80000000;
+		/* 32 bit read so align, we really want status at offset 6 */
+		ciaa |= PCI_COMMAND;
+		E1000_WRITE_REG(hw, E1000_CIAA, ciaa);
+		ciad = E1000_READ_REG(hw, E1000_CIAD);
+		ciaa &= 0x7FFFFFFF;
+		/* disable debug mode asap after reading data */
+		E1000_WRITE_REG(hw, E1000_CIAA, ciaa);
+		/* Get the upper 16 bits which will be the PCI status reg */
+		ciad >>= 16;
+		if (ciad & (PCI_STATUS_REC_MASTER_ABORT |
+			    PCI_STATUS_REC_TARGET_ABORT |
+			    PCI_STATUS_SIG_SYSTEM_ERROR)) {
+			netdev_err(netdev, "VF %d suffered error\n", vf);
+			/* Issue VFLR */
+			ciaa = (vf << 16) | 0x80000000;
+			ciaa |= 0xA8;
+			E1000_WRITE_REG(hw, E1000_CIAA, ciaa);
+			ciad = 0x00008000;  /* VFLR */
+			E1000_WRITE_REG(hw, E1000_CIAD, ciad);
+			ciaa &= 0x7FFFFFFF;
+			E1000_WRITE_REG(hw, E1000_CIAA, ciaa);
+		}
+	}
+dma_timer_reset:
+	/* Reset the timer */
+	if (!test_bit(__IGB_DOWN, &adapter->state))
+		mod_timer(&adapter->dma_err_timer,
+			  round_jiffies(jiffies + HZ / 10));
+}
+
+/**
+ * igb_dma_err_timer - Timer Call-back
+ * @data: pointer to adapter cast into an unsigned long
+ **/
+#ifdef HAVE_TIMER_SETUP
+static void igb_dma_err_timer(struct timer_list *t)
+{
+	struct igb_adapter *adapter = from_timer(adapter, t, dma_err_timer);
+#else
+static void igb_dma_err_timer(unsigned long data)
+{
+	struct igb_adapter *adapter = (struct igb_adapter *)data;
+#endif
+	/* Do the rest outside of interrupt context */
+	schedule_work(&adapter->dma_err_task);
+}
+
+enum latency_range {
+	lowest_latency = 0,
+	low_latency = 1,
+	bulk_latency = 2,
+	latency_invalid = 255
+};
+
+/**
+ * igb_update_ring_itr - update the dynamic ITR value based on packet size
+ *
+ *      Stores a new ITR value based on strictly on packet size.  This
+ *      algorithm is less sophisticated than that used in igb_update_itr,
+ *      due to the difficulty of synchronizing statistics across multiple
+ *      receive rings.  The divisors and thresholds used by this function
+ *      were determined based on theoretical maximum wire speed and testing
+ *      data, in order to minimize response time while increasing bulk
+ *      throughput.
+ *      This functionality is controlled by the InterruptThrottleRate module
+ *      parameter (see igb_param.c)
+ *      NOTE:  This function is called only when operating in a multiqueue
+ *             receive environment.
+ * @q_vector: pointer to q_vector
+ **/
+static void igb_update_ring_itr(struct igb_q_vector *q_vector)
+{
+	int new_val = q_vector->itr_val;
+	int avg_wire_size = 0;
+	struct igb_adapter *adapter = q_vector->adapter;
+	unsigned int packets;
+
+	/* For non-gigabit speeds, just fix the interrupt rate at 4000
+	 * ints/sec - ITR timer value of 120 ticks.
+	 */
+	switch (adapter->link_speed) {
+	case SPEED_10:
+	case SPEED_100:
+		new_val = IGB_4K_ITR;
+		goto set_itr_val;
+	default:
+		break;
+	}
+
+	packets = q_vector->rx.total_packets;
+	if (packets)
+		avg_wire_size = q_vector->rx.total_bytes / packets;
+
+	packets = q_vector->tx.total_packets;
+	if (packets)
+		avg_wire_size = max_t(u32, avg_wire_size,
+		                      q_vector->tx.total_bytes / packets);
+
+	/* if avg_wire_size isn't set no work was done */
+	if (!avg_wire_size)
+		goto clear_counts;
+
+	/* Add 24 bytes to size to account for CRC, preamble, and gap */
+	avg_wire_size += 24;
+
+	/* Don't starve jumbo frames */
+	avg_wire_size = min(avg_wire_size, 3000);
+
+	/* Give a little boost to mid-size frames */
+	if ((avg_wire_size > 300) && (avg_wire_size < 1200))
+		new_val = avg_wire_size / 3;
+	else
+		new_val = avg_wire_size / 2;
+
+	/* conservative mode (itr 3) eliminates the lowest_latency setting */
+	if (new_val < IGB_20K_ITR &&
+	    ((q_vector->rx.ring && adapter->rx_itr_setting == 3) ||
+	     (!q_vector->rx.ring && adapter->tx_itr_setting == 3)))
+		new_val = IGB_20K_ITR;
+
+set_itr_val:
+	if (new_val != q_vector->itr_val) {
+		q_vector->itr_val = new_val;
+		q_vector->set_itr = 1;
+	}
+clear_counts:
+	q_vector->rx.total_bytes = 0;
+	q_vector->rx.total_packets = 0;
+	q_vector->tx.total_bytes = 0;
+	q_vector->tx.total_packets = 0;
+}
+
+/**
+ * igb_update_itr - update the dynamic ITR value based on statistics
+ *      Stores a new ITR value based on packets and byte
+ *      counts during the last interrupt.  The advantage of per interrupt
+ *      computation is faster updates and more accurate ITR for the current
+ *      traffic pattern.  Constants in this function were computed
+ *      based on theoretical maximum wire speed and thresholds were set based
+ *      on testing data as well as attempting to minimize response time
+ *      while increasing bulk throughput.
+ *      this functionality is controlled by the InterruptThrottleRate module
+ *      parameter (see igb_param.c)
+ *      NOTE:  These calculations are only valid when operating in a single-
+ *             queue environment.
+ * @q_vector: pointer to q_vector
+ * @ring_container: ring info to update the itr for
+ **/
+static void igb_update_itr(struct igb_q_vector *q_vector,
+			   struct igb_ring_container *ring_container)
+{
+	unsigned int packets = ring_container->total_packets;
+	unsigned int bytes = ring_container->total_bytes;
+	u8 itrval = ring_container->itr;
+
+	/* no packets, exit with status unchanged */
+	if (packets == 0)
+		return;
+
+	switch (itrval) {
+	case lowest_latency:
+		/* handle TSO and jumbo frames */
+		if (bytes/packets > 8000)
+			itrval = bulk_latency;
+		else if ((packets < 5) && (bytes > 512))
+			itrval = low_latency;
+		break;
+	case low_latency:  /* 50 usec aka 20000 ints/s */
+		if (bytes > 10000) {
+			/* this if handles the TSO accounting */
+			if (bytes/packets > 8000) {
+				itrval = bulk_latency;
+			} else if ((packets < 10) || ((bytes/packets) > 1200)) {
+				itrval = bulk_latency;
+			} else if (packets > 35) {
+				itrval = lowest_latency;
+			}
+		} else if (bytes/packets > 2000) {
+			itrval = bulk_latency;
+		} else if (packets <= 2 && bytes < 512) {
+			itrval = lowest_latency;
+		}
+		break;
+	case bulk_latency: /* 250 usec aka 4000 ints/s */
+		if (bytes > 25000) {
+			if (packets > 35)
+				itrval = low_latency;
+		} else if (bytes < 1500) {
+			itrval = low_latency;
+		}
+		break;
+	}
+
+	/* clear work counters since we have the values we need */
+	ring_container->total_bytes = 0;
+	ring_container->total_packets = 0;
+
+	/* write updated itr to ring container */
+	ring_container->itr = itrval;
+}
+
+static void igb_set_itr(struct igb_q_vector *q_vector)
+{
+	struct igb_adapter *adapter = q_vector->adapter;
+	u32 new_itr = q_vector->itr_val;
+	u8 current_itr = 0;
+
+	/* for non-gigabit speeds, just fix the interrupt rate at 4000 */
+	switch (adapter->link_speed) {
+	case SPEED_10:
+	case SPEED_100:
+		current_itr = 0;
+		new_itr = IGB_4K_ITR;
+		goto set_itr_now;
+	default:
+		break;
+	}
+
+	igb_update_itr(q_vector, &q_vector->tx);
+	igb_update_itr(q_vector, &q_vector->rx);
+
+	current_itr = max(q_vector->rx.itr, q_vector->tx.itr);
+
+	/* conservative mode (itr 3) eliminates the lowest_latency setting */
+	if (current_itr == lowest_latency &&
+	    ((q_vector->rx.ring && adapter->rx_itr_setting == 3) ||
+	     (!q_vector->rx.ring && adapter->tx_itr_setting == 3)))
+		current_itr = low_latency;
+
+	switch (current_itr) {
+	/* counts and packets in update_itr are dependent on these numbers */
+	case lowest_latency:
+		new_itr = IGB_70K_ITR; /* 70,000 ints/sec */
+		break;
+	case low_latency:
+		new_itr = IGB_20K_ITR; /* 20,000 ints/sec */
+		break;
+	case bulk_latency:
+		new_itr = IGB_4K_ITR;  /* 4,000 ints/sec */
+		break;
+	default:
+		break;
+	}
+
+set_itr_now:
+	if (new_itr != q_vector->itr_val) {
+		/* this attempts to bias the interrupt rate towards Bulk
+		 * by adding intermediate steps when interrupt rate is
+		 * increasing */
+		new_itr = new_itr > q_vector->itr_val ?
+		             max((new_itr * q_vector->itr_val) /
+		                 (new_itr + (q_vector->itr_val >> 2)),
+				 new_itr) :
+			     new_itr;
+		/* Don't write the value here; it resets the adapter's
+		 * internal timer, and causes us to delay far longer than
+		 * we should between interrupts.  Instead, we write the ITR
+		 * value at the beginning of the next interrupt so the timing
+		 * ends up being correct.
+		 */
+		q_vector->itr_val = new_itr;
+		q_vector->set_itr = 1;
+	}
+}
+
+void igb_tx_ctxtdesc(struct igb_ring *tx_ring, u32 vlan_macip_lens,
+		     u32 type_tucmd, u32 mss_l4len_idx)
+{
+	struct e1000_adv_tx_context_desc *context_desc;
+	u16 i = tx_ring->next_to_use;
+
+	context_desc = IGB_TX_CTXTDESC(tx_ring, i);
+
+	i++;
+	tx_ring->next_to_use = (i < tx_ring->count) ? i : 0;
+
+	/* set bits to identify this as an advanced context descriptor */
+	type_tucmd |= E1000_TXD_CMD_DEXT | E1000_ADVTXD_DTYP_CTXT;
+
+	/* For 82575, context index must be unique per ring. */
+	if (test_bit(IGB_RING_FLAG_TX_CTX_IDX, &tx_ring->flags))
+		mss_l4len_idx |= tx_ring->reg_idx << 4;
+
+	context_desc->vlan_macip_lens	= cpu_to_le32(vlan_macip_lens);
+	context_desc->seqnum_seed	= 0;
+	context_desc->type_tucmd_mlhl	= cpu_to_le32(type_tucmd);
+	context_desc->mss_l4len_idx	= cpu_to_le32(mss_l4len_idx);
+}
+
+static int igb_tso(struct igb_ring *tx_ring,
+		   struct igb_tx_buffer *first,
+		   u8 *hdr_len)
+{
+#ifdef NETIF_F_TSO
+	struct sk_buff *skb = first->skb;
+	u32 vlan_macip_lens, type_tucmd;
+	u32 mss_l4len_idx, l4len;
+
+	if (skb->ip_summed != CHECKSUM_PARTIAL)
+		return 0;
+
+	if (!skb_is_gso(skb))
+#endif /* NETIF_F_TSO */
+		return 0;
+#ifdef NETIF_F_TSO
+
+	if (skb_header_cloned(skb)) {
+		int err = pskb_expand_head(skb, 0, 0, GFP_ATOMIC);
+		if (err)
+			return err;
+	}
+
+	/* ADV DTYP TUCMD MKRLOC/ISCSIHEDLEN */
+	type_tucmd = E1000_ADVTXD_TUCMD_L4T_TCP;
+
+	if (first->protocol == __constant_htons(ETH_P_IP)) {
+		struct iphdr *iph = ip_hdr(skb);
+		iph->tot_len = 0;
+		iph->check = 0;
+		tcp_hdr(skb)->check = ~csum_tcpudp_magic(iph->saddr,
+							 iph->daddr, 0,
+							 IPPROTO_TCP,
+							 0);
+		type_tucmd |= E1000_ADVTXD_TUCMD_IPV4;
+		first->tx_flags |= IGB_TX_FLAGS_TSO |
+				   IGB_TX_FLAGS_CSUM |
+				   IGB_TX_FLAGS_IPV4;
+#ifdef NETIF_F_TSO6
+	} else if (skb_is_gso_v6(skb)) {
+		ipv6_hdr(skb)->payload_len = 0;
+		tcp_hdr(skb)->check = ~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,
+						       &ipv6_hdr(skb)->daddr,
+						       0, IPPROTO_TCP, 0);
+		first->tx_flags |= IGB_TX_FLAGS_TSO |
+				   IGB_TX_FLAGS_CSUM;
+#endif
+	}
+
+	/* compute header lengths */
+	l4len = tcp_hdrlen(skb);
+	*hdr_len = skb_transport_offset(skb) + l4len;
+
+	/* update gso size and bytecount with header size */
+	first->gso_segs = skb_shinfo(skb)->gso_segs;
+	first->bytecount += (first->gso_segs - 1) * *hdr_len;
+
+	/* MSS L4LEN IDX */
+	mss_l4len_idx = l4len << E1000_ADVTXD_L4LEN_SHIFT;
+	mss_l4len_idx |= skb_shinfo(skb)->gso_size << E1000_ADVTXD_MSS_SHIFT;
+
+	/* VLAN MACLEN IPLEN */
+	vlan_macip_lens = skb_network_header_len(skb);
+	vlan_macip_lens |= skb_network_offset(skb) << E1000_ADVTXD_MACLEN_SHIFT;
+	vlan_macip_lens |= first->tx_flags & IGB_TX_FLAGS_VLAN_MASK;
+
+	igb_tx_ctxtdesc(tx_ring, vlan_macip_lens, type_tucmd, mss_l4len_idx);
+
+	return 1;
+#endif  /* NETIF_F_TSO */
+}
+
+static void igb_tx_csum(struct igb_ring *tx_ring, struct igb_tx_buffer *first)
+{
+	struct sk_buff *skb = first->skb;
+	u32 vlan_macip_lens = 0;
+	u32 mss_l4len_idx = 0;
+	u32 type_tucmd = 0;
+
+	if (skb->ip_summed != CHECKSUM_PARTIAL) {
+		if (!(first->tx_flags & IGB_TX_FLAGS_VLAN))
+			return;
+	} else {
+		u8 nexthdr = 0;
+		switch (first->protocol) {
+		case __constant_htons(ETH_P_IP):
+			vlan_macip_lens |= skb_network_header_len(skb);
+			type_tucmd |= E1000_ADVTXD_TUCMD_IPV4;
+			nexthdr = ip_hdr(skb)->protocol;
+			break;
+#ifdef NETIF_F_IPV6_CSUM
+		case __constant_htons(ETH_P_IPV6):
+			vlan_macip_lens |= skb_network_header_len(skb);
+			nexthdr = ipv6_hdr(skb)->nexthdr;
+			break;
+#endif
+		default:
+			if (unlikely(net_ratelimit())) {
+				dev_warn(tx_ring->dev,
+				 "partial checksum but proto=%x!\n",
+				 first->protocol);
+			}
+			break;
+		}
+
+		switch (nexthdr) {
+		case IPPROTO_TCP:
+			type_tucmd |= E1000_ADVTXD_TUCMD_L4T_TCP;
+			mss_l4len_idx = tcp_hdrlen(skb) <<
+					E1000_ADVTXD_L4LEN_SHIFT;
+			break;
+#ifdef HAVE_SCTP
+		case IPPROTO_SCTP:
+			type_tucmd |= E1000_ADVTXD_TUCMD_L4T_SCTP;
+			mss_l4len_idx = sizeof(struct sctphdr) <<
+					E1000_ADVTXD_L4LEN_SHIFT;
+			break;
+#endif
+		case IPPROTO_UDP:
+			mss_l4len_idx = sizeof(struct udphdr) <<
+					E1000_ADVTXD_L4LEN_SHIFT;
+			break;
+		default:
+			if (unlikely(net_ratelimit())) {
+				dev_warn(tx_ring->dev,
+				 "partial checksum but l4 proto=%x!\n",
+				 nexthdr);
+			}
+			break;
+		}
+
+		/* update TX checksum flag */
+		first->tx_flags |= IGB_TX_FLAGS_CSUM;
+	}
+
+	vlan_macip_lens |= skb_network_offset(skb) << E1000_ADVTXD_MACLEN_SHIFT;
+	vlan_macip_lens |= first->tx_flags & IGB_TX_FLAGS_VLAN_MASK;
+
+	igb_tx_ctxtdesc(tx_ring, vlan_macip_lens, type_tucmd, mss_l4len_idx);
+}
+
+#define IGB_SET_FLAG(_input, _flag, _result) \
+	((_flag <= _result) ? \
+	 ((u32)(_input & _flag) * (_result / _flag)) : \
+	 ((u32)(_input & _flag) / (_flag / _result)))
+
+static u32 igb_tx_cmd_type(struct sk_buff *skb, u32 tx_flags)
+{
+	/* set type for advanced descriptor with frame checksum insertion */
+	u32 cmd_type = E1000_ADVTXD_DTYP_DATA |
+		       E1000_ADVTXD_DCMD_DEXT |
+		       E1000_ADVTXD_DCMD_IFCS;
+
+	/* set HW vlan bit if vlan is present */
+	cmd_type |= IGB_SET_FLAG(tx_flags, IGB_TX_FLAGS_VLAN,
+				 (E1000_ADVTXD_DCMD_VLE));
+
+	/* set segmentation bits for TSO */
+	cmd_type |= IGB_SET_FLAG(tx_flags, IGB_TX_FLAGS_TSO,
+				 (E1000_ADVTXD_DCMD_TSE));
+
+	/* set timestamp bit if present */
+	cmd_type |= IGB_SET_FLAG(tx_flags, IGB_TX_FLAGS_TSTAMP,
+				 (E1000_ADVTXD_MAC_TSTAMP));
+
+	return cmd_type;
+}
+
+static void igb_tx_olinfo_status(struct igb_ring *tx_ring,
+				 union e1000_adv_tx_desc *tx_desc,
+				 u32 tx_flags, unsigned int paylen)
+{
+	u32 olinfo_status = paylen << E1000_ADVTXD_PAYLEN_SHIFT;
+
+	/* 82575 requires a unique index per ring */
+	if (test_bit(IGB_RING_FLAG_TX_CTX_IDX, &tx_ring->flags))
+		olinfo_status |= tx_ring->reg_idx << 4;
+
+	/* insert L4 checksum */
+	olinfo_status |= IGB_SET_FLAG(tx_flags,
+				      IGB_TX_FLAGS_CSUM,
+				      (E1000_TXD_POPTS_TXSM << 8));
+
+	/* insert IPv4 checksum */
+	olinfo_status |= IGB_SET_FLAG(tx_flags,
+				      IGB_TX_FLAGS_IPV4,
+				      (E1000_TXD_POPTS_IXSM << 8));
+
+	tx_desc->read.olinfo_status = cpu_to_le32(olinfo_status);
+}
+
+static void igb_tx_map(struct igb_ring *tx_ring,
+		       struct igb_tx_buffer *first,
+		       const u8 hdr_len)
+{
+	struct sk_buff *skb = first->skb;
+	struct igb_tx_buffer *tx_buffer;
+	union e1000_adv_tx_desc *tx_desc;
+	struct skb_frag_struct *frag;
+	dma_addr_t dma;
+	unsigned int data_len, size;
+	u32 tx_flags = first->tx_flags;
+	u32 cmd_type = igb_tx_cmd_type(skb, tx_flags);
+	u16 i = tx_ring->next_to_use;
+
+	tx_desc = IGB_TX_DESC(tx_ring, i);
+
+	igb_tx_olinfo_status(tx_ring, tx_desc, tx_flags, skb->len - hdr_len);
+
+	size = skb_headlen(skb);
+	data_len = skb->data_len;
+
+	dma = dma_map_single(tx_ring->dev, skb->data, size, DMA_TO_DEVICE);
+
+	tx_buffer = first;
+
+	for (frag = &skb_shinfo(skb)->frags[0];; frag++) {
+		if (dma_mapping_error(tx_ring->dev, dma))
+			goto dma_error;
+
+		/* record length, and DMA address */
+		dma_unmap_len_set(tx_buffer, len, size);
+		dma_unmap_addr_set(tx_buffer, dma, dma);
+
+		tx_desc->read.buffer_addr = cpu_to_le64(dma);
+
+		while (unlikely(size > IGB_MAX_DATA_PER_TXD)) {
+			tx_desc->read.cmd_type_len =
+				cpu_to_le32(cmd_type ^ IGB_MAX_DATA_PER_TXD);
+
+			i++;
+			tx_desc++;
+			if (i == tx_ring->count) {
+				tx_desc = IGB_TX_DESC(tx_ring, 0);
+				i = 0;
+			}
+			tx_desc->read.olinfo_status = 0;
+
+			dma += IGB_MAX_DATA_PER_TXD;
+			size -= IGB_MAX_DATA_PER_TXD;
+
+			tx_desc->read.buffer_addr = cpu_to_le64(dma);
+		}
+
+		if (likely(!data_len))
+			break;
+
+		tx_desc->read.cmd_type_len = cpu_to_le32(cmd_type ^ size);
+
+		i++;
+		tx_desc++;
+		if (i == tx_ring->count) {
+			tx_desc = IGB_TX_DESC(tx_ring, 0);
+			i = 0;
+		}
+		tx_desc->read.olinfo_status = 0;
+
+		size = skb_frag_size(frag);
+		data_len -= size;
+
+		dma = skb_frag_dma_map(tx_ring->dev, frag, 0,
+				       size, DMA_TO_DEVICE);
+
+		tx_buffer = &tx_ring->tx_buffer_info[i];
+	}
+
+	/* write last descriptor with RS and EOP bits */
+	cmd_type |= size | IGB_TXD_DCMD;
+	tx_desc->read.cmd_type_len = cpu_to_le32(cmd_type);
+
+	netdev_tx_sent_queue(txring_txq(tx_ring), first->bytecount);
+	/* set the timestamp */
+	first->time_stamp = jiffies;
+
+	/*
+	 * Force memory writes to complete before letting h/w know there
+	 * are new descriptors to fetch.  (Only applicable for weak-ordered
+	 * memory model archs, such as IA-64).
+	 *
+	 * We also need this memory barrier to make certain all of the
+	 * status bits have been updated before next_to_watch is written.
+	 */
+	wmb();
+
+	/* set next_to_watch value indicating a packet is present */
+	first->next_to_watch = tx_desc;
+
+	i++;
+	if (i == tx_ring->count)
+		i = 0;
+
+	tx_ring->next_to_use = i;
+
+	writel(i, tx_ring->tail);
+
+	/* we need this if more than one processor can write to our tail
+	 * at a time, it syncronizes IO on IA64/Altix systems */
+	mmiowb();
+
+	return;
+
+dma_error:
+	dev_err(tx_ring->dev, "TX DMA map failed\n");
+
+	/* clear dma mappings for failed tx_buffer_info map */
+	for (;;) {
+		tx_buffer = &tx_ring->tx_buffer_info[i];
+		igb_unmap_and_free_tx_resource(tx_ring, tx_buffer);
+		if (tx_buffer == first)
+			break;
+		if (i == 0)
+			i = tx_ring->count;
+		i--;
+	}
+
+	tx_ring->next_to_use = i;
+}
+
+static int __igb_maybe_stop_tx(struct igb_ring *tx_ring, const u16 size)
+{
+	struct net_device *netdev = netdev_ring(tx_ring);
+
+	if (netif_is_multiqueue(netdev))
+		netif_stop_subqueue(netdev, ring_queue_index(tx_ring));
+	else
+		netif_stop_queue(netdev);
+
+	/* Herbert's original patch had:
+	 *  smp_mb__after_netif_stop_queue();
+	 * but since that doesn't exist yet, just open code it. */
+	smp_mb();
+
+	/* We need to check again in a case another CPU has just
+	 * made room available. */
+	if (igb_desc_unused(tx_ring) < size)
+		return -EBUSY;
+
+	/* A reprieve! */
+	if (netif_is_multiqueue(netdev))
+		netif_wake_subqueue(netdev, ring_queue_index(tx_ring));
+	else
+		netif_wake_queue(netdev);
+
+	tx_ring->tx_stats.restart_queue++;
+
+	return 0;
+}
+
+static inline int igb_maybe_stop_tx(struct igb_ring *tx_ring, const u16 size)
+{
+	if (igb_desc_unused(tx_ring) >= size)
+		return 0;
+	return __igb_maybe_stop_tx(tx_ring, size);
+}
+
+netdev_tx_t igb_xmit_frame_ring(struct sk_buff *skb,
+				struct igb_ring *tx_ring)
+{
+	struct igb_tx_buffer *first;
+	int tso;
+	u32 tx_flags = 0;
+#if PAGE_SIZE > IGB_MAX_DATA_PER_TXD
+	unsigned short f;
+#endif
+	u16 count = TXD_USE_COUNT(skb_headlen(skb));
+	__be16 protocol = vlan_get_protocol(skb);
+	u8 hdr_len = 0;
+
+	/*
+	 * need: 1 descriptor per page * PAGE_SIZE/IGB_MAX_DATA_PER_TXD,
+	 *       + 1 desc for skb_headlen/IGB_MAX_DATA_PER_TXD,
+	 *       + 2 desc gap to keep tail from touching head,
+	 *       + 1 desc for context descriptor,
+	 * otherwise try next time
+	 */
+#if PAGE_SIZE > IGB_MAX_DATA_PER_TXD
+	for (f = 0; f < skb_shinfo(skb)->nr_frags; f++)
+		count += TXD_USE_COUNT(skb_shinfo(skb)->frags[f].size);
+#else
+	count += skb_shinfo(skb)->nr_frags;
+#endif
+	if (igb_maybe_stop_tx(tx_ring, count + 3)) {
+		/* this is a hard error */
+		return NETDEV_TX_BUSY;
+	}
+
+	/* record the location of the first descriptor for this packet */
+	first = &tx_ring->tx_buffer_info[tx_ring->next_to_use];
+	first->skb = skb;
+	first->bytecount = skb->len;
+	first->gso_segs = 1;
+
+	skb_tx_timestamp(skb);
+
+#ifdef HAVE_PTP_1588_CLOCK
+	if (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)) {
+		struct igb_adapter *adapter = netdev_priv(tx_ring->netdev);
+		if (!adapter->ptp_tx_skb) {
+			skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+			tx_flags |= IGB_TX_FLAGS_TSTAMP;
+
+			adapter->ptp_tx_skb = skb_get(skb);
+			adapter->ptp_tx_start = jiffies;
+			if (adapter->hw.mac.type == e1000_82576)
+				schedule_work(&adapter->ptp_tx_work);
+		}
+	}
+#endif /* HAVE_PTP_1588_CLOCK */
+
+	if (vlan_tx_tag_present(skb)) {
+		tx_flags |= IGB_TX_FLAGS_VLAN;
+		tx_flags |= (vlan_tx_tag_get(skb) << IGB_TX_FLAGS_VLAN_SHIFT);
+	}
+
+	/* record initial flags and protocol */
+	first->tx_flags = tx_flags;
+	first->protocol = protocol;
+
+	tso = igb_tso(tx_ring, first, &hdr_len);
+	if (tso < 0)
+		goto out_drop;
+	else if (!tso)
+		igb_tx_csum(tx_ring, first);
+
+	igb_tx_map(tx_ring, first, hdr_len);
+
+#ifndef HAVE_TRANS_START_IN_QUEUE
+	netdev_ring(tx_ring)->trans_start = jiffies;
+
+#endif
+	/* Make sure there is space in the ring for the next send. */
+	igb_maybe_stop_tx(tx_ring, DESC_NEEDED);
+
+	return NETDEV_TX_OK;
+
+out_drop:
+	igb_unmap_and_free_tx_resource(tx_ring, first);
+
+	return NETDEV_TX_OK;
+}
+
+#ifdef HAVE_TX_MQ
+static inline struct igb_ring *igb_tx_queue_mapping(struct igb_adapter *adapter,
+                                                    struct sk_buff *skb)
+{
+	unsigned int r_idx = skb->queue_mapping;
+
+	if (r_idx >= adapter->num_tx_queues)
+		r_idx = r_idx % adapter->num_tx_queues;
+
+	return adapter->tx_ring[r_idx];
+}
+#else
+#define igb_tx_queue_mapping(_adapter, _skb) (_adapter)->tx_ring[0]
+#endif
+
+static netdev_tx_t igb_xmit_frame(struct sk_buff *skb,
+                                  struct net_device *netdev)
+{
+	struct igb_adapter *adapter = netdev_priv(netdev);
+
+	if (test_bit(__IGB_DOWN, &adapter->state)) {
+		dev_kfree_skb_any(skb);
+		return NETDEV_TX_OK;
+	}
+
+	if (skb->len <= 0) {
+		dev_kfree_skb_any(skb);
+		return NETDEV_TX_OK;
+	}
+
+	/*
+	 * The minimum packet size with TCTL.PSP set is 17 so pad the skb
+	 * in order to meet this minimum size requirement.
+	 */
+	if (skb->len < 17) {
+		if (skb_padto(skb, 17))
+			return NETDEV_TX_OK;
+		skb->len = 17;
+	}
+
+	return igb_xmit_frame_ring(skb, igb_tx_queue_mapping(adapter, skb));
+}
+
+/**
+ * igb_tx_timeout - Respond to a Tx Hang
+ * @netdev: network interface device structure
+ **/
+static void igb_tx_timeout(struct net_device *netdev)
+{
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	struct e1000_hw *hw = &adapter->hw;
+
+	/* Do the reset outside of interrupt context */
+	adapter->tx_timeout_count++;
+
+	if (hw->mac.type >= e1000_82580)
+		hw->dev_spec._82575.global_device_reset = true;
+
+	schedule_work(&adapter->reset_task);
+	E1000_WRITE_REG(hw, E1000_EICS,
+			(adapter->eims_enable_mask & ~adapter->eims_other));
+}
+
+static void igb_reset_task(struct work_struct *work)
+{
+	struct igb_adapter *adapter;
+	adapter = container_of(work, struct igb_adapter, reset_task);
+
+	igb_reinit_locked(adapter);
+}
+
+/**
+ * igb_get_stats - Get System Network Statistics
+ * @netdev: network interface device structure
+ *
+ * Returns the address of the device statistics structure.
+ * The statistics are updated here and also from the timer callback.
+ **/
+static struct net_device_stats *igb_get_stats(struct net_device *netdev)
+{
+	struct igb_adapter *adapter = netdev_priv(netdev);
+
+	if (!test_bit(__IGB_RESETTING, &adapter->state))
+		igb_update_stats(adapter);
+
+#ifdef HAVE_NETDEV_STATS_IN_NETDEV
+	/* only return the current stats */
+	return &netdev->stats;
+#else
+	/* only return the current stats */
+	return &adapter->net_stats;
+#endif /* HAVE_NETDEV_STATS_IN_NETDEV */
+}
+
+/**
+ * igb_change_mtu - Change the Maximum Transfer Unit
+ * @netdev: network interface device structure
+ * @new_mtu: new value for maximum frame size
+ *
+ * Returns 0 on success, negative on failure
+ **/
+static int igb_change_mtu(struct net_device *netdev, int new_mtu)
+{
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	struct e1000_hw *hw = &adapter->hw;
+	struct pci_dev *pdev = adapter->pdev;
+	int max_frame = new_mtu + ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN;
+
+	if ((new_mtu < 68) || (max_frame > MAX_JUMBO_FRAME_SIZE)) {
+		dev_err(pci_dev_to_dev(pdev), "Invalid MTU setting\n");
+		return -EINVAL;
+	}
+
+#define MAX_STD_JUMBO_FRAME_SIZE 9238
+	if (max_frame > MAX_STD_JUMBO_FRAME_SIZE) {
+		dev_err(pci_dev_to_dev(pdev), "MTU > 9216 not supported.\n");
+		return -EINVAL;
+	}
+
+	/* adjust max frame to be at least the size of a standard frame */
+	if (max_frame < (ETH_FRAME_LEN + ETH_FCS_LEN))
+		max_frame = ETH_FRAME_LEN + ETH_FCS_LEN;
+
+	while (test_and_set_bit(__IGB_RESETTING, &adapter->state))
+		usleep_range(1000, 2000);
+
+	/* igb_down has a dependency on max_frame_size */
+	adapter->max_frame_size = max_frame;
+
+	if (netif_running(netdev))
+		igb_down(adapter);
+
+	dev_info(pci_dev_to_dev(pdev), "changing MTU from %d to %d\n",
+	        netdev->mtu, new_mtu);
+	netdev->mtu = new_mtu;
+	hw->dev_spec._82575.mtu = new_mtu;
+
+	if (netif_running(netdev))
+		igb_up(adapter);
+	else
+		igb_reset(adapter);
+
+	clear_bit(__IGB_RESETTING, &adapter->state);
+
+	return 0;
+}
+
+/**
+ * igb_update_stats - Update the board statistics counters
+ * @adapter: board private structure
+ **/
+
+void igb_update_stats(struct igb_adapter *adapter)
+{
+#ifdef HAVE_NETDEV_STATS_IN_NETDEV
+	struct net_device_stats *net_stats = &adapter->netdev->stats;
+#else
+	struct net_device_stats *net_stats = &adapter->net_stats;
+#endif /* HAVE_NETDEV_STATS_IN_NETDEV */
+	struct e1000_hw *hw = &adapter->hw;
+#ifdef HAVE_PCI_ERS
+	struct pci_dev *pdev = adapter->pdev;
+#endif
+	u32 reg, mpc;
+	u16 phy_tmp;
+	int i;
+	u64 bytes, packets;
+#ifndef IGB_NO_LRO
+	u32 flushed = 0, coal = 0;
+	struct igb_q_vector *q_vector;
+#endif
+
+#define PHY_IDLE_ERROR_COUNT_MASK 0x00FF
+
+	/*
+	 * Prevent stats update while adapter is being reset, or if the pci
+	 * connection is down.
+	 */
+	if (adapter->link_speed == 0)
+		return;
+#ifdef HAVE_PCI_ERS
+	if (pci_channel_offline(pdev))
+		return;
+
+#endif
+#ifndef IGB_NO_LRO
+	for (i = 0; i < adapter->num_q_vectors; i++) {
+		q_vector = adapter->q_vector[i];
+		if (!q_vector)
+			continue;
+		flushed += q_vector->lrolist.stats.flushed;
+		coal += q_vector->lrolist.stats.coal;
+	}
+	adapter->lro_stats.flushed = flushed;
+	adapter->lro_stats.coal = coal;
+
+#endif
+	bytes = 0;
+	packets = 0;
+	for (i = 0; i < adapter->num_rx_queues; i++) {
+		u32 rqdpc_tmp = E1000_READ_REG(hw, E1000_RQDPC(i)) & 0x0FFF;
+		struct igb_ring *ring = adapter->rx_ring[i];
+		ring->rx_stats.drops += rqdpc_tmp;
+		net_stats->rx_fifo_errors += rqdpc_tmp;
+#ifdef CONFIG_IGB_VMDQ_NETDEV
+		if (!ring->vmdq_netdev) {
+			bytes += ring->rx_stats.bytes;
+			packets += ring->rx_stats.packets;
+		}
+#else
+		bytes += ring->rx_stats.bytes;
+		packets += ring->rx_stats.packets;
+#endif
+	}
+
+	net_stats->rx_bytes = bytes;
+	net_stats->rx_packets = packets;
+
+	bytes = 0;
+	packets = 0;
+	for (i = 0; i < adapter->num_tx_queues; i++) {
+		struct igb_ring *ring = adapter->tx_ring[i];
+#ifdef CONFIG_IGB_VMDQ_NETDEV
+		if (!ring->vmdq_netdev) {
+			bytes += ring->tx_stats.bytes;
+			packets += ring->tx_stats.packets;
+		}
+#else
+		bytes += ring->tx_stats.bytes;
+		packets += ring->tx_stats.packets;
+#endif
+	}
+	net_stats->tx_bytes = bytes;
+	net_stats->tx_packets = packets;
+
+	/* read stats registers */
+	adapter->stats.crcerrs += E1000_READ_REG(hw, E1000_CRCERRS);
+	adapter->stats.gprc += E1000_READ_REG(hw, E1000_GPRC);
+	adapter->stats.gorc += E1000_READ_REG(hw, E1000_GORCL);
+	E1000_READ_REG(hw, E1000_GORCH); /* clear GORCL */
+	adapter->stats.bprc += E1000_READ_REG(hw, E1000_BPRC);
+	adapter->stats.mprc += E1000_READ_REG(hw, E1000_MPRC);
+	adapter->stats.roc += E1000_READ_REG(hw, E1000_ROC);
+
+	adapter->stats.prc64 += E1000_READ_REG(hw, E1000_PRC64);
+	adapter->stats.prc127 += E1000_READ_REG(hw, E1000_PRC127);
+	adapter->stats.prc255 += E1000_READ_REG(hw, E1000_PRC255);
+	adapter->stats.prc511 += E1000_READ_REG(hw, E1000_PRC511);
+	adapter->stats.prc1023 += E1000_READ_REG(hw, E1000_PRC1023);
+	adapter->stats.prc1522 += E1000_READ_REG(hw, E1000_PRC1522);
+	adapter->stats.symerrs += E1000_READ_REG(hw, E1000_SYMERRS);
+	adapter->stats.sec += E1000_READ_REG(hw, E1000_SEC);
+
+	mpc = E1000_READ_REG(hw, E1000_MPC);
+	adapter->stats.mpc += mpc;
+	net_stats->rx_fifo_errors += mpc;
+	adapter->stats.scc += E1000_READ_REG(hw, E1000_SCC);
+	adapter->stats.ecol += E1000_READ_REG(hw, E1000_ECOL);
+	adapter->stats.mcc += E1000_READ_REG(hw, E1000_MCC);
+	adapter->stats.latecol += E1000_READ_REG(hw, E1000_LATECOL);
+	adapter->stats.dc += E1000_READ_REG(hw, E1000_DC);
+	adapter->stats.rlec += E1000_READ_REG(hw, E1000_RLEC);
+	adapter->stats.xonrxc += E1000_READ_REG(hw, E1000_XONRXC);
+	adapter->stats.xontxc += E1000_READ_REG(hw, E1000_XONTXC);
+	adapter->stats.xoffrxc += E1000_READ_REG(hw, E1000_XOFFRXC);
+	adapter->stats.xofftxc += E1000_READ_REG(hw, E1000_XOFFTXC);
+	adapter->stats.fcruc += E1000_READ_REG(hw, E1000_FCRUC);
+	adapter->stats.gptc += E1000_READ_REG(hw, E1000_GPTC);
+	adapter->stats.gotc += E1000_READ_REG(hw, E1000_GOTCL);
+	E1000_READ_REG(hw, E1000_GOTCH); /* clear GOTCL */
+	adapter->stats.rnbc += E1000_READ_REG(hw, E1000_RNBC);
+	adapter->stats.ruc += E1000_READ_REG(hw, E1000_RUC);
+	adapter->stats.rfc += E1000_READ_REG(hw, E1000_RFC);
+	adapter->stats.rjc += E1000_READ_REG(hw, E1000_RJC);
+	adapter->stats.tor += E1000_READ_REG(hw, E1000_TORH);
+	adapter->stats.tot += E1000_READ_REG(hw, E1000_TOTH);
+	adapter->stats.tpr += E1000_READ_REG(hw, E1000_TPR);
+
+	adapter->stats.ptc64 += E1000_READ_REG(hw, E1000_PTC64);
+	adapter->stats.ptc127 += E1000_READ_REG(hw, E1000_PTC127);
+	adapter->stats.ptc255 += E1000_READ_REG(hw, E1000_PTC255);
+	adapter->stats.ptc511 += E1000_READ_REG(hw, E1000_PTC511);
+	adapter->stats.ptc1023 += E1000_READ_REG(hw, E1000_PTC1023);
+	adapter->stats.ptc1522 += E1000_READ_REG(hw, E1000_PTC1522);
+
+	adapter->stats.mptc += E1000_READ_REG(hw, E1000_MPTC);
+	adapter->stats.bptc += E1000_READ_REG(hw, E1000_BPTC);
+
+	adapter->stats.tpt += E1000_READ_REG(hw, E1000_TPT);
+	adapter->stats.colc += E1000_READ_REG(hw, E1000_COLC);
+
+	adapter->stats.algnerrc += E1000_READ_REG(hw, E1000_ALGNERRC);
+	/* read internal phy sepecific stats */
+	reg = E1000_READ_REG(hw, E1000_CTRL_EXT);
+	if (!(reg & E1000_CTRL_EXT_LINK_MODE_MASK)) {
+		adapter->stats.rxerrc += E1000_READ_REG(hw, E1000_RXERRC);
+
+		/* this stat has invalid values on i210/i211 */
+		if ((hw->mac.type != e1000_i210) &&
+		    (hw->mac.type != e1000_i211))
+			adapter->stats.tncrs += E1000_READ_REG(hw, E1000_TNCRS);
+	}
+	adapter->stats.tsctc += E1000_READ_REG(hw, E1000_TSCTC);
+	adapter->stats.tsctfc += E1000_READ_REG(hw, E1000_TSCTFC);
+
+	adapter->stats.iac += E1000_READ_REG(hw, E1000_IAC);
+	adapter->stats.icrxoc += E1000_READ_REG(hw, E1000_ICRXOC);
+	adapter->stats.icrxptc += E1000_READ_REG(hw, E1000_ICRXPTC);
+	adapter->stats.icrxatc += E1000_READ_REG(hw, E1000_ICRXATC);
+	adapter->stats.ictxptc += E1000_READ_REG(hw, E1000_ICTXPTC);
+	adapter->stats.ictxatc += E1000_READ_REG(hw, E1000_ICTXATC);
+	adapter->stats.ictxqec += E1000_READ_REG(hw, E1000_ICTXQEC);
+	adapter->stats.ictxqmtc += E1000_READ_REG(hw, E1000_ICTXQMTC);
+	adapter->stats.icrxdmtc += E1000_READ_REG(hw, E1000_ICRXDMTC);
+
+	/* Fill out the OS statistics structure */
+	net_stats->multicast = adapter->stats.mprc;
+	net_stats->collisions = adapter->stats.colc;
+
+	/* Rx Errors */
+
+	/* RLEC on some newer hardware can be incorrect so build
+	 * our own version based on RUC and ROC */
+	net_stats->rx_errors = adapter->stats.rxerrc +
+		adapter->stats.crcerrs + adapter->stats.algnerrc +
+		adapter->stats.ruc + adapter->stats.roc +
+		adapter->stats.cexterr;
+	net_stats->rx_length_errors = adapter->stats.ruc +
+				      adapter->stats.roc;
+	net_stats->rx_crc_errors = adapter->stats.crcerrs;
+	net_stats->rx_frame_errors = adapter->stats.algnerrc;
+	net_stats->rx_missed_errors = adapter->stats.mpc;
+
+	/* Tx Errors */
+	net_stats->tx_errors = adapter->stats.ecol +
+			       adapter->stats.latecol;
+	net_stats->tx_aborted_errors = adapter->stats.ecol;
+	net_stats->tx_window_errors = adapter->stats.latecol;
+	net_stats->tx_carrier_errors = adapter->stats.tncrs;
+
+	/* Tx Dropped needs to be maintained elsewhere */
+
+	/* Phy Stats */
+	if (hw->phy.media_type == e1000_media_type_copper) {
+		if ((adapter->link_speed == SPEED_1000) &&
+		   (!e1000_read_phy_reg(hw, PHY_1000T_STATUS, &phy_tmp))) {
+			phy_tmp &= PHY_IDLE_ERROR_COUNT_MASK;
+			adapter->phy_stats.idle_errors += phy_tmp;
+		}
+	}
+
+	/* Management Stats */
+	adapter->stats.mgptc += E1000_READ_REG(hw, E1000_MGTPTC);
+	adapter->stats.mgprc += E1000_READ_REG(hw, E1000_MGTPRC);
+	if (hw->mac.type > e1000_82580) {
+		adapter->stats.o2bgptc += E1000_READ_REG(hw, E1000_O2BGPTC);
+		adapter->stats.o2bspc += E1000_READ_REG(hw, E1000_O2BSPC);
+		adapter->stats.b2ospc += E1000_READ_REG(hw, E1000_B2OSPC);
+		adapter->stats.b2ogprc += E1000_READ_REG(hw, E1000_B2OGPRC);
+	}
+}
+
+static irqreturn_t igb_msix_other(int irq, void *data)
+{
+	struct igb_adapter *adapter = data;
+	struct e1000_hw *hw = &adapter->hw;
+	u32 icr = E1000_READ_REG(hw, E1000_ICR);
+	/* reading ICR causes bit 31 of EICR to be cleared */
+
+	if (icr & E1000_ICR_DRSTA)
+		schedule_work(&adapter->reset_task);
+
+	if (icr & E1000_ICR_DOUTSYNC) {
+		/* HW is reporting DMA is out of sync */
+		adapter->stats.doosync++;
+		/* The DMA Out of Sync is also indication of a spoof event
+		 * in IOV mode. Check the Wrong VM Behavior register to
+		 * see if it is really a spoof event. */
+		igb_check_wvbr(adapter);
+	}
+
+	/* Check for a mailbox event */
+	if (icr & E1000_ICR_VMMB)
+		igb_msg_task(adapter);
+
+	if (icr & E1000_ICR_LSC) {
+		hw->mac.get_link_status = 1;
+		/* guard against interrupt when we're going down */
+		if (!test_bit(__IGB_DOWN, &adapter->state))
+			mod_timer(&adapter->watchdog_timer, jiffies + 1);
+	}
+
+#ifdef HAVE_PTP_1588_CLOCK
+	if (icr & E1000_ICR_TS) {
+		u32 tsicr = E1000_READ_REG(hw, E1000_TSICR);
+
+		if (tsicr & E1000_TSICR_TXTS) {
+			/* acknowledge the interrupt */
+			E1000_WRITE_REG(hw, E1000_TSICR, E1000_TSICR_TXTS);
+			/* retrieve hardware timestamp */
+			schedule_work(&adapter->ptp_tx_work);
+		}
+	}
+#endif /* HAVE_PTP_1588_CLOCK */
+
+	/* Check for MDD event */
+	if (icr & E1000_ICR_MDDET)
+		igb_process_mdd_event(adapter);
+
+	E1000_WRITE_REG(hw, E1000_EIMS, adapter->eims_other);
+
+	return IRQ_HANDLED;
+}
+
+static void igb_write_itr(struct igb_q_vector *q_vector)
+{
+	struct igb_adapter *adapter = q_vector->adapter;
+	u32 itr_val = q_vector->itr_val & 0x7FFC;
+
+	if (!q_vector->set_itr)
+		return;
+
+	if (!itr_val)
+		itr_val = 0x4;
+
+	if (adapter->hw.mac.type == e1000_82575)
+		itr_val |= itr_val << 16;
+	else
+		itr_val |= E1000_EITR_CNT_IGNR;
+
+	writel(itr_val, q_vector->itr_register);
+	q_vector->set_itr = 0;
+}
+
+static irqreturn_t igb_msix_ring(int irq, void *data)
+{
+	struct igb_q_vector *q_vector = data;
+
+	/* Write the ITR value calculated from the previous interrupt. */
+	igb_write_itr(q_vector);
+
+	napi_schedule(&q_vector->napi);
+
+	return IRQ_HANDLED;
+}
+
+#ifdef IGB_DCA
+static void igb_update_tx_dca(struct igb_adapter *adapter,
+			      struct igb_ring *tx_ring,
+			      int cpu)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 txctrl = dca3_get_tag(tx_ring->dev, cpu);
+
+	if (hw->mac.type != e1000_82575)
+		txctrl <<= E1000_DCA_TXCTRL_CPUID_SHIFT_82576;
+
+	/*
+	 * We can enable relaxed ordering for reads, but not writes when
+	 * DCA is enabled.  This is due to a known issue in some chipsets
+	 * which will cause the DCA tag to be cleared.
+	 */
+	txctrl |= E1000_DCA_TXCTRL_DESC_RRO_EN |
+		  E1000_DCA_TXCTRL_DATA_RRO_EN |
+		  E1000_DCA_TXCTRL_DESC_DCA_EN;
+
+	E1000_WRITE_REG(hw, E1000_DCA_TXCTRL(tx_ring->reg_idx), txctrl);
+}
+
+static void igb_update_rx_dca(struct igb_adapter *adapter,
+			      struct igb_ring *rx_ring,
+			      int cpu)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 rxctrl = dca3_get_tag(&adapter->pdev->dev, cpu);
+
+	if (hw->mac.type != e1000_82575)
+		rxctrl <<= E1000_DCA_RXCTRL_CPUID_SHIFT_82576;
+
+	/*
+	 * We can enable relaxed ordering for reads, but not writes when
+	 * DCA is enabled.  This is due to a known issue in some chipsets
+	 * which will cause the DCA tag to be cleared.
+	 */
+	rxctrl |= E1000_DCA_RXCTRL_DESC_RRO_EN |
+		  E1000_DCA_RXCTRL_DESC_DCA_EN;
+
+	E1000_WRITE_REG(hw, E1000_DCA_RXCTRL(rx_ring->reg_idx), rxctrl);
+}
+
+static void igb_update_dca(struct igb_q_vector *q_vector)
+{
+	struct igb_adapter *adapter = q_vector->adapter;
+	int cpu = get_cpu();
+
+	if (q_vector->cpu == cpu)
+		goto out_no_update;
+
+	if (q_vector->tx.ring)
+		igb_update_tx_dca(adapter, q_vector->tx.ring, cpu);
+
+	if (q_vector->rx.ring)
+		igb_update_rx_dca(adapter, q_vector->rx.ring, cpu);
+
+	q_vector->cpu = cpu;
+out_no_update:
+	put_cpu();
+}
+
+static void igb_setup_dca(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	int i;
+
+	if (!(adapter->flags & IGB_FLAG_DCA_ENABLED))
+		return;
+
+	/* Always use CB2 mode, difference is masked in the CB driver. */
+	E1000_WRITE_REG(hw, E1000_DCA_CTRL, E1000_DCA_CTRL_DCA_MODE_CB2);
+
+	for (i = 0; i < adapter->num_q_vectors; i++) {
+		adapter->q_vector[i]->cpu = -1;
+		igb_update_dca(adapter->q_vector[i]);
+	}
+}
+
+static int __igb_notify_dca(struct device *dev, void *data)
+{
+	struct net_device *netdev = dev_get_drvdata(dev);
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	struct pci_dev *pdev = adapter->pdev;
+	struct e1000_hw *hw = &adapter->hw;
+	unsigned long event = *(unsigned long *)data;
+
+	switch (event) {
+	case DCA_PROVIDER_ADD:
+		/* if already enabled, don't do it again */
+		if (adapter->flags & IGB_FLAG_DCA_ENABLED)
+			break;
+		if (dca_add_requester(dev) == E1000_SUCCESS) {
+			adapter->flags |= IGB_FLAG_DCA_ENABLED;
+			dev_info(pci_dev_to_dev(pdev), "DCA enabled\n");
+			igb_setup_dca(adapter);
+			break;
+		}
+		/* Fall Through since DCA is disabled. */
+	case DCA_PROVIDER_REMOVE:
+		if (adapter->flags & IGB_FLAG_DCA_ENABLED) {
+			/* without this a class_device is left
+			 * hanging around in the sysfs model */
+			dca_remove_requester(dev);
+			dev_info(pci_dev_to_dev(pdev), "DCA disabled\n");
+			adapter->flags &= ~IGB_FLAG_DCA_ENABLED;
+			E1000_WRITE_REG(hw, E1000_DCA_CTRL, E1000_DCA_CTRL_DCA_DISABLE);
+		}
+		break;
+	}
+
+	return E1000_SUCCESS;
+}
+
+static int igb_notify_dca(struct notifier_block *nb, unsigned long event,
+                          void *p)
+{
+	int ret_val;
+
+	ret_val = driver_for_each_device(&igb_driver.driver, NULL, &event,
+	                                 __igb_notify_dca);
+
+	return ret_val ? NOTIFY_BAD : NOTIFY_DONE;
+}
+#endif /* IGB_DCA */
+
+static int igb_vf_configure(struct igb_adapter *adapter, int vf)
+{
+	unsigned char mac_addr[ETH_ALEN];
+
+	random_ether_addr(mac_addr);
+	igb_set_vf_mac(adapter, vf, mac_addr);
+
+#ifdef IFLA_VF_MAX
+#ifdef HAVE_VF_SPOOFCHK_CONFIGURE
+	/* By default spoof check is enabled for all VFs */
+	adapter->vf_data[vf].spoofchk_enabled = true;
+#endif
+#endif
+
+	return true;
+}
+
+static void igb_ping_all_vfs(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 ping;
+	int i;
+
+	for (i = 0 ; i < adapter->vfs_allocated_count; i++) {
+		ping = E1000_PF_CONTROL_MSG;
+		if (adapter->vf_data[i].flags & IGB_VF_FLAG_CTS)
+			ping |= E1000_VT_MSGTYPE_CTS;
+		e1000_write_mbx(hw, &ping, 1, i);
+	}
+}
+
+/**
+ *  igb_mta_set_ - Set multicast filter table address
+ *  @adapter: pointer to the adapter structure
+ *  @hash_value: determines the MTA register and bit to set
+ *
+ *  The multicast table address is a register array of 32-bit registers.
+ *  The hash_value is used to determine what register the bit is in, the
+ *  current value is read, the new bit is OR'd in and the new value is
+ *  written back into the register.
+ **/
+void igb_mta_set(struct igb_adapter *adapter, u32 hash_value)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 hash_bit, hash_reg, mta;
+
+	/*
+	 * The MTA is a register array of 32-bit registers. It is
+	 * treated like an array of (32*mta_reg_count) bits.  We want to
+	 * set bit BitArray[hash_value]. So we figure out what register
+	 * the bit is in, read it, OR in the new bit, then write
+	 * back the new value.  The (hw->mac.mta_reg_count - 1) serves as a
+	 * mask to bits 31:5 of the hash value which gives us the
+	 * register we're modifying.  The hash bit within that register
+	 * is determined by the lower 5 bits of the hash value.
+	 */
+	hash_reg = (hash_value >> 5) & (hw->mac.mta_reg_count - 1);
+	hash_bit = hash_value & 0x1F;
+
+	mta = E1000_READ_REG_ARRAY(hw, E1000_MTA, hash_reg);
+
+	mta |= (1 << hash_bit);
+
+	E1000_WRITE_REG_ARRAY(hw, E1000_MTA, hash_reg, mta);
+	E1000_WRITE_FLUSH(hw);
+}
+
+static int igb_set_vf_promisc(struct igb_adapter *adapter, u32 *msgbuf, u32 vf)
+{
+
+	struct e1000_hw *hw = &adapter->hw;
+	u32 vmolr = E1000_READ_REG(hw, E1000_VMOLR(vf));
+	struct vf_data_storage *vf_data = &adapter->vf_data[vf];
+
+	vf_data->flags &= ~(IGB_VF_FLAG_UNI_PROMISC |
+	                    IGB_VF_FLAG_MULTI_PROMISC);
+	vmolr &= ~(E1000_VMOLR_ROPE | E1000_VMOLR_ROMPE | E1000_VMOLR_MPME);
+
+#ifdef IGB_ENABLE_VF_PROMISC
+	if (*msgbuf & E1000_VF_SET_PROMISC_UNICAST) {
+		vmolr |= E1000_VMOLR_ROPE;
+		vf_data->flags |= IGB_VF_FLAG_UNI_PROMISC;
+		*msgbuf &= ~E1000_VF_SET_PROMISC_UNICAST;
+	}
+#endif
+	if (*msgbuf & E1000_VF_SET_PROMISC_MULTICAST) {
+		vmolr |= E1000_VMOLR_MPME;
+		vf_data->flags |= IGB_VF_FLAG_MULTI_PROMISC;
+		*msgbuf &= ~E1000_VF_SET_PROMISC_MULTICAST;
+	} else {
+		/*
+		 * if we have hashes and we are clearing a multicast promisc
+		 * flag we need to write the hashes to the MTA as this step
+		 * was previously skipped
+		 */
+		if (vf_data->num_vf_mc_hashes > 30) {
+			vmolr |= E1000_VMOLR_MPME;
+		} else if (vf_data->num_vf_mc_hashes) {
+			int j;
+			vmolr |= E1000_VMOLR_ROMPE;
+			for (j = 0; j < vf_data->num_vf_mc_hashes; j++)
+				igb_mta_set(adapter, vf_data->vf_mc_hashes[j]);
+		}
+	}
+
+	E1000_WRITE_REG(hw, E1000_VMOLR(vf), vmolr);
+
+	/* there are flags left unprocessed, likely not supported */
+	if (*msgbuf & E1000_VT_MSGINFO_MASK)
+		return -EINVAL;
+
+	return 0;
+
+}
+
+static int igb_set_vf_multicasts(struct igb_adapter *adapter,
+				  u32 *msgbuf, u32 vf)
+{
+	int n = (msgbuf[0] & E1000_VT_MSGINFO_MASK) >> E1000_VT_MSGINFO_SHIFT;
+	u16 *hash_list = (u16 *)&msgbuf[1];
+	struct vf_data_storage *vf_data = &adapter->vf_data[vf];
+	int i;
+
+	/* salt away the number of multicast addresses assigned
+	 * to this VF for later use to restore when the PF multi cast
+	 * list changes
+	 */
+	vf_data->num_vf_mc_hashes = n;
+
+	/* only up to 30 hash values supported */
+	if (n > 30)
+		n = 30;
+
+	/* store the hashes for later use */
+	for (i = 0; i < n; i++)
+		vf_data->vf_mc_hashes[i] = hash_list[i];
+
+	/* Flush and reset the mta with the new values */
+	igb_set_rx_mode(adapter->netdev);
+
+	return 0;
+}
+
+static void igb_restore_vf_multicasts(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	struct vf_data_storage *vf_data;
+	int i, j;
+
+	for (i = 0; i < adapter->vfs_allocated_count; i++) {
+		u32 vmolr = E1000_READ_REG(hw, E1000_VMOLR(i));
+		vmolr &= ~(E1000_VMOLR_ROMPE | E1000_VMOLR_MPME);
+
+		vf_data = &adapter->vf_data[i];
+
+		if ((vf_data->num_vf_mc_hashes > 30) ||
+		    (vf_data->flags & IGB_VF_FLAG_MULTI_PROMISC)) {
+			vmolr |= E1000_VMOLR_MPME;
+		} else if (vf_data->num_vf_mc_hashes) {
+			vmolr |= E1000_VMOLR_ROMPE;
+			for (j = 0; j < vf_data->num_vf_mc_hashes; j++)
+				igb_mta_set(adapter, vf_data->vf_mc_hashes[j]);
+		}
+		E1000_WRITE_REG(hw, E1000_VMOLR(i), vmolr);
+	}
+}
+
+static void igb_clear_vf_vfta(struct igb_adapter *adapter, u32 vf)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 pool_mask, reg, vid;
+	u16 vlan_default;
+	int i;
+
+	pool_mask = 1 << (E1000_VLVF_POOLSEL_SHIFT + vf);
+
+	/* Find the vlan filter for this id */
+	for (i = 0; i < E1000_VLVF_ARRAY_SIZE; i++) {
+		reg = E1000_READ_REG(hw, E1000_VLVF(i));
+
+		/* remove the vf from the pool */
+		reg &= ~pool_mask;
+
+		/* if pool is empty then remove entry from vfta */
+		if (!(reg & E1000_VLVF_POOLSEL_MASK) &&
+		    (reg & E1000_VLVF_VLANID_ENABLE)) {
+			reg = 0;
+			vid = reg & E1000_VLVF_VLANID_MASK;
+			igb_vfta_set(adapter, vid, FALSE);
+		}
+
+		E1000_WRITE_REG(hw, E1000_VLVF(i), reg);
+	}
+
+	adapter->vf_data[vf].vlans_enabled = 0;
+
+	vlan_default = adapter->vf_data[vf].default_vf_vlan_id;
+	if (vlan_default)
+		igb_vlvf_set(adapter, vlan_default, true, vf);
+}
+
+s32 igb_vlvf_set(struct igb_adapter *adapter, u32 vid, bool add, u32 vf)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 reg, i;
+
+	/* The vlvf table only exists on 82576 hardware and newer */
+	if (hw->mac.type < e1000_82576)
+		return -1;
+
+	/* we only need to do this if VMDq is enabled */
+	if (!adapter->vmdq_pools)
+		return -1;
+
+	/* Find the vlan filter for this id */
+	for (i = 0; i < E1000_VLVF_ARRAY_SIZE; i++) {
+		reg = E1000_READ_REG(hw, E1000_VLVF(i));
+		if ((reg & E1000_VLVF_VLANID_ENABLE) &&
+		    vid == (reg & E1000_VLVF_VLANID_MASK))
+			break;
+	}
+
+	if (add) {
+		if (i == E1000_VLVF_ARRAY_SIZE) {
+			/* Did not find a matching VLAN ID entry that was
+			 * enabled.  Search for a free filter entry, i.e.
+			 * one without the enable bit set
+			 */
+			for (i = 0; i < E1000_VLVF_ARRAY_SIZE; i++) {
+				reg = E1000_READ_REG(hw, E1000_VLVF(i));
+				if (!(reg & E1000_VLVF_VLANID_ENABLE))
+					break;
+			}
+		}
+		if (i < E1000_VLVF_ARRAY_SIZE) {
+			/* Found an enabled/available entry */
+			reg |= 1 << (E1000_VLVF_POOLSEL_SHIFT + vf);
+
+			/* if !enabled we need to set this up in vfta */
+			if (!(reg & E1000_VLVF_VLANID_ENABLE)) {
+				/* add VID to filter table */
+				igb_vfta_set(adapter, vid, TRUE);
+				reg |= E1000_VLVF_VLANID_ENABLE;
+			}
+			reg &= ~E1000_VLVF_VLANID_MASK;
+			reg |= vid;
+			E1000_WRITE_REG(hw, E1000_VLVF(i), reg);
+
+			/* do not modify RLPML for PF devices */
+			if (vf >= adapter->vfs_allocated_count)
+				return E1000_SUCCESS;
+
+			if (!adapter->vf_data[vf].vlans_enabled) {
+				u32 size;
+				reg = E1000_READ_REG(hw, E1000_VMOLR(vf));
+				size = reg & E1000_VMOLR_RLPML_MASK;
+				size += 4;
+				reg &= ~E1000_VMOLR_RLPML_MASK;
+				reg |= size;
+				E1000_WRITE_REG(hw, E1000_VMOLR(vf), reg);
+			}
+
+			adapter->vf_data[vf].vlans_enabled++;
+		}
+	} else {
+		if (i < E1000_VLVF_ARRAY_SIZE) {
+			/* remove vf from the pool */
+			reg &= ~(1 << (E1000_VLVF_POOLSEL_SHIFT + vf));
+			/* if pool is empty then remove entry from vfta */
+			if (!(reg & E1000_VLVF_POOLSEL_MASK)) {
+				reg = 0;
+				igb_vfta_set(adapter, vid, FALSE);
+			}
+			E1000_WRITE_REG(hw, E1000_VLVF(i), reg);
+
+			/* do not modify RLPML for PF devices */
+			if (vf >= adapter->vfs_allocated_count)
+				return E1000_SUCCESS;
+
+			adapter->vf_data[vf].vlans_enabled--;
+			if (!adapter->vf_data[vf].vlans_enabled) {
+				u32 size;
+				reg = E1000_READ_REG(hw, E1000_VMOLR(vf));
+				size = reg & E1000_VMOLR_RLPML_MASK;
+				size -= 4;
+				reg &= ~E1000_VMOLR_RLPML_MASK;
+				reg |= size;
+				E1000_WRITE_REG(hw, E1000_VMOLR(vf), reg);
+			}
+		}
+	}
+	return E1000_SUCCESS;
+}
+
+#ifdef IFLA_VF_MAX
+static void igb_set_vmvir(struct igb_adapter *adapter, u32 vid, u32 vf)
+{
+	struct e1000_hw *hw = &adapter->hw;
+
+	if (vid)
+		E1000_WRITE_REG(hw, E1000_VMVIR(vf), (vid | E1000_VMVIR_VLANA_DEFAULT));
+	else
+		E1000_WRITE_REG(hw, E1000_VMVIR(vf), 0);
+}
+
+static int igb_ndo_set_vf_vlan(struct net_device *netdev,
+#ifdef HAVE_VF_VLAN_PROTO
+			       int vf, u16 vlan, u8 qos, __be16 vlan_proto)
+#else
+			       int vf, u16 vlan, u8 qos)
+#endif
+{
+	int err = 0;
+	struct igb_adapter *adapter = netdev_priv(netdev);
+
+	/* VLAN IDs accepted range 0-4094 */
+	if ((vf >= adapter->vfs_allocated_count) || (vlan > VLAN_VID_MASK-1) || (qos > 7))
+		return -EINVAL;
+
+#ifdef HAVE_VF_VLAN_PROTO
+	if (vlan_proto != htons(ETH_P_8021Q))
+		return -EPROTONOSUPPORT;
+#endif
+
+	if (vlan || qos) {
+		err = igb_vlvf_set(adapter, vlan, !!vlan, vf);
+		if (err)
+			goto out;
+		igb_set_vmvir(adapter, vlan | (qos << VLAN_PRIO_SHIFT), vf);
+		igb_set_vmolr(adapter, vf, !vlan);
+		adapter->vf_data[vf].pf_vlan = vlan;
+		adapter->vf_data[vf].pf_qos = qos;
+		igb_set_vf_vlan_strip(adapter, vf, true);
+		dev_info(&adapter->pdev->dev,
+			 "Setting VLAN %d, QOS 0x%x on VF %d\n", vlan, qos, vf);
+		if (test_bit(__IGB_DOWN, &adapter->state)) {
+			dev_warn(&adapter->pdev->dev,
+				 "The VF VLAN has been set,"
+				 " but the PF device is not up.\n");
+			dev_warn(&adapter->pdev->dev,
+				 "Bring the PF device up before"
+				 " attempting to use the VF device.\n");
+		}
+	} else {
+		if (adapter->vf_data[vf].pf_vlan)
+			dev_info(&adapter->pdev->dev,
+				 "Clearing VLAN on VF %d\n", vf);
+		igb_vlvf_set(adapter, adapter->vf_data[vf].pf_vlan,
+				   false, vf);
+		igb_set_vmvir(adapter, vlan, vf);
+		igb_set_vmolr(adapter, vf, true);
+		igb_set_vf_vlan_strip(adapter, vf, false);
+		adapter->vf_data[vf].pf_vlan = 0;
+		adapter->vf_data[vf].pf_qos = 0;
+       }
+out:
+       return err;
+}
+
+#ifdef HAVE_VF_SPOOFCHK_CONFIGURE
+static int igb_ndo_set_vf_spoofchk(struct net_device *netdev, int vf,
+				bool setting)
+{
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	struct e1000_hw *hw = &adapter->hw;
+	u32 dtxswc, reg_offset;
+
+	if (!adapter->vfs_allocated_count)
+		return -EOPNOTSUPP;
+
+	if (vf >= adapter->vfs_allocated_count)
+		return -EINVAL;
+
+	reg_offset = (hw->mac.type == e1000_82576) ? E1000_DTXSWC : E1000_TXSWC;
+	dtxswc = E1000_READ_REG(hw, reg_offset);
+	if (setting)
+		dtxswc |= ((1 << vf) |
+			   (1 << (vf + E1000_DTXSWC_VLAN_SPOOF_SHIFT)));
+	else
+		dtxswc &= ~((1 << vf) |
+			    (1 << (vf + E1000_DTXSWC_VLAN_SPOOF_SHIFT)));
+	E1000_WRITE_REG(hw, reg_offset, dtxswc);
+
+	adapter->vf_data[vf].spoofchk_enabled = setting;
+	return E1000_SUCCESS;
+}
+#endif /* HAVE_VF_SPOOFCHK_CONFIGURE */
+#endif /* IFLA_VF_MAX */
+
+static int igb_find_vlvf_entry(struct igb_adapter *adapter, int vid)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	int i;
+	u32 reg;
+
+	/* Find the vlan filter for this id */
+	for (i = 0; i < E1000_VLVF_ARRAY_SIZE; i++) {
+		reg = E1000_READ_REG(hw, E1000_VLVF(i));
+		if ((reg & E1000_VLVF_VLANID_ENABLE) &&
+		    vid == (reg & E1000_VLVF_VLANID_MASK))
+			break;
+	}
+
+	if (i >= E1000_VLVF_ARRAY_SIZE)
+		i = -1;
+
+	return i;
+}
+
+static int igb_set_vf_vlan(struct igb_adapter *adapter, u32 *msgbuf, u32 vf)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	int add = (msgbuf[0] & E1000_VT_MSGINFO_MASK) >> E1000_VT_MSGINFO_SHIFT;
+	int vid = (msgbuf[1] & E1000_VLVF_VLANID_MASK);
+	int err = 0;
+
+	if (vid)
+		igb_set_vf_vlan_strip(adapter, vf, true);
+	else
+		igb_set_vf_vlan_strip(adapter, vf, false);
+
+	/* If in promiscuous mode we need to make sure the PF also has
+	 * the VLAN filter set.
+	 */
+	if (add && (adapter->netdev->flags & IFF_PROMISC))
+		err = igb_vlvf_set(adapter, vid, add,
+				   adapter->vfs_allocated_count);
+	if (err)
+		goto out;
+
+	err = igb_vlvf_set(adapter, vid, add, vf);
+
+	if (err)
+		goto out;
+
+	/* Go through all the checks to see if the VLAN filter should
+	 * be wiped completely.
+	 */
+	if (!add && (adapter->netdev->flags & IFF_PROMISC)) {
+		u32 vlvf, bits;
+
+		int regndx = igb_find_vlvf_entry(adapter, vid);
+		if (regndx < 0)
+			goto out;
+		/* See if any other pools are set for this VLAN filter
+		 * entry other than the PF.
+		 */
+		vlvf = bits = E1000_READ_REG(hw, E1000_VLVF(regndx));
+		bits &= 1 << (E1000_VLVF_POOLSEL_SHIFT +
+			      adapter->vfs_allocated_count);
+		/* If the filter was removed then ensure PF pool bit
+		 * is cleared if the PF only added itself to the pool
+		 * because the PF is in promiscuous mode.
+		 */
+		if ((vlvf & VLAN_VID_MASK) == vid &&
+#ifndef HAVE_VLAN_RX_REGISTER
+		    !test_bit(vid, adapter->active_vlans) &&
+#endif
+		    !bits)
+			igb_vlvf_set(adapter, vid, add,
+				     adapter->vfs_allocated_count);
+	}
+
+out:
+	return err;
+}
+
+static inline void igb_vf_reset(struct igb_adapter *adapter, u32 vf)
+{
+	struct e1000_hw *hw = &adapter->hw;
+
+	/* clear flags except flag that the PF has set the MAC */
+	adapter->vf_data[vf].flags &= IGB_VF_FLAG_PF_SET_MAC;
+	adapter->vf_data[vf].last_nack = jiffies;
+
+	/* reset offloads to defaults */
+	igb_set_vmolr(adapter, vf, true);
+
+	/* reset vlans for device */
+	igb_clear_vf_vfta(adapter, vf);
+#ifdef IFLA_VF_MAX
+	if (adapter->vf_data[vf].pf_vlan)
+		igb_ndo_set_vf_vlan(adapter->netdev, vf,
+				    adapter->vf_data[vf].pf_vlan,
+#ifdef HAVE_VF_VLAN_PROTO
+				    adapter->vf_data[vf].pf_qos,
+				    htons(ETH_P_8021Q));
+#else
+				    adapter->vf_data[vf].pf_qos);
+#endif
+	else
+		igb_clear_vf_vfta(adapter, vf);
+#endif
+
+	/* reset multicast table array for vf */
+	adapter->vf_data[vf].num_vf_mc_hashes = 0;
+
+	/* Flush and reset the mta with the new values */
+	igb_set_rx_mode(adapter->netdev);
+
+	/*
+	 * Reset the VFs TDWBAL and TDWBAH registers which are not
+	 * cleared by a VFLR
+	 */
+	E1000_WRITE_REG(hw, E1000_TDWBAH(vf), 0);
+	E1000_WRITE_REG(hw, E1000_TDWBAL(vf), 0);
+	if (hw->mac.type == e1000_82576) {
+		E1000_WRITE_REG(hw, E1000_TDWBAH(IGB_MAX_VF_FUNCTIONS + vf), 0);
+		E1000_WRITE_REG(hw, E1000_TDWBAL(IGB_MAX_VF_FUNCTIONS + vf), 0);
+	}
+}
+
+static void igb_vf_reset_event(struct igb_adapter *adapter, u32 vf)
+{
+	unsigned char *vf_mac = adapter->vf_data[vf].vf_mac_addresses;
+
+	/* generate a new mac address as we were hotplug removed/added */
+	if (!(adapter->vf_data[vf].flags & IGB_VF_FLAG_PF_SET_MAC))
+		random_ether_addr(vf_mac);
+
+	/* process remaining reset events */
+	igb_vf_reset(adapter, vf);
+}
+
+static void igb_vf_reset_msg(struct igb_adapter *adapter, u32 vf)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	unsigned char *vf_mac = adapter->vf_data[vf].vf_mac_addresses;
+	u32 reg, msgbuf[3];
+	u8 *addr = (u8 *)(&msgbuf[1]);
+
+	/* process all the same items cleared in a function level reset */
+	igb_vf_reset(adapter, vf);
+
+	/* set vf mac address */
+	igb_del_mac_filter(adapter, vf_mac, vf);
+	igb_add_mac_filter(adapter, vf_mac, vf);
+
+	/* enable transmit and receive for vf */
+	reg = E1000_READ_REG(hw, E1000_VFTE);
+	E1000_WRITE_REG(hw, E1000_VFTE, reg | (1 << vf));
+	reg = E1000_READ_REG(hw, E1000_VFRE);
+	E1000_WRITE_REG(hw, E1000_VFRE, reg | (1 << vf));
+
+	adapter->vf_data[vf].flags |= IGB_VF_FLAG_CTS;
+
+	/* reply to reset with ack and vf mac address */
+	msgbuf[0] = E1000_VF_RESET | E1000_VT_MSGTYPE_ACK;
+	memcpy(addr, vf_mac, 6);
+	e1000_write_mbx(hw, msgbuf, 3, vf);
+}
+
+static int igb_set_vf_mac_addr(struct igb_adapter *adapter, u32 *msg, int vf)
+{
+	/*
+	 * The VF MAC Address is stored in a packed array of bytes
+	 * starting at the second 32 bit word of the msg array
+	 */
+	unsigned char *addr = (unsigned char *)&msg[1];
+	int err = -1;
+
+	if (is_valid_ether_addr(addr))
+		err = igb_set_vf_mac(adapter, vf, addr);
+
+	return err;
+}
+
+static void igb_rcv_ack_from_vf(struct igb_adapter *adapter, u32 vf)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	struct vf_data_storage *vf_data = &adapter->vf_data[vf];
+	u32 msg = E1000_VT_MSGTYPE_NACK;
+
+	/* if device isn't clear to send it shouldn't be reading either */
+	if (!(vf_data->flags & IGB_VF_FLAG_CTS) &&
+	    time_after(jiffies, vf_data->last_nack + (2 * HZ))) {
+		e1000_write_mbx(hw, &msg, 1, vf);
+		vf_data->last_nack = jiffies;
+	}
+}
+
+static void igb_rcv_msg_from_vf(struct igb_adapter *adapter, u32 vf)
+{
+	struct pci_dev *pdev = adapter->pdev;
+	u32 msgbuf[E1000_VFMAILBOX_SIZE];
+	struct e1000_hw *hw = &adapter->hw;
+	struct vf_data_storage *vf_data = &adapter->vf_data[vf];
+	s32 retval;
+
+	retval = e1000_read_mbx(hw, msgbuf, E1000_VFMAILBOX_SIZE, vf);
+
+	if (retval) {
+		dev_err(pci_dev_to_dev(pdev), "Error receiving message from VF\n");
+		return;
+	}
+
+	/* this is a message we already processed, do nothing */
+	if (msgbuf[0] & (E1000_VT_MSGTYPE_ACK | E1000_VT_MSGTYPE_NACK))
+		return;
+
+	/*
+	 * until the vf completes a reset it should not be
+	 * allowed to start any configuration.
+	 */
+
+	if (msgbuf[0] == E1000_VF_RESET) {
+		igb_vf_reset_msg(adapter, vf);
+		return;
+	}
+
+	if (!(vf_data->flags & IGB_VF_FLAG_CTS)) {
+		msgbuf[0] = E1000_VT_MSGTYPE_NACK;
+		if (time_after(jiffies, vf_data->last_nack + (2 * HZ))) {
+			e1000_write_mbx(hw, msgbuf, 1, vf);
+			vf_data->last_nack = jiffies;
+		}
+		return;
+	}
+
+	switch ((msgbuf[0] & 0xFFFF)) {
+	case E1000_VF_SET_MAC_ADDR:
+		retval = -EINVAL;
+#ifndef IGB_DISABLE_VF_MAC_SET
+		if (!(vf_data->flags & IGB_VF_FLAG_PF_SET_MAC))
+			retval = igb_set_vf_mac_addr(adapter, msgbuf, vf);
+		else
+			DPRINTK(DRV, INFO,
+				"VF %d attempted to override administratively "
+				"set MAC address\nReload the VF driver to "
+				"resume operations\n", vf);
+#endif
+		break;
+	case E1000_VF_SET_PROMISC:
+		retval = igb_set_vf_promisc(adapter, msgbuf, vf);
+		break;
+	case E1000_VF_SET_MULTICAST:
+		retval = igb_set_vf_multicasts(adapter, msgbuf, vf);
+		break;
+	case E1000_VF_SET_LPE:
+		retval = igb_set_vf_rlpml(adapter, msgbuf[1], vf);
+		break;
+	case E1000_VF_SET_VLAN:
+		retval = -1;
+#ifdef IFLA_VF_MAX
+		if (vf_data->pf_vlan)
+			DPRINTK(DRV, INFO,
+				"VF %d attempted to override administratively "
+				"set VLAN tag\nReload the VF driver to "
+				"resume operations\n", vf);
+		else
+#endif
+			retval = igb_set_vf_vlan(adapter, msgbuf, vf);
+		break;
+	default:
+		dev_err(pci_dev_to_dev(pdev), "Unhandled Msg %08x\n", msgbuf[0]);
+		retval = -E1000_ERR_MBX;
+		break;
+	}
+
+	/* notify the VF of the results of what it sent us */
+	if (retval)
+		msgbuf[0] |= E1000_VT_MSGTYPE_NACK;
+	else
+		msgbuf[0] |= E1000_VT_MSGTYPE_ACK;
+
+	msgbuf[0] |= E1000_VT_MSGTYPE_CTS;
+
+	e1000_write_mbx(hw, msgbuf, 1, vf);
+}
+
+static void igb_msg_task(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 vf;
+
+	for (vf = 0; vf < adapter->vfs_allocated_count; vf++) {
+		/* process any reset requests */
+		if (!e1000_check_for_rst(hw, vf))
+			igb_vf_reset_event(adapter, vf);
+
+		/* process any messages pending */
+		if (!e1000_check_for_msg(hw, vf))
+			igb_rcv_msg_from_vf(adapter, vf);
+
+		/* process any acks */
+		if (!e1000_check_for_ack(hw, vf))
+			igb_rcv_ack_from_vf(adapter, vf);
+	}
+}
+
+/**
+ *  igb_set_uta - Set unicast filter table address
+ *  @adapter: board private structure
+ *
+ *  The unicast table address is a register array of 32-bit registers.
+ *  The table is meant to be used in a way similar to how the MTA is used
+ *  however due to certain limitations in the hardware it is necessary to
+ *  set all the hash bits to 1 and use the VMOLR ROPE bit as a promiscuous
+ *  enable bit to allow vlan tag stripping when promiscuous mode is enabled
+ **/
+static void igb_set_uta(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	int i;
+
+	/* The UTA table only exists on 82576 hardware and newer */
+	if (hw->mac.type < e1000_82576)
+		return;
+
+	/* we only need to do this if VMDq is enabled */
+	if (!adapter->vmdq_pools)
+		return;
+
+	for (i = 0; i < hw->mac.uta_reg_count; i++)
+		E1000_WRITE_REG_ARRAY(hw, E1000_UTA, i, ~0);
+}
+
+/**
+ * igb_intr_msi - Interrupt Handler
+ * @irq: interrupt number
+ * @data: pointer to a network interface device structure
+ **/
+static irqreturn_t igb_intr_msi(int irq, void *data)
+{
+	struct igb_adapter *adapter = data;
+	struct igb_q_vector *q_vector = adapter->q_vector[0];
+	struct e1000_hw *hw = &adapter->hw;
+	/* read ICR disables interrupts using IAM */
+	u32 icr = E1000_READ_REG(hw, E1000_ICR);
+
+	igb_write_itr(q_vector);
+
+	if (icr & E1000_ICR_DRSTA)
+		schedule_work(&adapter->reset_task);
+
+	if (icr & E1000_ICR_DOUTSYNC) {
+		/* HW is reporting DMA is out of sync */
+		adapter->stats.doosync++;
+	}
+
+	if (icr & (E1000_ICR_RXSEQ | E1000_ICR_LSC)) {
+		hw->mac.get_link_status = 1;
+		if (!test_bit(__IGB_DOWN, &adapter->state))
+			mod_timer(&adapter->watchdog_timer, jiffies + 1);
+	}
+
+#ifdef HAVE_PTP_1588_CLOCK
+	if (icr & E1000_ICR_TS) {
+		u32 tsicr = E1000_READ_REG(hw, E1000_TSICR);
+
+		if (tsicr & E1000_TSICR_TXTS) {
+			/* acknowledge the interrupt */
+			E1000_WRITE_REG(hw, E1000_TSICR, E1000_TSICR_TXTS);
+			/* retrieve hardware timestamp */
+			schedule_work(&adapter->ptp_tx_work);
+		}
+	}
+#endif /* HAVE_PTP_1588_CLOCK */
+
+	napi_schedule(&q_vector->napi);
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * igb_intr - Legacy Interrupt Handler
+ * @irq: interrupt number
+ * @data: pointer to a network interface device structure
+ **/
+static irqreturn_t igb_intr(int irq, void *data)
+{
+	struct igb_adapter *adapter = data;
+	struct igb_q_vector *q_vector = adapter->q_vector[0];
+	struct e1000_hw *hw = &adapter->hw;
+	/* Interrupt Auto-Mask...upon reading ICR, interrupts are masked.  No
+	 * need for the IMC write */
+	u32 icr = E1000_READ_REG(hw, E1000_ICR);
+
+	/* IMS will not auto-mask if INT_ASSERTED is not set, and if it is
+	 * not set, then the adapter didn't send an interrupt */
+	if (!(icr & E1000_ICR_INT_ASSERTED))
+		return IRQ_NONE;
+
+	igb_write_itr(q_vector);
+
+	if (icr & E1000_ICR_DRSTA)
+		schedule_work(&adapter->reset_task);
+
+	if (icr & E1000_ICR_DOUTSYNC) {
+		/* HW is reporting DMA is out of sync */
+		adapter->stats.doosync++;
+	}
+
+	if (icr & (E1000_ICR_RXSEQ | E1000_ICR_LSC)) {
+		hw->mac.get_link_status = 1;
+		/* guard against interrupt when we're going down */
+		if (!test_bit(__IGB_DOWN, &adapter->state))
+			mod_timer(&adapter->watchdog_timer, jiffies + 1);
+	}
+
+#ifdef HAVE_PTP_1588_CLOCK
+	if (icr & E1000_ICR_TS) {
+		u32 tsicr = E1000_READ_REG(hw, E1000_TSICR);
+
+		if (tsicr & E1000_TSICR_TXTS) {
+			/* acknowledge the interrupt */
+			E1000_WRITE_REG(hw, E1000_TSICR, E1000_TSICR_TXTS);
+			/* retrieve hardware timestamp */
+			schedule_work(&adapter->ptp_tx_work);
+		}
+	}
+#endif /* HAVE_PTP_1588_CLOCK */
+
+	napi_schedule(&q_vector->napi);
+
+	return IRQ_HANDLED;
+}
+
+void igb_ring_irq_enable(struct igb_q_vector *q_vector)
+{
+	struct igb_adapter *adapter = q_vector->adapter;
+	struct e1000_hw *hw = &adapter->hw;
+
+	if ((q_vector->rx.ring && (adapter->rx_itr_setting & 3)) ||
+	    (!q_vector->rx.ring && (adapter->tx_itr_setting & 3))) {
+		if ((adapter->num_q_vectors == 1) && !adapter->vf_data)
+			igb_set_itr(q_vector);
+		else
+			igb_update_ring_itr(q_vector);
+	}
+
+	if (!test_bit(__IGB_DOWN, &adapter->state)) {
+		if (adapter->msix_entries)
+			E1000_WRITE_REG(hw, E1000_EIMS, q_vector->eims_value);
+		else
+			igb_irq_enable(adapter);
+	}
+}
+
+/**
+ * igb_poll - NAPI Rx polling callback
+ * @napi: napi polling structure
+ * @budget: count of how many packets we should handle
+ **/
+static int igb_poll(struct napi_struct *napi, int budget)
+{
+	struct igb_q_vector *q_vector = container_of(napi, struct igb_q_vector, napi);
+	bool clean_complete = true;
+
+#ifdef IGB_DCA
+	if (q_vector->adapter->flags & IGB_FLAG_DCA_ENABLED)
+		igb_update_dca(q_vector);
+#endif
+	if (q_vector->tx.ring)
+		clean_complete = igb_clean_tx_irq(q_vector);
+
+	if (q_vector->rx.ring)
+		clean_complete &= igb_clean_rx_irq(q_vector, budget);
+
+#ifndef HAVE_NETDEV_NAPI_LIST
+	/* if netdev is disabled we need to stop polling */
+	if (!netif_running(q_vector->adapter->netdev))
+		clean_complete = true;
+
+#endif
+	/* If all work not completed, return budget and keep polling */
+	if (!clean_complete)
+		return budget;
+
+	/* If not enough Rx work done, exit the polling mode */
+	napi_complete(napi);
+	igb_ring_irq_enable(q_vector);
+
+	return 0;
+}
+
+/**
+ * igb_clean_tx_irq - Reclaim resources after transmit completes
+ * @q_vector: pointer to q_vector containing needed info
+ * returns TRUE if ring is completely cleaned
+ **/
+static bool igb_clean_tx_irq(struct igb_q_vector *q_vector)
+{
+	struct igb_adapter *adapter = q_vector->adapter;
+	struct igb_ring *tx_ring = q_vector->tx.ring;
+	struct igb_tx_buffer *tx_buffer;
+	union e1000_adv_tx_desc *tx_desc;
+	unsigned int total_bytes = 0, total_packets = 0;
+	unsigned int budget = q_vector->tx.work_limit;
+	unsigned int i = tx_ring->next_to_clean;
+
+	if (test_bit(__IGB_DOWN, &adapter->state))
+		return true;
+
+	tx_buffer = &tx_ring->tx_buffer_info[i];
+	tx_desc = IGB_TX_DESC(tx_ring, i);
+	i -= tx_ring->count;
+
+	do {
+		union e1000_adv_tx_desc *eop_desc = tx_buffer->next_to_watch;
+
+		/* if next_to_watch is not set then there is no work pending */
+		if (!eop_desc)
+			break;
+
+		/* prevent any other reads prior to eop_desc */
+		read_barrier_depends();
+
+		/* if DD is not set pending work has not been completed */
+		if (!(eop_desc->wb.status & cpu_to_le32(E1000_TXD_STAT_DD)))
+			break;
+
+		/* clear next_to_watch to prevent false hangs */
+		tx_buffer->next_to_watch = NULL;
+
+		/* update the statistics for this packet */
+		total_bytes += tx_buffer->bytecount;
+		total_packets += tx_buffer->gso_segs;
+
+		/* free the skb */
+		dev_kfree_skb_any(tx_buffer->skb);
+
+		/* unmap skb header data */
+		dma_unmap_single(tx_ring->dev,
+		                 dma_unmap_addr(tx_buffer, dma),
+		                 dma_unmap_len(tx_buffer, len),
+		                 DMA_TO_DEVICE);
+
+		/* clear tx_buffer data */
+		tx_buffer->skb = NULL;
+		dma_unmap_len_set(tx_buffer, len, 0);
+
+		/* clear last DMA location and unmap remaining buffers */
+		while (tx_desc != eop_desc) {
+			tx_buffer++;
+			tx_desc++;
+			i++;
+			if (unlikely(!i)) {
+				i -= tx_ring->count;
+				tx_buffer = tx_ring->tx_buffer_info;
+				tx_desc = IGB_TX_DESC(tx_ring, 0);
+			}
+
+			/* unmap any remaining paged data */
+			if (dma_unmap_len(tx_buffer, len)) {
+				dma_unmap_page(tx_ring->dev,
+				               dma_unmap_addr(tx_buffer, dma),
+				               dma_unmap_len(tx_buffer, len),
+				               DMA_TO_DEVICE);
+				dma_unmap_len_set(tx_buffer, len, 0);
+			}
+		}
+
+		/* move us one more past the eop_desc for start of next pkt */
+		tx_buffer++;
+		tx_desc++;
+		i++;
+		if (unlikely(!i)) {
+			i -= tx_ring->count;
+			tx_buffer = tx_ring->tx_buffer_info;
+			tx_desc = IGB_TX_DESC(tx_ring, 0);
+		}
+
+		/* issue prefetch for next Tx descriptor */
+		prefetch(tx_desc);
+
+		/* update budget accounting */
+		budget--;
+	} while (likely(budget));
+
+	netdev_tx_completed_queue(txring_txq(tx_ring),
+				  total_packets, total_bytes);
+
+	i += tx_ring->count;
+	tx_ring->next_to_clean = i;
+	tx_ring->tx_stats.bytes += total_bytes;
+	tx_ring->tx_stats.packets += total_packets;
+	q_vector->tx.total_bytes += total_bytes;
+	q_vector->tx.total_packets += total_packets;
+
+#ifdef DEBUG
+	if (test_bit(IGB_RING_FLAG_TX_DETECT_HANG, &tx_ring->flags) &&
+	    !(adapter->disable_hw_reset && adapter->tx_hang_detected)) {
+#else
+	if (test_bit(IGB_RING_FLAG_TX_DETECT_HANG, &tx_ring->flags)) {
+#endif
+		struct e1000_hw *hw = &adapter->hw;
+
+		/* Detect a transmit hang in hardware, this serializes the
+		 * check with the clearing of time_stamp and movement of i */
+		clear_bit(IGB_RING_FLAG_TX_DETECT_HANG, &tx_ring->flags);
+		if (tx_buffer->next_to_watch &&
+		    time_after(jiffies, tx_buffer->time_stamp +
+		               (adapter->tx_timeout_factor * HZ))
+		    && !(E1000_READ_REG(hw, E1000_STATUS) &
+		         E1000_STATUS_TXOFF)) {
+
+			/* detected Tx unit hang */
+#ifdef DEBUG
+			adapter->tx_hang_detected = TRUE;
+			if (adapter->disable_hw_reset) {
+				DPRINTK(DRV, WARNING,
+					"Deactivating netdev watchdog timer\n");
+				if (del_timer(&netdev_ring(tx_ring)->watchdog_timer))
+					dev_put(netdev_ring(tx_ring));
+#ifndef HAVE_NET_DEVICE_OPS
+				netdev_ring(tx_ring)->tx_timeout = NULL;
+#endif
+			}
+#endif /* DEBUG */
+			dev_err(tx_ring->dev,
+				"Detected Tx Unit Hang\n"
+				"  Tx Queue             <%d>\n"
+				"  TDH                  <%x>\n"
+				"  TDT                  <%x>\n"
+				"  next_to_use          <%x>\n"
+				"  next_to_clean        <%x>\n"
+				"buffer_info[next_to_clean]\n"
+				"  time_stamp           <%lx>\n"
+				"  next_to_watch        <%p>\n"
+				"  jiffies              <%lx>\n"
+				"  desc.status          <%x>\n",
+				tx_ring->queue_index,
+				E1000_READ_REG(hw, E1000_TDH(tx_ring->reg_idx)),
+				readl(tx_ring->tail),
+				tx_ring->next_to_use,
+				tx_ring->next_to_clean,
+				tx_buffer->time_stamp,
+				tx_buffer->next_to_watch,
+				jiffies,
+				tx_buffer->next_to_watch->wb.status);
+			if (netif_is_multiqueue(netdev_ring(tx_ring)))
+				netif_stop_subqueue(netdev_ring(tx_ring),
+						    ring_queue_index(tx_ring));
+			else
+				netif_stop_queue(netdev_ring(tx_ring));
+
+			/* we are about to reset, no point in enabling stuff */
+			return true;
+		}
+	}
+
+#define TX_WAKE_THRESHOLD (DESC_NEEDED * 2)
+	if (unlikely(total_packets &&
+		     netif_carrier_ok(netdev_ring(tx_ring)) &&
+		     igb_desc_unused(tx_ring) >= TX_WAKE_THRESHOLD)) {
+		/* Make sure that anybody stopping the queue after this
+		 * sees the new next_to_clean.
+		 */
+		smp_mb();
+		if (netif_is_multiqueue(netdev_ring(tx_ring))) {
+			if (__netif_subqueue_stopped(netdev_ring(tx_ring),
+						     ring_queue_index(tx_ring)) &&
+			    !(test_bit(__IGB_DOWN, &adapter->state))) {
+				netif_wake_subqueue(netdev_ring(tx_ring),
+						    ring_queue_index(tx_ring));
+				tx_ring->tx_stats.restart_queue++;
+			}
+		} else {
+			if (netif_queue_stopped(netdev_ring(tx_ring)) &&
+			    !(test_bit(__IGB_DOWN, &adapter->state))) {
+				netif_wake_queue(netdev_ring(tx_ring));
+				tx_ring->tx_stats.restart_queue++;
+			}
+		}
+	}
+
+	return !!budget;
+}
+
+#ifdef HAVE_VLAN_RX_REGISTER
+/**
+ * igb_receive_skb - helper function to handle rx indications
+ * @q_vector: structure containing interrupt and ring information
+ * @skb: packet to send up
+ **/
+static void igb_receive_skb(struct igb_q_vector *q_vector,
+                            struct sk_buff *skb)
+{
+	struct vlan_group **vlgrp = netdev_priv(skb->dev);
+
+	if (IGB_CB(skb)->vid) {
+		if (*vlgrp) {
+			vlan_gro_receive(&q_vector->napi, *vlgrp,
+					 IGB_CB(skb)->vid, skb);
+		} else {
+			dev_kfree_skb_any(skb);
+		}
+	} else {
+		napi_gro_receive(&q_vector->napi, skb);
+	}
+}
+
+#endif /* HAVE_VLAN_RX_REGISTER */
+#ifndef CONFIG_IGB_DISABLE_PACKET_SPLIT
+/**
+ * igb_reuse_rx_page - page flip buffer and store it back on the ring
+ * @rx_ring: rx descriptor ring to store buffers on
+ * @old_buff: donor buffer to have page reused
+ *
+ * Synchronizes page for reuse by the adapter
+ **/
+static void igb_reuse_rx_page(struct igb_ring *rx_ring,
+			      struct igb_rx_buffer *old_buff)
+{
+	struct igb_rx_buffer *new_buff;
+	u16 nta = rx_ring->next_to_alloc;
+
+	new_buff = &rx_ring->rx_buffer_info[nta];
+
+	/* update, and store next to alloc */
+	nta++;
+	rx_ring->next_to_alloc = (nta < rx_ring->count) ? nta : 0;
+
+	/* transfer page from old buffer to new buffer */
+	memcpy(new_buff, old_buff, sizeof(struct igb_rx_buffer));
+
+	/* sync the buffer for use by the device */
+	dma_sync_single_range_for_device(rx_ring->dev, old_buff->dma,
+					 old_buff->page_offset,
+					 IGB_RX_BUFSZ,
+					 DMA_FROM_DEVICE);
+}
+
+static bool igb_can_reuse_rx_page(struct igb_rx_buffer *rx_buffer,
+				  struct page *page,
+				  unsigned int truesize)
+{
+	/* avoid re-using remote pages */
+	if (unlikely(page_to_nid(page) != numa_node_id()))
+		return false;
+
+#if (PAGE_SIZE < 8192)
+	/* if we are only owner of page we can reuse it */
+	if (unlikely(page_count(page) != 1))
+		return false;
+
+	/* flip page offset to other buffer */
+	rx_buffer->page_offset ^= IGB_RX_BUFSZ;
+
+#else
+	/* move offset up to the next cache line */
+	rx_buffer->page_offset += truesize;
+
+	if (rx_buffer->page_offset > (PAGE_SIZE - IGB_RX_BUFSZ))
+		return false;
+#endif
+
+	/* bump ref count on page before it is given to the stack */
+	get_page(page);
+
+	return true;
+}
+
+/**
+ * igb_add_rx_frag - Add contents of Rx buffer to sk_buff
+ * @rx_ring: rx descriptor ring to transact packets on
+ * @rx_buffer: buffer containing page to add
+ * @rx_desc: descriptor containing length of buffer written by hardware
+ * @skb: sk_buff to place the data into
+ *
+ * This function will add the data contained in rx_buffer->page to the skb.
+ * This is done either through a direct copy if the data in the buffer is
+ * less than the skb header size, otherwise it will just attach the page as
+ * a frag to the skb.
+ *
+ * The function will then update the page offset if necessary and return
+ * true if the buffer can be reused by the adapter.
+ **/
+static bool igb_add_rx_frag(struct igb_ring *rx_ring,
+			    struct igb_rx_buffer *rx_buffer,
+			    union e1000_adv_rx_desc *rx_desc,
+			    struct sk_buff *skb)
+{
+	struct page *page = rx_buffer->page;
+	unsigned int size = le16_to_cpu(rx_desc->wb.upper.length);
+#if (PAGE_SIZE < 8192)
+	unsigned int truesize = IGB_RX_BUFSZ;
+#else
+	unsigned int truesize = ALIGN(size, L1_CACHE_BYTES);
+#endif
+
+	if ((size <= IGB_RX_HDR_LEN) && !skb_is_nonlinear(skb)) {
+		unsigned char *va = page_address(page) + rx_buffer->page_offset;
+
+#ifdef HAVE_PTP_1588_CLOCK
+		if (igb_test_staterr(rx_desc, E1000_RXDADV_STAT_TSIP)) {
+			igb_ptp_rx_pktstamp(rx_ring->q_vector, va, skb);
+			va += IGB_TS_HDR_LEN;
+			size -= IGB_TS_HDR_LEN;
+		}
+#endif /* HAVE_PTP_1588_CLOCK */
+
+		memcpy(__skb_put(skb, size), va, ALIGN(size, sizeof(long)));
+
+		/* we can reuse buffer as-is, just make sure it is local */
+		if (likely(page_to_nid(page) == numa_node_id()))
+			return true;
+
+		/* this page cannot be reused so discard it */
+		put_page(page);
+		return false;
+	}
+
+	skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page,
+			rx_buffer->page_offset, size, truesize);
+
+	return igb_can_reuse_rx_page(rx_buffer, page, truesize);
+}
+
+static struct sk_buff *igb_fetch_rx_buffer(struct igb_ring *rx_ring,
+					   union e1000_adv_rx_desc *rx_desc,
+					   struct sk_buff *skb)
+{
+	struct igb_rx_buffer *rx_buffer;
+	struct page *page;
+
+	rx_buffer = &rx_ring->rx_buffer_info[rx_ring->next_to_clean];
+
+	page = rx_buffer->page;
+	prefetchw(page);
+
+	if (likely(!skb)) {
+		void *page_addr = page_address(page) +
+				  rx_buffer->page_offset;
+
+		/* prefetch first cache line of first page */
+		prefetch(page_addr);
+#if L1_CACHE_BYTES < 128
+		prefetch(page_addr + L1_CACHE_BYTES);
+#endif
+
+		/* allocate a skb to store the frags */
+		skb = netdev_alloc_skb_ip_align(rx_ring->netdev,
+						IGB_RX_HDR_LEN);
+		if (unlikely(!skb)) {
+			rx_ring->rx_stats.alloc_failed++;
+			return NULL;
+		}
+
+		/*
+		 * we will be copying header into skb->data in
+		 * pskb_may_pull so it is in our interest to prefetch
+		 * it now to avoid a possible cache miss
+		 */
+		prefetchw(skb->data);
+	}
+
+	/* we are reusing so sync this buffer for CPU use */
+	dma_sync_single_range_for_cpu(rx_ring->dev,
+				      rx_buffer->dma,
+				      rx_buffer->page_offset,
+				      IGB_RX_BUFSZ,
+				      DMA_FROM_DEVICE);
+
+	/* pull page into skb */
+	if (igb_add_rx_frag(rx_ring, rx_buffer, rx_desc, skb)) {
+		/* hand second half of page back to the ring */
+		igb_reuse_rx_page(rx_ring, rx_buffer);
+	} else {
+		/* we are not reusing the buffer so unmap it */
+		dma_unmap_page(rx_ring->dev, rx_buffer->dma,
+			       PAGE_SIZE, DMA_FROM_DEVICE);
+	}
+
+	/* clear contents of rx_buffer */
+	rx_buffer->page = NULL;
+
+	return skb;
+}
+
+#endif
+static inline void igb_rx_checksum(struct igb_ring *ring,
+				   union e1000_adv_rx_desc *rx_desc,
+				   struct sk_buff *skb)
+{
+	skb_checksum_none_assert(skb);
+
+	/* Ignore Checksum bit is set */
+	if (igb_test_staterr(rx_desc, E1000_RXD_STAT_IXSM))
+		return;
+
+	/* Rx checksum disabled via ethtool */
+	if (!(netdev_ring(ring)->features & NETIF_F_RXCSUM))
+		return;
+
+	/* TCP/UDP checksum error bit is set */
+	if (igb_test_staterr(rx_desc,
+			     E1000_RXDEXT_STATERR_TCPE |
+			     E1000_RXDEXT_STATERR_IPE)) {
+		/*
+		 * work around errata with sctp packets where the TCPE aka
+		 * L4E bit is set incorrectly on 64 byte (60 byte w/o crc)
+		 * packets, (aka let the stack check the crc32c)
+		 */
+		if (!((skb->len == 60) &&
+		      test_bit(IGB_RING_FLAG_RX_SCTP_CSUM, &ring->flags)))
+			ring->rx_stats.csum_err++;
+
+		/* let the stack verify checksum errors */
+		return;
+	}
+	/* It must be a TCP or UDP packet with a valid checksum */
+	if (igb_test_staterr(rx_desc, E1000_RXD_STAT_TCPCS |
+				      E1000_RXD_STAT_UDPCS))
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+}
+
+#ifdef NETIF_F_RXHASH
+static inline void igb_rx_hash(struct igb_ring *ring,
+			       union e1000_adv_rx_desc *rx_desc,
+			       struct sk_buff *skb)
+{
+	if (netdev_ring(ring)->features & NETIF_F_RXHASH)
+		skb_set_hash(skb, le32_to_cpu(rx_desc->wb.lower.hi_dword.rss),
+			     PKT_HASH_TYPE_L3);
+}
+
+#endif
+#ifndef IGB_NO_LRO
+#ifdef CONFIG_IGB_DISABLE_PACKET_SPLIT
+/**
+ * igb_merge_active_tail - merge active tail into lro skb
+ * @tail: pointer to active tail in frag_list
+ *
+ * This function merges the length and data of an active tail into the
+ * skb containing the frag_list.  It resets the tail's pointer to the head,
+ * but it leaves the heads pointer to tail intact.
+ **/
+static inline struct sk_buff *igb_merge_active_tail(struct sk_buff *tail)
+{
+	struct sk_buff *head = IGB_CB(tail)->head;
+
+	if (!head)
+		return tail;
+
+	head->len += tail->len;
+	head->data_len += tail->len;
+	head->truesize += tail->len;
+
+	IGB_CB(tail)->head = NULL;
+
+	return head;
+}
+
+/**
+ * igb_add_active_tail - adds an active tail into the skb frag_list
+ * @head: pointer to the start of the skb
+ * @tail: pointer to active tail to add to frag_list
+ *
+ * This function adds an active tail to the end of the frag list.  This tail
+ * will still be receiving data so we cannot yet ad it's stats to the main
+ * skb.  That is done via igb_merge_active_tail.
+ **/
+static inline void igb_add_active_tail(struct sk_buff *head, struct sk_buff *tail)
+{
+	struct sk_buff *old_tail = IGB_CB(head)->tail;
+
+	if (old_tail) {
+		igb_merge_active_tail(old_tail);
+		old_tail->next = tail;
+	} else {
+		skb_shinfo(head)->frag_list = tail;
+	}
+
+	IGB_CB(tail)->head = head;
+	IGB_CB(head)->tail = tail;
+
+	IGB_CB(head)->append_cnt++;
+}
+
+/**
+ * igb_close_active_frag_list - cleanup pointers on a frag_list skb
+ * @head: pointer to head of an active frag list
+ *
+ * This function will clear the frag_tail_tracker pointer on an active
+ * frag_list and returns true if the pointer was actually set
+ **/
+static inline bool igb_close_active_frag_list(struct sk_buff *head)
+{
+	struct sk_buff *tail = IGB_CB(head)->tail;
+
+	if (!tail)
+		return false;
+
+	igb_merge_active_tail(tail);
+
+	IGB_CB(head)->tail = NULL;
+
+	return true;
+}
+
+#endif /* CONFIG_IGB_DISABLE_PACKET_SPLIT */
+/**
+ * igb_can_lro - returns true if packet is TCP/IPV4 and LRO is enabled
+ * @adapter: board private structure
+ * @rx_desc: pointer to the rx descriptor
+ * @skb: pointer to the skb to be merged
+ *
+ **/
+static inline bool igb_can_lro(struct igb_ring *rx_ring,
+			       union e1000_adv_rx_desc *rx_desc,
+			       struct sk_buff *skb)
+{
+	struct iphdr *iph = (struct iphdr *)skb->data;
+	__le16 pkt_info = rx_desc->wb.lower.lo_dword.hs_rss.pkt_info;
+
+	/* verify hardware indicates this is IPv4/TCP */
+	if((!(pkt_info & cpu_to_le16(E1000_RXDADV_PKTTYPE_TCP)) ||
+	    !(pkt_info & cpu_to_le16(E1000_RXDADV_PKTTYPE_IPV4))))
+		return false;
+
+	/* .. and LRO is enabled */
+	if (!(netdev_ring(rx_ring)->features & NETIF_F_LRO))
+		return false;
+
+	/* .. and we are not in promiscuous mode */
+	if (netdev_ring(rx_ring)->flags & IFF_PROMISC)
+		return false;
+
+	/* .. and the header is large enough for us to read IP/TCP fields */
+	if (!pskb_may_pull(skb, sizeof(struct igb_lrohdr)))
+		return false;
+
+	/* .. and there are no VLANs on packet */
+	if (skb->protocol != __constant_htons(ETH_P_IP))
+		return false;
+
+	/* .. and we are version 4 with no options */
+	if (*(u8 *)iph != 0x45)
+		return false;
+
+	/* .. and the packet is not fragmented */
+	if (iph->frag_off & htons(IP_MF | IP_OFFSET))
+		return false;
+
+	/* .. and that next header is TCP */
+	if (iph->protocol != IPPROTO_TCP)
+		return false;
+
+	return true;
+}
+
+static inline struct igb_lrohdr *igb_lro_hdr(struct sk_buff *skb)
+{
+	return (struct igb_lrohdr *)skb->data;
+}
+
+/**
+ * igb_lro_flush - Indicate packets to upper layer.
+ *
+ * Update IP and TCP header part of head skb if more than one
+ * skb's chained and indicate packets to upper layer.
+ **/
+static void igb_lro_flush(struct igb_q_vector *q_vector,
+			  struct sk_buff *skb)
+{
+	struct igb_lro_list *lrolist = &q_vector->lrolist;
+
+	__skb_unlink(skb, &lrolist->active);
+
+	if (IGB_CB(skb)->append_cnt) {
+		struct igb_lrohdr *lroh = igb_lro_hdr(skb);
+
+#ifdef CONFIG_IGB_DISABLE_PACKET_SPLIT
+		/* close any active lro contexts */
+		igb_close_active_frag_list(skb);
+
+#endif
+		/* incorporate ip header and re-calculate checksum */
+		lroh->iph.tot_len = ntohs(skb->len);
+		lroh->iph.check = 0;
+
+		/* header length is 5 since we know no options exist */
+		lroh->iph.check = ip_fast_csum((u8 *)lroh, 5);
+
+		/* clear TCP checksum to indicate we are an LRO frame */
+		lroh->th.check = 0;
+
+		/* incorporate latest timestamp into the tcp header */
+		if (IGB_CB(skb)->tsecr) {
+			lroh->ts[2] = IGB_CB(skb)->tsecr;
+			lroh->ts[1] = htonl(IGB_CB(skb)->tsval);
+		}
+#ifdef NETIF_F_GSO
+
+		skb_shinfo(skb)->gso_size = IGB_CB(skb)->mss;
+		skb_shinfo(skb)->gso_type = SKB_GSO_TCPV4;
+#endif
+	}
+
+#ifdef HAVE_VLAN_RX_REGISTER
+	igb_receive_skb(q_vector, skb);
+#else
+	napi_gro_receive(&q_vector->napi, skb);
+#endif
+	lrolist->stats.flushed++;
+}
+
+static void igb_lro_flush_all(struct igb_q_vector *q_vector)
+{
+	struct igb_lro_list *lrolist = &q_vector->lrolist;
+	struct sk_buff *skb, *tmp;
+
+	skb_queue_reverse_walk_safe(&lrolist->active, skb, tmp)
+		igb_lro_flush(q_vector, skb);
+}
+
+/*
+ * igb_lro_header_ok - Main LRO function.
+ **/
+static void igb_lro_header_ok(struct sk_buff *skb)
+{
+	struct igb_lrohdr *lroh = igb_lro_hdr(skb);
+	u16 opt_bytes, data_len;
+
+#ifdef CONFIG_IGB_DISABLE_PACKET_SPLIT
+	IGB_CB(skb)->tail = NULL;
+#endif
+	IGB_CB(skb)->tsecr = 0;
+	IGB_CB(skb)->append_cnt = 0;
+	IGB_CB(skb)->mss = 0;
+
+	/* ensure that the checksum is valid */
+	if (skb->ip_summed != CHECKSUM_UNNECESSARY)
+		return;
+
+	/* If we see CE codepoint in IP header, packet is not mergeable */
+	if (INET_ECN_is_ce(ipv4_get_dsfield(&lroh->iph)))
+		return;
+
+	/* ensure no bits set besides ack or psh */
+	if (lroh->th.fin || lroh->th.syn || lroh->th.rst ||
+	    lroh->th.urg || lroh->th.ece || lroh->th.cwr ||
+	    !lroh->th.ack)
+		return;
+
+	/* store the total packet length */
+	data_len = ntohs(lroh->iph.tot_len);
+
+	/* remove any padding from the end of the skb */
+	__pskb_trim(skb, data_len);
+
+	/* remove header length from data length */
+	data_len -= sizeof(struct igb_lrohdr);
+
+	/*
+	 * check for timestamps. Since the only option we handle are timestamps,
+	 * we only have to handle the simple case of aligned timestamps
+	 */
+	opt_bytes = (lroh->th.doff << 2) - sizeof(struct tcphdr);
+	if (opt_bytes != 0) {
+		if ((opt_bytes != TCPOLEN_TSTAMP_ALIGNED) ||
+		    !pskb_may_pull(skb, sizeof(struct igb_lrohdr) +
+					TCPOLEN_TSTAMP_ALIGNED) ||
+		    (lroh->ts[0] != htonl((TCPOPT_NOP << 24) |
+					     (TCPOPT_NOP << 16) |
+					     (TCPOPT_TIMESTAMP << 8) |
+					      TCPOLEN_TIMESTAMP)) ||
+		    (lroh->ts[2] == 0)) {
+			return;
+		}
+
+		IGB_CB(skb)->tsval = ntohl(lroh->ts[1]);
+		IGB_CB(skb)->tsecr = lroh->ts[2];
+
+		data_len -= TCPOLEN_TSTAMP_ALIGNED;
+	}
+
+	/* record data_len as mss for the packet */
+	IGB_CB(skb)->mss = data_len;
+	IGB_CB(skb)->next_seq = ntohl(lroh->th.seq);
+}
+
+#ifndef CONFIG_IGB_DISABLE_PACKET_SPLIT
+static void igb_merge_frags(struct sk_buff *lro_skb, struct sk_buff *new_skb)
+{
+	struct skb_shared_info *sh_info;
+	struct skb_shared_info *new_skb_info;
+	unsigned int data_len;
+
+	sh_info = skb_shinfo(lro_skb);
+	new_skb_info = skb_shinfo(new_skb);
+
+	/* copy frags into the last skb */
+	memcpy(sh_info->frags + sh_info->nr_frags,
+	       new_skb_info->frags,
+	       new_skb_info->nr_frags * sizeof(skb_frag_t));
+
+	/* copy size data over */
+	sh_info->nr_frags += new_skb_info->nr_frags;
+	data_len = IGB_CB(new_skb)->mss;
+	lro_skb->len += data_len;
+	lro_skb->data_len += data_len;
+	lro_skb->truesize += data_len;
+
+	/* wipe record of data from new_skb */
+	new_skb_info->nr_frags = 0;
+	new_skb->len = new_skb->data_len = 0;
+	dev_kfree_skb_any(new_skb);
+}
+
+#endif /* CONFIG_IGB_DISABLE_PACKET_SPLIT */
+/**
+ * igb_lro_receive - if able, queue skb into lro chain
+ * @q_vector: structure containing interrupt and ring information
+ * @new_skb: pointer to current skb being checked
+ *
+ * Checks whether the skb given is eligible for LRO and if that's
+ * fine chains it to the existing lro_skb based on flowid. If an LRO for
+ * the flow doesn't exist create one.
+ **/
+static void igb_lro_receive(struct igb_q_vector *q_vector,
+			    struct sk_buff *new_skb)
+{
+	struct sk_buff *lro_skb;
+	struct igb_lro_list *lrolist = &q_vector->lrolist;
+	struct igb_lrohdr *lroh = igb_lro_hdr(new_skb);
+	__be32 saddr = lroh->iph.saddr;
+	__be32 daddr = lroh->iph.daddr;
+	__be32 tcp_ports = *(__be32 *)&lroh->th;
+	u16 data_len;
+#ifdef HAVE_VLAN_RX_REGISTER
+	u16 vid = IGB_CB(new_skb)->vid;
+#else
+	u16 vid = new_skb->vlan_tci;
+#endif
+
+	igb_lro_header_ok(new_skb);
+
+	/*
+	 * we have a packet that might be eligible for LRO,
+	 * so see if it matches anything we might expect
+	 */
+	skb_queue_walk(&lrolist->active, lro_skb) {
+		if (*(__be32 *)&igb_lro_hdr(lro_skb)->th != tcp_ports ||
+		    igb_lro_hdr(lro_skb)->iph.saddr != saddr ||
+		    igb_lro_hdr(lro_skb)->iph.daddr != daddr)
+			continue;
+
+#ifdef HAVE_VLAN_RX_REGISTER
+		if (IGB_CB(lro_skb)->vid != vid)
+#else
+		if (lro_skb->vlan_tci != vid)
+#endif
+			continue;
+
+		/* out of order packet */
+		if (IGB_CB(lro_skb)->next_seq != IGB_CB(new_skb)->next_seq) {
+			igb_lro_flush(q_vector, lro_skb);
+			IGB_CB(new_skb)->mss = 0;
+			break;
+		}
+
+		/* TCP timestamp options have changed */
+		if (!IGB_CB(lro_skb)->tsecr != !IGB_CB(new_skb)->tsecr) {
+			igb_lro_flush(q_vector, lro_skb);
+			break;
+		}
+
+		/* make sure timestamp values are increasing */
+		if (IGB_CB(lro_skb)->tsecr &&
+		    IGB_CB(lro_skb)->tsval > IGB_CB(new_skb)->tsval) {
+			igb_lro_flush(q_vector, lro_skb);
+			IGB_CB(new_skb)->mss = 0;
+			break;
+		}
+
+		data_len = IGB_CB(new_skb)->mss;
+
+		/* Check for all of the above below
+		 *   malformed header
+		 *   no tcp data
+		 *   resultant packet would be too large
+		 *   new skb is larger than our current mss
+		 *   data would remain in header
+		 *   we would consume more frags then the sk_buff contains
+		 *   ack sequence numbers changed
+		 *   window size has changed
+		 */
+		if (data_len == 0 ||
+		    data_len > IGB_CB(lro_skb)->mss ||
+		    data_len > IGB_CB(lro_skb)->free ||
+#ifndef CONFIG_IGB_DISABLE_PACKET_SPLIT
+		    data_len != new_skb->data_len ||
+		    skb_shinfo(new_skb)->nr_frags >=
+		    (MAX_SKB_FRAGS - skb_shinfo(lro_skb)->nr_frags) ||
+#endif
+		    igb_lro_hdr(lro_skb)->th.ack_seq != lroh->th.ack_seq ||
+		    igb_lro_hdr(lro_skb)->th.window != lroh->th.window) {
+			igb_lro_flush(q_vector, lro_skb);
+			break;
+		}
+
+		/* Remove IP and TCP header*/
+		skb_pull(new_skb, new_skb->len - data_len);
+
+		/* update timestamp and timestamp echo response */
+		IGB_CB(lro_skb)->tsval = IGB_CB(new_skb)->tsval;
+		IGB_CB(lro_skb)->tsecr = IGB_CB(new_skb)->tsecr;
+
+		/* update sequence and free space */
+		IGB_CB(lro_skb)->next_seq += data_len;
+		IGB_CB(lro_skb)->free -= data_len;
+
+		/* update append_cnt */
+		IGB_CB(lro_skb)->append_cnt++;
+
+#ifndef CONFIG_IGB_DISABLE_PACKET_SPLIT
+		/* if header is empty pull pages into current skb */
+		igb_merge_frags(lro_skb, new_skb);
+#else
+		/* chain this new skb in frag_list */
+		igb_add_active_tail(lro_skb, new_skb);
+#endif
+
+		if ((data_len < IGB_CB(lro_skb)->mss) || lroh->th.psh ||
+		    skb_shinfo(lro_skb)->nr_frags == MAX_SKB_FRAGS) {
+			igb_lro_hdr(lro_skb)->th.psh |= lroh->th.psh;
+			igb_lro_flush(q_vector, lro_skb);
+		}
+
+		lrolist->stats.coal++;
+		return;
+	}
+
+	if (IGB_CB(new_skb)->mss && !lroh->th.psh) {
+		/* if we are at capacity flush the tail */
+		if (skb_queue_len(&lrolist->active) >= IGB_LRO_MAX) {
+			lro_skb = skb_peek_tail(&lrolist->active);
+			if (lro_skb)
+				igb_lro_flush(q_vector, lro_skb);
+		}
+
+		/* update sequence and free space */
+		IGB_CB(new_skb)->next_seq += IGB_CB(new_skb)->mss;
+		IGB_CB(new_skb)->free = 65521 - new_skb->len;
+
+		/* .. and insert at the front of the active list */
+		__skb_queue_head(&lrolist->active, new_skb);
+
+		lrolist->stats.coal++;
+		return;
+	}
+
+	/* packet not handled by any of the above, pass it to the stack */
+#ifdef HAVE_VLAN_RX_REGISTER
+	igb_receive_skb(q_vector, new_skb);
+#else
+	napi_gro_receive(&q_vector->napi, new_skb);
+#endif
+}
+
+#endif /* IGB_NO_LRO */
+/**
+ * igb_process_skb_fields - Populate skb header fields from Rx descriptor
+ * @rx_ring: rx descriptor ring packet is being transacted on
+ * @rx_desc: pointer to the EOP Rx descriptor
+ * @skb: pointer to current skb being populated
+ *
+ * This function checks the ring, descriptor, and packet information in
+ * order to populate the hash, checksum, VLAN, timestamp, protocol, and
+ * other fields within the skb.
+ **/
+static void igb_process_skb_fields(struct igb_ring *rx_ring,
+				   union e1000_adv_rx_desc *rx_desc,
+				   struct sk_buff *skb)
+{
+	struct net_device *dev = rx_ring->netdev;
+	__le16 pkt_info = rx_desc->wb.lower.lo_dword.hs_rss.pkt_info;
+
+#ifdef NETIF_F_RXHASH
+	igb_rx_hash(rx_ring, rx_desc, skb);
+
+#endif
+	igb_rx_checksum(rx_ring, rx_desc, skb);
+
+    /* update packet type stats */
+	if (pkt_info & cpu_to_le16(E1000_RXDADV_PKTTYPE_IPV4))
+		rx_ring->rx_stats.ipv4_packets++;
+	else if (pkt_info & cpu_to_le16(E1000_RXDADV_PKTTYPE_IPV4_EX))
+		rx_ring->rx_stats.ipv4e_packets++;
+	else if (pkt_info & cpu_to_le16(E1000_RXDADV_PKTTYPE_IPV6))
+		rx_ring->rx_stats.ipv6_packets++;
+	else if (pkt_info & cpu_to_le16(E1000_RXDADV_PKTTYPE_IPV6_EX))
+		rx_ring->rx_stats.ipv6e_packets++;
+	else if (pkt_info & cpu_to_le16(E1000_RXDADV_PKTTYPE_TCP))
+		rx_ring->rx_stats.tcp_packets++;
+	else if (pkt_info & cpu_to_le16(E1000_RXDADV_PKTTYPE_UDP))
+		rx_ring->rx_stats.udp_packets++;
+	else if (pkt_info & cpu_to_le16(E1000_RXDADV_PKTTYPE_SCTP))
+		rx_ring->rx_stats.sctp_packets++;
+	else if (pkt_info & cpu_to_le16(E1000_RXDADV_PKTTYPE_NFS))
+		rx_ring->rx_stats.nfs_packets++;
+
+#ifdef HAVE_PTP_1588_CLOCK
+	igb_ptp_rx_hwtstamp(rx_ring, rx_desc, skb);
+#endif /* HAVE_PTP_1588_CLOCK */
+
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
+	if ((dev->features & NETIF_F_HW_VLAN_CTAG_RX) &&
+#else
+	if ((dev->features & NETIF_F_HW_VLAN_RX) &&
+#endif
+	    igb_test_staterr(rx_desc, E1000_RXD_STAT_VP)) {
+		u16 vid = 0;
+		if (igb_test_staterr(rx_desc, E1000_RXDEXT_STATERR_LB) &&
+		    test_bit(IGB_RING_FLAG_RX_LB_VLAN_BSWAP, &rx_ring->flags))
+			vid = be16_to_cpu(rx_desc->wb.upper.vlan);
+		else
+			vid = le16_to_cpu(rx_desc->wb.upper.vlan);
+#ifdef HAVE_VLAN_RX_REGISTER
+		IGB_CB(skb)->vid = vid;
+	} else {
+		IGB_CB(skb)->vid = 0;
+#else
+
+#ifdef HAVE_VLAN_PROTOCOL
+		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vid);
+#else
+		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vid);
+#endif
+
+
+#endif
+	}
+
+	skb_record_rx_queue(skb, rx_ring->queue_index);
+
+	skb->protocol = eth_type_trans(skb, dev);
+}
+
+/**
+ * igb_is_non_eop - process handling of non-EOP buffers
+ * @rx_ring: Rx ring being processed
+ * @rx_desc: Rx descriptor for current buffer
+ *
+ * This function updates next to clean.  If the buffer is an EOP buffer
+ * this function exits returning false, otherwise it will place the
+ * sk_buff in the next buffer to be chained and return true indicating
+ * that this is in fact a non-EOP buffer.
+ **/
+static bool igb_is_non_eop(struct igb_ring *rx_ring,
+			   union e1000_adv_rx_desc *rx_desc)
+{
+	u32 ntc = rx_ring->next_to_clean + 1;
+
+	/* fetch, update, and store next to clean */
+	ntc = (ntc < rx_ring->count) ? ntc : 0;
+	rx_ring->next_to_clean = ntc;
+
+	prefetch(IGB_RX_DESC(rx_ring, ntc));
+
+	if (likely(igb_test_staterr(rx_desc, E1000_RXD_STAT_EOP)))
+		return false;
+
+	return true;
+}
+
+#ifdef CONFIG_IGB_DISABLE_PACKET_SPLIT
+/* igb_clean_rx_irq -- * legacy */
+static bool igb_clean_rx_irq(struct igb_q_vector *q_vector, int budget)
+{
+	struct igb_ring *rx_ring = q_vector->rx.ring;
+	unsigned int total_bytes = 0, total_packets = 0;
+	u16 cleaned_count = igb_desc_unused(rx_ring);
+
+	do {
+		struct igb_rx_buffer *rx_buffer;
+		union e1000_adv_rx_desc *rx_desc;
+		struct sk_buff *skb;
+		u16 ntc;
+
+		/* return some buffers to hardware, one at a time is too slow */
+		if (cleaned_count >= IGB_RX_BUFFER_WRITE) {
+			igb_alloc_rx_buffers(rx_ring, cleaned_count);
+			cleaned_count = 0;
+		}
+
+		ntc = rx_ring->next_to_clean;
+		rx_desc = IGB_RX_DESC(rx_ring, ntc);
+		rx_buffer = &rx_ring->rx_buffer_info[ntc];
+
+		if (!igb_test_staterr(rx_desc, E1000_RXD_STAT_DD))
+			break;
+
+		/*
+		 * This memory barrier is needed to keep us from reading
+		 * any other fields out of the rx_desc until we know the
+		 * RXD_STAT_DD bit is set
+		 */
+		rmb();
+
+		skb = rx_buffer->skb;
+
+		prefetch(skb->data);
+
+		/* pull the header of the skb in */
+		__skb_put(skb, le16_to_cpu(rx_desc->wb.upper.length));
+
+		/* clear skb reference in buffer info structure */
+		rx_buffer->skb = NULL;
+
+		cleaned_count++;
+
+		BUG_ON(igb_is_non_eop(rx_ring, rx_desc));
+
+		dma_unmap_single(rx_ring->dev, rx_buffer->dma,
+				 rx_ring->rx_buffer_len,
+				 DMA_FROM_DEVICE);
+		rx_buffer->dma = 0;
+
+		if (igb_test_staterr(rx_desc,
+				     E1000_RXDEXT_ERR_FRAME_ERR_MASK)) {
+			dev_kfree_skb_any(skb);
+			continue;
+		}
+
+		total_bytes += skb->len;
+
+		/* populate checksum, timestamp, VLAN, and protocol */
+		igb_process_skb_fields(rx_ring, rx_desc, skb);
+
+#ifndef IGB_NO_LRO
+		if (igb_can_lro(rx_ring, rx_desc, skb))
+			igb_lro_receive(q_vector, skb);
+		else
+#endif
+#ifdef HAVE_VLAN_RX_REGISTER
+			igb_receive_skb(q_vector, skb);
+#else
+			napi_gro_receive(&q_vector->napi, skb);
+#endif
+
+#ifndef NETIF_F_GRO
+		netdev_ring(rx_ring)->last_rx = jiffies;
+
+#endif
+		/* update budget accounting */
+		total_packets++;
+	} while (likely(total_packets < budget));
+
+	rx_ring->rx_stats.packets += total_packets;
+	rx_ring->rx_stats.bytes += total_bytes;
+	q_vector->rx.total_packets += total_packets;
+	q_vector->rx.total_bytes += total_bytes;
+
+	if (cleaned_count)
+		igb_alloc_rx_buffers(rx_ring, cleaned_count);
+
+#ifndef IGB_NO_LRO
+	igb_lro_flush_all(q_vector);
+
+#endif /* IGB_NO_LRO */
+	return total_packets < budget;
+}
+#else /* CONFIG_IGB_DISABLE_PACKET_SPLIT */
+/**
+ * igb_get_headlen - determine size of header for LRO/GRO
+ * @data: pointer to the start of the headers
+ * @max_len: total length of section to find headers in
+ *
+ * This function is meant to determine the length of headers that will
+ * be recognized by hardware for LRO, and GRO offloads.  The main
+ * motivation of doing this is to only perform one pull for IPv4 TCP
+ * packets so that we can do basic things like calculating the gso_size
+ * based on the average data per packet.
+ **/
+static unsigned int igb_get_headlen(unsigned char *data,
+				    unsigned int max_len)
+{
+	union {
+		unsigned char *network;
+		/* l2 headers */
+		struct ethhdr *eth;
+		struct vlan_hdr *vlan;
+		/* l3 headers */
+		struct iphdr *ipv4;
+		struct ipv6hdr *ipv6;
+	} hdr;
+	__be16 protocol;
+	u8 nexthdr = 0;	/* default to not TCP */
+	u8 hlen;
+
+	/* this should never happen, but better safe than sorry */
+	if (max_len < ETH_HLEN)
+		return max_len;
+
+	/* initialize network frame pointer */
+	hdr.network = data;
+
+	/* set first protocol and move network header forward */
+	protocol = hdr.eth->h_proto;
+	hdr.network += ETH_HLEN;
+
+	/* handle any vlan tag if present */
+	if (protocol == __constant_htons(ETH_P_8021Q)) {
+		if ((hdr.network - data) > (max_len - VLAN_HLEN))
+			return max_len;
+
+		protocol = hdr.vlan->h_vlan_encapsulated_proto;
+		hdr.network += VLAN_HLEN;
+	}
+
+	/* handle L3 protocols */
+	if (protocol == __constant_htons(ETH_P_IP)) {
+		if ((hdr.network - data) > (max_len - sizeof(struct iphdr)))
+			return max_len;
+
+		/* access ihl as a u8 to avoid unaligned access on ia64 */
+		hlen = (hdr.network[0] & 0x0F) << 2;
+
+		/* verify hlen meets minimum size requirements */
+		if (hlen < sizeof(struct iphdr))
+			return hdr.network - data;
+
+		/* record next protocol if header is present */
+		if (!(hdr.ipv4->frag_off & htons(IP_OFFSET)))
+			nexthdr = hdr.ipv4->protocol;
+#ifdef NETIF_F_TSO6
+	} else if (protocol == __constant_htons(ETH_P_IPV6)) {
+		if ((hdr.network - data) > (max_len - sizeof(struct ipv6hdr)))
+			return max_len;
+
+		/* record next protocol */
+		nexthdr = hdr.ipv6->nexthdr;
+		hlen = sizeof(struct ipv6hdr);
+#endif /* NETIF_F_TSO6 */
+	} else {
+		return hdr.network - data;
+	}
+
+	/* relocate pointer to start of L4 header */
+	hdr.network += hlen;
+
+	/* finally sort out TCP */
+	if (nexthdr == IPPROTO_TCP) {
+		if ((hdr.network - data) > (max_len - sizeof(struct tcphdr)))
+			return max_len;
+
+		/* access doff as a u8 to avoid unaligned access on ia64 */
+		hlen = (hdr.network[12] & 0xF0) >> 2;
+
+		/* verify hlen meets minimum size requirements */
+		if (hlen < sizeof(struct tcphdr))
+			return hdr.network - data;
+
+		hdr.network += hlen;
+	} else if (nexthdr == IPPROTO_UDP) {
+		if ((hdr.network - data) > (max_len - sizeof(struct udphdr)))
+			return max_len;
+
+		hdr.network += sizeof(struct udphdr);
+	}
+
+	/*
+	 * If everything has gone correctly hdr.network should be the
+	 * data section of the packet and will be the end of the header.
+	 * If not then it probably represents the end of the last recognized
+	 * header.
+	 */
+	if ((hdr.network - data) < max_len)
+		return hdr.network - data;
+	else
+		return max_len;
+}
+
+/**
+ * igb_pull_tail - igb specific version of skb_pull_tail
+ * @rx_ring: rx descriptor ring packet is being transacted on
+ * @rx_desc: pointer to the EOP Rx descriptor
+ * @skb: pointer to current skb being adjusted
+ *
+ * This function is an igb specific version of __pskb_pull_tail.  The
+ * main difference between this version and the original function is that
+ * this function can make several assumptions about the state of things
+ * that allow for significant optimizations versus the standard function.
+ * As a result we can do things like drop a frag and maintain an accurate
+ * truesize for the skb.
+ */
+static void igb_pull_tail(struct igb_ring *rx_ring,
+			  union e1000_adv_rx_desc *rx_desc,
+			  struct sk_buff *skb)
+{
+	struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[0];
+	unsigned char *va;
+	unsigned int pull_len;
+
+	/*
+	 * it is valid to use page_address instead of kmap since we are
+	 * working with pages allocated out of the lomem pool per
+	 * alloc_page(GFP_ATOMIC)
+	 */
+	va = skb_frag_address(frag);
+
+#ifdef HAVE_PTP_1588_CLOCK
+	if (igb_test_staterr(rx_desc, E1000_RXDADV_STAT_TSIP)) {
+		/* retrieve timestamp from buffer */
+		igb_ptp_rx_pktstamp(rx_ring->q_vector, va, skb);
+
+		/* update pointers to remove timestamp header */
+		skb_frag_size_sub(frag, IGB_TS_HDR_LEN);
+		frag->page_offset += IGB_TS_HDR_LEN;
+		skb->data_len -= IGB_TS_HDR_LEN;
+		skb->len -= IGB_TS_HDR_LEN;
+
+		/* move va to start of packet data */
+		va += IGB_TS_HDR_LEN;
+	}
+#endif /* HAVE_PTP_1588_CLOCK */
+
+	/*
+	 * we need the header to contain the greater of either ETH_HLEN or
+	 * 60 bytes if the skb->len is less than 60 for skb_pad.
+	 */
+	pull_len = igb_get_headlen(va, IGB_RX_HDR_LEN);
+
+	/* align pull length to size of long to optimize memcpy performance */
+	skb_copy_to_linear_data(skb, va, ALIGN(pull_len, sizeof(long)));
+
+	/* update all of the pointers */
+	skb_frag_size_sub(frag, pull_len);
+	frag->page_offset += pull_len;
+	skb->data_len -= pull_len;
+	skb->tail += pull_len;
+}
+
+/**
+ * igb_cleanup_headers - Correct corrupted or empty headers
+ * @rx_ring: rx descriptor ring packet is being transacted on
+ * @rx_desc: pointer to the EOP Rx descriptor
+ * @skb: pointer to current skb being fixed
+ *
+ * Address the case where we are pulling data in on pages only
+ * and as such no data is present in the skb header.
+ *
+ * In addition if skb is not at least 60 bytes we need to pad it so that
+ * it is large enough to qualify as a valid Ethernet frame.
+ *
+ * Returns true if an error was encountered and skb was freed.
+ **/
+static bool igb_cleanup_headers(struct igb_ring *rx_ring,
+				union e1000_adv_rx_desc *rx_desc,
+				struct sk_buff *skb)
+{
+
+	if (unlikely((igb_test_staterr(rx_desc,
+				       E1000_RXDEXT_ERR_FRAME_ERR_MASK)))) {
+		struct net_device *netdev = rx_ring->netdev;
+		if (!(netdev->features & NETIF_F_RXALL)) {
+			dev_kfree_skb_any(skb);
+			return true;
+		}
+	}
+
+	/* place header in linear portion of buffer */
+	if (skb_is_nonlinear(skb))
+		igb_pull_tail(rx_ring, rx_desc, skb);
+
+	/* if skb_pad returns an error the skb was freed */
+	if (unlikely(skb->len < 60)) {
+		int pad_len = 60 - skb->len;
+
+		if (skb_pad(skb, pad_len))
+			return true;
+		__skb_put(skb, pad_len);
+	}
+
+	return false;
+}
+
+/* igb_clean_rx_irq -- * packet split */
+static bool igb_clean_rx_irq(struct igb_q_vector *q_vector, int budget)
+{
+	struct igb_ring *rx_ring = q_vector->rx.ring;
+	struct sk_buff *skb = rx_ring->skb;
+	unsigned int total_bytes = 0, total_packets = 0;
+	u16 cleaned_count = igb_desc_unused(rx_ring);
+
+	do {
+		union e1000_adv_rx_desc *rx_desc;
+
+		/* return some buffers to hardware, one at a time is too slow */
+		if (cleaned_count >= IGB_RX_BUFFER_WRITE) {
+			igb_alloc_rx_buffers(rx_ring, cleaned_count);
+			cleaned_count = 0;
+		}
+
+		rx_desc = IGB_RX_DESC(rx_ring, rx_ring->next_to_clean);
+
+		if (!igb_test_staterr(rx_desc, E1000_RXD_STAT_DD))
+			break;
+
+		/*
+		 * This memory barrier is needed to keep us from reading
+		 * any other fields out of the rx_desc until we know the
+		 * RXD_STAT_DD bit is set
+		 */
+		rmb();
+
+		/* retrieve a buffer from the ring */
+		skb = igb_fetch_rx_buffer(rx_ring, rx_desc, skb);
+
+		/* exit if we failed to retrieve a buffer */
+		if (!skb)
+			break;
+
+		cleaned_count++;
+
+		/* fetch next buffer in frame if non-eop */
+		if (igb_is_non_eop(rx_ring, rx_desc))
+			continue;
+
+		/* verify the packet layout is correct */
+		if (igb_cleanup_headers(rx_ring, rx_desc, skb)) {
+			skb = NULL;
+			continue;
+		}
+
+		/* probably a little skewed due to removing CRC */
+		total_bytes += skb->len;
+
+		/* populate checksum, timestamp, VLAN, and protocol */
+		igb_process_skb_fields(rx_ring, rx_desc, skb);
+
+#ifndef IGB_NO_LRO
+		if (igb_can_lro(rx_ring, rx_desc, skb))
+			igb_lro_receive(q_vector, skb);
+		else
+#endif
+#ifdef HAVE_VLAN_RX_REGISTER
+			igb_receive_skb(q_vector, skb);
+#else
+			napi_gro_receive(&q_vector->napi, skb);
+#endif
+#ifndef NETIF_F_GRO
+
+		netdev_ring(rx_ring)->last_rx = jiffies;
+#endif
+
+		/* reset skb pointer */
+		skb = NULL;
+
+		/* update budget accounting */
+		total_packets++;
+	} while (likely(total_packets < budget));
+
+	/* place incomplete frames back on ring for completion */
+	rx_ring->skb = skb;
+
+	rx_ring->rx_stats.packets += total_packets;
+	rx_ring->rx_stats.bytes += total_bytes;
+	q_vector->rx.total_packets += total_packets;
+	q_vector->rx.total_bytes += total_bytes;
+
+	if (cleaned_count)
+		igb_alloc_rx_buffers(rx_ring, cleaned_count);
+
+#ifndef IGB_NO_LRO
+	igb_lro_flush_all(q_vector);
+
+#endif /* IGB_NO_LRO */
+	return total_packets < budget;
+}
+#endif /* CONFIG_IGB_DISABLE_PACKET_SPLIT */
+
+#ifdef CONFIG_IGB_DISABLE_PACKET_SPLIT
+static bool igb_alloc_mapped_skb(struct igb_ring *rx_ring,
+				 struct igb_rx_buffer *bi)
+{
+	struct sk_buff *skb = bi->skb;
+	dma_addr_t dma = bi->dma;
+
+	if (dma)
+		return true;
+
+	if (likely(!skb)) {
+		skb = netdev_alloc_skb_ip_align(netdev_ring(rx_ring),
+						rx_ring->rx_buffer_len);
+		bi->skb = skb;
+		if (!skb) {
+			rx_ring->rx_stats.alloc_failed++;
+			return false;
+		}
+
+		/* initialize skb for ring */
+		skb_record_rx_queue(skb, ring_queue_index(rx_ring));
+	}
+
+	dma = dma_map_single(rx_ring->dev, skb->data,
+			     rx_ring->rx_buffer_len, DMA_FROM_DEVICE);
+
+	/* if mapping failed free memory back to system since
+	 * there isn't much point in holding memory we can't use
+	 */
+	if (dma_mapping_error(rx_ring->dev, dma)) {
+		dev_kfree_skb_any(skb);
+		bi->skb = NULL;
+
+		rx_ring->rx_stats.alloc_failed++;
+		return false;
+	}
+
+	bi->dma = dma;
+	return true;
+}
+
+#else /* CONFIG_IGB_DISABLE_PACKET_SPLIT */
+static bool igb_alloc_mapped_page(struct igb_ring *rx_ring,
+				  struct igb_rx_buffer *bi)
+{
+	struct page *page = bi->page;
+	dma_addr_t dma;
+
+	/* since we are recycling buffers we should seldom need to alloc */
+	if (likely(page))
+		return true;
+
+	/* alloc new page for storage */
+	page = alloc_page(GFP_ATOMIC | __GFP_COLD);
+	if (unlikely(!page)) {
+		rx_ring->rx_stats.alloc_failed++;
+		return false;
+	}
+
+	/* map page for use */
+	dma = dma_map_page(rx_ring->dev, page, 0, PAGE_SIZE, DMA_FROM_DEVICE);
+
+	/*
+	 * if mapping failed free memory back to system since
+	 * there isn't much point in holding memory we can't use
+	 */
+	if (dma_mapping_error(rx_ring->dev, dma)) {
+		__free_page(page);
+
+		rx_ring->rx_stats.alloc_failed++;
+		return false;
+	}
+
+	bi->dma = dma;
+	bi->page = page;
+	bi->page_offset = 0;
+
+	return true;
+}
+
+#endif /* CONFIG_IGB_DISABLE_PACKET_SPLIT */
+/**
+ * igb_alloc_rx_buffers - Replace used receive buffers; packet split
+ * @adapter: address of board private structure
+ **/
+void igb_alloc_rx_buffers(struct igb_ring *rx_ring, u16 cleaned_count)
+{
+	union e1000_adv_rx_desc *rx_desc;
+	struct igb_rx_buffer *bi;
+	u16 i = rx_ring->next_to_use;
+
+	/* nothing to do */
+	if (!cleaned_count)
+		return;
+
+	rx_desc = IGB_RX_DESC(rx_ring, i);
+	bi = &rx_ring->rx_buffer_info[i];
+	i -= rx_ring->count;
+
+	do {
+#ifdef CONFIG_IGB_DISABLE_PACKET_SPLIT
+		if (!igb_alloc_mapped_skb(rx_ring, bi))
+#else
+		if (!igb_alloc_mapped_page(rx_ring, bi))
+#endif /* CONFIG_IGB_DISABLE_PACKET_SPLIT */
+			break;
+
+		/*
+		 * Refresh the desc even if buffer_addrs didn't change
+		 * because each write-back erases this info.
+		 */
+#ifdef CONFIG_IGB_DISABLE_PACKET_SPLIT
+		rx_desc->read.pkt_addr = cpu_to_le64(bi->dma);
+#else
+		rx_desc->read.pkt_addr = cpu_to_le64(bi->dma + bi->page_offset);
+#endif
+
+		rx_desc++;
+		bi++;
+		i++;
+		if (unlikely(!i)) {
+			rx_desc = IGB_RX_DESC(rx_ring, 0);
+			bi = rx_ring->rx_buffer_info;
+			i -= rx_ring->count;
+		}
+
+		/* clear the hdr_addr for the next_to_use descriptor */
+		rx_desc->read.hdr_addr = 0;
+
+		cleaned_count--;
+	} while (cleaned_count);
+
+	i += rx_ring->count;
+
+	if (rx_ring->next_to_use != i) {
+		/* record the next descriptor to use */
+		rx_ring->next_to_use = i;
+
+#ifndef CONFIG_IGB_DISABLE_PACKET_SPLIT
+		/* update next to alloc since we have filled the ring */
+		rx_ring->next_to_alloc = i;
+
+#endif
+		/*
+		 * Force memory writes to complete before letting h/w
+		 * know there are new descriptors to fetch.  (Only
+		 * applicable for weak-ordered memory model archs,
+		 * such as IA-64).
+		 */
+		wmb();
+		writel(i, rx_ring->tail);
+	}
+}
+
+#ifdef SIOCGMIIPHY
+/**
+ * igb_mii_ioctl -
+ * @netdev:
+ * @ifreq:
+ * @cmd:
+ **/
+static int igb_mii_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd)
+{
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	struct mii_ioctl_data *data = if_mii(ifr);
+
+	if (adapter->hw.phy.media_type != e1000_media_type_copper)
+		return -EOPNOTSUPP;
+
+	switch (cmd) {
+	case SIOCGMIIPHY:
+		data->phy_id = adapter->hw.phy.addr;
+		break;
+	case SIOCGMIIREG:
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (e1000_read_phy_reg(&adapter->hw, data->reg_num & 0x1F,
+				   &data->val_out))
+			return -EIO;
+		break;
+	case SIOCSMIIREG:
+	default:
+		return -EOPNOTSUPP;
+	}
+	return E1000_SUCCESS;
+}
+
+#endif
+/**
+ * igb_ioctl -
+ * @netdev:
+ * @ifreq:
+ * @cmd:
+ **/
+static int igb_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd)
+{
+	switch (cmd) {
+#ifdef SIOCGMIIPHY
+	case SIOCGMIIPHY:
+	case SIOCGMIIREG:
+	case SIOCSMIIREG:
+		return igb_mii_ioctl(netdev, ifr, cmd);
+#endif
+#ifdef HAVE_PTP_1588_CLOCK
+	case SIOCSHWTSTAMP:
+		return igb_ptp_hwtstamp_ioctl(netdev, ifr, cmd);
+#endif /* HAVE_PTP_1588_CLOCK */
+#ifdef ETHTOOL_OPS_COMPAT
+	case SIOCETHTOOL:
+		return ethtool_ioctl(ifr);
+#endif
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+s32 e1000_read_pcie_cap_reg(struct e1000_hw *hw, u32 reg, u16 *value)
+{
+	struct igb_adapter *adapter = hw->back;
+	u16 cap_offset;
+
+	cap_offset = pci_find_capability(adapter->pdev, PCI_CAP_ID_EXP);
+	if (!cap_offset)
+		return -E1000_ERR_CONFIG;
+
+	pci_read_config_word(adapter->pdev, cap_offset + reg, value);
+
+	return E1000_SUCCESS;
+}
+
+s32 e1000_write_pcie_cap_reg(struct e1000_hw *hw, u32 reg, u16 *value)
+{
+	struct igb_adapter *adapter = hw->back;
+	u16 cap_offset;
+
+	cap_offset = pci_find_capability(adapter->pdev, PCI_CAP_ID_EXP);
+	if (!cap_offset)
+		return -E1000_ERR_CONFIG;
+
+	pci_write_config_word(adapter->pdev, cap_offset + reg, *value);
+
+	return E1000_SUCCESS;
+}
+
+#ifdef HAVE_VLAN_RX_REGISTER
+static void igb_vlan_mode(struct net_device *netdev, struct vlan_group *vlgrp)
+#else
+void igb_vlan_mode(struct net_device *netdev, u32 features)
+#endif
+{
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	struct e1000_hw *hw = &adapter->hw;
+	u32 ctrl, rctl;
+	int i;
+#ifdef HAVE_VLAN_RX_REGISTER
+	bool enable = !!vlgrp;
+
+	igb_irq_disable(adapter);
+
+	adapter->vlgrp = vlgrp;
+
+	if (!test_bit(__IGB_DOWN, &adapter->state))
+		igb_irq_enable(adapter);
+#else
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
+	bool enable = !!(features & NETIF_F_HW_VLAN_CTAG_RX);
+#else
+	bool enable = !!(features & NETIF_F_HW_VLAN_RX);
+#endif
+#endif
+
+	if (enable) {
+		/* enable VLAN tag insert/strip */
+		ctrl = E1000_READ_REG(hw, E1000_CTRL);
+		ctrl |= E1000_CTRL_VME;
+		E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+
+		/* Disable CFI check */
+		rctl = E1000_READ_REG(hw, E1000_RCTL);
+		rctl &= ~E1000_RCTL_CFIEN;
+		E1000_WRITE_REG(hw, E1000_RCTL, rctl);
+	} else {
+		/* disable VLAN tag insert/strip */
+		ctrl = E1000_READ_REG(hw, E1000_CTRL);
+		ctrl &= ~E1000_CTRL_VME;
+		E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+	}
+
+#ifndef CONFIG_IGB_VMDQ_NETDEV
+	for (i = 0; i < adapter->vmdq_pools; i++) {
+		igb_set_vf_vlan_strip(adapter,
+				      adapter->vfs_allocated_count + i,
+				      enable);
+	}
+
+#else
+	igb_set_vf_vlan_strip(adapter,
+			      adapter->vfs_allocated_count,
+			      enable);
+
+	for (i = 1; i < adapter->vmdq_pools; i++) {
+#ifdef HAVE_VLAN_RX_REGISTER
+		struct igb_vmdq_adapter *vadapter;
+		vadapter = netdev_priv(adapter->vmdq_netdev[i-1]);
+		enable = !!vadapter->vlgrp;
+#else
+		struct net_device *vnetdev;
+		vnetdev = adapter->vmdq_netdev[i-1];
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
+		enable = !!(vnetdev->features & NETIF_F_HW_VLAN_CTAG_RX);
+#else
+		enable = !!(vnetdev->features & NETIF_F_HW_VLAN_RX);
+#endif
+#endif
+		igb_set_vf_vlan_strip(adapter,
+				      adapter->vfs_allocated_count + i,
+				      enable);
+	}
+
+#endif
+	igb_rlpml_set(adapter);
+}
+
+#ifdef HAVE_VLAN_PROTOCOL
+static int igb_vlan_rx_add_vid(struct net_device *netdev, __be16 proto, u16 vid)
+#elif defined HAVE_INT_NDO_VLAN_RX_ADD_VID
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
+static int igb_vlan_rx_add_vid(struct net_device *netdev,
+			       __always_unused __be16 proto, u16 vid)
+#else
+static int igb_vlan_rx_add_vid(struct net_device *netdev, u16 vid)
+#endif
+#else
+static void igb_vlan_rx_add_vid(struct net_device *netdev, u16 vid)
+#endif
+{
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	int pf_id = adapter->vfs_allocated_count;
+
+	/* attempt to add filter to vlvf array */
+	igb_vlvf_set(adapter, vid, TRUE, pf_id);
+
+	/* add the filter since PF can receive vlans w/o entry in vlvf */
+	igb_vfta_set(adapter, vid, TRUE);
+#ifndef HAVE_NETDEV_VLAN_FEATURES
+
+	/* Copy feature flags from netdev to the vlan netdev for this vid.
+	 * This allows things like TSO to bubble down to our vlan device.
+	 * There is no need to update netdev for vlan 0 (DCB), since it
+	 * wouldn't has v_netdev.
+	 */
+	if (adapter->vlgrp) {
+		struct vlan_group *vlgrp = adapter->vlgrp;
+		struct net_device *v_netdev = vlan_group_get_device(vlgrp, vid);
+		if (v_netdev) {
+			v_netdev->features |= netdev->features;
+			vlan_group_set_device(vlgrp, vid, v_netdev);
+		}
+	}
+#endif
+#ifndef HAVE_VLAN_RX_REGISTER
+
+	set_bit(vid, adapter->active_vlans);
+#endif
+#ifdef HAVE_INT_NDO_VLAN_RX_ADD_VID
+	return 0;
+#endif
+}
+
+#ifdef HAVE_VLAN_PROTOCOL
+static int igb_vlan_rx_kill_vid(struct net_device *netdev, __be16 proto, u16 vid)
+#elif defined HAVE_INT_NDO_VLAN_RX_ADD_VID
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
+static int igb_vlan_rx_kill_vid(struct net_device *netdev,
+				__always_unused __be16 proto, u16 vid)
+#else
+static int igb_vlan_rx_kill_vid(struct net_device *netdev, u16 vid)
+#endif
+#else
+static void igb_vlan_rx_kill_vid(struct net_device *netdev, u16 vid)
+#endif
+{
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	int pf_id = adapter->vfs_allocated_count;
+	s32 err;
+
+#ifdef HAVE_VLAN_RX_REGISTER
+	igb_irq_disable(adapter);
+
+	vlan_group_set_device(adapter->vlgrp, vid, NULL);
+
+	if (!test_bit(__IGB_DOWN, &adapter->state))
+		igb_irq_enable(adapter);
+
+#endif /* HAVE_VLAN_RX_REGISTER */
+	/* remove vlan from VLVF table array */
+	err = igb_vlvf_set(adapter, vid, FALSE, pf_id);
+
+	/* if vid was not present in VLVF just remove it from table */
+	if (err)
+		igb_vfta_set(adapter, vid, FALSE);
+#ifndef HAVE_VLAN_RX_REGISTER
+
+	clear_bit(vid, adapter->active_vlans);
+#endif
+#ifdef HAVE_INT_NDO_VLAN_RX_ADD_VID
+	return 0;
+#endif
+}
+
+static void igb_restore_vlan(struct igb_adapter *adapter)
+{
+#ifdef HAVE_VLAN_RX_REGISTER
+	igb_vlan_mode(adapter->netdev, adapter->vlgrp);
+
+	if (adapter->vlgrp) {
+		u16 vid;
+		for (vid = 0; vid < VLAN_N_VID; vid++) {
+			if (!vlan_group_get_device(adapter->vlgrp, vid))
+				continue;
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
+			igb_vlan_rx_add_vid(adapter->netdev,
+					    htons(ETH_P_8021Q), vid);
+#else
+			igb_vlan_rx_add_vid(adapter->netdev, vid);
+#endif
+		}
+	}
+#else
+	u16 vid;
+
+	igb_vlan_mode(adapter->netdev, adapter->netdev->features);
+
+	for_each_set_bit(vid, adapter->active_vlans, VLAN_N_VID)
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
+		igb_vlan_rx_add_vid(adapter->netdev,
+				    htons(ETH_P_8021Q), vid);
+#else
+		igb_vlan_rx_add_vid(adapter->netdev, vid);
+#endif
+#endif
+}
+
+int igb_set_spd_dplx(struct igb_adapter *adapter, u16 spddplx)
+{
+	struct pci_dev *pdev = adapter->pdev;
+	struct e1000_mac_info *mac = &adapter->hw.mac;
+
+	mac->autoneg = 0;
+
+	/* SerDes device's does not support 10Mbps Full/duplex
+	 * and 100Mbps Half duplex
+	 */
+	if (adapter->hw.phy.media_type == e1000_media_type_internal_serdes) {
+		switch (spddplx) {
+		case SPEED_10 + DUPLEX_HALF:
+		case SPEED_10 + DUPLEX_FULL:
+		case SPEED_100 + DUPLEX_HALF:
+			dev_err(pci_dev_to_dev(pdev),
+				"Unsupported Speed/Duplex configuration\n");
+			return -EINVAL;
+		default:
+			break;
+		}
+	}
+
+	switch (spddplx) {
+	case SPEED_10 + DUPLEX_HALF:
+		mac->forced_speed_duplex = ADVERTISE_10_HALF;
+		break;
+	case SPEED_10 + DUPLEX_FULL:
+		mac->forced_speed_duplex = ADVERTISE_10_FULL;
+		break;
+	case SPEED_100 + DUPLEX_HALF:
+		mac->forced_speed_duplex = ADVERTISE_100_HALF;
+		break;
+	case SPEED_100 + DUPLEX_FULL:
+		mac->forced_speed_duplex = ADVERTISE_100_FULL;
+		break;
+	case SPEED_1000 + DUPLEX_FULL:
+		mac->autoneg = 1;
+		adapter->hw.phy.autoneg_advertised = ADVERTISE_1000_FULL;
+		break;
+	case SPEED_1000 + DUPLEX_HALF: /* not supported */
+	default:
+		dev_err(pci_dev_to_dev(pdev), "Unsupported Speed/Duplex configuration\n");
+		return -EINVAL;
+	}
+
+	/* clear MDI, MDI(-X) override is only allowed when autoneg enabled */
+	adapter->hw.phy.mdix = AUTO_ALL_MODES;
+
+	return 0;
+}
+
+static int __igb_shutdown(struct pci_dev *pdev, bool *enable_wake,
+			  bool runtime)
+{
+	struct net_device *netdev = pci_get_drvdata(pdev);
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	struct e1000_hw *hw = &adapter->hw;
+	u32 ctrl, rctl, status;
+	u32 wufc = runtime ? E1000_WUFC_LNKC : adapter->wol;
+#ifdef CONFIG_PM
+	int retval = 0;
+#endif
+
+	netif_device_detach(netdev);
+
+	status = E1000_READ_REG(hw, E1000_STATUS);
+	if (status & E1000_STATUS_LU)
+		wufc &= ~E1000_WUFC_LNKC;
+
+	if (netif_running(netdev))
+		__igb_close(netdev, true);
+
+	igb_clear_interrupt_scheme(adapter);
+
+#ifdef CONFIG_PM
+	retval = pci_save_state(pdev);
+	if (retval)
+		return retval;
+#endif
+
+	if (wufc) {
+		igb_setup_rctl(adapter);
+		igb_set_rx_mode(netdev);
+
+		/* turn on all-multi mode if wake on multicast is enabled */
+		if (wufc & E1000_WUFC_MC) {
+			rctl = E1000_READ_REG(hw, E1000_RCTL);
+			rctl |= E1000_RCTL_MPE;
+			E1000_WRITE_REG(hw, E1000_RCTL, rctl);
+		}
+
+		ctrl = E1000_READ_REG(hw, E1000_CTRL);
+		/* phy power management enable */
+		#define E1000_CTRL_EN_PHY_PWR_MGMT 0x00200000
+		ctrl |= E1000_CTRL_ADVD3WUC;
+		E1000_WRITE_REG(hw, E1000_CTRL, ctrl);
+
+		/* Allow time for pending master requests to run */
+		e1000_disable_pcie_master(hw);
+
+		E1000_WRITE_REG(hw, E1000_WUC, E1000_WUC_PME_EN);
+		E1000_WRITE_REG(hw, E1000_WUFC, wufc);
+	} else {
+		E1000_WRITE_REG(hw, E1000_WUC, 0);
+		E1000_WRITE_REG(hw, E1000_WUFC, 0);
+	}
+
+	*enable_wake = wufc || adapter->en_mng_pt;
+	if (!*enable_wake)
+		igb_power_down_link(adapter);
+	else
+		igb_power_up_link(adapter);
+
+	/* Release control of h/w to f/w.  If f/w is AMT enabled, this
+	 * would have already happened in close and is redundant. */
+	igb_release_hw_control(adapter);
+
+	pci_disable_device(pdev);
+
+	return 0;
+}
+
+#ifdef CONFIG_PM
+#ifdef HAVE_SYSTEM_SLEEP_PM_OPS
+static int igb_suspend(struct device *dev)
+#else
+static int igb_suspend(struct pci_dev *pdev, pm_message_t state)
+#endif /* HAVE_SYSTEM_SLEEP_PM_OPS */
+{
+#ifdef HAVE_SYSTEM_SLEEP_PM_OPS
+	struct pci_dev *pdev = to_pci_dev(dev);
+#endif /* HAVE_SYSTEM_SLEEP_PM_OPS */
+	int retval;
+	bool wake;
+
+	retval = __igb_shutdown(pdev, &wake, 0);
+	if (retval)
+		return retval;
+
+	if (wake) {
+		pci_prepare_to_sleep(pdev);
+	} else {
+		pci_wake_from_d3(pdev, false);
+		pci_set_power_state(pdev, PCI_D3hot);
+	}
+
+	return 0;
+}
+
+#ifdef HAVE_SYSTEM_SLEEP_PM_OPS
+static int igb_resume(struct device *dev)
+#else
+static int igb_resume(struct pci_dev *pdev)
+#endif /* HAVE_SYSTEM_SLEEP_PM_OPS */
+{
+#ifdef HAVE_SYSTEM_SLEEP_PM_OPS
+	struct pci_dev *pdev = to_pci_dev(dev);
+#endif /* HAVE_SYSTEM_SLEEP_PM_OPS */
+	struct net_device *netdev = pci_get_drvdata(pdev);
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	struct e1000_hw *hw = &adapter->hw;
+	u32 err;
+
+	pci_set_power_state(pdev, PCI_D0);
+	pci_restore_state(pdev);
+	pci_save_state(pdev);
+
+	err = pci_enable_device_mem(pdev);
+	if (err) {
+		dev_err(pci_dev_to_dev(pdev),
+			"igb: Cannot enable PCI device from suspend\n");
+		return err;
+	}
+	pci_set_master(pdev);
+
+	pci_enable_wake(pdev, PCI_D3hot, 0);
+	pci_enable_wake(pdev, PCI_D3cold, 0);
+
+	if (igb_init_interrupt_scheme(adapter, true)) {
+		dev_err(pci_dev_to_dev(pdev), "Unable to allocate memory for queues\n");
+		return -ENOMEM;
+	}
+
+	igb_reset(adapter);
+
+	/* let the f/w know that the h/w is now under the control of the
+	 * driver. */
+	igb_get_hw_control(adapter);
+
+	E1000_WRITE_REG(hw, E1000_WUS, ~0);
+
+	if (netdev->flags & IFF_UP) {
+		rtnl_lock();
+		err = __igb_open(netdev, true);
+		rtnl_unlock();
+		if (err)
+			return err;
+	}
+
+	netif_device_attach(netdev);
+
+	return 0;
+}
+
+#ifdef CONFIG_PM_RUNTIME
+#ifdef HAVE_SYSTEM_SLEEP_PM_OPS
+static int igb_runtime_idle(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev);
+	struct net_device *netdev = pci_get_drvdata(pdev);
+	struct igb_adapter *adapter = netdev_priv(netdev);
+
+	if (!igb_has_link(adapter))
+		pm_schedule_suspend(dev, MSEC_PER_SEC * 5);
+
+	return -EBUSY;
+}
+
+static int igb_runtime_suspend(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev);
+	int retval;
+	bool wake;
+
+	retval = __igb_shutdown(pdev, &wake, 1);
+	if (retval)
+		return retval;
+
+	if (wake) {
+		pci_prepare_to_sleep(pdev);
+	} else {
+		pci_wake_from_d3(pdev, false);
+		pci_set_power_state(pdev, PCI_D3hot);
+	}
+
+	return 0;
+}
+
+static int igb_runtime_resume(struct device *dev)
+{
+	return igb_resume(dev);
+}
+#endif /* HAVE_SYSTEM_SLEEP_PM_OPS */
+#endif /* CONFIG_PM_RUNTIME */
+#endif /* CONFIG_PM */
+
+#ifdef USE_REBOOT_NOTIFIER
+/* only want to do this for 2.4 kernels? */
+static int igb_notify_reboot(struct notifier_block *nb, unsigned long event,
+                             void *p)
+{
+	struct pci_dev *pdev = NULL;
+	bool wake;
+
+	switch (event) {
+	case SYS_DOWN:
+	case SYS_HALT:
+	case SYS_POWER_OFF:
+		while ((pdev = pci_find_device(PCI_ANY_ID, PCI_ANY_ID, pdev))) {
+			if (pci_dev_driver(pdev) == &igb_driver) {
+				__igb_shutdown(pdev, &wake, 0);
+				if (event == SYS_POWER_OFF) {
+					pci_wake_from_d3(pdev, wake);
+					pci_set_power_state(pdev, PCI_D3hot);
+				}
+			}
+		}
+	}
+	return NOTIFY_DONE;
+}
+#else
+static void igb_shutdown(struct pci_dev *pdev)
+{
+	bool wake = false;
+
+	__igb_shutdown(pdev, &wake, 0);
+
+	if (system_state == SYSTEM_POWER_OFF) {
+		pci_wake_from_d3(pdev, wake);
+		pci_set_power_state(pdev, PCI_D3hot);
+	}
+}
+#endif /* USE_REBOOT_NOTIFIER */
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+/*
+ * Polling 'interrupt' - used by things like netconsole to send skbs
+ * without having to re-enable interrupts. It's not called while
+ * the interrupt routine is executing.
+ */
+static void igb_netpoll(struct net_device *netdev)
+{
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	struct e1000_hw *hw = &adapter->hw;
+	struct igb_q_vector *q_vector;
+	int i;
+
+	for (i = 0; i < adapter->num_q_vectors; i++) {
+		q_vector = adapter->q_vector[i];
+		if (adapter->msix_entries)
+			E1000_WRITE_REG(hw, E1000_EIMC, q_vector->eims_value);
+		else
+			igb_irq_disable(adapter);
+		napi_schedule(&q_vector->napi);
+	}
+}
+#endif /* CONFIG_NET_POLL_CONTROLLER */
+
+#ifdef HAVE_PCI_ERS
+#define E1000_DEV_ID_82576_VF 0x10CA
+/**
+ * igb_io_error_detected - called when PCI error is detected
+ * @pdev: Pointer to PCI device
+ * @state: The current pci connection state
+ *
+ * This function is called after a PCI bus error affecting
+ * this device has been detected.
+ */
+static pci_ers_result_t igb_io_error_detected(struct pci_dev *pdev,
+					      pci_channel_state_t state)
+{
+	struct net_device *netdev = pci_get_drvdata(pdev);
+	struct igb_adapter *adapter = netdev_priv(netdev);
+
+#ifdef CONFIG_PCI_IOV__UNUSED
+	struct pci_dev *bdev, *vfdev;
+	u32 dw0, dw1, dw2, dw3;
+	int vf, pos;
+	u16 req_id, pf_func;
+
+	if (!(adapter->flags & IGB_FLAG_DETECT_BAD_DMA))
+		goto skip_bad_vf_detection;
+
+	bdev = pdev->bus->self;
+	while (bdev && (pci_pcie_type(bdev) != PCI_EXP_TYPE_ROOT_PORT))
+		bdev = bdev->bus->self;
+
+	if (!bdev)
+		goto skip_bad_vf_detection;
+
+	pos = pci_find_ext_capability(bdev, PCI_EXT_CAP_ID_ERR);
+	if (!pos)
+		goto skip_bad_vf_detection;
+
+	pci_read_config_dword(bdev, pos + PCI_ERR_HEADER_LOG, &dw0);
+	pci_read_config_dword(bdev, pos + PCI_ERR_HEADER_LOG + 4, &dw1);
+	pci_read_config_dword(bdev, pos + PCI_ERR_HEADER_LOG + 8, &dw2);
+	pci_read_config_dword(bdev, pos + PCI_ERR_HEADER_LOG + 12, &dw3);
+
+	req_id = dw1 >> 16;
+	/* On the 82576 if bit 7 of the requestor ID is set then it's a VF */
+	if (!(req_id & 0x0080))
+		goto skip_bad_vf_detection;
+
+	pf_func = req_id & 0x01;
+	if ((pf_func & 1) == (pdev->devfn & 1)) {
+
+		vf = (req_id & 0x7F) >> 1;
+		dev_err(pci_dev_to_dev(pdev),
+			"VF %d has caused a PCIe error\n", vf);
+		dev_err(pci_dev_to_dev(pdev),
+			"TLP: dw0: %8.8x\tdw1: %8.8x\tdw2: "
+			"%8.8x\tdw3: %8.8x\n",
+			dw0, dw1, dw2, dw3);
+
+		/* Find the pci device of the offending VF */
+		vfdev = pci_get_device(PCI_VENDOR_ID_INTEL,
+				       E1000_DEV_ID_82576_VF, NULL);
+		while (vfdev) {
+			if (vfdev->devfn == (req_id & 0xFF))
+				break;
+			vfdev = pci_get_device(PCI_VENDOR_ID_INTEL,
+					       E1000_DEV_ID_82576_VF, vfdev);
+		}
+		/*
+		 * There's a slim chance the VF could have been hot plugged,
+		 * so if it is no longer present we don't need to issue the
+		 * VFLR.  Just clean up the AER in that case.
+		 */
+		if (vfdev) {
+			dev_err(pci_dev_to_dev(pdev),
+				"Issuing VFLR to VF %d\n", vf);
+			pci_write_config_dword(vfdev, 0xA8, 0x00008000);
+		}
+
+		pci_cleanup_aer_uncorrect_error_status(pdev);
+	}
+
+	/*
+	 * Even though the error may have occurred on the other port
+	 * we still need to increment the vf error reference count for
+	 * both ports because the I/O resume function will be called
+	 * for both of them.
+	 */
+	adapter->vferr_refcount++;
+
+	return PCI_ERS_RESULT_RECOVERED;
+
+skip_bad_vf_detection:
+#endif /* CONFIG_PCI_IOV */
+
+	netif_device_detach(netdev);
+
+	if (state == pci_channel_io_perm_failure)
+		return PCI_ERS_RESULT_DISCONNECT;
+
+	if (netif_running(netdev))
+		igb_down(adapter);
+	pci_disable_device(pdev);
+
+	/* Request a slot slot reset. */
+	return PCI_ERS_RESULT_NEED_RESET;
+}
+
+/**
+ * igb_io_slot_reset - called after the pci bus has been reset.
+ * @pdev: Pointer to PCI device
+ *
+ * Restart the card from scratch, as if from a cold-boot. Implementation
+ * resembles the first-half of the igb_resume routine.
+ */
+static pci_ers_result_t igb_io_slot_reset(struct pci_dev *pdev)
+{
+	struct net_device *netdev = pci_get_drvdata(pdev);
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	struct e1000_hw *hw = &adapter->hw;
+	pci_ers_result_t result;
+
+	if (pci_enable_device_mem(pdev)) {
+		dev_err(pci_dev_to_dev(pdev),
+			"Cannot re-enable PCI device after reset.\n");
+		result = PCI_ERS_RESULT_DISCONNECT;
+	} else {
+		pci_set_master(pdev);
+		pci_restore_state(pdev);
+		pci_save_state(pdev);
+
+		pci_enable_wake(pdev, PCI_D3hot, 0);
+		pci_enable_wake(pdev, PCI_D3cold, 0);
+
+		schedule_work(&adapter->reset_task);
+		E1000_WRITE_REG(hw, E1000_WUS, ~0);
+		result = PCI_ERS_RESULT_RECOVERED;
+	}
+
+	pci_cleanup_aer_uncorrect_error_status(pdev);
+
+	return result;
+}
+
+/**
+ * igb_io_resume - called when traffic can start flowing again.
+ * @pdev: Pointer to PCI device
+ *
+ * This callback is called when the error recovery driver tells us that
+ * its OK to resume normal operation. Implementation resembles the
+ * second-half of the igb_resume routine.
+ */
+static void igb_io_resume(struct pci_dev *pdev)
+{
+	struct net_device *netdev = pci_get_drvdata(pdev);
+	struct igb_adapter *adapter = netdev_priv(netdev);
+
+	if (adapter->vferr_refcount) {
+		dev_info(pci_dev_to_dev(pdev), "Resuming after VF err\n");
+		adapter->vferr_refcount--;
+		return;
+	}
+
+	if (netif_running(netdev)) {
+		if (igb_up(adapter)) {
+			dev_err(pci_dev_to_dev(pdev), "igb_up failed after reset\n");
+			return;
+		}
+	}
+
+	netif_device_attach(netdev);
+
+	/* let the f/w know that the h/w is now under the control of the
+	 * driver. */
+	igb_get_hw_control(adapter);
+}
+
+#endif /* HAVE_PCI_ERS */
+
+int igb_add_mac_filter(struct igb_adapter *adapter, u8 *addr, u16 queue)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	int i;
+
+	if (is_zero_ether_addr(addr))
+		return 0;
+
+	for (i = 0; i < hw->mac.rar_entry_count; i++) {
+		if (adapter->mac_table[i].state & IGB_MAC_STATE_IN_USE)
+			continue;
+		adapter->mac_table[i].state = (IGB_MAC_STATE_MODIFIED |
+						   IGB_MAC_STATE_IN_USE);
+		memcpy(adapter->mac_table[i].addr, addr, ETH_ALEN);
+		adapter->mac_table[i].queue = queue;
+		igb_sync_mac_table(adapter);
+		return 0;
+	}
+	return -ENOMEM;
+}
+int igb_del_mac_filter(struct igb_adapter *adapter, u8* addr, u16 queue)
+{
+	/* search table for addr, if found, set to 0 and sync */
+	int i;
+	struct e1000_hw *hw = &adapter->hw;
+
+	if (is_zero_ether_addr(addr))
+		return 0;
+	for (i = 0; i < hw->mac.rar_entry_count; i++) {
+		if (ether_addr_equal(addr, adapter->mac_table[i].addr) &&
+		    adapter->mac_table[i].queue == queue) {
+			adapter->mac_table[i].state = IGB_MAC_STATE_MODIFIED;
+			memset(adapter->mac_table[i].addr, 0, ETH_ALEN);
+			adapter->mac_table[i].queue = 0;
+			igb_sync_mac_table(adapter);
+			return 0;
+		}
+	}
+	return -ENOMEM;
+}
+static int igb_set_vf_mac(struct igb_adapter *adapter,
+                          int vf, unsigned char *mac_addr)
+{
+	igb_del_mac_filter(adapter, adapter->vf_data[vf].vf_mac_addresses, vf);
+	memcpy(adapter->vf_data[vf].vf_mac_addresses, mac_addr, ETH_ALEN);
+
+	igb_add_mac_filter(adapter, mac_addr, vf);
+
+	return 0;
+}
+
+#ifdef IFLA_VF_MAX
+static int igb_ndo_set_vf_mac(struct net_device *netdev, int vf, u8 *mac)
+{
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	if (!is_valid_ether_addr(mac) || (vf >= adapter->vfs_allocated_count))
+		return -EINVAL;
+	adapter->vf_data[vf].flags |= IGB_VF_FLAG_PF_SET_MAC;
+	dev_info(&adapter->pdev->dev, "setting MAC %pM on VF %d\n", mac, vf);
+	dev_info(&adapter->pdev->dev, "Reload the VF driver to make this"
+				      " change effective.\n");
+	if (test_bit(__IGB_DOWN, &adapter->state)) {
+		dev_warn(&adapter->pdev->dev, "The VF MAC address has been set,"
+			 " but the PF device is not up.\n");
+		dev_warn(&adapter->pdev->dev, "Bring the PF device up before"
+			 " attempting to use the VF device.\n");
+	}
+	return igb_set_vf_mac(adapter, vf, mac);
+}
+
+static int igb_link_mbps(int internal_link_speed)
+{
+	switch (internal_link_speed) {
+	case SPEED_100:
+		return 100;
+	case SPEED_1000:
+		return 1000;
+	case SPEED_2500:
+		return 2500;
+	default:
+		return 0;
+	}
+}
+
+static void igb_set_vf_rate_limit(struct e1000_hw *hw, int vf, int tx_rate,
+			int link_speed)
+{
+	int rf_dec, rf_int;
+	u32 bcnrc_val;
+
+	if (tx_rate != 0) {
+		/* Calculate the rate factor values to set */
+		rf_int = link_speed / tx_rate;
+		rf_dec = (link_speed - (rf_int * tx_rate));
+		rf_dec = (rf_dec * (1<<E1000_RTTBCNRC_RF_INT_SHIFT)) / tx_rate;
+
+		bcnrc_val = E1000_RTTBCNRC_RS_ENA;
+		bcnrc_val |= ((rf_int<<E1000_RTTBCNRC_RF_INT_SHIFT) &
+				E1000_RTTBCNRC_RF_INT_MASK);
+		bcnrc_val |= (rf_dec & E1000_RTTBCNRC_RF_DEC_MASK);
+	} else {
+		bcnrc_val = 0;
+	}
+
+	E1000_WRITE_REG(hw, E1000_RTTDQSEL, vf); /* vf X uses queue X */
+	/*
+	 * Set global transmit compensation time to the MMW_SIZE in RTTBCNRM
+	 * register. MMW_SIZE=0x014 if 9728-byte jumbo is supported.
+	 */
+	E1000_WRITE_REG(hw, E1000_RTTBCNRM(0), 0x14);
+	E1000_WRITE_REG(hw, E1000_RTTBCNRC, bcnrc_val);
+}
+
+static void igb_check_vf_rate_limit(struct igb_adapter *adapter)
+{
+	int actual_link_speed, i;
+	bool reset_rate = false;
+
+	/* VF TX rate limit was not set */
+	if ((adapter->vf_rate_link_speed == 0) ||
+		(adapter->hw.mac.type != e1000_82576))
+		return;
+
+	actual_link_speed = igb_link_mbps(adapter->link_speed);
+	if (actual_link_speed != adapter->vf_rate_link_speed) {
+		reset_rate = true;
+		adapter->vf_rate_link_speed = 0;
+		dev_info(&adapter->pdev->dev,
+		"Link speed has been changed. VF Transmit rate is disabled\n");
+	}
+
+	for (i = 0; i < adapter->vfs_allocated_count; i++) {
+		if (reset_rate)
+			adapter->vf_data[i].tx_rate = 0;
+
+		igb_set_vf_rate_limit(&adapter->hw, i,
+			adapter->vf_data[i].tx_rate, actual_link_speed);
+	}
+}
+
+#ifdef HAVE_VF_MIN_MAX_TXRATE
+static int igb_ndo_set_vf_bw(struct net_device *netdev, int vf, int min_tx_rate,
+			     int tx_rate)
+#else /* HAVE_VF_MIN_MAX_TXRATE */
+static int igb_ndo_set_vf_bw(struct net_device *netdev, int vf, int tx_rate)
+#endif /* HAVE_VF_MIN_MAX_TXRATE */
+{
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	struct e1000_hw *hw = &adapter->hw;
+	int actual_link_speed;
+
+	if (hw->mac.type != e1000_82576)
+		return -EOPNOTSUPP;
+
+#ifdef HAVE_VF_MIN_MAX_TXRATE
+	if (min_tx_rate)
+		return -EINVAL;
+#endif /* HAVE_VF_MIN_MAX_TXRATE */
+
+	actual_link_speed = igb_link_mbps(adapter->link_speed);
+	if ((vf >= adapter->vfs_allocated_count) ||
+		(!(E1000_READ_REG(hw, E1000_STATUS) & E1000_STATUS_LU)) ||
+		(tx_rate < 0) || (tx_rate > actual_link_speed))
+		return -EINVAL;
+
+	adapter->vf_rate_link_speed = actual_link_speed;
+	adapter->vf_data[vf].tx_rate = (u16)tx_rate;
+	igb_set_vf_rate_limit(hw, vf, tx_rate, actual_link_speed);
+
+	return 0;
+}
+
+static int igb_ndo_get_vf_config(struct net_device *netdev,
+				 int vf, struct ifla_vf_info *ivi)
+{
+	struct igb_adapter *adapter = netdev_priv(netdev);
+	if (vf >= adapter->vfs_allocated_count)
+		return -EINVAL;
+	ivi->vf = vf;
+	memcpy(&ivi->mac, adapter->vf_data[vf].vf_mac_addresses, ETH_ALEN);
+#ifdef HAVE_VF_MIN_MAX_TXRATE
+	ivi->max_tx_rate = adapter->vf_data[vf].tx_rate;
+	ivi->min_tx_rate = 0;
+#else /* HAVE_VF_MIN_MAX_TXRATE */
+	ivi->tx_rate = adapter->vf_data[vf].tx_rate;
+#endif /* HAVE_VF_MIN_MAX_TXRATE */
+	ivi->vlan = adapter->vf_data[vf].pf_vlan;
+	ivi->qos = adapter->vf_data[vf].pf_qos;
+#ifdef HAVE_VF_SPOOFCHK_CONFIGURE
+	ivi->spoofchk = adapter->vf_data[vf].spoofchk_enabled;
+#endif
+	return 0;
+}
+#endif
+static void igb_vmm_control(struct igb_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	int count;
+	u32 reg;
+
+	switch (hw->mac.type) {
+	case e1000_82575:
+	default:
+		/* replication is not supported for 82575 */
+		return;
+	case e1000_82576:
+		/* notify HW that the MAC is adding vlan tags */
+		reg = E1000_READ_REG(hw, E1000_DTXCTL);
+		reg |= (E1000_DTXCTL_VLAN_ADDED |
+			E1000_DTXCTL_SPOOF_INT);
+		E1000_WRITE_REG(hw, E1000_DTXCTL, reg);
+	case e1000_82580:
+		/* enable replication vlan tag stripping */
+		reg = E1000_READ_REG(hw, E1000_RPLOLR);
+		reg |= E1000_RPLOLR_STRVLAN;
+		E1000_WRITE_REG(hw, E1000_RPLOLR, reg);
+	case e1000_i350:
+	case e1000_i354:
+		/* none of the above registers are supported by i350 */
+		break;
+	}
+
+	/* Enable Malicious Driver Detection */
+	if ((adapter->vfs_allocated_count) &&
+	    (adapter->mdd)) {
+		if (hw->mac.type == e1000_i350)
+			igb_enable_mdd(adapter);
+	}
+
+		/* enable replication and loopback support */
+		count = adapter->vfs_allocated_count || adapter->vmdq_pools;
+		if (adapter->flags & IGB_FLAG_LOOPBACK_ENABLE && count)
+			e1000_vmdq_set_loopback_pf(hw, 1);
+		e1000_vmdq_set_anti_spoofing_pf(hw,
+			adapter->vfs_allocated_count || adapter->vmdq_pools,
+			adapter->vfs_allocated_count);
+	e1000_vmdq_set_replication_pf(hw, adapter->vfs_allocated_count ||
+				      adapter->vmdq_pools);
+}
+
+static void igb_init_fw(struct igb_adapter *adapter)
+{
+	struct e1000_fw_drv_info fw_cmd;
+	struct e1000_hw *hw = &adapter->hw;
+	int i;
+	u16 mask;
+
+	if (hw->mac.type == e1000_i210)
+		mask = E1000_SWFW_EEP_SM;
+	else
+		mask = E1000_SWFW_PHY0_SM;
+	/* i211 parts do not support this feature */
+	if (hw->mac.type == e1000_i211)
+		hw->mac.arc_subsystem_valid = false;
+
+	if (!hw->mac.ops.acquire_swfw_sync(hw, mask)) {
+		for (i = 0; i <= FW_MAX_RETRIES; i++) {
+			E1000_WRITE_REG(hw, E1000_FWSTS, E1000_FWSTS_FWRI);
+			fw_cmd.hdr.cmd = FW_CMD_DRV_INFO;
+			fw_cmd.hdr.buf_len = FW_CMD_DRV_INFO_LEN;
+			fw_cmd.hdr.cmd_or_resp.cmd_resv = FW_CMD_RESERVED;
+			fw_cmd.port_num = hw->bus.func;
+			fw_cmd.drv_version = FW_FAMILY_DRV_VER;
+			fw_cmd.hdr.checksum = 0;
+			fw_cmd.hdr.checksum = e1000_calculate_checksum((u8 *)&fw_cmd,
+			                                           (FW_HDR_LEN +
+			                                            fw_cmd.hdr.buf_len));
+			 e1000_host_interface_command(hw, (u8*)&fw_cmd,
+			                             sizeof(fw_cmd));
+			if (fw_cmd.hdr.cmd_or_resp.ret_status == FW_STATUS_SUCCESS)
+				break;
+		}
+	} else
+		dev_warn(pci_dev_to_dev(adapter->pdev),
+			 "Unable to get semaphore, firmware init failed.\n");
+	hw->mac.ops.release_swfw_sync(hw, mask);
+}
+
+static void igb_init_dmac(struct igb_adapter *adapter, u32 pba)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 dmac_thr;
+	u16 hwm;
+	u32 status;
+
+	if (hw->mac.type == e1000_i211)
+		return;
+
+	if (hw->mac.type > e1000_82580) {
+		if (adapter->dmac != IGB_DMAC_DISABLE) {
+			u32 reg;
+
+			/* force threshold to 0.  */
+			E1000_WRITE_REG(hw, E1000_DMCTXTH, 0);
+
+			/*
+			 * DMA Coalescing high water mark needs to be greater
+			 * than the Rx threshold. Set hwm to PBA - max frame
+			 * size in 16B units, capping it at PBA - 6KB.
+			 */
+			hwm = 64 * pba - adapter->max_frame_size / 16;
+			if (hwm < 64 * (pba - 6))
+				hwm = 64 * (pba - 6);
+			reg = E1000_READ_REG(hw, E1000_FCRTC);
+			reg &= ~E1000_FCRTC_RTH_COAL_MASK;
+			reg |= ((hwm << E1000_FCRTC_RTH_COAL_SHIFT)
+				& E1000_FCRTC_RTH_COAL_MASK);
+			E1000_WRITE_REG(hw, E1000_FCRTC, reg);
+
+			/*
+			 * Set the DMA Coalescing Rx threshold to PBA - 2 * max
+			 * frame size, capping it at PBA - 10KB.
+			 */
+			dmac_thr = pba - adapter->max_frame_size / 512;
+			if (dmac_thr < pba - 10)
+				dmac_thr = pba - 10;
+			reg = E1000_READ_REG(hw, E1000_DMACR);
+			reg &= ~E1000_DMACR_DMACTHR_MASK;
+			reg |= ((dmac_thr << E1000_DMACR_DMACTHR_SHIFT)
+				& E1000_DMACR_DMACTHR_MASK);
+
+			/* transition to L0x or L1 if available..*/
+			reg |= (E1000_DMACR_DMAC_EN | E1000_DMACR_DMAC_LX_MASK);
+
+			/* Check if status is 2.5Gb backplane connection
+			 * before configuration of watchdog timer, which is
+			 * in msec values in 12.8usec intervals
+			 * watchdog timer= msec values in 32usec intervals
+			 * for non 2.5Gb connection
+			 */
+			if (hw->mac.type == e1000_i354) {
+				status = E1000_READ_REG(hw, E1000_STATUS);
+				if ((status & E1000_STATUS_2P5_SKU) &&
+				    (!(status & E1000_STATUS_2P5_SKU_OVER)))
+					reg |= ((adapter->dmac * 5) >> 6);
+				else
+					reg |= ((adapter->dmac) >> 5);
+			} else {
+				reg |= ((adapter->dmac) >> 5);
+			}
+
+			/*
+			 * Disable BMC-to-OS Watchdog enable
+			 * on devices that support OS-to-BMC
+			 */
+			if (hw->mac.type != e1000_i354)
+				reg &= ~E1000_DMACR_DC_BMC2OSW_EN;
+			E1000_WRITE_REG(hw, E1000_DMACR, reg);
+
+			/* no lower threshold to disable coalescing(smart fifb)-UTRESH=0*/
+			E1000_WRITE_REG(hw, E1000_DMCRTRH, 0);
+
+			/* This sets the time to wait before requesting
+			 * transition to low power state to number of usecs
+			 * needed to receive 1 512 byte frame at gigabit
+			 * line rate. On i350 device, time to make transition
+			 * to Lx state is delayed by 4 usec with flush disable
+			 * bit set to avoid losing mailbox interrupts
+			 */
+			reg = E1000_READ_REG(hw, E1000_DMCTLX);
+			if (hw->mac.type == e1000_i350)
+				reg |= IGB_DMCTLX_DCFLUSH_DIS;
+
+			/* in 2.5Gb connection, TTLX unit is 0.4 usec
+			 * which is 0x4*2 = 0xA. But delay is still 4 usec
+			 */
+			if (hw->mac.type == e1000_i354) {
+				status = E1000_READ_REG(hw, E1000_STATUS);
+				if ((status & E1000_STATUS_2P5_SKU) &&
+				    (!(status & E1000_STATUS_2P5_SKU_OVER)))
+					reg |= 0xA;
+				else
+					reg |= 0x4;
+			} else {
+				reg |= 0x4;
+			}
+			E1000_WRITE_REG(hw, E1000_DMCTLX, reg);
+
+			/* free space in tx packet buffer to wake from DMA coal */
+			E1000_WRITE_REG(hw, E1000_DMCTXTH, (IGB_MIN_TXPBSIZE -
+				(IGB_TX_BUF_4096 + adapter->max_frame_size)) >> 6);
+
+			/* make low power state decision controlled by DMA coal */
+			reg = E1000_READ_REG(hw, E1000_PCIEMISC);
+			reg &= ~E1000_PCIEMISC_LX_DECISION;
+			E1000_WRITE_REG(hw, E1000_PCIEMISC, reg);
+		} /* endif adapter->dmac is not disabled */
+	} else if (hw->mac.type == e1000_82580) {
+		u32 reg = E1000_READ_REG(hw, E1000_PCIEMISC);
+		E1000_WRITE_REG(hw, E1000_PCIEMISC,
+		                reg & ~E1000_PCIEMISC_LX_DECISION);
+		E1000_WRITE_REG(hw, E1000_DMACR, 0);
+	}
+}
+
+#ifdef HAVE_I2C_SUPPORT
+/*  igb_read_i2c_byte - Reads 8 bit word over I2C
+ *  @hw: pointer to hardware structure
+ *  @byte_offset: byte offset to read
+ *  @dev_addr: device address
+ *  @data: value read
+ *
+ *  Performs byte read operation over I2C interface at
+ *  a specified device address.
+ */
+s32 igb_read_i2c_byte(struct e1000_hw *hw, u8 byte_offset,
+				u8 dev_addr, u8 *data)
+{
+	struct igb_adapter *adapter = container_of(hw, struct igb_adapter, hw);
+	struct i2c_client *this_client = adapter->i2c_client;
+	s32 status;
+	u16 swfw_mask = 0;
+
+	if (!this_client)
+		return E1000_ERR_I2C;
+
+	swfw_mask = E1000_SWFW_PHY0_SM;
+
+	if (hw->mac.ops.acquire_swfw_sync(hw, swfw_mask)
+	    != E1000_SUCCESS)
+		return E1000_ERR_SWFW_SYNC;
+
+	status = i2c_smbus_read_byte_data(this_client, byte_offset);
+	hw->mac.ops.release_swfw_sync(hw, swfw_mask);
+
+	if (status < 0)
+		return E1000_ERR_I2C;
+	else {
+		*data = status;
+		return E1000_SUCCESS;
+	}
+}
+
+/*  igb_write_i2c_byte - Writes 8 bit word over I2C
+ *  @hw: pointer to hardware structure
+ *  @byte_offset: byte offset to write
+ *  @dev_addr: device address
+ *  @data: value to write
+ *
+ *  Performs byte write operation over I2C interface at
+ *  a specified device address.
+ */
+s32 igb_write_i2c_byte(struct e1000_hw *hw, u8 byte_offset,
+				 u8 dev_addr, u8 data)
+{
+	struct igb_adapter *adapter = container_of(hw, struct igb_adapter, hw);
+	struct i2c_client *this_client = adapter->i2c_client;
+	s32 status;
+	u16 swfw_mask = E1000_SWFW_PHY0_SM;
+
+	if (!this_client)
+		return E1000_ERR_I2C;
+
+	if (hw->mac.ops.acquire_swfw_sync(hw, swfw_mask) != E1000_SUCCESS)
+		return E1000_ERR_SWFW_SYNC;
+	status = i2c_smbus_write_byte_data(this_client, byte_offset, data);
+	hw->mac.ops.release_swfw_sync(hw, swfw_mask);
+
+	if (status)
+		return E1000_ERR_I2C;
+	else
+		return E1000_SUCCESS;
+}
+#endif /*  HAVE_I2C_SUPPORT */
+/* igb_main.c */
+
+
+/**
+ * igb_probe - Device Initialization Routine
+ * @pdev: PCI device information struct
+ * @ent: entry in igb_pci_tbl
+ *
+ * Returns 0 on success, negative on failure
+ *
+ * igb_probe initializes an adapter identified by a pci_dev structure.
+ * The OS initialization, configuring of the adapter private structure,
+ * and a hardware reset occur.
+ **/
+int igb_kni_probe(struct pci_dev *pdev,
+			       struct net_device **lad_dev)
+{
+	struct net_device *netdev;
+	struct igb_adapter *adapter;
+	struct e1000_hw *hw;
+	u16 eeprom_data = 0;
+	u8 pba_str[E1000_PBANUM_LENGTH];
+	s32 ret_val;
+	static int global_quad_port_a; /* global quad port a indication */
+	int i, err, pci_using_dac = 0;
+	static int cards_found;
+
+	err = pci_enable_device_mem(pdev);
+	if (err)
+		return err;
+
+#ifdef NO_KNI
+	pci_using_dac = 0;
+	err = dma_set_mask(pci_dev_to_dev(pdev), DMA_BIT_MASK(64));
+	if (!err) {
+		err = dma_set_coherent_mask(pci_dev_to_dev(pdev), DMA_BIT_MASK(64));
+		if (!err)
+			pci_using_dac = 1;
+	} else {
+		err = dma_set_mask(pci_dev_to_dev(pdev), DMA_BIT_MASK(32));
+		if (err) {
+			err = dma_set_coherent_mask(pci_dev_to_dev(pdev), DMA_BIT_MASK(32));
+			if (err) {
+				IGB_ERR("No usable DMA configuration, "
+				        "aborting\n");
+				goto err_dma;
+			}
+		}
+	}
+
+#ifndef HAVE_ASPM_QUIRKS
+	/* 82575 requires that the pci-e link partner disable the L0s state */
+	switch (pdev->device) {
+	case E1000_DEV_ID_82575EB_COPPER:
+	case E1000_DEV_ID_82575EB_FIBER_SERDES:
+	case E1000_DEV_ID_82575GB_QUAD_COPPER:
+		pci_disable_link_state(pdev, PCIE_LINK_STATE_L0S);
+	default:
+		break;
+	}
+
+#endif /* HAVE_ASPM_QUIRKS */
+	err = pci_request_selected_regions(pdev,
+	                                   pci_select_bars(pdev,
+                                                           IORESOURCE_MEM),
+	                                   igb_driver_name);
+	if (err)
+		goto err_pci_reg;
+
+	pci_enable_pcie_error_reporting(pdev);
+
+	pci_set_master(pdev);
+
+	err = -ENOMEM;
+#endif /* NO_KNI */
+#ifdef HAVE_TX_MQ
+	netdev = alloc_etherdev_mq(sizeof(struct igb_adapter),
+	                           IGB_MAX_TX_QUEUES);
+#else
+	netdev = alloc_etherdev(sizeof(struct igb_adapter));
+#endif /* HAVE_TX_MQ */
+	if (!netdev)
+		goto err_alloc_etherdev;
+
+	SET_MODULE_OWNER(netdev);
+	SET_NETDEV_DEV(netdev, &pdev->dev);
+
+	//pci_set_drvdata(pdev, netdev);
+	adapter = netdev_priv(netdev);
+	adapter->netdev = netdev;
+	adapter->pdev = pdev;
+	hw = &adapter->hw;
+	hw->back = adapter;
+	adapter->port_num = hw->bus.func;
+	adapter->msg_enable = (1 << debug) - 1;
+
+#ifdef HAVE_PCI_ERS
+	err = pci_save_state(pdev);
+	if (err)
+		goto err_ioremap;
+#endif
+	err = -EIO;
+	hw->hw_addr = ioremap(pci_resource_start(pdev, 0),
+	                      pci_resource_len(pdev, 0));
+	if (!hw->hw_addr)
+		goto err_ioremap;
+
+#ifdef HAVE_NET_DEVICE_OPS
+	netdev->netdev_ops = &igb_netdev_ops;
+#else /* HAVE_NET_DEVICE_OPS */
+	netdev->open = &igb_open;
+	netdev->stop = &igb_close;
+	netdev->get_stats = &igb_get_stats;
+#ifdef HAVE_SET_RX_MODE
+	netdev->set_rx_mode = &igb_set_rx_mode;
+#endif
+	netdev->set_multicast_list = &igb_set_rx_mode;
+	netdev->set_mac_address = &igb_set_mac;
+	netdev->change_mtu = &igb_change_mtu;
+	netdev->do_ioctl = &igb_ioctl;
+#ifdef HAVE_TX_TIMEOUT
+	netdev->tx_timeout = &igb_tx_timeout;
+#endif
+	netdev->vlan_rx_register = igb_vlan_mode;
+	netdev->vlan_rx_add_vid = igb_vlan_rx_add_vid;
+	netdev->vlan_rx_kill_vid = igb_vlan_rx_kill_vid;
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	netdev->poll_controller = igb_netpoll;
+#endif
+	netdev->hard_start_xmit = &igb_xmit_frame;
+#endif /* HAVE_NET_DEVICE_OPS */
+	igb_set_ethtool_ops(netdev);
+#ifdef HAVE_TX_TIMEOUT
+	netdev->watchdog_timeo = 5 * HZ;
+#endif
+
+	strncpy(netdev->name, pci_name(pdev), sizeof(netdev->name) - 1);
+
+	adapter->bd_number = cards_found;
+
+	/* setup the private structure */
+	err = igb_sw_init(adapter);
+	if (err)
+		goto err_sw_init;
+
+	e1000_get_bus_info(hw);
+
+	hw->phy.autoneg_wait_to_complete = FALSE;
+	hw->mac.adaptive_ifs = FALSE;
+
+	/* Copper options */
+	if (hw->phy.media_type == e1000_media_type_copper) {
+		hw->phy.mdix = AUTO_ALL_MODES;
+		hw->phy.disable_polarity_correction = FALSE;
+		hw->phy.ms_type = e1000_ms_hw_default;
+	}
+
+	if (e1000_check_reset_block(hw))
+		dev_info(pci_dev_to_dev(pdev),
+			"PHY reset is blocked due to SOL/IDER session.\n");
+
+	/*
+	 * features is initialized to 0 in allocation, it might have bits
+	 * set by igb_sw_init so we should use an or instead of an
+	 * assignment.
+	 */
+	netdev->features |= NETIF_F_SG |
+			    NETIF_F_IP_CSUM |
+#ifdef NETIF_F_IPV6_CSUM
+			    NETIF_F_IPV6_CSUM |
+#endif
+#ifdef NETIF_F_TSO
+			    NETIF_F_TSO |
+#ifdef NETIF_F_TSO6
+			    NETIF_F_TSO6 |
+#endif
+#endif /* NETIF_F_TSO */
+#ifdef NETIF_F_RXHASH
+			    NETIF_F_RXHASH |
+#endif
+			    NETIF_F_RXCSUM |
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
+			    NETIF_F_HW_VLAN_CTAG_RX |
+			    NETIF_F_HW_VLAN_CTAG_TX;
+#else
+			    NETIF_F_HW_VLAN_RX |
+			    NETIF_F_HW_VLAN_TX;
+#endif
+
+	if (hw->mac.type >= e1000_82576)
+		netdev->features |= NETIF_F_SCTP_CSUM;
+
+#ifdef HAVE_NDO_SET_FEATURES
+	/* copy netdev features into list of user selectable features */
+	netdev->hw_features |= netdev->features;
+#ifndef IGB_NO_LRO
+
+	/* give us the option of enabling LRO later */
+	netdev->hw_features |= NETIF_F_LRO;
+#endif
+#else
+#ifdef NETIF_F_GRO
+
+	/* this is only needed on kernels prior to 2.6.39 */
+	netdev->features |= NETIF_F_GRO;
+#endif
+#endif
+
+	/* set this bit last since it cannot be part of hw_features */
+#ifdef NETIF_F_HW_VLAN_CTAG_FILTER
+	netdev->features |= NETIF_F_HW_VLAN_CTAG_FILTER;
+#else
+	netdev->features |= NETIF_F_HW_VLAN_FILTER;
+#endif
+
+#ifdef HAVE_NETDEV_VLAN_FEATURES
+	netdev->vlan_features |= NETIF_F_TSO |
+				 NETIF_F_TSO6 |
+				 NETIF_F_IP_CSUM |
+				 NETIF_F_IPV6_CSUM |
+				 NETIF_F_SG;
+
+#endif
+	if (pci_using_dac)
+		netdev->features |= NETIF_F_HIGHDMA;
+
+#ifdef NO_KNI
+	adapter->en_mng_pt = e1000_enable_mng_pass_thru(hw);
+#ifdef DEBUG
+	if (adapter->dmac != IGB_DMAC_DISABLE)
+		printk("%s: DMA Coalescing is enabled..\n", netdev->name);
+#endif
+
+	/* before reading the NVM, reset the controller to put the device in a
+	 * known good starting state */
+	e1000_reset_hw(hw);
+#endif /* NO_KNI */
+
+	/* make sure the NVM is good */
+	if (e1000_validate_nvm_checksum(hw) < 0) {
+		dev_err(pci_dev_to_dev(pdev), "The NVM Checksum Is Not"
+		        " Valid\n");
+		err = -EIO;
+		goto err_eeprom;
+	}
+
+	/* copy the MAC address out of the NVM */
+	if (e1000_read_mac_addr(hw))
+		dev_err(pci_dev_to_dev(pdev), "NVM Read Error\n");
+	memcpy(netdev->dev_addr, hw->mac.addr, netdev->addr_len);
+#ifdef ETHTOOL_GPERMADDR
+	memcpy(netdev->perm_addr, hw->mac.addr, netdev->addr_len);
+
+	if (!is_valid_ether_addr(netdev->perm_addr)) {
+#else
+	if (!is_valid_ether_addr(netdev->dev_addr)) {
+#endif
+		dev_err(pci_dev_to_dev(pdev), "Invalid MAC Address\n");
+		err = -EIO;
+		goto err_eeprom;
+	}
+
+	memcpy(&adapter->mac_table[0].addr, hw->mac.addr, netdev->addr_len);
+	adapter->mac_table[0].queue = adapter->vfs_allocated_count;
+	adapter->mac_table[0].state = (IGB_MAC_STATE_DEFAULT | IGB_MAC_STATE_IN_USE);
+	igb_rar_set(adapter, 0);
+
+	/* get firmware version for ethtool -i */
+	igb_set_fw_version(adapter);
+
+	/* Check if Media Autosense is enabled */
+	if (hw->mac.type == e1000_82580)
+		igb_init_mas(adapter);
+
+#ifdef NO_KNI
+#ifdef HAVE_TIMER_SETUP
+	timer_setup(&adapter->watchdog_timer, &igb_watchdog, 0);
+	if (adapter->flags & IGB_FLAG_DETECT_BAD_DMA)
+		timer_setup(&adapter->dma_err_timer, &igb_dma_err_timer, 0);
+	timer_setup(&adapter->phy_info_timer, &igb_update_phy_info, 0);
+#else
+	setup_timer(&adapter->watchdog_timer, &igb_watchdog,
+	            (unsigned long) adapter);
+	if (adapter->flags & IGB_FLAG_DETECT_BAD_DMA)
+		setup_timer(&adapter->dma_err_timer, &igb_dma_err_timer,
+			    (unsigned long) adapter);
+	setup_timer(&adapter->phy_info_timer, &igb_update_phy_info,
+	            (unsigned long) adapter);
+#endif
+
+	INIT_WORK(&adapter->reset_task, igb_reset_task);
+	INIT_WORK(&adapter->watchdog_task, igb_watchdog_task);
+	if (adapter->flags & IGB_FLAG_DETECT_BAD_DMA)
+		INIT_WORK(&adapter->dma_err_task, igb_dma_err_task);
+#endif
+
+	/* Initialize link properties that are user-changeable */
+	adapter->fc_autoneg = true;
+	hw->mac.autoneg = true;
+	hw->phy.autoneg_advertised = 0x2f;
+
+	hw->fc.requested_mode = e1000_fc_default;
+	hw->fc.current_mode = e1000_fc_default;
+
+	e1000_validate_mdi_setting(hw);
+
+	/* By default, support wake on port A */
+	if (hw->bus.func == 0)
+		adapter->flags |= IGB_FLAG_WOL_SUPPORTED;
+
+	/* Check the NVM for wake support for non-port A ports */
+	if (hw->mac.type >= e1000_82580)
+		hw->nvm.ops.read(hw, NVM_INIT_CONTROL3_PORT_A +
+		                 NVM_82580_LAN_FUNC_OFFSET(hw->bus.func), 1,
+		                 &eeprom_data);
+	else if (hw->bus.func == 1)
+		e1000_read_nvm(hw, NVM_INIT_CONTROL3_PORT_B, 1, &eeprom_data);
+
+	if (eeprom_data & IGB_EEPROM_APME)
+		adapter->flags |= IGB_FLAG_WOL_SUPPORTED;
+
+	/* now that we have the eeprom settings, apply the special cases where
+	 * the eeprom may be wrong or the board simply won't support wake on
+	 * lan on a particular port */
+	switch (pdev->device) {
+	case E1000_DEV_ID_82575GB_QUAD_COPPER:
+		adapter->flags &= ~IGB_FLAG_WOL_SUPPORTED;
+		break;
+	case E1000_DEV_ID_82575EB_FIBER_SERDES:
+	case E1000_DEV_ID_82576_FIBER:
+	case E1000_DEV_ID_82576_SERDES:
+		/* Wake events only supported on port A for dual fiber
+		 * regardless of eeprom setting */
+		if (E1000_READ_REG(hw, E1000_STATUS) & E1000_STATUS_FUNC_1)
+			adapter->flags &= ~IGB_FLAG_WOL_SUPPORTED;
+		break;
+	case E1000_DEV_ID_82576_QUAD_COPPER:
+	case E1000_DEV_ID_82576_QUAD_COPPER_ET2:
+		/* if quad port adapter, disable WoL on all but port A */
+		if (global_quad_port_a != 0)
+			adapter->flags &= ~IGB_FLAG_WOL_SUPPORTED;
+		else
+			adapter->flags |= IGB_FLAG_QUAD_PORT_A;
+		/* Reset for multiple quad port adapters */
+		if (++global_quad_port_a == 4)
+			global_quad_port_a = 0;
+		break;
+	default:
+		/* If the device can't wake, don't set software support */
+		if (!device_can_wakeup(&adapter->pdev->dev))
+			adapter->flags &= ~IGB_FLAG_WOL_SUPPORTED;
+		break;
+	}
+
+	/* initialize the wol settings based on the eeprom settings */
+	if (adapter->flags & IGB_FLAG_WOL_SUPPORTED)
+		adapter->wol |= E1000_WUFC_MAG;
+
+	/* Some vendors want WoL disabled by default, but still supported */
+	if ((hw->mac.type == e1000_i350) &&
+	    (pdev->subsystem_vendor == PCI_VENDOR_ID_HP)) {
+		adapter->flags |= IGB_FLAG_WOL_SUPPORTED;
+		adapter->wol = 0;
+	}
+
+#ifdef NO_KNI
+	device_set_wakeup_enable(pci_dev_to_dev(adapter->pdev),
+				 adapter->flags & IGB_FLAG_WOL_SUPPORTED);
+
+	/* reset the hardware with the new settings */
+	igb_reset(adapter);
+	adapter->devrc = 0;
+
+#ifdef HAVE_I2C_SUPPORT
+	/* Init the I2C interface */
+	err = igb_init_i2c(adapter);
+	if (err) {
+		dev_err(&pdev->dev, "failed to init i2c interface\n");
+		goto err_eeprom;
+	}
+#endif /* HAVE_I2C_SUPPORT */
+
+	/* let the f/w know that the h/w is now under the control of the
+	 * driver. */
+	igb_get_hw_control(adapter);
+
+	strncpy(netdev->name, "eth%d", IFNAMSIZ);
+	err = register_netdev(netdev);
+	if (err)
+		goto err_register;
+
+#ifdef CONFIG_IGB_VMDQ_NETDEV
+	err = igb_init_vmdq_netdevs(adapter);
+	if (err)
+		goto err_register;
+#endif
+	/* carrier off reporting is important to ethtool even BEFORE open */
+	netif_carrier_off(netdev);
+
+#ifdef IGB_DCA
+	if (dca_add_requester(&pdev->dev) == E1000_SUCCESS) {
+		adapter->flags |= IGB_FLAG_DCA_ENABLED;
+		dev_info(pci_dev_to_dev(pdev), "DCA enabled\n");
+		igb_setup_dca(adapter);
+	}
+
+#endif
+#ifdef HAVE_PTP_1588_CLOCK
+	/* do hw tstamp init after resetting */
+	igb_ptp_init(adapter);
+#endif /* HAVE_PTP_1588_CLOCK */
+
+#endif /* NO_KNI */
+	dev_info(pci_dev_to_dev(pdev), "Intel(R) Gigabit Ethernet Network Connection\n");
+	/* print bus type/speed/width info */
+	dev_info(pci_dev_to_dev(pdev), "%s: (PCIe:%s:%s) ",
+	         netdev->name,
+	         ((hw->bus.speed == e1000_bus_speed_2500) ? "2.5GT/s" :
+	          (hw->bus.speed == e1000_bus_speed_5000) ? "5.0GT/s" :
+		  (hw->mac.type == e1000_i354) ? "integrated" :
+	                                                    "unknown"),
+	         ((hw->bus.width == e1000_bus_width_pcie_x4) ? "Width x4" :
+	          (hw->bus.width == e1000_bus_width_pcie_x2) ? "Width x2" :
+	          (hw->bus.width == e1000_bus_width_pcie_x1) ? "Width x1" :
+		  (hw->mac.type == e1000_i354) ? "integrated" :
+	           "unknown"));
+	dev_info(pci_dev_to_dev(pdev), "%s: MAC: ", netdev->name);
+	for (i = 0; i < 6; i++)
+		printk("%2.2x%c", netdev->dev_addr[i], i == 5 ? '\n' : ':');
+
+	ret_val = e1000_read_pba_string(hw, pba_str, E1000_PBANUM_LENGTH);
+	if (ret_val)
+		strncpy(pba_str, "Unknown", sizeof(pba_str) - 1);
+	dev_info(pci_dev_to_dev(pdev), "%s: PBA No: %s\n", netdev->name,
+		 pba_str);
+
+
+	/* Initialize the thermal sensor on i350 devices. */
+	if (hw->mac.type == e1000_i350) {
+		if (hw->bus.func == 0) {
+			u16 ets_word;
+
+			/*
+			 * Read the NVM to determine if this i350 device
+			 * supports an external thermal sensor.
+			 */
+			e1000_read_nvm(hw, NVM_ETS_CFG, 1, &ets_word);
+			if (ets_word != 0x0000 && ets_word != 0xFFFF)
+				adapter->ets = true;
+			else
+				adapter->ets = false;
+		}
+#ifdef NO_KNI
+#ifdef IGB_HWMON
+
+		igb_sysfs_init(adapter);
+#else
+#ifdef IGB_PROCFS
+
+		igb_procfs_init(adapter);
+#endif /* IGB_PROCFS */
+#endif /* IGB_HWMON */
+#endif /* NO_KNI */
+	} else {
+		adapter->ets = false;
+	}
+
+	if (hw->phy.media_type == e1000_media_type_copper) {
+		switch (hw->mac.type) {
+		case e1000_i350:
+		case e1000_i210:
+		case e1000_i211:
+			/* Enable EEE for internal copper PHY devices */
+			err = e1000_set_eee_i350(hw);
+			if ((!err) &&
+			    (adapter->flags & IGB_FLAG_EEE))
+				adapter->eee_advert =
+					MDIO_EEE_100TX | MDIO_EEE_1000T;
+			break;
+		case e1000_i354:
+			if ((E1000_READ_REG(hw, E1000_CTRL_EXT)) &
+			    (E1000_CTRL_EXT_LINK_MODE_SGMII)) {
+				err = e1000_set_eee_i354(hw);
+				if ((!err) &&
+				    (adapter->flags & IGB_FLAG_EEE))
+					adapter->eee_advert =
+					   MDIO_EEE_100TX | MDIO_EEE_1000T;
+			}
+			break;
+		default:
+			break;
+		}
+	}
+
+	/* send driver version info to firmware */
+	if (hw->mac.type >= e1000_i350)
+		igb_init_fw(adapter);
+
+#ifndef IGB_NO_LRO
+	if (netdev->features & NETIF_F_LRO)
+		dev_info(pci_dev_to_dev(pdev), "Internal LRO is enabled \n");
+	else
+		dev_info(pci_dev_to_dev(pdev), "LRO is disabled \n");
+#endif
+	dev_info(pci_dev_to_dev(pdev),
+	         "Using %s interrupts. %d rx queue(s), %d tx queue(s)\n",
+	         adapter->msix_entries ? "MSI-X" :
+	         (adapter->flags & IGB_FLAG_HAS_MSI) ? "MSI" : "legacy",
+	         adapter->num_rx_queues, adapter->num_tx_queues);
+
+	cards_found++;
+	*lad_dev = netdev;
+
+	pm_runtime_put_noidle(&pdev->dev);
+	return 0;
+
+//err_register:
+//	igb_release_hw_control(adapter);
+#ifdef HAVE_I2C_SUPPORT
+	memset(&adapter->i2c_adap, 0, sizeof(adapter->i2c_adap));
+#endif /* HAVE_I2C_SUPPORT */
+err_eeprom:
+//	if (!e1000_check_reset_block(hw))
+//		e1000_phy_hw_reset(hw);
+
+	if (hw->flash_address)
+		iounmap(hw->flash_address);
+err_sw_init:
+//	igb_clear_interrupt_scheme(adapter);
+//	igb_reset_sriov_capability(adapter);
+	iounmap(hw->hw_addr);
+err_ioremap:
+	free_netdev(netdev);
+err_alloc_etherdev:
+//	pci_release_selected_regions(pdev,
+//	                             pci_select_bars(pdev, IORESOURCE_MEM));
+//err_pci_reg:
+//err_dma:
+	pci_disable_device(pdev);
+	return err;
+}
+
+
+void igb_kni_remove(struct pci_dev *pdev)
+{
+	pci_disable_device(pdev);
+}
diff --git a/kernel/linux/kni/kni_dev.h b/kernel/linux/kni/kni_dev.h
index c1ca6789c..6738f8bd0 100644
--- a/kernel/linux/kni/kni_dev.h
+++ b/kernel/linux/kni/kni_dev.h
@@ -59,7 +59,7 @@ struct kni_dev {
 	struct rte_kni_fifo *rx_q;
 
 	/* queue for the allocated mbufs those can be used to save sk buffs */
-	struct rte_kni_fifo *alloc_q;
+	struct rte_kni_fifo *alloc_q;//kernel模块自此队列获取空闲的mbuf
 
 	/* free queue for the mbufs to be freed */
 	struct rte_kni_fifo *free_q;
@@ -80,7 +80,7 @@ struct kni_dev {
 	uint32_t mbuf_size;
 
 	/* buffers */
-	void *pa[MBUF_BURST_SZ];
+	void *pa[MBUF_BURST_SZ];//用于缓冲收到的报文指针
 	void *va[MBUF_BURST_SZ];
 	void *alloc_pa[MBUF_BURST_SZ];
 	void *alloc_va[MBUF_BURST_SZ];
diff --git a/kernel/linux/kni/kni_misc.c b/kernel/linux/kni/kni_misc.c
index 84ef03b5f..7ec5e9ccf 100644
--- a/kernel/linux/kni/kni_misc.c
+++ b/kernel/linux/kni/kni_misc.c
@@ -302,6 +302,7 @@ kni_ioctl_create(struct net *net, uint32_t ioctl_num,
 		return -EINVAL;
 
 	/* Copy kni info from user space */
+	//用户态配置下来的参数即为dev_info结构体
 	if (copy_from_user(&dev_info, (void *)ioctl_param, sizeof(dev_info)))
 		return -EFAULT;
 
@@ -329,6 +330,7 @@ kni_ioctl_create(struct net *net, uint32_t ioctl_num,
 	}
 	up_read(&knet->kni_list_lock);
 
+	//创建kni对应的netdev设备
 	net_dev = alloc_netdev(sizeof(struct kni_dev), dev_info.name,
 #ifdef NET_NAME_USER
 							NET_NAME_USER,
@@ -348,7 +350,7 @@ kni_ioctl_create(struct net *net, uint32_t ioctl_num,
 	strncpy(kni->name, dev_info.name, RTE_KNI_NAMESIZE);
 
 	/* Translate user space info into kernel space info */
-	kni->tx_q = phys_to_virt(dev_info.tx_phys);
+	kni->tx_q = phys_to_virt(dev_info.tx_phys);//设置发送队列
 	kni->rx_q = phys_to_virt(dev_info.rx_phys);
 	kni->alloc_q = phys_to_virt(dev_info.alloc_phys);
 	kni->free_q = phys_to_virt(dev_info.free_phys);
@@ -503,10 +505,11 @@ static const struct file_operations kni_fops = {
 	.owner = THIS_MODULE,
 	.open = kni_open,
 	.release = kni_release,
-	.unlocked_ioctl = (void *)kni_ioctl,
+	.unlocked_ioctl = (void *)kni_ioctl,//提供kni设备的创建初始化
 	.compat_ioctl = (void *)kni_compat_ioctl,
 };
 
+//kni设备的ops
 static struct miscdevice kni_misc = {
 	.minor = MISC_DYNAMIC_MINOR,
 	.name = KNI_DEVICE,
diff --git a/kernel/linux/kni/kni_net.c b/kernel/linux/kni/kni_net.c
index f25b1277b..7e835133a 100644
--- a/kernel/linux/kni/kni_net.c
+++ b/kernel/linux/kni/kni_net.c
@@ -262,14 +262,18 @@ kni_net_tx(struct sk_buff *skb, struct net_device *dev)
 	}
 
 	/* dequeue a mbuf from alloc_q */
+	//有skb需要发送，先从kni->alloc_q中申请一个mbuf,填充mbuf并传递给用户态
+	//然后释放skb
 	ret = kni_fifo_get(kni->alloc_q, &pkt_pa, 1);
 	if (likely(ret == 1)) {
+		//无空闲buf
 		void *data_kva;
 
 		pkt_kva = pa2kva(pkt_pa);
 		data_kva = kva2data_kva(pkt_kva);
 		pkt_va = pa2va(pkt_pa, pkt_kva);
 
+		//填充mbuf->data
 		len = skb->len;
 		memcpy(data_kva, skb->data, len);
 		if (unlikely(len < ETH_ZLEN)) {
@@ -280,8 +284,10 @@ kni_net_tx(struct sk_buff *skb, struct net_device *dev)
 		pkt_kva->data_len = len;
 
 		/* enqueue mbuf into tx_q */
+		//将报文存入tx_q队列，完成向用户态的发送
 		ret = kni_fifo_put(kni->tx_q, &pkt_va, 1);
 		if (unlikely(ret != 1)) {
+			//入队失败，报错
 			/* Failing should not happen */
 			pr_err("Fail to enqueue mbuf into tx_q\n");
 			goto drop;
@@ -293,6 +299,7 @@ kni_net_tx(struct sk_buff *skb, struct net_device *dev)
 	}
 
 	/* Free skb and update statistics */
+	//将源有的skb释放掉
 	dev_kfree_skb(skb);
 	dev->stats.tx_bytes += len;
 	dev->stats.tx_packets++;
@@ -322,9 +329,11 @@ kni_net_rx_normal(struct kni_dev *kni)
 	struct net_device *dev = kni->net_dev;
 
 	/* Get the number of free entries in free_q */
+	//先检查free_q上有多少个空间可用于存放空闲的mbuf
 	num_fq = kni_fifo_free_count(kni->free_q);
 	if (num_fq == 0) {
 		/* No room on the free_q, bail out */
+		//无空闲空间，则直接返回，不能收取
 		return;
 	}
 
@@ -332,17 +341,21 @@ kni_net_rx_normal(struct kni_dev *kni)
 	num_rx = min_t(uint32_t, num_fq, MBUF_BURST_SZ);
 
 	/* Burst dequeue from rx_q */
+	//自kni的收队列中取出报文，并申请skb,将kni的mbuf填充到skb中
 	num_rx = kni_fifo_get(kni->rx_q, kni->pa, num_rx);
 	if (num_rx == 0)
 		return;
 
 	/* Transfer received packets to netif */
 	for (i = 0; i < num_rx; i++) {
+		//已知的是物理地址，由物理地址反推kernel的虚地址
+		//然后kernel就可以访问这个buffer了
 		kva = pa2kva(kni->pa[i]);
-		len = kva->pkt_len;
+		len = kva->pkt_len;//取报文长度
 		data_kva = kva2data_kva(kva);
 		kni->va[i] = pa2va(kni->pa[i], kva);
 
+		//申请skb
 		skb = netdev_alloc_skb(dev, len);
 		if (!skb) {
 			/* Update statistics */
@@ -350,9 +363,11 @@ kni_net_rx_normal(struct kni_dev *kni)
 			continue;
 		}
 
+		//将mbuf中的内容copy到skb中
 		if (kva->nb_segs == 1) {
 			memcpy(skb_put(skb, len), data_kva, len);
 		} else {
+			//如果存在多片，则合并成一个skb
 			int nb_segs;
 			int kva_nb_segs = kva->nb_segs;
 
@@ -371,10 +386,12 @@ kni_net_rx_normal(struct kni_dev *kni)
 			}
 		}
 
+		//填充入接口，二层协议
 		skb->protocol = eth_type_trans(skb, dev);
 		skb->ip_summed = CHECKSUM_UNNECESSARY;
 
 		/* Call netif interface */
+		//使kernel收此接口报文（转入kernel协议栈）
 		netif_rx_ni(skb);
 
 		/* Update statistics */
@@ -383,6 +400,7 @@ kni_net_rx_normal(struct kni_dev *kni)
 	}
 
 	/* Burst enqueue mbufs into free_q */
+	//将mbuf入队列free_q，交给用户态去释放
 	ret = kni_fifo_put(kni->free_q, kni->va, num_rx);
 	if (ret != num_rx)
 		/* Failing should not happen */
@@ -745,6 +763,7 @@ static const struct net_device_ops kni_net_netdev_ops = {
 	.ndo_stop = kni_net_release,
 	.ndo_set_config = kni_net_config,
 	.ndo_change_rx_flags = kni_net_change_rx_flags,
+	//kni发包回调
 	.ndo_start_xmit = kni_net_tx,
 	.ndo_change_mtu = kni_net_change_mtu,
 	.ndo_tx_timeout = kni_net_tx_timeout,
diff --git a/lib/Makefile b/lib/Makefile
index 1ff00ba8c..3476dc880 100644
--- a/lib/Makefile
+++ b/lib/Makefile
@@ -1,12 +1,16 @@
 # SPDX-License-Identifier: BSD-3-Clause
 # Copyright(c) 2010-2017 Intel Corporation
 
+#由rte.sdkbuild.mk可知，每个模块的Makefile中的all目标将被执行用来编译
+
 include $(RTE_SDK)/mk/rte.vars.mk
 
+#在rte.vars.mk中我们载入.config文件得到配置变量，通过这些配置变量我们知道那个目录需要被编译
 DIRS-$(CONFIG_RTE_LIBRTE_KVARGS) += librte_kvargs
 DIRS-$(CONFIG_RTE_LIBRTE_EAL) += librte_eal
 DEPDIRS-librte_eal := librte_kvargs
 DIRS-$(CONFIG_RTE_LIBRTE_PCI) += librte_pci
+#指出librte_pci依赖于librte_eal
 DEPDIRS-librte_pci := librte_eal
 DIRS-$(CONFIG_RTE_LIBRTE_RING) += librte_ring
 DEPDIRS-librte_ring := librte_eal
diff --git a/lib/librte_bpf/bpf_def.h b/lib/librte_bpf/bpf_def.h
index d39992997..513569929 100644
--- a/lib/librte_bpf/bpf_def.h
+++ b/lib/librte_bpf/bpf_def.h
@@ -31,23 +31,25 @@ extern "C" {
  */
 
 /* Instruction classes */
+//指令类别
 #define BPF_CLASS(code) ((code) & 0x07)
-#define	BPF_LD		0x00
+#define	BPF_LD		0x00 //加载指令
 #define	BPF_LDX		0x01
-#define	BPF_ST		0x02
+#define	BPF_ST		0x02 //存储指令
 #define	BPF_STX		0x03
-#define	BPF_ALU		0x04
-#define	BPF_JMP		0x05
-#define	BPF_RET		0x06
-#define	BPF_MISC        0x07
+#define	BPF_ALU		0x04 //算术逻辑单元
+#define	BPF_JMP		0x05 //跳指令
+#define	BPF_RET		0x06 //return指令
+#define	BPF_MISC    0x07 //杂指令
 
 #define EBPF_ALU64	0x07
 
 /* ld/ldx fields */
+//操作位宽定义
 #define BPF_SIZE(code)  ((code) & 0x18)
-#define	BPF_W		0x00
-#define	BPF_H		0x08
-#define	BPF_B		0x10
+#define	BPF_W		0x00 //4字节
+#define	BPF_H		0x08 //2字节
+#define	BPF_B		0x10 //1字节
 #define	EBPF_DW		0x18
 
 #define BPF_MODE(code)  ((code) & 0xe0)
@@ -62,17 +64,17 @@ extern "C" {
 
 /* alu/jmp fields */
 #define BPF_OP(code)    ((code) & 0xf0)
-#define	BPF_ADD		0x00
-#define	BPF_SUB		0x10
+#define	BPF_ADD		0x00 //加
+#define	BPF_SUB		0x10 //减
 #define	BPF_MUL		0x20
 #define	BPF_DIV		0x30
 #define	BPF_OR		0x40
 #define	BPF_AND		0x50
-#define	BPF_LSH		0x60
+#define	BPF_LSH		0x60 //左多
 #define	BPF_RSH		0x70
-#define	BPF_NEG		0x80
-#define	BPF_MOD		0x90
-#define	BPF_XOR		0xa0
+#define	BPF_NEG		0x80 //求补码运算（取反加１）
+#define	BPF_MOD		0x90 //取余
+#define	BPF_XOR		0xa0 //异或
 
 #define EBPF_MOV	0xb0
 #define EBPF_ARSH	0xc0
@@ -132,11 +134,11 @@ enum {
  * eBPF instruction format
  */
 struct ebpf_insn {
-	uint8_t code;
-	uint8_t dst_reg:4;
-	uint8_t src_reg:4;
+	uint8_t code;//指令
+	uint8_t dst_reg:4;//目的寄存器编号
+	uint8_t src_reg:4;//源寄存器编号
 	int16_t off;
-	int32_t imm;
+	int32_t imm;//立即数
 };
 
 /*
diff --git a/lib/librte_bpf/bpf_exec.c b/lib/librte_bpf/bpf_exec.c
index 1bb226643..bf5c4d75f 100644
--- a/lib/librte_bpf/bpf_exec.c
+++ b/lib/librte_bpf/bpf_exec.c
@@ -26,6 +26,7 @@
 		(ins)->off : 0)
 
 #define BPF_JMP_CND_IMM(reg, ins, op, type)	\
+	/*针对目的寄存器与立即数，执行op操作，结果为真时，指令跳到off处继续执行，否则继续执行*/\
 	((ins) += \
 		((type)(reg)[(ins)->dst_reg] op (type)(ins)->imm) ? \
 		(ins)->off : 0)
@@ -34,16 +35,20 @@
 	((reg)[(ins)->dst_reg] = (type)(-(reg)[(ins)->dst_reg]))
 
 #define EBPF_MOV_ALU_REG(reg, ins, type)	\
+	/*目的寄存器中载入源寄存器的值*/\
 	((reg)[(ins)->dst_reg] = (type)(reg)[(ins)->src_reg])
 
 #define BPF_OP_ALU_REG(reg, ins, op, type)	\
+	/*逻辑运算，目的寄存器与源寄存器执行op操作，结果存入目的寄存器*/\
 	((reg)[(ins)->dst_reg] = \
 		(type)(reg)[(ins)->dst_reg] op (type)(reg)[(ins)->src_reg])
 
 #define EBPF_MOV_ALU_IMM(reg, ins, type)	\
+	/*目的寄存器中载入立即数*/\
 	((reg)[(ins)->dst_reg] = (type)(ins)->imm)
 
 #define BPF_OP_ALU_IMM(reg, ins, op, type)	\
+	/*逻辑运算，目的寄存器与立即数操作，再存入到目的寄存器*/\
 	((reg)[(ins)->dst_reg] = \
 		(type)(reg)[(ins)->dst_reg] op (type)(ins)->imm)
 
@@ -58,6 +63,7 @@
 } while (0)
 
 #define BPF_LD_REG(reg, ins, type)	\
+	/*将源寄存器中存放的指针地址偏移off后，将其内容赋给目的寄存器*/\
 	((reg)[(ins)->dst_reg] = \
 		*(type *)(uintptr_t)((reg)[(ins)->src_reg] + (ins)->off))
 
@@ -112,15 +118,18 @@ bpf_alu_le(uint64_t reg[EBPF_REG_NUM], const struct ebpf_insn *ins)
 	}
 }
 
+//bpf程序执行
 static inline uint64_t
-bpf_exec(const struct rte_bpf *bpf, uint64_t reg[EBPF_REG_NUM])
+bpf_exec(const struct rte_bpf *bpf, uint64_t reg[EBPF_REG_NUM]/*寄存器*/)
 {
 	const struct ebpf_insn *ins;
 
+	//遍历执行所有指令（jump指令控制指定的跳跃，即控制ins指针移动）
 	for (ins = bpf->prm.ins; ; ins++) {
 		switch (ins->code) {
 		/* 32 bit ALU IMM operations */
 		case (BPF_ALU | BPF_ADD | BPF_K):
+			//有key标记，按加立即数操作
 			BPF_OP_ALU_IMM(reg, ins, +, uint32_t);
 			break;
 		case (BPF_ALU | BPF_SUB | BPF_K):
@@ -199,6 +208,7 @@ bpf_exec(const struct rte_bpf *bpf, uint64_t reg[EBPF_REG_NUM])
 			bpf_alu_le(reg, ins);
 			break;
 		/* 64 bit ALU IMM operations */
+			//64位指令
 		case (EBPF_ALU64 | BPF_ADD | BPF_K):
 			BPF_OP_ALU_IMM(reg, ins, +, uint64_t);
 			break;
@@ -401,6 +411,8 @@ bpf_exec(const struct rte_bpf *bpf, uint64_t reg[EBPF_REG_NUM])
 			BPF_JMP_CND_REG(reg, ins, &, uint64_t);
 			break;
 		/* call instructions */
+			//call指令，调用ins->imm指定的符号，并传入1,2,3,4,5号寄存器值，执行结果保存在0号寄存器中
+			//call指令结束后，继续执行
 		case (BPF_JMP | EBPF_CALL):
 			reg[EBPF_REG_0] = bpf->prm.xsym[ins->imm].func.val(
 				reg[EBPF_REG_1], reg[EBPF_REG_2],
@@ -408,9 +420,11 @@ bpf_exec(const struct rte_bpf *bpf, uint64_t reg[EBPF_REG_NUM])
 				reg[EBPF_REG_5]);
 			break;
 		/* return instruction */
+			//退出执行，返回0号寄存器
 		case (BPF_JMP | EBPF_EXIT):
 			return reg[EBPF_REG_0];
 		default:
+			//遇到非法执令，报错
 			RTE_BPF_LOG(ERR,
 				"%s(%p): invalid opcode %#x at pc: %#zx;\n",
 				__func__, bpf, ins->code,
@@ -432,10 +446,11 @@ rte_bpf_exec_burst(const struct rte_bpf *bpf, void *ctx[], uint64_t rc[],
 	uint64_t reg[EBPF_REG_NUM];
 	uint64_t stack[MAX_BPF_STACK_SIZE / sizeof(uint64_t)];
 
+	//执行num个bpf程序
 	for (i = 0; i != num; i++) {
 
-		reg[EBPF_REG_1] = (uintptr_t)ctx[i];
-		reg[EBPF_REG_10] = (uintptr_t)(stack + RTE_DIM(stack));
+		reg[EBPF_REG_1] = (uintptr_t)ctx[i];//参数
+		reg[EBPF_REG_10] = (uintptr_t)(stack + RTE_DIM(stack));//栈底指针（没有指出栈顶，故栈是定长的）
 
 		rc[i] = bpf_exec(bpf, reg);
 	}
diff --git a/lib/librte_bpf/bpf_impl.h b/lib/librte_bpf/bpf_impl.h
index 03ba0ae11..d5726d5a0 100644
--- a/lib/librte_bpf/bpf_impl.h
+++ b/lib/librte_bpf/bpf_impl.h
@@ -17,7 +17,7 @@ extern "C" {
 struct rte_bpf {
 	struct rte_bpf_prm prm;
 	struct rte_bpf_jit jit;
-	size_t sz;
+	size_t sz;//size,大小
 	uint32_t stack_sz;
 };
 
diff --git a/lib/librte_bpf/bpf_jit_x86.c b/lib/librte_bpf/bpf_jit_x86.c
index f70cd6be5..edafc853d 100644
--- a/lib/librte_bpf/bpf_jit_x86.c
+++ b/lib/librte_bpf/bpf_jit_x86.c
@@ -1313,6 +1313,7 @@ emit(struct bpf_jit_state *st, const struct rte_bpf *bpf)
 /*
  * produce a native ISA version of the given BPF code.
  */
+//将bpf代码转换为x86指令
 int
 bpf_jit_x86(struct rte_bpf *bpf)
 {
diff --git a/lib/librte_bpf/bpf_load.c b/lib/librte_bpf/bpf_load.c
index 2a3b901d7..3732a896c 100644
--- a/lib/librte_bpf/bpf_load.c
+++ b/lib/librte_bpf/bpf_load.c
@@ -27,11 +27,14 @@ bpf_load(const struct rte_bpf_prm *prm)
 	struct rte_bpf *bpf;
 	size_t sz, bsz, insz, xsz;
 
+	//符号表大小
 	xsz =  prm->nb_xsym * sizeof(prm->xsym[0]);
+	//指令表大小
 	insz = prm->nb_ins * sizeof(prm->ins[0]);
 	bsz = sizeof(bpf[0]);
 	sz = insz + xsz + bsz;
 
+	//申请sz字节内存
 	buf = mmap(NULL, sz, PROT_READ | PROT_WRITE,
 		MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
 	if (buf == MAP_FAILED)
@@ -42,9 +45,12 @@ bpf_load(const struct rte_bpf_prm *prm)
 
 	memcpy(&bpf->prm, prm, sizeof(bpf->prm));
 
+	//设置符号表
 	memcpy(buf + bsz, prm->xsym, xsz);
+	//设置指令
 	memcpy(buf + bsz + xsz, prm->ins, insz);
 
+	//指向符号表，及指令
 	bpf->prm.xsym = (void *)(buf + bsz);
 	bpf->prm.ins = (void *)(buf + bsz + xsz);
 
@@ -63,14 +69,16 @@ bpf_check_xsym(const struct rte_bpf_xsym *xsym)
 		return -EINVAL;
 
 	if (xsym->type == RTE_BPF_XTYPE_VAR) {
+		//符号为变量
 		if (xsym->var.desc.type == RTE_BPF_ARG_UNDEF)
 			return -EINVAL;
 	} else if (xsym->type == RTE_BPF_XTYPE_FUNC) {
-
+		//符号为函数
 		if (xsym->func.nb_args > EBPF_FUNC_MAX_ARGS)
 			return -EINVAL;
 
 		/* check function arguments */
+		//确保所有参数均给出
 		for (i = 0; i != xsym->func.nb_args; i++) {
 			if (xsym->func.args[i].type == RTE_BPF_ARG_UNDEF)
 				return -EINVAL;
@@ -99,6 +107,7 @@ rte_bpf_load(const struct rte_bpf_prm *prm)
 		return NULL;
 	}
 
+	//符号检查
 	rc = 0;
 	for (i = 0; i != prm->nb_xsym && rc == 0; i++)
 		rc = bpf_check_xsym(prm->xsym + i);
@@ -109,6 +118,7 @@ rte_bpf_load(const struct rte_bpf_prm *prm)
 		return NULL;
 	}
 
+	//加载prm中的符号表及指令
 	bpf = bpf_load(prm);
 	if (bpf == NULL) {
 		rte_errno = ENOMEM;
diff --git a/lib/librte_bpf/bpf_load_elf.c b/lib/librte_bpf/bpf_load_elf.c
index 2b11adeb5..a53da8297 100644
--- a/lib/librte_bpf/bpf_load_elf.c
+++ b/lib/librte_bpf/bpf_load_elf.c
@@ -92,6 +92,7 @@ resolve_xsym(const char *sn, size_t ofs, struct ebpf_insn *ins, size_t ins_sz,
 		ins[idx].imm = fidx;
 	/* for variable we need to store its absolute address */
 	} else {
+		//针对符号，载入其地址对应值，做为立即数（这个用起来需要在bpf代码编写时就知道此符号的固定地址）
 		ins[idx].imm = (uintptr_t)prm->xsym[fidx].var.val;
 		ins[idx + 1].imm =
 			(uint64_t)(uintptr_t)prm->xsym[fidx].var.val >> 32;
@@ -276,6 +277,7 @@ bpf_load_elf(const struct rte_bpf_prm *prm, int32_t fd, const char *section)
 	elf_version(EV_CURRENT);
 	elf = elf_begin(fd, ELF_C_READ, NULL);
 
+	//查找段名称
 	rc = find_elf_code(elf, section, &sd, &sidx);
 	if (rc == 0)
 		rc = elf_reloc_code(elf, sd, sidx, prm);
@@ -301,6 +303,7 @@ rte_bpf_elf_load(const struct rte_bpf_prm *prm, const char *fname,
 	int32_t fd, rc;
 	struct rte_bpf *bpf;
 
+	//参数检查
 	if (prm == NULL || fname == NULL || sname == NULL) {
 		rte_errno = EINVAL;
 		return NULL;
diff --git a/lib/librte_bpf/bpf_pkt.c b/lib/librte_bpf/bpf_pkt.c
index 6e8248f0d..b30a96fa0 100644
--- a/lib/librte_bpf/bpf_pkt.c
+++ b/lib/librte_bpf/bpf_pkt.c
@@ -183,25 +183,31 @@ apply_filter(struct rte_mbuf *mb[], const uint64_t rc[], uint32_t num,
 
 		/* filter matches */
 		if (rc[i] != 0)
+			//返回非０，认为mbuf匹配成功
 			mb[j++] = mb[i];
 		/* no match */
 		else
+			//mbuf没有匹配
 			dr[k++] = mb[i];
 	}
 
 	if (drop != 0) {
+		//如果指定需要drop,则释放dr数组中的mbuf
 		/* free filtered out mbufs */
 		for (i = 0; i != k; i++)
 			rte_pktmbuf_free(dr[i]);
 	} else {
+		//未指定drop,则将其copy到mb的后面
 		/* copy filtered out mbufs beyond good ones */
 		for (i = 0; i != k; i++)
 			mb[j + i] = dr[i];
 	}
 
+	//返回有多少mbuf被匹配
 	return j;
 }
 
+//执行bpf虚机，过滤num个mbuf
 static inline uint32_t
 pkt_filter_vm(const struct rte_bpf *bpf, struct rte_mbuf *mb[], uint32_t num,
 	uint32_t drop)
@@ -244,7 +250,9 @@ pkt_filter_mb_vm(const struct rte_bpf *bpf, struct rte_mbuf *mb[], uint32_t num,
 {
 	uint64_t rc[num];
 
+	//针对num个mbuf执行bpf,各mbuf情况返回值为rc
 	rte_bpf_exec_burst(bpf, (void **)mb, rc, num);
+	//按rc结果filter
 	return apply_filter(mb, rc, num, drop);
 }
 
@@ -353,6 +361,7 @@ bpf_rx_callback_mb_vm(__rte_unused uint16_t port, __rte_unused uint16_t queue,
 	cbi = user_param;
 	bpf_eth_cbi_inuse(cbi);
 	rc = (cbi->cb != NULL) ?
+		//bpf方式过滤，不匹配的直接drop
 		pkt_filter_mb_vm(cbi->bpf, pkt, nb_pkts, 1) :
 		nb_pkts;
 	bpf_eth_cbi_unuse(cbi);
@@ -386,6 +395,7 @@ bpf_tx_callback_mb_vm(__rte_unused uint16_t port, __rte_unused uint16_t queue,
 	cbi = user_param;
 	bpf_eth_cbi_inuse(cbi);
 	rc = (cbi->cb != NULL) ?
+			//bpf方式过滤，不匹配的不drop
 		pkt_filter_mb_vm(cbi->bpf, pkt, nb_pkts, 0) :
 		nb_pkts;
 	bpf_eth_cbi_unuse(cbi);
@@ -521,6 +531,7 @@ bpf_eth_elf_load(struct bpf_eth_cbh *cbh, uint16_t port, uint16_t queue,
 			queue >= RTE_MAX_QUEUES_PER_PORT)
 		return -EINVAL;
 
+	//按rx,tx　返回不同的回调
 	if (cbh->type == BPF_ETH_RX)
 		frx = select_rx_callback(prm->prog_arg.type, flags);
 	else
@@ -557,6 +568,7 @@ bpf_eth_elf_load(struct bpf_eth_cbh *cbh, uint16_t port, uint16_t queue,
 	bc->bpf = bpf;
 	bc->jit = jit;
 
+	//为接口队列挂载回调，在报文收到时，可以执行
 	if (cbh->type == BPF_ETH_RX)
 		bc->cb = rte_eth_add_rx_callback(port, queue, frx, bc);
 	else
diff --git a/lib/librte_bpf/bpf_validate.c b/lib/librte_bpf/bpf_validate.c
index 6bd6f78e9..5214cef8c 100644
--- a/lib/librte_bpf/bpf_validate.c
+++ b/lib/librte_bpf/bpf_validate.c
@@ -85,7 +85,7 @@ struct bpf_ins_check {
 	struct {
 		uint16_t dreg;
 		uint16_t sreg;
-	} mask;
+	} mask;//可用哪些目的寄存器，可用哪些源寄存器
 	struct {
 		uint16_t min;
 		uint16_t max;
@@ -93,7 +93,7 @@ struct bpf_ins_check {
 	struct {
 		uint32_t min;
 		uint32_t max;
-	} imm;
+	} imm;//立即数取值
 	const char * (*check)(const struct ebpf_insn *);
 	const char * (*eval)(struct bpf_verifier *, const struct ebpf_insn *);
 };
@@ -593,6 +593,7 @@ eval_defined(const struct bpf_reg_val *dst, const struct bpf_reg_val *src)
 	return NULL;
 }
 
+//alu执行检查
 static const char *
 eval_alu(struct bpf_verifier *bvf, const struct ebpf_insn *ins)
 {
@@ -603,6 +604,7 @@ eval_alu(struct bpf_verifier *bvf, const struct ebpf_insn *ins)
 	struct bpf_eval_state *st;
 	struct bpf_reg_val *rd, rs;
 
+	//区分是32位操作数，还是64位操作数
 	opsz = (BPF_CLASS(ins->code) == BPF_ALU) ?
 		sizeof(uint32_t) : sizeof(uint64_t);
 	opsz = opsz * CHAR_BIT;
@@ -619,6 +621,7 @@ eval_alu(struct bpf_verifier *bvf, const struct ebpf_insn *ins)
 
 	eval_apply_mask(rd, msk);
 
+	//取指令对应的操作码
 	op = BPF_OP(ins->code);
 
 	err = eval_defined((op != EBPF_MOV) ? rd : NULL,
@@ -626,6 +629,7 @@ eval_alu(struct bpf_verifier *bvf, const struct ebpf_insn *ins)
 	if (err != NULL)
 		return err;
 
+	//执行算术运算指令
 	if (op == BPF_ADD)
 		eval_add(rd, &rs, msk);
 	else if (op == BPF_SUB)
@@ -1660,20 +1664,25 @@ check_syntax(const struct ebpf_insn *ins)
 	if (ins_chk[op].mask.dreg == 0)
 		return "invalid opcode";
 
+	//指令op不能操作指定指出的寄存器（目的寄存器）
 	if ((ins_chk[op].mask.dreg & 1 << ins->dst_reg) == 0)
 		return "invalid dst-reg field";
 
+	//指令op不能操作指定指出的寄存器（源寄存器）
 	if ((ins_chk[op].mask.sreg & 1 << ins->src_reg) == 0)
 		return "invalid src-reg field";
 
+	//偏移取值是否合法
 	off = ins->off;
 	if (ins_chk[op].off.min > off || ins_chk[op].off.max < off)
 		return "invalid off field";
 
+	//立即数取值是否合法
 	imm = ins->imm;
 	if (ins_chk[op].imm.min > imm || ins_chk[op].imm.max < imm)
 		return "invalid imm field";
 
+	//采用check回调检查
 	if (ins_chk[op].check != NULL)
 		return ins_chk[op].check(ins);
 
@@ -1888,12 +1897,14 @@ validate(struct bpf_verifier *bvf)
 	const struct ebpf_insn *ins;
 	const char *err;
 
+	//遍历每条指令
 	rc = 0;
 	for (i = 0; i < bvf->prm->nb_ins; i++) {
 
 		ins = bvf->prm->ins + i;
 		node = bvf->in + i;
 
+		//检查指令语法
 		err = check_syntax(ins);
 		if (err != 0) {
 			RTE_BPF_LOG(ERR, "%s: %s at pc: %u\n",
@@ -2217,6 +2228,7 @@ bpf_validate(struct rte_bpf *bpf)
 	struct bpf_verifier bvf;
 
 	/* check input argument type, don't allow mbuf ptr on 32-bit */
+	//程序参数类型检查
 	if (bpf->prm.prog_arg.type != RTE_BPF_ARG_RAW &&
 			bpf->prm.prog_arg.type != RTE_BPF_ARG_PTR &&
 			(sizeof(uint64_t) != sizeof(uintptr_t) ||
@@ -2234,6 +2246,7 @@ bpf_validate(struct rte_bpf *bpf)
 	rc = validate(&bvf);
 
 	if (rc == 0) {
+		//校验通过
 		rc = evst_pool_init(&bvf);
 		if (rc == 0)
 			rc = evaluate(&bvf);
diff --git a/lib/librte_bpf/rte_bpf.h b/lib/librte_bpf/rte_bpf.h
index cbf1cddac..7908280ce 100644
--- a/lib/librte_bpf/rte_bpf.h
+++ b/lib/librte_bpf/rte_bpf.h
@@ -73,8 +73,8 @@ struct rte_bpf_xsym {
 		struct {
 			uint64_t (*val)(uint64_t, uint64_t, uint64_t,
 				uint64_t, uint64_t);
-			uint32_t nb_args;
-			struct rte_bpf_arg args[EBPF_FUNC_MAX_ARGS];
+			uint32_t nb_args;//函数参数数目
+			struct rte_bpf_arg args[EBPF_FUNC_MAX_ARGS];//参数
 			/**< Function arguments descriptions. */
 			struct rte_bpf_arg ret; /**< function return value. */
 		} func;
@@ -89,11 +89,14 @@ struct rte_bpf_xsym {
  * Input parameters for loading eBPF code.
  */
 struct rte_bpf_prm {
+	//指令
 	const struct ebpf_insn *ins; /**< array of eBPF instructions */
+	//指令数
 	uint32_t nb_ins;            /**< number of instructions in ins */
-	const struct rte_bpf_xsym *xsym;
 	/**< array of external symbols that eBPF code is allowed to reference */
-	uint32_t nb_xsym; /**< number of elements in xsym */
+	const struct rte_bpf_xsym *xsym;//外部符号表
+	uint32_t nb_xsym; /**< number of elements in xsym */ //符号表大小
+	//程序参数
 	struct rte_bpf_arg prog_arg; /**< eBPF program input arg description */
 };
 
diff --git a/lib/librte_cfgfile/rte_cfgfile.c b/lib/librte_cfgfile/rte_cfgfile.c
index 9049fd9c2..210c4380a 100644
--- a/lib/librte_cfgfile/rte_cfgfile.c
+++ b/lib/librte_cfgfile/rte_cfgfile.c
@@ -13,18 +13,22 @@
 
 #include "rte_cfgfile.h"
 
+//提供了一种非常简单的ini文件解析功能（所有value均为char*类型）
+//可以在其上添加section的metadata,用于指出有哪些段，这些段有哪些entry
+//这些entry的value是什么类型，哪些是必须的，哪些是可选的，它们之间如何依赖
+
 struct rte_cfgfile_section {
 	char name[CFG_NAME_LEN];
-	int num_entries;
-	int allocated_entries;
-	struct rte_cfgfile_entry *entries;
+	int num_entries;//此段内有多少个配置项
+	int allocated_entries;//此段内申请了多少个配置项
+	struct rte_cfgfile_entry *entries;//配置项
 };
 
 struct rte_cfgfile {
 	int flags;
-	int num_sections;
-	int allocated_sections;
-	struct rte_cfgfile_section *sections;
+	int num_sections;//使用了多少个段
+	int allocated_sections;//申请了多少个段
+	struct rte_cfgfile_section *sections;//段
 };
 
 static int cfgfile_logtype;
@@ -35,10 +39,10 @@ static int cfgfile_logtype;
 
 /** when we resize a file structure, how many extra entries
  * for new sections do we add in */
-#define CFG_ALLOC_SECTION_BATCH 8
+#define CFG_ALLOC_SECTION_BATCH 8 //一次最多申请多少个section
 /** when we resize a section structure, how many extra entries
  * for new entries do we add in */
-#define CFG_ALLOC_ENTRY_BATCH 16
+#define CFG_ALLOC_ENTRY_BATCH 16 //一次最多申请多少个entry
 
 /**
  * Default cfgfile load parameters.
@@ -59,6 +63,7 @@ static const char valid_comment_chars[] = {
 	'@'
 };
 
+//对string执行strip
 static unsigned
 _strip(char *str, unsigned len)
 {
@@ -86,6 +91,7 @@ _strip(char *str, unsigned len)
 	return newlen;
 }
 
+//给定sectionname获得section结构体
 static struct rte_cfgfile_section *
 _get_section(struct rte_cfgfile *cfg, const char *sectionname)
 {
@@ -99,6 +105,7 @@ _get_section(struct rte_cfgfile *cfg, const char *sectionname)
 	return NULL;
 }
 
+//添加配置项
 static int
 _add_entry(struct rte_cfgfile_section *section, const char *entryname,
 		const char *entryvalue)
@@ -128,6 +135,7 @@ _add_entry(struct rte_cfgfile_section *section, const char *entryname,
 	return 0;
 }
 
+//检查配置指明的配置字符是否合乎规定
 static int
 rte_cfgfile_check_params(const struct rte_cfgfile_parameters *params)
 {
@@ -156,6 +164,7 @@ rte_cfgfile_check_params(const struct rte_cfgfile_parameters *params)
 	return 0;
 }
 
+//加载配置文件
 struct rte_cfgfile *
 rte_cfgfile_load(const char *filename, int flags)
 {
@@ -163,6 +172,7 @@ rte_cfgfile_load(const char *filename, int flags)
 					    &default_cfgfile_params);
 }
 
+//配置文件解析
 struct rte_cfgfile *
 rte_cfgfile_load_with_params(const char *filename, int flags,
 			     const struct rte_cfgfile_parameters *params)
@@ -174,33 +184,41 @@ rte_cfgfile_load_with_params(const char *filename, int flags,
 	if (rte_cfgfile_check_params(params))
 		return NULL;
 
+	//打开配置文件
 	FILE *f = fopen(filename, "r");
 	if (f == NULL)
 		return NULL;
 
 	cfg = rte_cfgfile_create(flags);
 
+	//读配置文件
 	while (fgets(buffer, sizeof(buffer), f) != NULL) {
 		char *pos;
 		size_t len = strnlen(buffer, sizeof(buffer));
-		lineno++;
+		lineno++;//计数行号
 		if ((len >= sizeof(buffer) - 1) && (buffer[len-1] != '\n')) {
+			//配置过长，报错（当前采用固定缓冲区)
 			CFG_LOG(ERR, " line %d - no \\n found on string. "
 					"Check if line too long\n", lineno);
 			goto error1;
 		}
+
 		/* skip parsing if comment character found */
+		//去除注释的字符后面的内容（这里有个bug,例下例示，假设#号为注释符）
+		//配置：“abcdef\#abcdef#really comments”将检查不出来
 		pos = memchr(buffer, params->comment_character, len);
 		if (pos != NULL && (*(pos-1) != '\\')) {
 			*pos = '\0';
-			len = pos -  buffer;
+			len = pos -  buffer;//有效字符串长度
 		}
 
 		len = _strip(buffer, len);
 		/* skip lines without useful content */
 		if (buffer[0] != '[' && memchr(buffer, '=', len) == NULL)
+			//非段（section)开始，且非vlaue开始，不处理(这个处理比较欠考虑）
 			continue;
 
+		//提取section
 		if (buffer[0] == '[') {
 			/* section heading line */
 			char *end = memchr(buffer, ']', len);
@@ -211,10 +229,11 @@ rte_cfgfile_load_with_params(const char *filename, int flags,
 				goto error1;
 			}
 			*end = '\0';
-			_strip(&buffer[1], end - &buffer[1]);
+			_strip(&buffer[1], end - &buffer[1]);//section 名称
 
 			rte_cfgfile_add_section(cfg, &buffer[1]);
 		} else {
+			//提取key,value
 			/* key and value line */
 			char *split[2] = {NULL};
 
@@ -229,10 +248,10 @@ rte_cfgfile_load_with_params(const char *filename, int flags,
 			*split[1] = '\0';
 			split[1]++;
 
-			_strip(split[0], strlen(split[0]));
-			_strip(split[1], strlen(split[1]));
-			char *end = memchr(split[1], '\\', strlen(split[1]));
+			_strip(split[0], strlen(split[0]));//strip key
+			_strip(split[1], strlen(split[1]));//strip value
 
+			//处理"\#"这种转义情况，将其转换为#
 			size_t split_len = strlen(split[1]) + 1;
 			while (end != NULL) {
 				if (*(end+1) == params->comment_character) {
@@ -243,6 +262,7 @@ rte_cfgfile_load_with_params(const char *filename, int flags,
 				end = memchr(end, '\\', strlen(end));
 			}
 
+			//是否支持empty value
 			if (!(flags & CFG_FLAG_EMPTY_VALUES) &&
 					(*split[1] == '\0')) {
 				CFG_LOG(ERR,
@@ -251,9 +271,11 @@ rte_cfgfile_load_with_params(const char *filename, int flags,
 				goto error1;
 			}
 
+			//当前不存在段，报错
 			if (cfg->num_sections == 0)
 				goto error1;
 
+			//将配置加入
 			_add_entry(&cfg->sections[cfg->num_sections - 1],
 					split[0], split[1]);
 		}
@@ -266,6 +288,7 @@ rte_cfgfile_load_with_params(const char *filename, int flags,
 	return NULL;
 }
 
+//创建空的cfgfile
 struct rte_cfgfile *
 rte_cfgfile_create(int flags)
 {
@@ -288,6 +311,7 @@ rte_cfgfile_create(int flags)
 
 	cfg->allocated_sections = CFG_ALLOC_SECTION_BATCH;
 
+	//初始化每个section
 	for (i = 0; i < CFG_ALLOC_SECTION_BATCH; i++) {
 		cfg->sections[i].entries = calloc(CFG_ALLOC_ENTRY_BATCH,
 					  sizeof(struct rte_cfgfile_entry));
@@ -299,6 +323,7 @@ rte_cfgfile_create(int flags)
 		cfg->sections[i].allocated_entries = CFG_ALLOC_ENTRY_BATCH;
 	}
 
+	//是否需要添加global section
 	if (flags & CFG_FLAG_GLOBAL_SECTION)
 		rte_cfgfile_add_section(cfg, "GLOBAL");
 
@@ -318,6 +343,7 @@ rte_cfgfile_create(int flags)
 	return NULL;
 }
 
+//增加section
 int
 rte_cfgfile_add_section(struct rte_cfgfile *cfg, const char *sectionname)
 {
@@ -330,6 +356,8 @@ rte_cfgfile_add_section(struct rte_cfgfile *cfg, const char *sectionname)
 		return -EINVAL;
 
 	/* resize overall struct if we don't have room for more	sections */
+	//检查是否所有已申请的段已使用完，如果是，则采用realloc扩大分配（由于结构体原因，仅
+	//增加了section,entry并没有被增大（当然也不需要）。
 	if (cfg->num_sections == cfg->allocated_sections) {
 
 		struct rte_cfgfile_section *n_sections =
@@ -341,6 +369,7 @@ rte_cfgfile_add_section(struct rte_cfgfile *cfg, const char *sectionname)
 		if (n_sections == NULL)
 			return -ENOMEM;
 
+		//初始化新增部分
 		for (i = 0; i < CFG_ALLOC_SECTION_BATCH; i++) {
 			n_sections[i + cfg->allocated_sections].num_entries = 0;
 			n_sections[i +
@@ -351,6 +380,7 @@ rte_cfgfile_add_section(struct rte_cfgfile *cfg, const char *sectionname)
 		cfg->allocated_sections += CFG_ALLOC_SECTION_BATCH;
 	}
 
+	//设置cfg->num_sections这一段为sectionname,增加num_sections
 	strlcpy(cfg->sections[cfg->num_sections].name, sectionname,
 		sizeof(cfg->sections[0].name));
 	cfg->sections[cfg->num_sections].num_entries = 0;
@@ -359,6 +389,7 @@ rte_cfgfile_add_section(struct rte_cfgfile *cfg, const char *sectionname)
 	return 0;
 }
 
+//动态添加配置
 int rte_cfgfile_add_entry(struct rte_cfgfile *cfg,
 		const char *sectionname, const char *entryname,
 		const char *entryvalue)
@@ -383,6 +414,7 @@ int rte_cfgfile_add_entry(struct rte_cfgfile *cfg,
 	return ret;
 }
 
+//修改配置
 int rte_cfgfile_set_entry(struct rte_cfgfile *cfg, const char *sectionname,
 		const char *entryname, const char *entryvalue)
 {
@@ -411,6 +443,7 @@ int rte_cfgfile_set_entry(struct rte_cfgfile *cfg, const char *sectionname,
 	return -EINVAL;
 }
 
+//将配置保存成文件
 int rte_cfgfile_save(struct rte_cfgfile *cfg, const char *filename)
 {
 	int i, j;
@@ -435,6 +468,7 @@ int rte_cfgfile_save(struct rte_cfgfile *cfg, const char *filename)
 	return fclose(f);
 }
 
+//配置文件释放
 int rte_cfgfile_close(struct rte_cfgfile *cfg)
 {
 	int i;
@@ -458,6 +492,7 @@ int rte_cfgfile_close(struct rte_cfgfile *cfg)
 	return 0;
 }
 
+//返回名称为sectionname的段，有多少个
 int
 rte_cfgfile_num_sections(struct rte_cfgfile *cfg, const char *sectionname,
 size_t length)
@@ -471,6 +506,7 @@ size_t length)
 	return num_sections;
 }
 
+//获取当前配置文件的sections,并将其填充在sections中，最多填充max_sections项
 int
 rte_cfgfile_sections(struct rte_cfgfile *cfg, char *sections[],
 	int max_sections)
@@ -483,12 +519,14 @@ rte_cfgfile_sections(struct rte_cfgfile *cfg, char *sections[],
 	return i;
 }
 
+//检查指定section段是否存在
 int
 rte_cfgfile_has_section(struct rte_cfgfile *cfg, const char *sectionname)
 {
 	return _get_section(cfg, sectionname) != NULL;
 }
 
+//有多少个section entries
 int
 rte_cfgfile_section_num_entries(struct rte_cfgfile *cfg,
 	const char *sectionname)
@@ -499,6 +537,7 @@ rte_cfgfile_section_num_entries(struct rte_cfgfile *cfg,
 	return s->num_entries;
 }
 
+//给定段索引，取段名称及段entry数目
 int
 rte_cfgfile_section_num_entries_by_index(struct rte_cfgfile *cfg,
 	char *sectionname, int index)
@@ -511,6 +550,8 @@ rte_cfgfile_section_num_entries_by_index(struct rte_cfgfile *cfg,
 	strlcpy(sectionname, sect->name, CFG_NAME_LEN);
 	return sect->num_entries;
 }
+
+//给定段名称，获取段内的entry,最多获取max_entries个
 int
 rte_cfgfile_section_entries(struct rte_cfgfile *cfg, const char *sectionname,
 		struct rte_cfgfile_entry *entries, int max_entries)
@@ -524,6 +565,7 @@ rte_cfgfile_section_entries(struct rte_cfgfile *cfg, const char *sectionname,
 	return i;
 }
 
+//给段索引获取段名称，段内的entriy,最多获取max_entries个
 int
 rte_cfgfile_section_entries_by_index(struct rte_cfgfile *cfg, int index,
 		char *sectionname,
@@ -541,6 +583,7 @@ rte_cfgfile_section_entries_by_index(struct rte_cfgfile *cfg, int index,
 	return i;
 }
 
+//给定段名，entry名，取其对应的配置值
 const char *
 rte_cfgfile_get_entry(struct rte_cfgfile *cfg, const char *sectionname,
 		const char *entryname)
@@ -556,6 +599,7 @@ rte_cfgfile_get_entry(struct rte_cfgfile *cfg, const char *sectionname,
 	return NULL;
 }
 
+//给定段名，entry名，检查是否存在其对应的配置
 int
 rte_cfgfile_has_entry(struct rte_cfgfile *cfg, const char *sectionname,
 		const char *entryname)
diff --git a/lib/librte_cfgfile/rte_cfgfile.h b/lib/librte_cfgfile/rte_cfgfile.h
index b2030fa66..ac473fe98 100644
--- a/lib/librte_cfgfile/rte_cfgfile.h
+++ b/lib/librte_cfgfile/rte_cfgfile.h
@@ -40,7 +40,7 @@ struct rte_cfgfile_entry {
 /** Configuration file operation optional arguments */
 struct rte_cfgfile_parameters {
 	/** Config file comment character; one of '!', '#', '%', ';', '@' */
-	char comment_character;
+	char comment_character;//配置文件注释字符串
 };
 
 /**@{ cfgfile load operation flags */
@@ -50,13 +50,13 @@ enum {
 	 * defined section.  These entries can be accessed in the "GLOBAL"
 	 * section.
 	 */
-	CFG_FLAG_GLOBAL_SECTION = 1,
+	CFG_FLAG_GLOBAL_SECTION = 1,//是否需要增加global段
 
 	/**
 	 * Indicates that file supports key value entries where the value can
 	 * be zero length (e.g., "key=").
 	 */
-	CFG_FLAG_EMPTY_VALUES = 2,
+	CFG_FLAG_EMPTY_VALUES = 2,//是否容许空的value
 };
 /**@} */
 
diff --git a/lib/librte_cmdline/cmdline.c b/lib/librte_cmdline/cmdline.c
index 53cda84c1..3db2e0a8c 100644
--- a/lib/librte_cmdline/cmdline.c
+++ b/lib/librte_cmdline/cmdline.c
@@ -28,13 +28,14 @@ cmdline_valid_buffer(struct rdline *rdl, const char *buf,
 {
 	struct cmdline *cl = rdl->opaque;
 	int ret;
+	//解析命令行
 	ret = cmdline_parse(cl, buf);
 	if (ret == CMDLINE_PARSE_AMBIGUOUS)
-		cmdline_printf(cl, "Ambiguous command\n");
+		cmdline_printf(cl, "Ambiguous command\n");//歧义的命令
 	else if (ret == CMDLINE_PARSE_NOMATCH)
-		cmdline_printf(cl, "Command not found\n");
+		cmdline_printf(cl, "Command not found\n");//未知的命令
 	else if (ret == CMDLINE_PARSE_BAD_ARGS)
-		cmdline_printf(cl, "Bad arguments\n");
+		cmdline_printf(cl, "Bad arguments\n");//命令参数有误
 }
 
 static int
@@ -46,6 +47,7 @@ cmdline_complete_buffer(struct rdline *rdl, const char *buf,
 	return cmdline_complete(cl, buf, state, dstbuf, dstsize);
 }
 
+//向s_out中写入单个字符
 int
 cmdline_write_char(struct rdline *rdl, char c)
 {
@@ -63,7 +65,7 @@ cmdline_write_char(struct rdline *rdl, char c)
 	return ret;
 }
 
-
+//设置提示述符
 void
 cmdline_set_prompt(struct cmdline *cl, const char *prompt)
 {
@@ -83,7 +85,7 @@ cmdline_new(cmdline_parse_ctx_t *ctx, const char *prompt, int s_in, int s_out)
 
 	cl = malloc(sizeof(struct cmdline));
 	if (cl == NULL)
-		return NULL;
+		return NULL;//申请内存失败
 	memset(cl, 0, sizeof(struct cmdline));
 	cl->s_in = s_in;
 	cl->s_out = s_out;
@@ -118,6 +120,7 @@ cmdline_free(struct cmdline *cl)
 	free(cl);
 }
 
+//命令行输出语句
 void
 cmdline_printf(const struct cmdline *cl, const char *fmt, ...)
 {
@@ -133,6 +136,7 @@ cmdline_printf(const struct cmdline *cl, const char *fmt, ...)
 	va_end(ap);
 }
 
+//命令行字符串输入
 int
 cmdline_in(struct cmdline *cl, const char *buf, int size)
 {
@@ -159,8 +163,9 @@ cmdline_in(struct cmdline *cl, const char *buf, int size)
 				same = 0;
 			buflen = strnlen(buffer, RDLINE_BUF_SIZE);
 			if (buflen > 1 && !same)
+				//将命令加入到历史中
 				rdline_add_history(&cl->rdl, buffer);
-			rdline_newline(&cl->rdl, cl->prompt);
+			rdline_newline(&cl->rdl, cl->prompt);//显示新行，显示提示符
 		}
 		else if (ret == RDLINE_RES_EOF)
 			return -1;
@@ -178,6 +183,7 @@ cmdline_quit(struct cmdline *cl)
 	rdline_quit(&cl->rdl);
 }
 
+//读取命令行输入
 int
 cmdline_poll(struct cmdline *cl)
 {
@@ -189,22 +195,24 @@ cmdline_poll(struct cmdline *cl)
 	if (!cl)
 		return -EINVAL;
 	else if (cl->rdl.status == RDLINE_EXITED)
+		//如果要求命令行退出，则直接返回退出
 		return RDLINE_EXITED;
 
 	pfd.fd = cl->s_in;
 	pfd.events = POLLIN;
 	pfd.revents = 0;
 
-	status = poll(&pfd, 1, 0);
+	status = poll(&pfd, 1, 0);//自标准输入中检测事件
 	if (status < 0)
 		return status;
 	else if (status > 0) {
 		c = -1;
-		read_status = read(cl->s_in, &c, 1);
+		read_status = read(cl->s_in, &c, 1);//读一个字符
 		if (read_status < 0)
+			//读取失败，返回
 			return read_status;
 
-		status = cmdline_in(cl, &c, 1);
+		status = cmdline_in(cl, &c, 1);//输入一个字符
 		if (status < 0 && cl->rdl.status != RDLINE_EXITED)
 			return status;
 	}
diff --git a/lib/librte_cmdline/cmdline.h b/lib/librte_cmdline/cmdline.h
index 27d2effdf..07c3dab5f 100644
--- a/lib/librte_cmdline/cmdline.h
+++ b/lib/librte_cmdline/cmdline.h
@@ -21,13 +21,14 @@
 extern "C" {
 #endif
 
+//非常差的库，基本是个垃圾
 struct cmdline {
-	int s_in;
-	int s_out;
-	cmdline_parse_ctx_t *ctx;
+	int s_in;//输入fd
+	int s_out;//输出fd
+	cmdline_parse_ctx_t *ctx;//支持的命令数组
 	struct rdline rdl;
-	char prompt[RDLINE_PROMPT_SIZE];
-	struct termios oldterm;
+	char prompt[RDLINE_PROMPT_SIZE];//命令行提示符
+	struct termios oldterm;//启动前版本的term设备
 };
 
 struct cmdline *cmdline_new(cmdline_parse_ctx_t *ctx, const char *prompt, int s_in, int s_out);
diff --git a/lib/librte_cmdline/cmdline_cirbuf.c b/lib/librte_cmdline/cmdline_cirbuf.c
index 829a8af56..3d4f327d9 100644
--- a/lib/librte_cmdline/cmdline_cirbuf.c
+++ b/lib/librte_cmdline/cmdline_cirbuf.c
@@ -11,6 +11,7 @@
 #include "cmdline_cirbuf.h"
 
 
+//初始化circle buffer
 int
 cirbuf_init(struct cirbuf *cbuf, char *buf, unsigned int start, unsigned int maxlen)
 {
@@ -31,6 +32,7 @@ cirbuf_add_buf_head(struct cirbuf *cbuf, const char *c, unsigned int n)
 {
 	unsigned int e;
 
+	//要放入的字符，必须有足够的空间
 	if (!cbuf || !c || !n || n > CIRBUF_GET_FREELEN(cbuf))
 		return -EINVAL;
 
@@ -49,8 +51,8 @@ cirbuf_add_buf_head(struct cirbuf *cbuf, const char *c, unsigned int n)
 		memcpy(cbuf->buf + cbuf->maxlen - n + (cbuf->start + e), c,
 		       n - (cbuf->start + e));
 	}
-	cbuf->len += n;
-	cbuf->start += (cbuf->maxlen - n + e);
+	cbuf->len += n;//有效字符数增加
+	cbuf->start += (cbuf->maxlen - n + e);//start反着向前走
 	cbuf->start %= cbuf->maxlen;
 	return n;
 }
@@ -88,13 +90,17 @@ cirbuf_add_buf_tail(struct cirbuf *cbuf, const char *c, unsigned int n)
 }
 
 /* add at head */
-
+//在cbuf->start指定的位置前加一个'c'
 static inline void
 __cirbuf_add_head(struct cirbuf * cbuf, char c)
 {
 	if (!CIRBUF_IS_EMPTY(cbuf)) {
+		//当不为空时，cbuf->start指向已用的空间
+		//start向前减
 		cbuf->start += (cbuf->maxlen - 1);
 		cbuf->start %= cbuf->maxlen;
+		//
+		//cbuf->start = ((cbuf->start + 1) % cbuf->maxlen)
 	}
 	cbuf->buf[cbuf->start] = c;
 	cbuf->len ++;
@@ -103,6 +109,7 @@ __cirbuf_add_head(struct cirbuf * cbuf, char c)
 int
 cirbuf_add_head_safe(struct cirbuf * cbuf, char c)
 {
+	//会检查是否为满
 	if (cbuf && !CIRBUF_IS_FULL(cbuf)) {
 		__cirbuf_add_head(cbuf, c);
 		return 0;
@@ -110,6 +117,7 @@ cirbuf_add_head_safe(struct cirbuf * cbuf, char c)
 	return -EINVAL;
 }
 
+//不检查是否为满，直接加入
 void
 cirbuf_add_head(struct cirbuf * cbuf, char c)
 {
@@ -122,16 +130,18 @@ static inline void
 __cirbuf_add_tail(struct cirbuf * cbuf, char c)
 {
 	if (!CIRBUF_IS_EMPTY(cbuf)) {
+		//当buf不为空时，end向后移
 		cbuf->end ++;
 		cbuf->end %= cbuf->maxlen;
 	}
-	cbuf->buf[cbuf->end] = c;
+	cbuf->buf[cbuf->end] = c;//存入数据
 	cbuf->len ++;
 }
 
 int
 cirbuf_add_tail_safe(struct cirbuf * cbuf, char c)
 {
+	//会检查是否为满
 	if (cbuf && !CIRBUF_IS_FULL(cbuf)) {
 		__cirbuf_add_tail(cbuf, c);
 		return 0;
@@ -139,6 +149,7 @@ cirbuf_add_tail_safe(struct cirbuf * cbuf, char c)
 	return -EINVAL;
 }
 
+//不检查是否为满
 void
 cirbuf_add_tail(struct cirbuf * cbuf, char c)
 {
@@ -146,16 +157,19 @@ cirbuf_add_tail(struct cirbuf * cbuf, char c)
 }
 
 
+//字符向左移
 static inline void
 __cirbuf_shift_left(struct cirbuf *cbuf)
 {
 	unsigned int i;
 	char tmp = cbuf->buf[cbuf->start];
 
+	//将cbuf->start指向的字符向左移一个索引位
 	for (i=0 ; i<cbuf->len ; i++) {
 		cbuf->buf[(cbuf->start+i)%cbuf->maxlen] =
 			cbuf->buf[(cbuf->start+i+1)%cbuf->maxlen];
 	}
+	//start左移一位，end左移一位
 	cbuf->buf[(cbuf->start-1+cbuf->maxlen)%cbuf->maxlen] = tmp;
 	cbuf->start += (cbuf->maxlen - 1);
 	cbuf->start %= cbuf->maxlen;
@@ -163,6 +177,7 @@ __cirbuf_shift_left(struct cirbuf *cbuf)
 	cbuf->end %= cbuf->maxlen;
 }
 
+//字符向右移
 static inline void
 __cirbuf_shift_right(struct cirbuf *cbuf)
 {
@@ -181,6 +196,7 @@ __cirbuf_shift_right(struct cirbuf *cbuf)
 }
 
 /* XXX we could do a better algorithm here... */
+//使start移动到0位置（通过左移或者右移来实现）
 int
 cirbuf_align_left(struct cirbuf * cbuf)
 {
@@ -202,6 +218,7 @@ cirbuf_align_left(struct cirbuf * cbuf)
 }
 
 /* XXX we could do a better algorithm here... */
+//使end移动到cbuf->maxlen-1位置
 int
 cirbuf_align_right(struct cirbuf * cbuf)
 {
@@ -224,6 +241,7 @@ cirbuf_align_right(struct cirbuf * cbuf)
 
 /* buffer del */
 
+//移除掉从start位置开始size字节的数据
 int
 cirbuf_del_buf_head(struct cirbuf *cbuf, unsigned int size)
 {
@@ -244,6 +262,7 @@ cirbuf_del_buf_head(struct cirbuf *cbuf, unsigned int size)
 
 /* buffer del */
 
+//移除掉从end结尾向前size字节的数据
 int
 cirbuf_del_buf_tail(struct cirbuf *cbuf, unsigned int size)
 {
@@ -263,7 +282,7 @@ cirbuf_del_buf_tail(struct cirbuf *cbuf, unsigned int size)
 }
 
 /* del at head */
-
+//移掉一个字符
 static inline void
 __cirbuf_del_head(struct cirbuf * cbuf)
 {
@@ -291,7 +310,7 @@ cirbuf_del_head(struct cirbuf * cbuf)
 }
 
 /* del at tail */
-
+//从end位置向前移掉一个字符
 static inline void
 __cirbuf_del_tail(struct cirbuf * cbuf)
 {
@@ -319,7 +338,7 @@ cirbuf_del_tail(struct cirbuf * cbuf)
 }
 
 /* convert to buffer */
-
+//从start位置到end位置取最多size个字符
 int
 cirbuf_get_buf_head(struct cirbuf *cbuf, char *c, unsigned int size)
 {
@@ -357,7 +376,7 @@ cirbuf_get_buf_head(struct cirbuf *cbuf, char *c, unsigned int size)
 }
 
 /* convert to buffer */
-
+//从end向前，最多取size个字符
 int
 cirbuf_get_buf_tail(struct cirbuf *cbuf, char *c, unsigned int size)
 {
@@ -396,7 +415,7 @@ cirbuf_get_buf_tail(struct cirbuf *cbuf, char *c, unsigned int size)
 }
 
 /* get head or get tail */
-
+//取start指向的字符
 char
 cirbuf_get_head(struct cirbuf * cbuf)
 {
@@ -404,7 +423,7 @@ cirbuf_get_head(struct cirbuf * cbuf)
 }
 
 /* get head or get tail */
-
+//取end指向的字符
 char
 cirbuf_get_tail(struct cirbuf * cbuf)
 {
diff --git a/lib/librte_cmdline/cmdline_cirbuf.h b/lib/librte_cmdline/cmdline_cirbuf.h
index c23b211ad..780b90391 100644
--- a/lib/librte_cmdline/cmdline_cirbuf.h
+++ b/lib/librte_cmdline/cmdline_cirbuf.h
@@ -17,11 +17,11 @@ extern "C" {
  * This structure is the header of a cirbuf type.
  */
 struct cirbuf {
-	unsigned int maxlen;    /**< total len of the fifo (number of elements) */
-	unsigned int start;     /**< indice of the first elt */
-	unsigned int end;       /**< indice of the last elt */
-	unsigned int len;       /**< current len of fifo */
-	char *buf;
+	unsigned int maxlen;    /**< total len of the fifo (number of elements) */ //缓冲区长度
+	unsigned int start;     /**< indice of the first elt */ //数据存放起始位置
+	unsigned int end;       /**< indice of the last elt */ //数据消费起始位置
+	unsigned int len;       /**< current len of fifo */ //当前占用的长度
+	char *buf;//缓冲区
 };
 
 #ifdef RTE_LIBRTE_CMDLINE_DEBUG
@@ -46,6 +46,7 @@ int cirbuf_init(struct cirbuf *cbuf, char *buf, unsigned int start, unsigned int
 /**
  * Return 1 if the circular buffer is empty
  */
+//必须为空
 #define CIRBUF_IS_EMPTY(cirbuf) ((cirbuf)->len == 0)
 
 /**
diff --git a/lib/librte_cmdline/cmdline_parse.c b/lib/librte_cmdline/cmdline_parse.c
index b57b30e8f..1826347aa 100644
--- a/lib/librte_cmdline/cmdline_parse.c
+++ b/lib/librte_cmdline/cmdline_parse.c
@@ -30,6 +30,7 @@
 
 /* isblank() needs _XOPEN_SOURCE >= 600 || _ISOC99_SOURCE, so use our
  * own. */
+//空字符检测
 static int
 isblank2(char c)
 {
@@ -39,6 +40,7 @@ isblank2(char c)
 	return 0;
 }
 
+//行尾字符检测
 static int
 isendofline(char c)
 {
@@ -48,6 +50,7 @@ isendofline(char c)
 	return 0;
 }
 
+//'#'号字符
 static int
 iscomment(char c)
 {
@@ -56,9 +59,11 @@ iscomment(char c)
 	return 0;
 }
 
+//检查token是否结束
 int
 cmdline_isendoftoken(char c)
 {
+	//字符串结束，或者注释开始，或者空格，或者换行
 	if (!c || iscomment(c) || isblank2(c) || isendofline(c))
 		return 1;
 	return 0;
@@ -85,6 +90,7 @@ nb_common_chars(const char * s1, const char * s2)
 	return i;
 }
 
+//调用命令行处理函数
 /** Retrieve either static or dynamic token at a given index. */
 static cmdline_parse_token_hdr_t *
 get_token(cmdline_parse_inst_t *inst, unsigned int index)
@@ -96,7 +102,7 @@ get_token(cmdline_parse_inst_t *inst, unsigned int index)
 		return inst->tokens[index];
 	/* generate dynamic token */
 	token_p = NULL;
-	inst->f(&token_p, NULL, &inst->tokens[index]);
+	inst->f(&token_p, NULL, &inst->tokens[index]);//执行命令行回调
 	return token_p;
 }
 
@@ -118,21 +124,24 @@ match_inst(cmdline_parse_inst_t *inst, const char *buf,
 		memset(resbuf, 0, resbuf_size);
 	/* check if we match all tokens of inst */
 	while (!nb_match_token || i < nb_match_token) {
+		//取inst命令第i个token
 		token_p = get_token(inst, i);
 		if (!token_p)
-			break;
+			break;//无i token,匹配失败
 		memcpy(&token_hdr, token_p, sizeof(token_hdr));
 
 		debug_printf("TK\n");
 		/* skip spaces */
+		//跳过前导的空字符
 		while (isblank2(*buf)) {
 			buf++;
 		}
 
 		/* end of buf */
 		if ( isendofline(*buf) || iscomment(*buf) )
-			break;
+			break;//包含空字符的串，无法匹配，跳出
 
+		//调用token的解析函数，完成命令行中此token的解析
 		if (resbuf == NULL) {
 			n = token_hdr.ops->parse(token_p, buf, NULL, 0);
 		} else {
@@ -152,8 +161,9 @@ match_inst(cmdline_parse_inst_t *inst, const char *buf,
 		}
 
 		if (n < 0)
-			break;
+			break;//解析失败
 
+		//解析成功，切换到下一个token的处理
 		debug_printf("TK parsed (len=%d)\n", n);
 		i++;
 		buf += n;
@@ -166,9 +176,9 @@ match_inst(cmdline_parse_inst_t *inst, const char *buf,
 	/* in case we want to match a specific num of token */
 	if (nb_match_token) {
 		if (i == nb_match_token) {
-			return 0;
+			return 0;//指定match token数时，且已匹配完成，则返回0
 		}
-		return i;
+		return i;//否则返回匹配数
 	}
 
 	/* we don't match all the tokens */
@@ -190,6 +200,23 @@ match_inst(cmdline_parse_inst_t *inst, const char *buf,
 }
 
 
+//实现命令行解析
+//dpdk中的命令行功能，使用起来非常麻烦，解析起来了也事情比较多,写得比较屎
+/**
+ * 这块应这样做
+ * 1。定义已知的类型（dpdk中也有这种概念，例如cmdline_parse_token_string_t），
+ * 2。针对已知类型，定义字面型式，例如"<string>","<uin8>","<ipv4>" 及其对应的parse函数，可自主扩展
+ * 3. 如果（1），（2）完成，则对于任意命令 均可写成token流形式，例如 ifconfig <string> <ipv4> <ipmask>
+ * 4. （3）步可实现命令配的匹配及参数提取，可定义validate对参数进入联想调验
+ * 5。定义命令集对应的help,incomplete函数
+ * 6。 命令提供对应的f函数，完成命令对应的业务
+ *
+ * 以下可实现：
+ * 1。用户通过2步定义的元素，定义自已的命令，如果需要命令层次需定义相应的分隔符
+ * 2. 用户提供此命令对应的校验函数   ＊大工作
+ * 3。用户提供此命令对应的help文字
+ * 4。用户提供此命令对应的f函数完成业务 ＊大工作
+ */
 int
 cmdline_parse(struct cmdline *cl, const char * buf)
 {
@@ -221,14 +248,17 @@ cmdline_parse(struct cmdline *cl, const char * buf)
 	 * - count line length
 	 */
 	curbuf = buf;
+	//如果*curbuf非换行符，则继续循环（用于在buf中找出换行符位置）
 	while (! isendofline(*curbuf)) {
 		if ( *curbuf == '\0' ) {
 			debug_printf("Incomplete buf (len=%d)\n", linelen);
 			return 0;
 		}
+		//遇到注释符
 		if ( iscomment(*curbuf) ) {
 			comment = 1;
 		}
+		//如果非注释符，非空字符，则为需要解析字符
 		if ( ! isblank2(*curbuf) && ! comment) {
 			parse_it = 1;
 		}
@@ -237,11 +267,13 @@ cmdline_parse(struct cmdline *cl, const char * buf)
 	}
 
 	/* skip all endofline chars */
+	//跳过第一个endofline后面的其它endofline
 	while (isendofline(buf[linelen])) {
 		linelen++;
 	}
 
 	/* empty line */
+	//空行（含全注释行，目前不支持半注释行）
 	if ( parse_it == 0 ) {
 		debug_printf("Empty line (len=%d)\n", linelen);
 		return linelen;
@@ -251,6 +283,7 @@ cmdline_parse(struct cmdline *cl, const char * buf)
 		     linelen, linelen > 64 ? 64 : linelen, buf);
 
 	/* parse it !! */
+	//遍历命令数组，找到匹配此行的命令
 	inst = ctx[inst_num];
 	while (inst) {
 		debug_printf("INST %d\n", inst_num);
@@ -291,6 +324,7 @@ cmdline_parse(struct cmdline *cl, const char * buf)
 	}
 
 	/* call func */
+	//有命令回调，执行命令回调
 	if (f) {
 		f(result.buf, cl, data);
 	}
@@ -470,6 +504,7 @@ cmdline_complete(struct cmdline *cl, const char *buf, int *state,
 			}
 			(*state)++;
 			if (token_p && token_hdr.ops->get_help) {
+				//提取帮助信息
 				token_hdr.ops->get_help(token_p, tmpbuf,
 							sizeof(tmpbuf));
 				help_str = inst->help_str;
diff --git a/lib/librte_cmdline/cmdline_parse.h b/lib/librte_cmdline/cmdline_parse.h
index e4d802fff..cc58ecbc8 100644
--- a/lib/librte_cmdline/cmdline_parse.h
+++ b/lib/librte_cmdline/cmdline_parse.h
@@ -35,7 +35,7 @@ extern "C" {
  */
 struct cmdline_token_hdr {
 	struct cmdline_token_ops *ops;
-	unsigned int offset;
+	unsigned int offset;//需要填充的结构体中成员的偏移量（结构体在解析中给出）
 };
 typedef struct cmdline_token_hdr cmdline_parse_token_hdr_t;
 
@@ -63,8 +63,10 @@ struct cmdline_token_ops {
 	int (*parse)(cmdline_parse_token_hdr_t *, const char *, void *,
 		unsigned int);
 	/** return the num of possible choices for this token */
+	//返回可能的选择
 	int (*complete_get_nb)(cmdline_parse_token_hdr_t *);
 	/** return the elt x for this token (token, idx, dstbuf, size) */
+	//返回具体的某一个可能的选择（即选择的字面值）
 	int (*complete_get_elt)(cmdline_parse_token_hdr_t *, int, char *,
 		unsigned int);
 	/** get help for this token (token, dstbuf, size) */
@@ -131,7 +133,7 @@ struct cmdline;
 struct cmdline_inst {
 	/* f(parsed_struct, data) */
 	void (*f)(void *, struct cmdline *, void *);
-	void *data;
+	void *data;//f参数
 	const char *help_str;
 	cmdline_parse_token_hdr_t *tokens[];
 };
diff --git a/lib/librte_cmdline/cmdline_parse_etheraddr.c b/lib/librte_cmdline/cmdline_parse_etheraddr.c
index 2cb8dd2a1..9aaddf432 100644
--- a/lib/librte_cmdline/cmdline_parse_etheraddr.c
+++ b/lib/librte_cmdline/cmdline_parse_etheraddr.c
@@ -26,6 +26,8 @@ struct cmdline_token_ops cmdline_token_etheraddr_ops = {
 	.get_help = cmdline_get_help_etheraddr,
 };
 
+
+//解析mac地址
 int
 cmdline_parse_etheraddr(__attribute__((unused)) cmdline_parse_token_hdr_t *tk,
 	const char *buf, void *res, unsigned ressize)
@@ -40,6 +42,7 @@ cmdline_parse_etheraddr(__attribute__((unused)) cmdline_parse_token_hdr_t *tk,
 	if (!buf || ! *buf)
 		return -1;
 
+	//找出token结束位置
 	while (!cmdline_isendoftoken(buf[token_len]))
 		token_len++;
 
@@ -57,6 +60,7 @@ cmdline_parse_etheraddr(__attribute__((unused)) cmdline_parse_token_hdr_t *tk,
 	return token_len;
 }
 
+//返回帮助信息
 int
 cmdline_get_help_etheraddr(__attribute__((unused)) cmdline_parse_token_hdr_t *tk,
 			       char *dstbuf, unsigned int size)
diff --git a/lib/librte_cmdline/cmdline_parse_ipaddr.c b/lib/librte_cmdline/cmdline_parse_ipaddr.c
index 4de5ba35a..947a0b907 100644
--- a/lib/librte_cmdline/cmdline_parse_ipaddr.c
+++ b/lib/librte_cmdline/cmdline_parse_ipaddr.c
@@ -32,6 +32,7 @@ struct cmdline_token_ops cmdline_token_ipaddr_ops = {
 #define PREFIXMAX 128
 #define V4PREFIXMAX 32
 
+//ip地址解析
 int
 cmdline_parse_ipaddr(cmdline_parse_token_hdr_t *tk, const char *buf, void *res,
 	unsigned ressize)
@@ -79,6 +80,7 @@ cmdline_parse_ipaddr(cmdline_parse_token_hdr_t *tk, const char *buf, void *res,
 	}
 
 	/* convert the IP addr */
+	//如果支持ipv4地址，则进行ipv4地址解析
 	if ((tk2->ipaddr_data.flags & CMDLINE_IPADDR_V4) &&
 	    inet_pton(AF_INET, ip_str, &ipaddr.addr.ipv4) == 1 &&
 		prefixlen <= V4PREFIXMAX) {
@@ -87,6 +89,7 @@ cmdline_parse_ipaddr(cmdline_parse_token_hdr_t *tk, const char *buf, void *res,
 			memcpy(res, &ipaddr, sizeof(ipaddr));
 		return token_len;
 	}
+	//如果支持ipv6地址，则进行ipv6地址解析
 	if ((tk2->ipaddr_data.flags & CMDLINE_IPADDR_V6) &&
 	    inet_pton(AF_INET6, ip_str, &ipaddr.addr.ipv6) == 1) {
 		ipaddr.family = AF_INET6;
diff --git a/lib/librte_cmdline/cmdline_parse_num.c b/lib/librte_cmdline/cmdline_parse_num.c
index 478f181b4..51e59883d 100644
--- a/lib/librte_cmdline/cmdline_parse_num.c
+++ b/lib/librte_cmdline/cmdline_parse_num.c
@@ -22,6 +22,7 @@
 #define debug_printf(args...) do {} while(0)
 #endif
 
+//数字类型解析
 struct cmdline_token_ops cmdline_token_num_ops = {
 	.parse = cmdline_parse_num,
 	.complete_get_nb = NULL,
@@ -96,6 +97,7 @@ check_res_size(struct cmdline_token_num_data *nd, unsigned ressize)
 }
 
 /* parse an int */
+//解析数字
 int
 cmdline_parse_num(cmdline_parse_token_hdr_t *tk, const char *srcbuf, void *res,
 	unsigned ressize)
@@ -119,10 +121,11 @@ cmdline_parse_num(cmdline_parse_token_hdr_t *tk, const char *srcbuf, void *res,
 
 	/* check that we have enough room in res */
 	if (res) {
-		if (check_res_size(&nd, ressize) < 0)
+		if (check_res_size(&nd, ressize) < 0) //参数检查
 			return -1;
 	}
 
+	//小型的数字识别器代码
 	while ( st != ERROR && c && ! cmdline_isendoftoken(c) ) {
 		debug_printf("%c %x -> ", c, c);
 		switch (st) {
@@ -325,6 +328,7 @@ cmdline_parse_num(cmdline_parse_token_hdr_t *tk, const char *srcbuf, void *res,
 
 
 /* parse an int */
+//数字解析帮助信息
 int
 cmdline_get_help_num(cmdline_parse_token_hdr_t *tk, char *dstbuf, unsigned int size)
 {
diff --git a/lib/librte_cmdline/cmdline_parse_num.h b/lib/librte_cmdline/cmdline_parse_num.h
index 58b28cad7..81b35dcdc 100644
--- a/lib/librte_cmdline/cmdline_parse_num.h
+++ b/lib/librte_cmdline/cmdline_parse_num.h
@@ -30,7 +30,7 @@ struct cmdline_token_num_data {
 
 struct cmdline_token_num {
 	struct cmdline_token_hdr hdr;
-	struct cmdline_token_num_data num_data;
+	struct cmdline_token_num_data num_data;//整数的类型
 };
 typedef struct cmdline_token_num cmdline_parse_token_num_t;
 
diff --git a/lib/librte_cmdline/cmdline_parse_string.c b/lib/librte_cmdline/cmdline_parse_string.c
index 9cf41d0f7..2fd286e7f 100644
--- a/lib/librte_cmdline/cmdline_parse_string.c
+++ b/lib/librte_cmdline/cmdline_parse_string.c
@@ -27,6 +27,7 @@ struct cmdline_token_ops cmdline_token_string_ops = {
 #define ANYSTRINGS_HELP   "Any STRINGS"
 #define FIXEDSTRING_HELP  "Fixed STRING"
 
+//取字符串有效长度 ‘＃’号以后的字符将被忽略
 static unsigned int
 get_token_len(const char *s)
 {
@@ -41,16 +42,18 @@ get_token_len(const char *s)
 	return i;
 }
 
+//获取下一个可选token(采用#号分隔）
 static const char *
 get_next_token(const char *s)
 {
 	unsigned int i;
 	i = get_token_len(s);
 	if (s[i] == '#')
-		return s+i+1;
+		return s+i+1;//跳过#号
 	return NULL;
 }
 
+//支持三种形式，已知的单token方式（容许选择），未知的单token方式（不容许选择），未知的string形式
 int
 cmdline_parse_string(cmdline_parse_token_hdr_t *tk, const char *buf, void *res,
 	unsigned ressize)
@@ -72,31 +75,38 @@ cmdline_parse_string(cmdline_parse_token_hdr_t *tk, const char *buf, void *res,
 
 	/* fixed string (known single token) */
 	if ((sd->str != NULL) && (strcmp(sd->str, TOKEN_STRING_MULTI) != 0)) {
+		//配置了字符串值，且字符串值不为“空串”
 		str = sd->str;
 		do {
 			token_len = get_token_len(str);
 
 			/* if token is too big... */
 			if (token_len >= STR_TOKEN_SIZE - 1) {
+				//过长不处理
 				continue;
 			}
 
 			if ( strncmp(buf, str, token_len) ) {
+				//不匹配
 				continue;
 			}
 
 			if ( !cmdline_isendoftoken(*(buf+token_len)) ) {
+				//需要继续匹配
 				continue;
 			}
 
 			break;
-		} while ( (str = get_next_token(str)) != NULL );
+		} while ( (str = get_next_token(str)) != NULL );//用'#'号来表示可选取任意
 
 		if (!str)
+			//未匹配此sd
 			return -1;
 	}
+	//多个string的情况
 	/* multi string */
 	else if (sd->str != NULL) {
+		//此时sd->str为“”空串，识别为string形式（多token)
 		if (ressize < STR_MULTI_TOKEN_SIZE)
 			return -1;
 
@@ -111,6 +121,7 @@ cmdline_parse_string(cmdline_parse_token_hdr_t *tk, const char *buf, void *res,
 	}
 	/* unspecified string (unknown single token) */
 	else {
+		//未知的单token形式
 		token_len = 0;
 		while(!cmdline_isendoftoken(buf[token_len]) &&
 		      token_len < (STR_TOKEN_SIZE-1))
@@ -122,6 +133,7 @@ cmdline_parse_string(cmdline_parse_token_hdr_t *tk, const char *buf, void *res,
 		}
 	}
 
+	//为res中写入分析出来的token,并将分析的token长度返回
 	if (res) {
 		if ((sd->str != NULL) && (strcmp(sd->str, TOKEN_STRING_MULTI) == 0))
 			/* we are sure that token_len is < STR_MULTI_TOKEN_SIZE-1 */
@@ -136,6 +148,7 @@ cmdline_parse_string(cmdline_parse_token_hdr_t *tk, const char *buf, void *res,
 	return token_len;
 }
 
+//返回有多少个token可选
 int cmdline_complete_get_nb_string(cmdline_parse_token_hdr_t *tk)
 {
 	struct cmdline_token_string *tk2;
@@ -150,15 +163,17 @@ int cmdline_complete_get_nb_string(cmdline_parse_token_hdr_t *tk)
 	sd = &tk2->string_data;
 
 	if (!sd->str)
+		//未知单token (无法选择）
 		return 0;
 
 	str = sd->str;
 	while( (str = get_next_token(str)) != NULL ) {
-		ret++;
+		ret++;//已知的单token(可以选择，容许在ret个中选择）
 	}
 	return ret;
 }
 
+//通过complete_get_nb可以知道有多少个token可选，采用此函数，返回具体的某一个可选token
 int cmdline_complete_get_elt_string(cmdline_parse_token_hdr_t *tk, int idx,
 				    char *dstbuf, unsigned int size)
 {
@@ -175,6 +190,7 @@ int cmdline_complete_get_elt_string(cmdline_parse_token_hdr_t *tk, int idx,
 
 	s = sd->str;
 
+	//返回第idx个可选的token
 	while (idx-- && s)
 		s = get_next_token(s);
 
@@ -190,7 +206,7 @@ int cmdline_complete_get_elt_string(cmdline_parse_token_hdr_t *tk, int idx,
 	return 0;
 }
 
-
+//帮助信息
 int cmdline_get_help_string(cmdline_parse_token_hdr_t *tk, char *dstbuf,
 			    unsigned int size)
 {
@@ -208,13 +224,13 @@ int cmdline_get_help_string(cmdline_parse_token_hdr_t *tk, char *dstbuf,
 
 	if (s) {
 		if (strcmp(s, TOKEN_STRING_MULTI) == 0)
-			snprintf(dstbuf, size, ANYSTRINGS_HELP);
+			snprintf(dstbuf, size, ANYSTRINGS_HELP);//指明容许任意的字符串
 		else if (get_next_token(s))
-			snprintf(dstbuf, size, CHOICESTRING_HELP);
+			snprintf(dstbuf, size, CHOICESTRING_HELP);//指明有多个可选
 		else
-			snprintf(dstbuf, size, FIXEDSTRING_HELP);
+			snprintf(dstbuf, size, FIXEDSTRING_HELP);//指明字符串常量
 	} else
-		snprintf(dstbuf, size, ANYSTRING_HELP);
+		snprintf(dstbuf, size, ANYSTRING_HELP);//指明任意的单个token
 
 	return 0;
 }
diff --git a/lib/librte_cmdline/cmdline_parse_string.h b/lib/librte_cmdline/cmdline_parse_string.h
index 52a26670e..a71a6504c 100644
--- a/lib/librte_cmdline/cmdline_parse_string.h
+++ b/lib/librte_cmdline/cmdline_parse_string.h
@@ -53,6 +53,7 @@ int cmdline_get_help_string(cmdline_parse_token_hdr_t *tk, char *dstbuf,
 */
 #define TOKEN_STRING_MULTI ""
 
+//指明字符串token(采用字符串类型解析函数）
 #define TOKEN_STRING_INITIALIZER(structure, field, string)  \
 {                                                           \
 	/* hdr */                                               \
diff --git a/lib/librte_cmdline/cmdline_rdline.c b/lib/librte_cmdline/cmdline_rdline.c
index 2cb53e38f..b3c87c629 100644
--- a/lib/librte_cmdline/cmdline_rdline.c
+++ b/lib/librte_cmdline/cmdline_rdline.c
@@ -48,6 +48,7 @@ rdline_init(struct rdline *rdl,
 	rdl->complete = complete;
 	rdl->write_char = write_char;
 	rdl->status = RDLINE_INIT;
+	//初始化history指针
 	return cirbuf_init(&rdl->history, rdl->history_buf, 0, RDLINE_HISTORY_BUF_SIZE);
 }
 
@@ -68,6 +69,7 @@ rdline_newline(struct rdline *rdl, const char *prompt)
 		memcpy(rdl->prompt, prompt, rdl->prompt_size);
 	rdl->prompt[RDLINE_PROMPT_SIZE-1] = '\0';
 
+	//输出提示语
 	for (i=0 ; i<rdl->prompt_size ; i++)
 		rdl->write_char(rdl, rdl->prompt[i]);
 	rdl->status = RDLINE_RUNNING;
@@ -83,6 +85,7 @@ rdline_stop(struct rdline *rdl)
 	rdl->status = RDLINE_INIT;
 }
 
+//设置命令行退出
 void
 rdline_quit(struct rdline *rdl)
 {
@@ -167,6 +170,7 @@ rdline_redisplay(struct rdline *rdl)
 	display_right_buffer(rdl, 1);
 }
 
+//输入一个字符'c'
 int
 rdline_char_in(struct rdline *rdl, char c)
 {
@@ -353,7 +357,7 @@ rdline_char_in(struct rdline *rdl, char c)
 			break;
 
 		/* autocomplete */
-		case CMDLINE_KEY_TAB:
+		case CMDLINE_KEY_TAB://收到按tab键，进行补全
 		case CMDLINE_KEY_HELP:
 			cirbuf_align_left(&rdl->left);
 			rdl->left_buf[CIRBUF_GET_LEN(&rdl->left)] = '\0';
@@ -417,6 +421,8 @@ rdline_char_in(struct rdline *rdl, char c)
 			if (rdl->history_cur_line != -1)
 				rdline_remove_first_history_item(rdl);
 
+			//收到回车键，校验命令行，检查是否可以进行命令解析
+			//进行cmdline_valid_buffer函数调用
 			if (rdl->validate)
 				rdl->validate(rdl, rdl->left_buf, CIRBUF_GET_LEN(&rdl->left)+2);
 			/* user may have stopped rdline */
diff --git a/lib/librte_cmdline/cmdline_rdline.h b/lib/librte_cmdline/cmdline_rdline.h
index d2170293d..f0d9666dd 100644
--- a/lib/librte_cmdline/cmdline_rdline.h
+++ b/lib/librte_cmdline/cmdline_rdline.h
@@ -46,9 +46,9 @@ extern "C" {
 #define RDLINE_HISTORY_MAX_LINE 64
 
 enum rdline_status {
-	RDLINE_INIT,
-	RDLINE_RUNNING,
-	RDLINE_EXITED
+	RDLINE_INIT,//初始状态
+	RDLINE_RUNNING,//运行状态
+	RDLINE_EXITED //退出状态
 };
 
 struct rdline;
@@ -68,21 +68,22 @@ struct rdline {
 	char left_buf[RDLINE_BUF_SIZE+2]; /* reserve 2 chars for the \n\0 */
 	char right_buf[RDLINE_BUF_SIZE];
 
+	//提示语
 	char prompt[RDLINE_PROMPT_SIZE];
-	unsigned int prompt_size;
+	unsigned int prompt_size;//提示语长度
 
 	char kill_buf[RDLINE_BUF_SIZE];
 	unsigned int kill_size;
 
 	/* history */
-	struct cirbuf history;
-	char history_buf[RDLINE_HISTORY_BUF_SIZE];
-	int history_cur_line;
+	struct cirbuf history;//命令行历史
+	char history_buf[RDLINE_HISTORY_BUF_SIZE];//cirbuf所需要缓冲区
+	int history_cur_line;//当前历史行
 
 	/* callbacks and func pointers */
 	rdline_write_char_t *write_char;
 	rdline_validate_t *validate;
-	rdline_complete_t *complete;
+	rdline_complete_t *complete;//补全回调
 
 	/* vt100 parser */
 	struct cmdline_vt100 vt100;
diff --git a/lib/librte_cmdline/cmdline_socket.c b/lib/librte_cmdline/cmdline_socket.c
index ecb3d82b6..d9430cbe0 100644
--- a/lib/librte_cmdline/cmdline_socket.c
+++ b/lib/librte_cmdline/cmdline_socket.c
@@ -25,8 +25,9 @@ cmdline_file_new(cmdline_parse_ctx_t *ctx, const char *prompt, const char *path)
 
 	/* everything else is checked in cmdline_new() */
 	if (!path)
-		return NULL;
+		return NULL;//无指出文件返回NULL
 
+	//打开cmd文件
 	fd = open(path, O_RDONLY, 0);
 	if (fd < 0) {
 		dprintf("open() failed\n");
@@ -47,7 +48,7 @@ cmdline_stdin_new(cmdline_parse_ctx_t *ctx, const char *prompt)
 	tcsetattr(0, TCSANOW, &term);
 	setbuf(stdin, NULL);
 
-	cl = cmdline_new(ctx, prompt, 0, 1);
+	cl = cmdline_new(ctx, prompt, 0, 1);//标准输入，标准输出
 
 	if (cl)
 		memcpy(&cl->oldterm, &oldterm, sizeof(term));
diff --git a/lib/librte_cmdline/cmdline_vt100.c b/lib/librte_cmdline/cmdline_vt100.c
index 662fc7345..74ec718ca 100644
--- a/lib/librte_cmdline/cmdline_vt100.c
+++ b/lib/librte_cmdline/cmdline_vt100.c
@@ -15,7 +15,7 @@
 #include "cmdline_vt100.h"
 
 const char *cmdline_vt100_commands[] = {
-	vt100_up_arr,
+	vt100_up_arr,//方向的映射
 	vt100_down_arr,
 	vt100_right_arr,
 	vt100_left_arr,
@@ -53,6 +53,7 @@ vt100_init(struct cmdline_vt100 *vt)
 }
 
 
+//匹配按键索引
 static int
 match_command(char *buf, unsigned int size)
 {
@@ -93,7 +94,7 @@ vt100_parser(struct cmdline_vt100 *vt, char ch)
 	switch (vt->state) {
 	case CMDLINE_VT100_INIT:
 		if (c == 033) {
-			vt->state = CMDLINE_VT100_ESCAPE;
+			vt->state = CMDLINE_VT100_ESCAPE;//转义状态
 		}
 		else {
 			vt->bufpos = 0;
diff --git a/lib/librte_distributor/rte_distributor_v20.c b/lib/librte_distributor/rte_distributor_v20.c
index 7a6fddf55..2a88ad227 100644
--- a/lib/librte_distributor/rte_distributor_v20.c
+++ b/lib/librte_distributor/rte_distributor_v20.c
@@ -93,6 +93,7 @@ add_to_backlog(struct rte_distributor_backlog *bl, int64_t item)
 	if (bl->count == RTE_DISTRIB_BACKLOG_SIZE)
 		return -1;
 
+	//将item加入到bl中
 	bl->pkts[(bl->start + bl->count++) & (RTE_DISTRIB_BACKLOG_MASK)]
 			= item;
 	return 0;
@@ -103,6 +104,7 @@ static int64_t
 backlog_pop(struct rte_distributor_backlog *bl)
 {
 	bl->count--;
+	//自bl中弹出item
 	return bl->pkts[bl->start++ & RTE_DISTRIB_BACKLOG_MASK];
 }
 
diff --git a/lib/librte_eal/common/arch/x86/rte_cpuflags.c b/lib/librte_eal/common/arch/x86/rte_cpuflags.c
index 6492df556..b593855ca 100644
--- a/lib/librte_eal/common/arch/x86/rte_cpuflags.c
+++ b/lib/librte_eal/common/arch/x86/rte_cpuflags.c
@@ -25,6 +25,7 @@ struct feature_entry {
 #define FEAT_DEF(name, leaf, subleaf, reg, bit) \
 	[RTE_CPUFLAG_##name] = {leaf, subleaf, reg, bit, #name },
 
+//x86_64 cpu功能表
 const struct feature_entry rte_cpu_feature_table[] = {
 	FEAT_DEF(SSE3, 0x00000001, 0, RTE_REG_ECX,  0)
 	FEAT_DEF(PCLMULQDQ, 0x00000001, 0, RTE_REG_ECX,  1)
@@ -122,6 +123,7 @@ const struct feature_entry rte_cpu_feature_table[] = {
 	FEAT_DEF(INVTSC, 0x80000007, 0, RTE_REG_EDX,  8)
 };
 
+//检查cpu功能是否开启
 int
 rte_cpu_get_flag_enabled(enum rte_cpu_flag_t feature)
 {
diff --git a/lib/librte_eal/common/eal_common_bus.c b/lib/librte_eal/common/eal_common_bus.c
index baa5b532a..94fc18f4a 100644
--- a/lib/librte_eal/common/eal_common_bus.c
+++ b/lib/librte_eal/common/eal_common_bus.c
@@ -13,9 +13,11 @@
 
 #include "eal_private.h"
 
+//注册的bus挂载此链上（在装载期间通过rte_bus_register完成注册）
 static struct rte_bus_list rte_bus_list =
 	TAILQ_HEAD_INITIALIZER(rte_bus_list);
 
+//注册bus
 void
 rte_bus_register(struct rte_bus *bus)
 {
@@ -26,12 +28,13 @@ rte_bus_register(struct rte_bus *bus)
 	RTE_VERIFY(bus->probe);
 	RTE_VERIFY(bus->find_device);
 	/* Buses supporting driver plug also require unplug. */
-	RTE_VERIFY(!bus->plug || bus->unplug);
+	RTE_VERIFY(!bus->plug || bus->unplug);//必须要有plug
 
 	TAILQ_INSERT_TAIL(&rte_bus_list, bus, next);
 	RTE_LOG(DEBUG, EAL, "Registered [%s] bus.\n", bus->name);
 }
 
+//bus解注册
 void
 rte_bus_unregister(struct rte_bus *bus)
 {
@@ -40,6 +43,7 @@ rte_bus_unregister(struct rte_bus *bus)
 }
 
 /* Scan all the buses for registered devices */
+//做系统所有bus的扫描，每个bus在scan时会将发现自已bus上的所有设备
 int
 rte_bus_scan(void)
 {
@@ -57,6 +61,7 @@ rte_bus_scan(void)
 }
 
 /* Probe all devices of all buses */
+//各总线探测设备
 int
 rte_bus_probe(void)
 {
@@ -64,6 +69,7 @@ rte_bus_probe(void)
 	struct rte_bus *bus, *vbus = NULL;
 
 	TAILQ_FOREACH(bus, &rte_bus_list, next) {
+		//virtual bus被安排在最后探测
 		if (!strcmp(bus->name, "vdev")) {
 			vbus = bus;
 			continue;
@@ -117,6 +123,7 @@ rte_bus_dump(FILE *f)
 	}
 }
 
+//遍历bus,针对每一个bus调用cmp函数，查找匹配的bus
 struct rte_bus *
 rte_bus_find(const struct rte_bus *start, rte_bus_cmp_t cmp,
 	     const void *data)
@@ -172,6 +179,7 @@ rte_bus_find_by_name(const char *busname)
 	return rte_bus_find(NULL, cmp_bus_name, (const void *)busname);
 }
 
+//检查bus是否认识此_name
 static int
 bus_can_parse(const struct rte_bus *bus, const void *_name)
 {
@@ -180,6 +188,7 @@ bus_can_parse(const struct rte_bus *bus, const void *_name)
 	return !(bus->parse && bus->parse(name, NULL) == 0);
 }
 
+//查找可解析给定名称设备的bus
 struct rte_bus *
 rte_bus_find_by_device_name(const char *str)
 {
diff --git a/lib/librte_eal/common/eal_common_cpuflags.c b/lib/librte_eal/common/eal_common_cpuflags.c
index dc5f75d05..3ccd1fe4f 100644
--- a/lib/librte_eal/common/eal_common_cpuflags.c
+++ b/lib/librte_eal/common/eal_common_cpuflags.c
@@ -7,10 +7,12 @@
 #include <rte_common.h>
 #include <rte_cpuflags.h>
 
+//检查指定的体系的flag在当前cpu中是否已开启
 int
 rte_cpu_is_supported(void)
 {
 	/* This is generated at compile-time by the build system */
+	//此宏来源于编译rte.cpuflags.mk定义，用于表示cpu flags
 	static const enum rte_cpu_flag_t compile_time_flags[] = {
 			RTE_COMPILE_TIME_CPUFLAGS
 	};
diff --git a/lib/librte_eal/common/eal_common_dev.c b/lib/librte_eal/common/eal_common_dev.c
index 9e4f09d83..383a43e7b 100644
--- a/lib/librte_eal/common/eal_common_dev.c
+++ b/lib/librte_eal/common/eal_common_dev.c
@@ -35,6 +35,7 @@ struct dev_event_callback {
 	rte_dev_event_cb_fn cb_fn;            /**< Callback address */
 	void *cb_arg;                         /**< Callback parameter */
 	char *dev_name;	 /**< Callback device name, NULL is for all device */
+	//指明callback正在执行
 	uint32_t active;                      /**< Callback is executing */
 };
 
@@ -425,6 +426,7 @@ rte_dev_remove(struct rte_device *dev)
 	return ret;
 }
 
+//注册设备事件处理回调（目前仅新增，删除设备两种事件）
 int
 rte_dev_event_callback_register(const char *device_name,
 				rte_dev_event_cb_fn cb_fn,
@@ -454,6 +456,7 @@ rte_dev_event_callback_register(const char *device_name,
 
 	/* create a new callback. */
 	if (event_cb == NULL) {
+		//未发现已存在的注册，创建回调
 		event_cb = malloc(sizeof(struct dev_event_callback));
 		if (event_cb != NULL) {
 			event_cb->cb_fn = cb_fn;
@@ -477,6 +480,7 @@ rte_dev_event_callback_register(const char *device_name,
 			goto error;
 		}
 	} else {
+		//报错，重复注册
 		RTE_LOG(ERR, EAL,
 			"The callback is already exist, no need "
 			"to register again.\n");
@@ -491,6 +495,7 @@ rte_dev_event_callback_register(const char *device_name,
 	return ret;
 }
 
+//解注册
 int
 rte_dev_event_callback_unregister(const char *device_name,
 				  rte_dev_event_cb_fn cb_fn,
@@ -536,6 +541,7 @@ rte_dev_event_callback_unregister(const char *device_name,
 	return ret;
 }
 
+//设备新增，删除事件触发
 void
 rte_dev_event_callback_process(const char *device_name,
 			       enum rte_dev_event_type event)
@@ -547,10 +553,11 @@ rte_dev_event_callback_process(const char *device_name,
 
 	rte_spinlock_lock(&dev_event_lock);
 
+	//遍历设备事件回调链
 	TAILQ_FOREACH(cb_lst, &dev_event_cbs, next) {
 		if (cb_lst->dev_name) {
 			if (strcmp(cb_lst->dev_name, device_name))
-				continue;
+				continue;//无法与事件设备相匹配，忽略
 		}
 		cb_lst->active = 1;
 		rte_spinlock_unlock(&dev_event_lock);
diff --git a/lib/librte_eal/common/eal_common_devargs.c b/lib/librte_eal/common/eal_common_devargs.c
index 2123773ef..f7133566d 100644
--- a/lib/librte_eal/common/eal_common_devargs.c
+++ b/lib/librte_eal/common/eal_common_devargs.c
@@ -25,6 +25,7 @@
 TAILQ_HEAD(rte_devargs_list, rte_devargs);
 
 /** Global list of user devices */
+//黑名单（-b)，白名单(-w)，虚拟设备（--vdev参数）均串在此链上。
 static struct rte_devargs_list devargs_list =
 	TAILQ_HEAD_INITIALIZER(devargs_list);
 
@@ -188,9 +189,11 @@ rte_devargs_parse(struct rte_devargs *da, const char *dev)
 	/* Retrieve eventual bus info */
 	do {
 		devname = dev;
+		//检查dev是否以某一bus名开头（即是否具有某一bus名称前缀）
 		bus = rte_bus_find(bus, bus_name_cmp, dev);
 		if (bus == NULL)
 			break;
+		//跳过bus前缀，查找此设备是否可以被bus解析，如果可以退出
 		devname = dev + strlen(bus->name) + 1;
 		if (rte_bus_find_by_device_name(devname) == bus)
 			break;
@@ -309,6 +312,7 @@ rte_devargs_add(enum rte_devtype devtype, const char *devargs_str)
 	if (devargs == NULL)
 		goto fail;
 
+	//解析dev,并将结果填充到devargs
 	if (rte_devargs_parse(devargs, dev))
 		goto fail;
 	devargs->type = devtype;
@@ -321,7 +325,7 @@ rte_devargs_add(enum rte_devtype devtype, const char *devargs_str)
 		else if (devargs->policy == RTE_DEV_BLACKLISTED)
 			bus->conf.scan_mode = RTE_BUS_SCAN_BLACKLIST;
 	}
-	TAILQ_INSERT_TAIL(&devargs_list, devargs, next);
+	TAILQ_INSERT_TAIL(&devargs_list, devargs, next);//添加设备参数
 	return 0;
 
 fail:
@@ -355,6 +359,7 @@ rte_devargs_remove(struct rte_devargs *devargs)
 }
 
 /* count the number of devices of a specified type */
+//获得指定类型设备的数目
 unsigned int
 rte_devargs_type_count(enum rte_devtype devtype)
 {
@@ -394,6 +399,7 @@ rte_devargs_next(const char *busname, const struct rte_devargs *start)
 	else
 		da = TAILQ_FIRST(&devargs_list);
 	while (da != NULL) {
+		//如果busname与da->bus的name相同，则返回此设备信息
 		if (busname == NULL ||
 		    (strcmp(busname, da->bus->name) == 0))
 			return da;
diff --git a/lib/librte_eal/common/eal_common_launch.c b/lib/librte_eal/common/eal_common_launch.c
index cf52d717f..65779939b 100644
--- a/lib/librte_eal/common/eal_common_launch.c
+++ b/lib/librte_eal/common/eal_common_launch.c
@@ -50,16 +50,20 @@ rte_eal_mp_remote_launch(int (*f)(void *), void *arg,
 	int master = rte_get_master_lcore();
 
 	/* check state of lcores */
+	//各slave必须处于wait状态
 	RTE_LCORE_FOREACH_SLAVE(lcore_id) {
 		if (lcore_config[lcore_id].state != WAIT)
 			return -EBUSY;
 	}
 
 	/* send messages to cores */
+	//逐个向slave安排任务，并等待答复
 	RTE_LCORE_FOREACH_SLAVE(lcore_id) {
 		rte_eal_remote_launch(f, arg, lcore_id);
 	}
 
+	//作为master，理论下也是要做事情的，这里做做样子，让master
+	//也处理处理任务。由上层控制，是否做
 	if (call_master == CALL_MASTER) {
 		lcore_config[master].ret = f(arg);
 		lcore_config[master].state = FINISHED;
@@ -81,6 +85,7 @@ rte_eal_get_lcore_state(unsigned lcore_id)
  * Do a rte_eal_wait_lcore() for every lcore. The return values are
  * ignored.
  */
+//等待各slave完成工作，并在确认完成后，将状态直为wait状态（下一步做什么，等待指令）
 void
 rte_eal_mp_wait_lcore(void)
 {
diff --git a/lib/librte_eal/common/eal_common_lcore.c b/lib/librte_eal/common/eal_common_lcore.c
index 39efadef1..fc5e2586c 100644
--- a/lib/librte_eal/common/eal_common_lcore.c
+++ b/lib/librte_eal/common/eal_common_lcore.c
@@ -79,6 +79,7 @@ unsigned int rte_get_next_lcore(unsigned int i, int skip_master, int wrap)
 		i %= RTE_MAX_LCORE;
 
 	while (i < RTE_MAX_LCORE) {
+	    //如果此core未占用，则检查是否需要跳过master
 		if (!rte_lcore_is_enabled(i) ||
 		    (skip_master && (i == rte_get_master_lcore()))) {
 			i++;
@@ -115,6 +116,7 @@ socket_id_cmp(const void *a, const void *b)
  * processors on the machine. The function will fill the cpu_info
  * structure.
  */
+//检测cpu,初始化lcore-config
 int
 rte_eal_cpu_init(void)
 {
@@ -123,7 +125,7 @@ rte_eal_cpu_init(void)
 	unsigned lcore_id;
 	unsigned count = 0;
 	unsigned int socket_id, prev_socket_id;
-	int lcore_to_socket_id[RTE_MAX_LCORE];
+	int lcore_to_socket_id[RTE_MAX_LCORE];//每个core对应的socket-id
 
 	/*
 	 * Parse the maximum set of logical cores, detect the subset of running
@@ -140,14 +142,16 @@ rte_eal_cpu_init(void)
 		lcore_to_socket_id[lcore_id] = socket_id;
 
 		/* in 1:1 mapping, record related cpu detected state */
+		//检查lcore_id是否存在
 		lcore_config[lcore_id].detected = eal_cpu_detected(lcore_id);
 		if (lcore_config[lcore_id].detected == 0) {
-			config->lcore_role[lcore_id] = ROLE_OFF;
+			config->lcore_role[lcore_id] = ROLE_OFF;//定为off
 			lcore_config[lcore_id].core_index = -1;
 			continue;
 		}
 
 		/* By default, lcore 1:1 map to cpu id */
+		//设置此core对应的cpuset
 		CPU_SET(lcore_id, &lcore_config[lcore_id].cpuset);
 
 		/* By default, each detected core is enabled */
@@ -169,9 +173,11 @@ rte_eal_cpu_init(void)
 	RTE_LOG(INFO, EAL, "Detected %u lcore(s)\n", config->lcore_count);
 
 	/* sort all socket id's in ascending order */
+	//对numa的记录，实现一次排序
 	qsort(lcore_to_socket_id, RTE_DIM(lcore_to_socket_id),
 			sizeof(lcore_to_socket_id[0]), socket_id_cmp);
 
+	//记录探测到的numa节点
 	prev_socket_id = -1;
 	config->numa_node_count = 0;
 	for (lcore_id = 0; lcore_id < RTE_MAX_LCORE; lcore_id++) {
diff --git a/lib/librte_eal/common/eal_common_log.c b/lib/librte_eal/common/eal_common_log.c
index e0a7bef03..cfcf8029f 100644
--- a/lib/librte_eal/common/eal_common_log.c
+++ b/lib/librte_eal/common/eal_common_log.c
@@ -18,6 +18,7 @@
 #include "eal_private.h"
 
 /* global log structure */
+//全局的log struct
 struct rte_logs rte_logs = {
 	.type = ~0,
 	.level = RTE_LOG_DEBUG,
@@ -54,8 +55,8 @@ struct log_cur_msg {
 };
 
 struct rte_log_dynamic_type {
-	const char *name;
-	uint32_t loglevel;
+	const char *name;//模块名称
+	uint32_t loglevel;//日志级别
 };
 
  /* per core log */
@@ -112,13 +113,14 @@ rte_log_get_level(uint32_t type)
 	return rte_logs.dynamic_types[type].loglevel;
 }
 
+//设置模块的日志级别
 int
 rte_log_set_level(uint32_t type, uint32_t level)
 {
 	if (type >= rte_logs.dynamic_types_len)
-		return -1;
+		return -1;//模块查找失败，报错
 	if (level > RTE_LOG_DEBUG)
-		return -1;
+		return -1;//日志级别错误
 
 	rte_logs.dynamic_types[type].loglevel = level;
 
@@ -141,6 +143,7 @@ rte_log_set_level_regexp(const char *regex, uint32_t level)
 	for (i = 0; i < rte_logs.dynamic_types_len; i++) {
 		if (rte_logs.dynamic_types[i].name == NULL)
 			continue;
+		//与name区配，则设置loglevel
 		if (regexec(&r, rte_logs.dynamic_types[i].name, 0,
 				NULL, 0) == 0)
 			rte_logs.dynamic_types[i].loglevel = level;
@@ -235,10 +238,11 @@ rte_log_lookup(const char *name)
 		if (rte_logs.dynamic_types[i].name == NULL)
 			continue;
 		if (strcmp(name, rte_logs.dynamic_types[i].name) == 0)
+			//此名称已存在，返回索引
 			return i;
 	}
 
-	return -1;
+	return -1;//名称不存在，返回-1
 }
 
 /* register an extended log type, assuming table is large enough, and id
@@ -267,8 +271,9 @@ rte_log_register(const char *name)
 
 	id = rte_log_lookup(name);
 	if (id >= 0)
-		return id;
+		return id;//返回此log对应的id
 
+	//不存在，注册此id
 	new_dynamic_types = realloc(rte_logs.dynamic_types,
 		sizeof(struct rte_log_dynamic_type) *
 		(rte_logs.dynamic_types_len + 1));
diff --git a/lib/librte_eal/common/eal_common_memzone.c b/lib/librte_eal/common/eal_common_memzone.c
index 99b8d6531..4ead1fc9b 100644
--- a/lib/librte_eal/common/eal_common_memzone.c
+++ b/lib/librte_eal/common/eal_common_memzone.c
@@ -71,6 +71,7 @@ memzone_reserve_aligned_thread_unsafe(const char *name, size_t len,
 
 	/* no more room in config */
 	if (arr->count >= arr->len) {
+	        //不能完成memzone的分配，空闲的memzone均已被填充
 		RTE_LOG(ERR, EAL, "%s(): No more room in config\n", __func__);
 		rte_errno = ENOSPC;
 		return NULL;
diff --git a/lib/librte_eal/common/eal_common_options.c b/lib/librte_eal/common/eal_common_options.c
index a7f9c5f9b..d2381bfa5 100644
--- a/lib/librte_eal/common/eal_common_options.c
+++ b/lib/librte_eal/common/eal_common_options.c
@@ -92,10 +92,11 @@ struct shared_driver {
 	TAILQ_ENTRY(shared_driver) next;
 
 	char    name[PATH_MAX];
-	void*   lib_handle;
+	void*   lib_handle;//so对应的handle
 };
 
 /* List of external loadable drivers */
+//参数指定的可外部加载的驱动（-d参数指定）
 static struct shared_driver_list solib_list =
 TAILQ_HEAD_INITIALIZER(solib_list);
 
@@ -122,10 +123,14 @@ struct device_option {
 static struct device_option_list devopt_list =
 TAILQ_HEAD_INITIALIZER(devopt_list);
 
+//master线程是否被指定
 static int master_lcore_parsed;
+//是否指定了内存-m参数
 static int mem_parsed;
+//是否指定了核
 static int core_parsed;
 
+//设备选项参数添加（将添加的选项串成一个链）
 static int
 eal_option_device_add(enum rte_devtype type, const char *optarg)
 {
@@ -151,6 +156,7 @@ eal_option_device_add(enum rte_devtype type, const char *optarg)
 	return 0;
 }
 
+//遍历设备选项参数链
 int
 eal_option_device_parse(void)
 {
@@ -158,7 +164,8 @@ eal_option_device_parse(void)
 	void *tmp;
 	int ret = 0;
 
-	TAILQ_FOREACH_SAFE(devopt, &devopt_list, next, tmp) {
+	TAILQ_FOREACH_SAFE(devopt, &devopt_list, next, tmp) {\
+		//如果rte_devargs_add添加失败，则后面所有的devopt将被直接移除
 		if (ret == 0) {
 			ret = rte_devargs_add(devopt->type, devopt->arg);
 			if (ret)
@@ -234,9 +241,10 @@ eal_plugin_add(const char *path)
 		return -1;
 	}
 	memset(solib, 0, sizeof(*solib));
+	//将path名称写入
 	strlcpy(solib->name, path, PATH_MAX-1);
 	solib->name[PATH_MAX-1] = 0;
-	TAILQ_INSERT_TAIL(&solib_list, solib, next);
+	TAILQ_INSERT_TAIL(&solib_list, solib, next);//生成一个solib,并加入
 
 	return 0;
 }
@@ -275,12 +283,14 @@ eal_plugindir_init(const char *path)
 	return (dent == NULL) ? 0 : -1;
 }
 
+//对于目录，递归加入。对于文件，则用dl_open打开。
 int
 eal_plugins_init(void)
 {
 	struct shared_driver *solib = NULL;
 	struct stat sb;
 
+	//如果default_solib_dir不为空，且为目录时，装载目录下so
 	if (*default_solib_dir != '\0' && stat(default_solib_dir, &sb) == 0 &&
 				S_ISDIR(sb.st_mode))
 		eal_plugin_add(default_solib_dir);
@@ -288,6 +298,7 @@ eal_plugins_init(void)
 	TAILQ_FOREACH(solib, &solib_list, next) {
 
 		if (stat(solib->name, &sb) == 0 && S_ISDIR(sb.st_mode)) {
+			//对于目录，则递归加入
 			if (eal_plugindir_init(solib->name) == -1) {
 				RTE_LOG(ERR, EAL,
 					"Cannot init plugin directory %s\n",
@@ -295,6 +306,7 @@ eal_plugins_init(void)
 				return -1;
 			}
 		} else {
+			//打开so文件
 			RTE_LOG(DEBUG, EAL, "open shared lib %s\n",
 				solib->name);
 			solib->lib_handle = dlopen(solib->name, RTLD_NOW);
@@ -326,6 +338,7 @@ static int xdigit2val(unsigned char c)
 	return val;
 }
 
+//指定service core
 static int
 eal_parse_service_coremask(const char *coremask)
 {
@@ -342,13 +355,13 @@ eal_parse_service_coremask(const char *coremask)
 	 * Remove 0x/0X if exists.
 	 */
 	while (isblank(*coremask))
-		coremask++;
+		coremask++;//跳过空字符
 	if (coremask[0] == '0' && ((coremask[1] == 'x')
 		|| (coremask[1] == 'X')))
-		coremask += 2;
+		coremask += 2;//跳过'0x'
 	i = strlen(coremask);
 	while ((i > 0) && isblank(coremask[i - 1]))
-		i--;
+		i--;//跳过结尾的空字符
 
 	if (i == 0)
 		return -1;
@@ -357,23 +370,23 @@ eal_parse_service_coremask(const char *coremask)
 		c = coremask[i];
 		if (isxdigit(c) == 0) {
 			/* invalid characters */
-			return -1;
+			return -1;//错误字符串
 		}
 		val = xdigit2val(c);
 		for (j = 0; j < BITS_PER_HEX && idx < RTE_MAX_LCORE;
 				j++, idx++) {
-			if ((1 << j) & val) {
+			if ((1 << j) & val) {//探测val的j位
 				/* handle master lcore already parsed */
 				uint32_t lcore = idx;
 				if (master_lcore_parsed &&
-						cfg->master_lcore == lcore) {
+						cfg->master_lcore == lcore) {//此core被占用了
 					RTE_LOG(ERR, EAL,
 						"lcore %u is master lcore, cannot use as service core\n",
 						idx);
 					return -1;
 				}
 
-				if (!lcore_config[idx].detected) {
+				if (!lcore_config[idx].detected) {//此core没有被发现
 					RTE_LOG(ERR, EAL,
 						"lcore %u unavailable\n", idx);
 					return -1;
@@ -461,40 +474,55 @@ eal_parse_coremask(const char *coremask, int *cores)
 	/* Remove all blank characters ahead and after .
 	 * Remove 0x/0X if exists.
 	 */
+	//跳过空字符
 	while (isblank(*coremask))
 		coremask++;
+
+	//跳过0x或者0X
 	if (coremask[0] == '0' && ((coremask[1] == 'x')
 		|| (coremask[1] == 'X')))
 		coremask += 2;
+
+	//跳过结尾的空字符
 	i = strlen(coremask);
 	while ((i > 0) && isblank(coremask[i - 1]))
 		i--;
+
+	//排除掉空串情况
 	if (i == 0)
 		return -1;
 
+	//返序遍历
 	for (i = i - 1; i >= 0 && idx < RTE_MAX_LCORE; i--) {
 		c = coremask[i];
 		if (isxdigit(c) == 0) {
 			/* invalid characters */
 			return -1;
 		}
-		val = xdigit2val(c);
+		val = xdigit2val(c);//取出字符值
+
+		//检查此字符相关的4个cpu
 		for (j = 0; j < BITS_PER_HEX && idx < RTE_MAX_LCORE; j++, idx++)
 		{
+			//idx对应的cpu被指定了
 			if ((1 << j) & val) {
 				cores[idx] = count;
 				count++;
 			}
 		}
 	}
+
 	for (; i >= 0; i--)
 		if (coremask[i] != '0')
+
 			return -1;
 	if (count == 0)
+	//用户指定为0的情况，返回error
 		return -1;
 	return 0;
 }
 
+//解析-l参数，当前支持样例 -l "2,3-9,10,21"这种形式
 static int
 eal_parse_service_corelist(const char *corelist)
 {
@@ -540,6 +568,7 @@ eal_parse_service_corelist(const char *corelist)
 					uint32_t lcore = idx;
 					if (cfg->master_lcore == lcore &&
 							master_lcore_parsed) {
+						//已成为master core,不能做为service core
 						RTE_LOG(ERR, EAL,
 							"Error: lcore %u is master lcore, cannot use as service core\n",
 							idx);
@@ -549,7 +578,7 @@ eal_parse_service_corelist(const char *corelist)
 						taken_lcore_count++;
 
 					lcore_config[idx].core_role =
-							ROLE_SERVICE;
+							ROLE_SERVICE;//指定为serivce core
 					count++;
 				}
 			}
@@ -583,6 +612,7 @@ eal_parse_corelist(const char *corelist, int *cores)
 		cores[idx] = -1;
 
 	/* Remove all blank characters ahead */
+	//跳过前导的空格
 	while (isblank(*corelist))
 		corelist++;
 
@@ -607,6 +637,7 @@ eal_parse_corelist(const char *corelist, int *cores)
 			max = idx;
 			if (min == RTE_MAX_LCORE)
 				min = idx;
+			//如果min与max有误，将被忽略
 			for (idx = min; idx <= max; idx++) {
 				if (cores[idx] == -1) {
 					cores[idx] = count;
@@ -830,11 +861,12 @@ eal_parse_lcores(const char *lcores)
 
 	/* Remove all blank characters ahead and after */
 	while (isblank(*lcores))
-		lcores++;
+		lcores++;//跳过前置空格
 
 	CPU_ZERO(&cpuset);
 
 	/* Reset lcore config */
+	//将role 重置
 	for (idx = 0; idx < RTE_MAX_LCORE; idx++) {
 		cfg->lcore_role[idx] = ROLE_OFF;
 		lcore_config[idx].core_index = -1;
@@ -844,7 +876,7 @@ eal_parse_lcores(const char *lcores)
 	/* Get list of cores */
 	do {
 		while (isblank(*lcores))
-			lcores++;
+			lcores++;//跳上次循环结束后，遇到的空格
 		if (*lcores == '\0')
 			goto err;
 
@@ -1064,6 +1096,7 @@ eal_parse_log_level(const char *arg)
 	return -1;
 }
 
+//依据不同自符串确定进程类型
 static enum rte_proc_type_t
 eal_parse_proc_type(const char *arg)
 {
@@ -1205,6 +1238,7 @@ eal_parse_common_option(int opt, const char *optarg,
 	switch (opt) {
 	/* blacklist */
 	case 'b':
+		//黑名单
 		if (w_used)
 			goto bw_used;
 		if (eal_option_device_add(RTE_DEVTYPE_BLACKLISTED_PCI,
@@ -1215,6 +1249,7 @@ eal_parse_common_option(int opt, const char *optarg,
 		break;
 	/* whitelist */
 	case 'w':
+		//白名单
 		if (b_used)
 			goto bw_used;
 		if (eal_option_device_add(RTE_DEVTYPE_WHITELISTED_PCI,
@@ -1225,6 +1260,7 @@ eal_parse_common_option(int opt, const char *optarg,
 		break;
 	/* coremask */
 	case 'c': {
+		//使用哪些core
 		int lcore_indexes[RTE_MAX_LCORE];
 
 		if (eal_service_cores_parsed())
@@ -1257,6 +1293,7 @@ eal_parse_common_option(int opt, const char *optarg,
 	}
 	/* corelist */
 	case 'l': {
+		//一种更易读的cpu指定形式
 		int lcore_indexes[RTE_MAX_LCORE];
 
 		if (eal_service_cores_parsed())
@@ -1290,6 +1327,7 @@ eal_parse_common_option(int opt, const char *optarg,
 	}
 	/* service coremask */
 	case 's':
+		//mask 形式
 		if (eal_parse_service_coremask(optarg) < 0) {
 			RTE_LOG(ERR, EAL, "invalid service coremask\n");
 			return -1;
@@ -1297,6 +1335,7 @@ eal_parse_common_option(int opt, const char *optarg,
 		break;
 	/* service corelist */
 	case 'S':
+		//core list 形式
 		if (eal_parse_service_corelist(optarg) < 0) {
 			RTE_LOG(ERR, EAL, "invalid service core list\n");
 			return -1;
@@ -1304,6 +1343,7 @@ eal_parse_common_option(int opt, const char *optarg,
 		break;
 	/* size of memory */
 	case 'm':
+		//使用的内存大小
 		conf->memory = atoi(optarg);
 		conf->memory *= 1024ULL;
 		conf->memory *= 1024ULL;
@@ -1336,6 +1376,7 @@ eal_parse_common_option(int opt, const char *optarg,
 		 * write message at highest log level so it can always
 		 * be seen
 		 * even if info or warning messages are disabled */
+		//显示版本号
 		RTE_LOG(CRIT, EAL, "RTE Version: '%s'\n", rte_version());
 		break;
 
@@ -1373,11 +1414,11 @@ eal_parse_common_option(int opt, const char *optarg,
 		conf->hugepage_unlink = 1;
 		break;
 
-	case OPT_PROC_TYPE_NUM:
+	case OPT_PROC_TYPE_NUM://多进程模式
 		conf->process_type = eal_parse_proc_type(optarg);
 		break;
 
-	case OPT_MASTER_LCORE_NUM:
+	case OPT_MASTER_LCORE_NUM://设置master线程在哪个core上
 		if (eal_parse_master_lcore(optarg) < 0) {
 			RTE_LOG(ERR, EAL, "invalid parameter for --"
 					OPT_MASTER_LCORE "\n");
@@ -1385,7 +1426,7 @@ eal_parse_common_option(int opt, const char *optarg,
 		}
 		break;
 
-	case OPT_VDEV_NUM:
+	case OPT_VDEV_NUM://虚设备驱动
 		if (eal_option_device_add(RTE_DEVTYPE_VIRTUAL,
 				optarg) < 0) {
 			return -1;
@@ -1393,6 +1434,7 @@ eal_parse_common_option(int opt, const char *optarg,
 		break;
 
 	case OPT_SYSLOG_NUM:
+		//syslog的输出位置
 		if (eal_parse_syslog(optarg, conf) < 0) {
 			RTE_LOG(ERR, EAL, "invalid parameters for --"
 					OPT_SYSLOG "\n");
@@ -1400,6 +1442,7 @@ eal_parse_common_option(int opt, const char *optarg,
 		}
 		break;
 
+	//--log-level处理
 	case OPT_LOG_LEVEL_NUM: {
 		if (eal_parse_log_level(optarg) < 0) {
 			RTE_LOG(ERR, EAL,
@@ -1467,10 +1510,12 @@ eal_auto_detect_cores(struct rte_config *cfg)
 	unsigned int removed = 0;
 	rte_cpuset_t affinity_set;
 
+	//获取当前cpu的亲呢性
 	if (pthread_getaffinity_np(pthread_self(), sizeof(rte_cpuset_t),
 				&affinity_set))
 		CPU_ZERO(&affinity_set);
 
+	//将没有绑定在此线程上的cpu置为role_off（未更新id)
 	for (lcore_id = 0; lcore_id < RTE_MAX_LCORE; lcore_id++) {
 		if (cfg->lcore_role[lcore_id] == ROLE_RTE &&
 		    !CPU_ISSET(lcore_id, &affinity_set)) {
@@ -1479,7 +1524,7 @@ eal_auto_detect_cores(struct rte_config *cfg)
 		}
 	}
 
-	cfg->lcore_count -= removed;
+	cfg->lcore_count -= removed;//core数量被移除
 }
 
 static void
@@ -1528,13 +1573,16 @@ eal_adjust_config(struct internal_config *internal_cfg)
 	int i;
 	struct rte_config *cfg = rte_eal_get_configuration();
 
+	//如果未指定core，则检查core
 	if (!core_parsed)
 		eal_auto_detect_cores(cfg);
 
+	//如果进程类型为auto,则检测进程类型
 	if (internal_config.process_type == RTE_PROC_AUTO)
 		internal_config.process_type = eal_proc_type_detect();
 
 	/* default master lcore is the first one */
+	//如果未指定master core，则选择第一个开启的core做为master core
 	if (!master_lcore_parsed) {
 		cfg->master_lcore = rte_get_next_lcore(-1, 0, 0);
 		if (cfg->master_lcore >= RTE_MAX_LCORE)
@@ -1546,22 +1594,26 @@ eal_adjust_config(struct internal_config *internal_cfg)
 
 	/* if no memory amounts were requested, this will result in 0 and
 	 * will be overridden later, right after eal_hugepage_info_init() */
+	//合入各socket上分配的内存
 	for (i = 0; i < RTE_MAX_NUMA_NODES; i++)
 		internal_cfg->memory += internal_cfg->socket_mem[i];
 
 	return 0;
 }
 
+//选项检查
 int
 eal_check_common_options(struct internal_config *internal_cfg)
 {
 	struct rte_config *cfg = rte_eal_get_configuration();
 
+	//master_core必须是开启了的
 	if (cfg->lcore_role[cfg->master_lcore] != ROLE_RTE) {
 		RTE_LOG(ERR, EAL, "Master lcore is not enabled for DPDK\n");
 		return -1;
 	}
 
+	//进程类型必须是有效的
 	if (internal_cfg->process_type == RTE_PROC_INVALID) {
 		RTE_LOG(ERR, EAL, "Invalid process type specified\n");
 		return -1;
@@ -1581,21 +1633,27 @@ eal_check_common_options(struct internal_config *internal_cfg)
 		RTE_LOG(ERR, EAL, "Invalid length of --" OPT_MBUF_POOL_OPS_NAME" option\n");
 		return -1;
 	}
+	//hugefile_prefix中不能有格式化符号
 	if (index(eal_get_hugefile_prefix(), '%') != NULL) {
 		RTE_LOG(ERR, EAL, "Invalid char, '%%', in --"OPT_FILE_PREFIX" "
 			"option\n");
 		return -1;
 	}
+
+	//-m 是指定非numa内存， --socket-mem 是指定numa内存，两者不可同时存在
 	if (mem_parsed && internal_cfg->force_sockets == 1) {
 		RTE_LOG(ERR, EAL, "Options -m and --"OPT_SOCKET_MEM" cannot "
 			"be specified at the same time\n");
 		return -1;
 	}
+
+	//--no-huge用于指定无大页， --socket-mem指定在numa的x上有大页，两者相互冲突
 	if (internal_cfg->no_hugetlbfs && internal_cfg->force_sockets == 1) {
 		RTE_LOG(ERR, EAL, "Option --"OPT_SOCKET_MEM" cannot "
 			"be specified together with --"OPT_NO_HUGE"\n");
 		return -1;
 	}
+	//配置冲突
 	if (internal_cfg->no_hugetlbfs && internal_cfg->hugepage_unlink &&
 			!internal_cfg->in_memory) {
 		RTE_LOG(ERR, EAL, "Option --"OPT_HUGE_UNLINK" cannot "
diff --git a/lib/librte_eal/common/eal_common_proc.c b/lib/librte_eal/common/eal_common_proc.c
index cbe8d10fc..5df364f4b 100644
--- a/lib/librte_eal/common/eal_common_proc.c
+++ b/lib/librte_eal/common/eal_common_proc.c
@@ -41,13 +41,14 @@ static char peer_name[PATH_MAX];
 
 struct action_entry {
 	TAILQ_ENTRY(action_entry) next;
-	char action_name[RTE_MP_MAX_NAME_LEN];
-	rte_mp_t action;
+	char action_name[RTE_MP_MAX_NAME_LEN];//名称
+	rte_mp_t action;//动作
 };
 
 /** Double linked list of actions. */
 TAILQ_HEAD(action_entry_list, action_entry);
 
+//初始化action entry list,用于串连所有注册的action entry
 static struct action_entry_list action_entry_list =
 	TAILQ_HEAD_INITIALIZER(action_entry_list);
 
@@ -164,6 +165,7 @@ rte_eal_primary_proc_alive(const char *config_file_path)
 	return !!ret;
 }
 
+//通过名称查找对应的action_entry
 static struct action_entry *
 find_action_entry_by_name(const char *name)
 {
@@ -177,6 +179,7 @@ find_action_entry_by_name(const char *name)
 	return entry;
 }
 
+//action名称长度校验
 static int
 validate_action_name(const char *name)
 {
@@ -197,11 +200,13 @@ validate_action_name(const char *name)
 	return 0;
 }
 
+//注册指定名称的action entry
 int
 rte_mp_action_register(const char *name, rte_mp_t action)
 {
 	struct action_entry *entry;
 
+	//校验action名称
 	if (validate_action_name(name) != 0)
 		return -1;
 
@@ -211,6 +216,7 @@ rte_mp_action_register(const char *name, rte_mp_t action)
 		return -1;
 	}
 
+	//初始化action entry
 	entry = malloc(sizeof(struct action_entry));
 	if (entry == NULL) {
 		rte_errno = ENOMEM;
@@ -221,21 +227,25 @@ rte_mp_action_register(const char *name, rte_mp_t action)
 
 	pthread_mutex_lock(&mp_mutex_action);
 	if (find_action_entry_by_name(name) != NULL) {
+		//检查对应的action是否已存在
 		pthread_mutex_unlock(&mp_mutex_action);
 		rte_errno = EEXIST;
 		free(entry);
 		return -1;
 	}
+	//将action entry添加到action_entry_list中
 	TAILQ_INSERT_TAIL(&action_entry_list, entry, next);
 	pthread_mutex_unlock(&mp_mutex_action);
 	return 0;
 }
 
+//解注册指定名称的action entry
 void
 rte_mp_action_unregister(const char *name)
 {
 	struct action_entry *entry;
 
+	//检查action entry是否合法
 	if (validate_action_name(name) != 0)
 		return;
 
@@ -245,12 +255,12 @@ rte_mp_action_unregister(const char *name)
 	}
 
 	pthread_mutex_lock(&mp_mutex_action);
-	entry = find_action_entry_by_name(name);
+	entry = find_action_entry_by_name(name);//找到指定的entry
 	if (entry == NULL) {
 		pthread_mutex_unlock(&mp_mutex_action);
 		return;
 	}
-	TAILQ_REMOVE(&action_entry_list, entry, next);
+	TAILQ_REMOVE(&action_entry_list, entry, next);//将其删除
 	pthread_mutex_unlock(&mp_mutex_action);
 	free(entry);
 }
@@ -276,6 +286,7 @@ read_msg(struct mp_msg_internal *m, struct sockaddr_un *s)
 	msgh.msg_control = control;
 	msgh.msg_controllen = sizeof(control);
 
+	//自mp_fd读取消息
 	msglen = recvmsg(mp_fd, &msgh, 0);
 	if (msglen < 0) {
 		RTE_LOG(ERR, EAL, "recvmsg failed, %s\n", strerror(errno));
@@ -288,6 +299,7 @@ read_msg(struct mp_msg_internal *m, struct sockaddr_un *s)
 	}
 
 	/* read auxiliary FDs if any */
+	//读取传递过来的fd
 	for (cmsg = CMSG_FIRSTHDR(&msgh); cmsg != NULL;
 		cmsg = CMSG_NXTHDR(&msgh, cmsg)) {
 		if ((cmsg->cmsg_level == SOL_SOCKET) &&
@@ -343,6 +355,7 @@ process_msg(struct mp_msg_internal *m, struct sockaddr_un *s)
 		return;
 	}
 
+	//查找消息中的名称是否对应了一个action entry
 	pthread_mutex_lock(&mp_mutex_action);
 	entry = find_action_entry_by_name(msg->name);
 	if (entry != NULL)
@@ -366,6 +379,7 @@ process_msg(struct mp_msg_internal *m, struct sockaddr_un *s)
 				msg->name);
 		}
 	} else if (action(msg, s->sun_path) < 0) {
+		//如果此消息对应了某个action entry,则交消息交由action entry处理
 		RTE_LOG(ERR, EAL, "Fail to handle message: %s\n", msg->name);
 	}
 }
@@ -377,6 +391,7 @@ mp_handle(void *arg __rte_unused)
 	struct sockaddr_un sa;
 
 	while (1) {
+		//收取消息，并处理
 		if (read_msg(&msg, &sa) == 0)
 			process_msg(&msg, &sa);
 	}
@@ -528,6 +543,7 @@ async_reply_handle(void *arg)
 		trigger_async_action(req);
 }
 
+//创建mp socket
 static int
 open_socket_fd(void)
 {
@@ -538,6 +554,7 @@ open_socket_fd(void)
 		snprintf(peer_name, sizeof(peer_name),
 				"%d_%"PRIx64, getpid(), rte_rdtsc());
 
+	//创建unix socket
 	mp_fd = socket(AF_UNIX, SOCK_DGRAM, 0);
 	if (mp_fd < 0) {
 		RTE_LOG(ERR, EAL, "failed to create unix socket\n");
@@ -547,6 +564,7 @@ open_socket_fd(void)
 	memset(&un, 0, sizeof(un));
 	un.sun_family = AF_UNIX;
 
+	//设置un.sun_path为eal_mp_socket_path(),此时peer_name为'\0'
 	create_socket_path(peer_name, un.sun_path, sizeof(un.sun_path));
 
 	unlink(un.sun_path); /* May still exist since last run */
@@ -575,6 +593,7 @@ close_socket_fd(void)
 	unlink(path);
 }
 
+//mp通道初始化（主从进程均会进来）
 int
 rte_mp_channel_init(void)
 {
@@ -585,21 +604,27 @@ rte_mp_channel_init(void)
 	/* in no shared files mode, we do not have secondary processes support,
 	 * so no need to initialize IPC.
 	 */
-	if (internal_config.no_shconf) {
+	if (internal_config.no_shconf) {//不share 配置文件，不初始化ipc(不支持从进程）
 		RTE_LOG(DEBUG, EAL, "No shared files mode enabled, IPC will be disabled\n");
 		rte_errno = ENOTSUP;
 		return -1;
 	}
 
+	//这一段代码写得非常屎，要完成的工作是
+	//1.锁住mp_dir_path
+	//2.删除其下所有文件
 	/* create filter path */
+	//构造'mp_socket/*',将'*'做为mp_filter
 	create_socket_path("*", path, sizeof(path));
 	strlcpy(mp_filter, basename(path), sizeof(mp_filter));
 
 	/* path may have been modified, so recreate it */
+	//构造'mp_socket/*',将'mp_socket/'做为mp_dir_path
 	create_socket_path("*", path, sizeof(path));
 	strlcpy(mp_dir_path, dirname(path), sizeof(mp_dir_path));
 
 	/* lock the directory */
+	//打开目录并锁住目录
 	dir_fd = open(mp_dir_path, O_RDONLY);
 	if (dir_fd < 0) {
 		RTE_LOG(ERR, EAL, "failed to open %s: %s\n",
@@ -614,11 +639,13 @@ rte_mp_channel_init(void)
 		return -1;
 	}
 
+	//创建mp socket
 	if (open_socket_fd() < 0) {
 		close(dir_fd);
 		return -1;
 	}
 
+	//创建mp消息处理线程
 	if (rte_ctrl_thread_create(&mp_handle_tid, "rte_mp_handle",
 			NULL, mp_handle, NULL) < 0) {
 		RTE_LOG(ERR, EAL, "failed to create mp thead: %s\n",
diff --git a/lib/librte_eal/common/eal_common_string_fns.c b/lib/librte_eal/common/eal_common_string_fns.c
index 60c5dd66f..cc9a12be2 100644
--- a/lib/librte_eal/common/eal_common_string_fns.c
+++ b/lib/librte_eal/common/eal_common_string_fns.c
@@ -25,14 +25,17 @@ rte_strsplit(char *string, int stringlen,
 			break;
 		if (tokstart) {
 			tokstart = 0;
+			//将string中的内容全部给tokens,然后再检查分隔符
 			tokens[tok++] = &string[i];
 		}
+
+		//发现delim符，置为'\0'，将上一个tokens中的string置为结束
 		if (string[i] == delim) {
 			string[i] = '\0';
 			tokstart = 1;
 		}
 	}
-	return tok;
+	return tok;//分割为多少组
 
 einval_error:
 	errno = EINVAL;
diff --git a/lib/librte_eal/common/eal_common_thread.c b/lib/librte_eal/common/eal_common_thread.c
index f9a8cf14d..a6b7bb704 100644
--- a/lib/librte_eal/common/eal_common_thread.c
+++ b/lib/librte_eal/common/eal_common_thread.c
@@ -38,6 +38,8 @@ rte_lcore_has_role(unsigned int lcore_id, enum rte_lcore_role_t role)
 	return cfg->lcore_role[lcore_id] == role;
 }
 
+//检查cpusetp中cpu对应的numa,如果cpusetp中包含的cpu有多个
+//且分属于不同的numa,则返回的numa为ANY,否则返回cpu对应的numa
 static int
 eal_cpuset_socket_id(rte_cpuset_t *cpusetp)
 {
@@ -49,13 +51,19 @@ eal_cpuset_socket_id(rte_cpuset_t *cpusetp)
 		return SOCKET_ID_ANY;
 
 	do {
+		//遍历检查cpusetp中包含的是那个cpu
 		if (!CPU_ISSET(cpu, cpusetp))
 			continue;
 
+		//如果未设置值，则使用此cpu对应的numa
 		if (socket_id == SOCKET_ID_ANY)
 			socket_id = eal_cpu_socket_id(cpu);
 
+		//取当前cpu对应的numa
 		sid = eal_cpu_socket_id(cpu);
+
+		//如果cpusetp中包含有多个cpu，且numa不相等，则将socket_id置为any
+		//且不再尝试
 		if (socket_id != sid) {
 			socket_id = SOCKET_ID_ANY;
 			break;
@@ -75,6 +83,7 @@ rte_thread_set_affinity(rte_cpuset_t *cpusetp)
 
 	tid = pthread_self();
 
+	//使线程绑定cpu
 	s = pthread_setaffinity_np(tid, sizeof(rte_cpuset_t), cpusetp);
 	if (s != 0) {
 		RTE_LOG(ERR, EAL, "pthread_setaffinity_np failed\n");
@@ -82,10 +91,12 @@ rte_thread_set_affinity(rte_cpuset_t *cpusetp)
 	}
 
 	/* store socket_id in TLS for quick access */
+	//设置当前线程默认使用的socket_id
 	RTE_PER_LCORE(_socket_id) =
 		eal_cpuset_socket_id(cpusetp);
 
 	/* store cpuset in TLS for quick access */
+	//存储本线程占用的cpuset
 	memmove(&RTE_PER_LCORE(_cpuset), cpusetp,
 		sizeof(rte_cpuset_t));
 
@@ -116,8 +127,10 @@ eal_thread_dump_affinity(char *str, unsigned size)
 	int ret;
 	unsigned int out = 0;
 
+	//获取当前线程的cpuset
 	rte_thread_get_affinity(&cpuset);
 
+	//收集cpuset中的cpu列表
 	for (cpu = 0; cpu < RTE_MAX_LCORE; cpu++) {
 		if (!CPU_ISSET(cpu, &cpuset))
 			continue;
@@ -156,15 +169,18 @@ static void *rte_thread_init(void *arg)
 	void *(*start_routine)(void *) = params->start_routine;
 	void *routine_arg = params->arg;
 
+	//等待barrier
 	ret = pthread_barrier_wait(&params->configured);
 	if (ret == PTHREAD_BARRIER_SERIAL_THREAD) {
 		pthread_barrier_destroy(&params->configured);
 		free(params);
 	}
 
+	//执行回调
 	return start_routine(routine_arg);
 }
 
+//创建线程，并配置cpu亲昵性
 int
 rte_ctrl_thread_create(pthread_t *thread, const char *name,
 		const pthread_attr_t *attr,
@@ -181,6 +197,7 @@ rte_ctrl_thread_create(pthread_t *thread, const char *name,
 	params->start_routine = start_routine;
 	params->arg = arg;
 
+	//初始化thread barrier,等待数为1+1
 	pthread_barrier_init(&params->configured, NULL, 2);
 
 	ret = pthread_create(thread, attr, rte_thread_init, (void *)params);
@@ -189,6 +206,7 @@ rte_ctrl_thread_create(pthread_t *thread, const char *name,
 		return -ret;
 	}
 
+	//修改线程名称
 	if (name != NULL) {
 		ret = rte_thread_setname(*thread, name);
 		if (ret < 0)
@@ -196,10 +214,12 @@ rte_ctrl_thread_create(pthread_t *thread, const char *name,
 				"Cannot set name for ctrl thread\n");
 	}
 
+	//设置线程亲昵性
 	ret = pthread_setaffinity_np(*thread, sizeof(*cpuset), cpuset);
 	if (ret)
 		goto fail;
 
+	//知会线程开始跑
 	ret = pthread_barrier_wait(&params->configured);
 	if (ret == PTHREAD_BARRIER_SERIAL_THREAD) {
 		pthread_barrier_destroy(&params->configured);
diff --git a/lib/librte_eal/common/eal_common_timer.c b/lib/librte_eal/common/eal_common_timer.c
index fa9ee1b22..1ebaae655 100644
--- a/lib/librte_eal/common/eal_common_timer.c
+++ b/lib/librte_eal/common/eal_common_timer.c
@@ -26,11 +26,14 @@ static uint64_t eal_tsc_resolution_hz;
 /* Pointer to user delay function */
 void (*rte_delay_us)(unsigned int) = NULL;
 
+//rte_delay_us的默认函数
 void
 rte_delay_us_block(unsigned int us)
 {
+	//利用cpu cycle来度量时间
 	const uint64_t start = rte_get_timer_cycles();
 	const uint64_t ticks = (uint64_t)us * rte_get_timer_hz() / 1E6;
+	//等待大于ticks的时间后，使函数跳出
 	while ((rte_get_timer_cycles() - start) < ticks)
 		rte_pause();
 }
@@ -60,7 +63,7 @@ rte_delay_us_sleep(unsigned int us)
 uint64_t
 rte_get_tsc_hz(void)
 {
-	return eal_tsc_resolution_hz;
+	return eal_tsc_resolution_hz;//认为每秒rdtsc增加数
 }
 
 static uint64_t
@@ -73,6 +76,7 @@ estimate_tsc_freq(void)
 	uint64_t start = rte_rdtsc();
 	sleep(1);
 	/* Round up to 10Mhz. 1E7 ~ 10Mhz */
+	//通过sleep(1)秒来获得时钟增加的量
 	return RTE_ALIGN_MUL_NEAR(rte_rdtsc() - start, CYC_PER_10MHZ);
 }
 
@@ -111,6 +115,7 @@ void rte_delay_us_callback_register(void (*userfunc)(unsigned int))
 
 RTE_INIT(rte_timer_init)
 {
+	//为rte_delay_us提供默认函数
 	/* set rte_delay_us_block as a delay function */
 	rte_delay_us_callback_register(rte_delay_us_block);
 }
diff --git a/lib/librte_eal/common/eal_filesystem.h b/lib/librte_eal/common/eal_filesystem.h
index 5d21f07c2..68daea364 100644
--- a/lib/librte_eal/common/eal_filesystem.h
+++ b/lib/librte_eal/common/eal_filesystem.h
@@ -33,6 +33,7 @@ const char *
 eal_get_hugefile_prefix(void);
 
 #define RUNTIME_CONFIG_FNAME "config"
+//获取runtime配置路径
 static inline const char *
 eal_runtime_config_path(void)
 {
diff --git a/lib/librte_eal/common/eal_internal_cfg.h b/lib/librte_eal/common/eal_internal_cfg.h
index a42f34923..e68515c41 100644
--- a/lib/librte_eal/common/eal_internal_cfg.h
+++ b/lib/librte_eal/common/eal_internal_cfg.h
@@ -26,23 +26,23 @@
  * mount points of hugepages
  */
 struct hugepage_info {
-	uint64_t hugepage_sz;   /**< size of a huge page */
-	char hugedir[PATH_MAX];    /**< dir where hugetlbfs is mounted */
+	uint64_t hugepage_sz;   /**< size of a huge page */ //每个大页的类型(页大小）
+	char hugedir[PATH_MAX];    /**< dir where hugetlbfs is mounted *///挂载目录名称
 	uint32_t num_pages[RTE_MAX_NUMA_NODES];
 	/**< number of hugepages of that size on each socket */
-	int lock_descriptor;    /**< file descriptor for hugepage dir */
+	int lock_descriptor;    /**< file descriptor for hugepage dir *///大页目录对应的文件描述符，用于lock
 };
 
 /**
  * internal configuration
  */
 struct internal_config {
-	volatile size_t memory;           /**< amount of asked memory */
-	volatile unsigned force_nchannel; /**< force number of channels */
-	volatile unsigned force_nrank;    /**< force number of ranks */
+	volatile size_t memory;           /**< amount of asked memory */ //-m给定了多少M内存
+	volatile unsigned force_nchannel; /**< force number of channels */ //内存通道参数
+	volatile unsigned force_nrank;    /**< force number of ranks */ //内存强制rank
 	volatile unsigned no_hugetlbfs;   /**< true to disable hugetlbfs */
 	unsigned hugepage_unlink;         /**< true to unlink backing files */
-	volatile unsigned no_pci;         /**< true to disable PCI */
+	volatile unsigned no_pci;         /**< true to disable PCI */ //是否禁用pci,debug用
 	volatile unsigned no_hpet;        /**< true to disable HPET */
 	volatile unsigned vmware_tsc_map; /**< true to use VMware TSC mapping
 										* instead of native TSC */
@@ -51,10 +51,13 @@ struct internal_config {
 	/**< true if DPDK should operate entirely in-memory and not create any
 	 * shared files or runtime data.
 	 */
+	//通过参数create-uio-dev指定
 	volatile unsigned create_uio_dev; /**< true to create /dev/uioX devices */
+	//多进程处理模式
 	volatile enum rte_proc_type_t process_type; /**< multi-process proc type */
 	/** true to try allocating memory on specific sockets */
 	volatile unsigned force_sockets;
+	//每个numa上的内存大小
 	volatile uint64_t socket_mem[RTE_MAX_NUMA_NODES]; /**< amount of memory per socket */
 	volatile unsigned force_socket_limits;
 	volatile uint64_t socket_limit[RTE_MAX_NUMA_NODES]; /**< limit amount of memory per socket */
@@ -69,15 +72,18 @@ struct internal_config {
 	/**< true if storing all pages within single files (per-page-size,
 	 * per-node) non-legacy mode only.
 	 */
+	//指定sys日志输出位置（默认是LOG_DAEMON）
 	volatile int syslog_facility;	  /**< facility passed to openlog() */
 	/** default interrupt mode for VFIO */
 	volatile enum rte_intr_mode vfio_intr_mode;
+    //大页文件前缀（默认rte)
 	char *hugefile_prefix;      /**< the base filename of hugetlbfs files */
+    //采用那个大页目录
 	char *hugepage_dir;         /**< specific hugetlbfs directory to use */
 	char *user_mbuf_pool_ops_name;
-			/**< user defined mbuf pool ops name */
-	unsigned num_hugepage_sizes;      /**< how many sizes on this system */
-	struct hugepage_info hugepage_info[MAX_HUGEPAGE_SIZES];
+	/**< user defined mbuf pool ops name */
+	unsigned num_hugepage_sizes;      /**< how many sizes on this system *///有多少种大页类型
+	struct hugepage_info hugepage_info[MAX_HUGEPAGE_SIZES];//每种大页的信息
 	enum rte_iova_mode iova_mode ;    /**< Set IOVA mode on this system  */
 	rte_cpuset_t ctrl_cpuset;         /**< cpuset for ctrl threads */
 	volatile unsigned int init_complete;
diff --git a/lib/librte_eal/common/eal_private.h b/lib/librte_eal/common/eal_private.h
index 8a9d493f0..f3defef74 100644
--- a/lib/librte_eal/common/eal_private.h
+++ b/lib/librte_eal/common/eal_private.h
@@ -16,22 +16,21 @@
  * Structure storing internal configuration (per-lcore)
  */
 struct lcore_config {
-	pthread_t thread_id;       /**< pthread identifier */
-	int pipe_master2slave[2];  /**< communication pipe with master */
-	int pipe_slave2master[2];  /**< communication pipe with master */
-
-	lcore_function_t * volatile f; /**< function to call */
-	void * volatile arg;       /**< argument of function */
-	volatile int ret;          /**< return value of function */
-
-	volatile enum rte_lcore_state_t state; /**< lcore state */
-	unsigned int socket_id;    /**< physical socket id for this lcore */
-	unsigned int core_id;      /**< core number on socket for this lcore */
-	int core_index;            /**< relative index, starting from 0 */
-	uint8_t core_role;         /**< role of core eg: OFF, RTE, SERVICE */
-	uint8_t detected;          /**< true if lcore was detected */
-
-	rte_cpuset_t cpuset;       /**< cpu set which the lcore affinity to */
+	pthread_t thread_id;       /**< pthread identifier */ //绑定的线程
+	int pipe_master2slave[2];  /**< communication pipe with master */ //master通信用
+	int pipe_slave2master[2];  /**< communication pipe with master */ //向master通信用
+
+	lcore_function_t * volatile f; /**< function to call */ //此核要执行的函数
+	void * volatile arg;       /**< argument of function */ //函数的参数
+	volatile int ret;          /**< return value of function */ //执行后的返回值（与状态配合使用）
+	volatile enum rte_lcore_state_t state; /**< lcore state */ //工作状态，执行函数用
+	unsigned int socket_id;    /**< physical socket id for this lcore */ //属于那个numa
+	unsigned int core_id;      /**< core number on socket for this lcore */ //物理core id(开超线程后，会出现两个线程一个core_id的情况）
+	int core_index;            /**< relative index, starting from 0 */ //core编号，如果不存在将为-1(最终按用户mask后的顺序）
+	uint8_t core_role;         /**< role of core eg: OFF, RTE, SERVICE */ //指定core的角色，例如ROLE_SERVICE
+	uint8_t detected;          /**< true if lcore was detected */ //是否被检测到
+
+	rte_cpuset_t cpuset;       /**< cpu set which the lcore affinity to */ //仅包含此core的cpuset
 };
 
 extern struct lcore_config lcore_config[RTE_MAX_LCORE];
@@ -40,12 +39,12 @@ extern struct lcore_config lcore_config[RTE_MAX_LCORE];
  * The global RTE configuration structure.
  */
 struct rte_config {
-	uint32_t master_lcore;       /**< Id of the master lcore */
-	uint32_t lcore_count;        /**< Number of available logical cores. */
-	uint32_t numa_node_count;    /**< Number of detected NUMA nodes. */
-	uint32_t numa_nodes[RTE_MAX_NUMA_NODES]; /**< List of detected NUMA nodes. */
+	uint32_t master_lcore;       /**< Id of the master lcore */ //master线程用那个逻辑core
+	uint32_t lcore_count;        /**< Number of available logical cores. */ //有多少个逻辑core（-c指定后，更新为有多少有效core)
+	uint32_t numa_node_count;    /**< Number of detected NUMA nodes. */ //系统有多少个numa节点（numa_nodes数组的大小）
+	uint32_t numa_nodes[RTE_MAX_NUMA_NODES]; /**< List of detected NUMA nodes. */ //记录系统中的numa id
 	uint32_t service_lcore_count;/**< Number of available service cores. */
-	enum rte_lcore_role_t lcore_role[RTE_MAX_LCORE]; /**< State of cores. */
+	enum rte_lcore_role_t lcore_role[RTE_MAX_LCORE]; /**< State of cores. */ //记录用户启用了那些core,未用哪些core(未用的core为role_off)
 
 	/** Primary or secondary configuration */
 	enum rte_proc_type_t process_type;
diff --git a/lib/librte_eal/common/include/arch/x86/rte_cycles.h b/lib/librte_eal/common/include/arch/x86/rte_cycles.h
index a461a4d73..ba15c658f 100644
--- a/lib/librte_eal/common/include/arch/x86/rte_cycles.h
+++ b/lib/librte_eal/common/include/arch/x86/rte_cycles.h
@@ -20,6 +20,7 @@ extern int rte_cycles_vmware_tsc_map;
 #include <rte_common.h>
 #include <rte_config.h>
 
+//取cpu计数器
 static inline uint64_t
 rte_rdtsc(void)
 {
@@ -56,6 +57,7 @@ rte_rdtsc_precise(void)
 	return rte_rdtsc();
 }
 
+//取当前tsc计数
 static inline uint64_t
 rte_get_tsc_cycles(void) { return rte_rdtsc(); }
 
diff --git a/lib/librte_eal/common/include/generic/rte_cycles.h b/lib/librte_eal/common/include/generic/rte_cycles.h
index 73d1fa7b9..9b262cfdd 100644
--- a/lib/librte_eal/common/include/generic/rte_cycles.h
+++ b/lib/librte_eal/common/include/generic/rte_cycles.h
@@ -90,6 +90,7 @@ int rte_eal_hpet_init(int make_default);
  * @return
  *   The number of cycles
  */
+//取当前时间
 static inline uint64_t
 rte_get_timer_cycles(void)
 {
diff --git a/lib/librte_eal/common/include/rte_bus.h b/lib/librte_eal/common/include/rte_bus.h
index d6a2494f6..60c011a4b 100644
--- a/lib/librte_eal/common/include/rte_bus.h
+++ b/lib/librte_eal/common/include/rte_bus.h
@@ -39,7 +39,7 @@ TAILQ_HEAD(rte_bus_list, rte_bus);
 enum rte_iova_mode {
 	RTE_IOVA_DC = 0,	/* Don't care mode */
 	RTE_IOVA_PA = (1 << 0), /* DMA using physical address */
-	RTE_IOVA_VA = (1 << 1)  /* DMA using virtual address */
+	RTE_IOVA_VA = (1 << 1)  /* DMA using virtual address */ //dma采用虚拟地址
 };
 
 /**
diff --git a/lib/librte_eal/common/include/rte_common.h b/lib/librte_eal/common/include/rte_common.h
index 459d082d1..0efb84ac5 100644
--- a/lib/librte_eal/common/include/rte_common.h
+++ b/lib/librte_eal/common/include/rte_common.h
@@ -200,6 +200,7 @@ static void __attribute__((destructor(RTE_PRIO(prio)), used)) func(void)
  * bigger than the first parameter. Second parameter must be a
  * power-of-two value.
  */
+//将val按align进行对齐（向下对齐）
 #define RTE_ALIGN_FLOOR(val, align) \
 	(typeof(val))((val) & (~((typeof(val))((align) - 1))))
 
@@ -701,10 +702,12 @@ rte_log2_u64(uint64_t v)
 #define RTE_FMT_TAIL(fmt, ...) __VA_ARGS__
 
 /** Mask value of type "tp" for the first "ln" bit set. */
+//使类型tp的前ln个bit置为０
 #define	RTE_LEN2MASK(ln, tp)	\
 	((tp)((uint64_t)-1 >> (sizeof(uint64_t) * CHAR_BIT - (ln))))
 
 /** Number of elements in the array. */
+//数组长度
 #define	RTE_DIM(a)	(sizeof (a) / sizeof ((a)[0]))
 
 /**
@@ -721,6 +724,7 @@ rte_log2_u64(uint64_t v)
  * @return
  *     Number.
  */
+//字符串转为数字，例如“1048576kB”
 static inline uint64_t
 rte_str_to_size(const char *str)
 {
diff --git a/lib/librte_eal/common/include/rte_debug.h b/lib/librte_eal/common/include/rte_debug.h
index 748d32c80..359a2feae 100644
--- a/lib/librte_eal/common/include/rte_debug.h
+++ b/lib/librte_eal/common/include/rte_debug.h
@@ -55,6 +55,7 @@ void rte_dump_registers(void);
 #else
 #define RTE_ASSERT(exp) do {} while (0)
 #endif
+//如果exp为假，则panic
 #define	RTE_VERIFY(exp)	do {                                                  \
 	if (unlikely(!(exp)))                                                           \
 		rte_panic("line %d\tassert \"%s\" failed\n", __LINE__, #exp); \
diff --git a/lib/librte_eal/common/include/rte_dev.h b/lib/librte_eal/common/include/rte_dev.h
index c25e09e3d..e287c3f56 100644
--- a/lib/librte_eal/common/include/rte_dev.h
+++ b/lib/librte_eal/common/include/rte_dev.h
@@ -44,6 +44,7 @@ typedef void (*rte_dev_event_cb_fn)(const char *device_name,
 					void *cb_arg);
 
 /* Macros to check for invalid function pointers */
+//如果func为空，返回retval
 #define RTE_FUNC_PTR_OR_ERR_RET(func, retval) do { \
 	if ((func) == NULL) \
 		return retval; \
@@ -78,9 +79,9 @@ enum rte_dev_policy {
  * A generic memory resource representation.
  */
 struct rte_mem_resource {
-	uint64_t phys_addr; /**< Physical address, 0 if not resource. */
-	uint64_t len;       /**< Length of the resource. */
-	void *addr;         /**< Virtual address, NULL when not mapped. */
+	uint64_t phys_addr; /**< Physical address, 0 if not resource. */ //起始地址
+	uint64_t len;       /**< Length of the resource. */ //资源长度
+	void *addr;         /**< Virtual address, NULL when not mapped. */ //这段地址的虚拟起始地址
 };
 
 /**
@@ -88,7 +89,7 @@ struct rte_mem_resource {
  */
 struct rte_driver {
 	TAILQ_ENTRY(rte_driver) next;  /**< Next in list. */
-	const char *name;                   /**< Driver name. */
+	const char *name;                   /**< Driver name. */ //驱动名称
 	const char *alias;              /**< Driver alias. */
 };
 
@@ -102,12 +103,12 @@ struct rte_driver {
  * A structure describing a generic device.
  */
 struct rte_device {
-	TAILQ_ENTRY(rte_device) next; /**< Next device */
-	const char *name;             /**< Device name */
-	const struct rte_driver *driver; /**< Driver assigned after probing */
+	TAILQ_ENTRY(rte_device) next; /**< Next device */ //用于将设备串连起来，例如一条bus一串设备
+	const char *name;             /**< Device name */ //设备名称
+	const struct rte_driver *driver;/**< Associated driver */ //采用哪个驱动
 	const struct rte_bus *bus;    /**< Bus handle assigned on scan */
-	int numa_node;                /**< NUMA node connection */
-	struct rte_devargs *devargs;  /**< Arguments for latest probing */
+	int numa_node;                /**< NUMA node connection */ //属于那个numa
+	struct rte_devargs *devargs;  /**< Arguments for latest probing */ //用户为此设备提供的参数
 };
 
 /**
@@ -213,6 +214,7 @@ __attribute__((used)) = RTE_STR(name)
 static const char DRV_EXP_TAG(name, pci_tbl_export)[] __attribute__((used)) = \
 RTE_STR(table)
 
+//定义驱动参数数组
 #define RTE_PMD_REGISTER_PARAM_STRING(name, str) \
 static const char DRV_EXP_TAG(name, param_string_export)[] \
 __attribute__((used)) = str
diff --git a/lib/librte_eal/common/include/rte_devargs.h b/lib/librte_eal/common/include/rte_devargs.h
index 882dfa0ab..c53825fe5 100644
--- a/lib/librte_eal/common/include/rte_devargs.h
+++ b/lib/librte_eal/common/include/rte_devargs.h
@@ -29,9 +29,9 @@ extern "C" {
  * Type of generic device
  */
 enum rte_devtype {
-	RTE_DEVTYPE_WHITELISTED_PCI,
-	RTE_DEVTYPE_BLACKLISTED_PCI,
-	RTE_DEVTYPE_VIRTUAL,
+	RTE_DEVTYPE_WHITELISTED_PCI,//白名单
+	RTE_DEVTYPE_BLACKLISTED_PCI,//黑名单
+	RTE_DEVTYPE_VIRTUAL,//虚拟设备
 };
 
 /**
@@ -56,7 +56,7 @@ struct rte_devargs {
 	RTE_STD_C11
 	union {
 	/** Arguments string as given by user or "" for no argument. */
-		char *args;
+		char *args;//设备参数
 		const char *drv_str;
 	};
 	struct rte_bus *bus; /**< bus handle. */
diff --git a/lib/librte_eal/common/include/rte_eal_interrupts.h b/lib/librte_eal/common/include/rte_eal_interrupts.h
index b370c0d26..8b6791826 100644
--- a/lib/librte_eal/common/include/rte_eal_interrupts.h
+++ b/lib/librte_eal/common/include/rte_eal_interrupts.h
@@ -70,9 +70,12 @@ struct rte_intr_handle {
 	RTE_STD_C11
 	union {
 		int vfio_dev_fd;  /**< VFIO device file descriptor */
+		//配置用fd
 		int uio_cfg_fd;  /**< UIO cfg file desc for uio_pci_generic */
 	};
+	//借助uio,用于处理中断的fd(读取中断事件）
 	int fd;	 /**< interrupt event file descriptor */
+	//采用哪种中断处理类型，例如uio
 	enum rte_intr_handle_type type;  /**< handle type */
 	uint32_t max_intr;             /**< max interrupt requested */
 	uint32_t nb_efd;               /**< number of available efd(event fd) */
diff --git a/lib/librte_eal/common/include/rte_function_versioning.h b/lib/librte_eal/common/include/rte_function_versioning.h
index c924351d5..ad250bcc4 100644
--- a/lib/librte_eal/common/include/rte_function_versioning.h
+++ b/lib/librte_eal/common/include/rte_function_versioning.h
@@ -44,6 +44,7 @@
  * Creates a symbol version table entry binding symbol <b>@DPDK_<n> to the internal
  * function name <b><e>
  */
+//符号版本，用于动态库兼容
 #define VERSION_SYMBOL(b, e, n) __asm__(".symver " RTE_STR(b) RTE_STR(e) ", " RTE_STR(b) "@DPDK_" RTE_STR(n))
 
 /*
diff --git a/lib/librte_eal/common/include/rte_lcore.h b/lib/librte_eal/common/include/rte_lcore.h
index 476b8ef3a..810fda6e0 100644
--- a/lib/librte_eal/common/include/rte_lcore.h
+++ b/lib/librte_eal/common/include/rte_lcore.h
@@ -47,6 +47,7 @@ enum rte_lcore_role_t rte_eal_lcore_role(unsigned int lcore_id);
  * @return
  *  Logical core ID (in EAL thread) or LCORE_ID_ANY (in non-EAL thread)
  */
+//当前core id
 static inline unsigned
 rte_lcore_id(void)
 {
@@ -59,6 +60,7 @@ rte_lcore_id(void)
  * @return
  *   the id of the master lcore
  */
+//获取master对应的core id
 unsigned int rte_get_master_lcore(void);
 
 /**
@@ -131,6 +133,7 @@ rte_socket_id_by_idx(unsigned int idx);
  * @return
  *   the ID of lcoreid's physical socket
  */
+//取logic core 对应的socket id
 unsigned int
 rte_lcore_to_socket_id(unsigned int lcore_id);
 
@@ -172,6 +175,7 @@ rte_lcore_cpuset(unsigned int lcore_id);
  * @return
  *   True if the given lcore is enabled; false otherwise.
  */
+//检查给定的core是否被启用了
 int rte_lcore_is_enabled(unsigned int lcore_id);
 
 /**
diff --git a/lib/librte_eal/common/include/rte_log.h b/lib/librte_eal/common/include/rte_log.h
index 1bb0e6694..90b4b2384 100644
--- a/lib/librte_eal/common/include/rte_log.h
+++ b/lib/librte_eal/common/include/rte_log.h
@@ -34,7 +34,7 @@ struct rte_logs {
 	uint32_t level; /**< Log level. */
 	FILE *file;     /**< Output file set by rte_openlog_stream, or NULL. */
 	size_t dynamic_types_len;
-	struct rte_log_dynamic_type *dynamic_types;
+	struct rte_log_dynamic_type *dynamic_types;//指出动态模块的日志级别
 };
 
 /** Global log information */
@@ -332,6 +332,7 @@ int rte_vlog(uint32_t level, uint32_t logtype, const char *format, va_list ap)
  *   - 0: Success.
  *   - Negative on error.
  */
+//此log性能不好
 #define RTE_LOG(l, t, ...)					\
 	 rte_log(RTE_LOG_ ## l,					\
 		 RTE_LOGTYPE_ ## t, # t ": " __VA_ARGS__)
diff --git a/lib/librte_eal/common/include/rte_memzone.h b/lib/librte_eal/common/include/rte_memzone.h
index f478fa9e6..ac304fcaa 100644
--- a/lib/librte_eal/common/include/rte_memzone.h
+++ b/lib/librte_eal/common/include/rte_memzone.h
@@ -39,6 +39,7 @@ extern "C" {
 #define RTE_MEMZONE_256MB          0x00020000   /**< Use 256MB pages. */
 #define RTE_MEMZONE_512MB          0x00040000   /**< Use 512MB pages. */
 #define RTE_MEMZONE_4GB            0x00080000   /**< Use 4GB pages. */
+//任意页标记
 #define RTE_MEMZONE_SIZE_HINT_ONLY 0x00000004   /**< Use available page size */
 #define RTE_MEMZONE_IOVA_CONTIG    0x00100000   /**< Ask for IOVA-contiguous memzone. */
 
diff --git a/lib/librte_eal/common/include/rte_pci.h b/lib/librte_eal/common/include/rte_pci.h
new file mode 100644
index 000000000..be176bdcf
--- /dev/null
+++ b/lib/librte_eal/common/include/rte_pci.h
@@ -0,0 +1,602 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright(c) 2010-2015 Intel Corporation. All rights reserved.
+ *   Copyright 2013-2014 6WIND S.A.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _RTE_PCI_H_
+#define _RTE_PCI_H_
+
+/**
+ * @file
+ *
+ * RTE PCI Interface
+ */
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <limits.h>
+#include <errno.h>
+#include <sys/queue.h>
+#include <stdint.h>
+#include <inttypes.h>
+
+#include <rte_debug.h>
+#include <rte_interrupts.h>
+#include <rte_dev.h>
+#include <rte_bus.h>
+
+/** Pathname of PCI devices directory. */
+const char *pci_get_sysfs_path(void);
+
+/** Formatting string for PCI device identifier: Ex: 0000:00:01.0 */
+#define PCI_PRI_FMT "%.4" PRIx16 ":%.2" PRIx8 ":%.2" PRIx8 ".%" PRIx8
+#define PCI_PRI_STR_SIZE sizeof("XXXXXXXX:XX:XX.X")//pci格式串
+
+/** Short formatting string, without domain, for PCI device: Ex: 00:01.0 */
+#define PCI_SHORT_PRI_FMT "%.2" PRIx8 ":%.2" PRIx8 ".%" PRIx8
+
+/** Nb. of values in PCI device identifier format string. */
+#define PCI_FMT_NVAL 4
+
+/** Nb. of values in PCI resource format. */
+#define PCI_RESOURCE_FMT_NVAL 3
+
+/** Maximum number of PCI resources. */
+#define PCI_MAX_RESOURCE 6
+
+/* Forward declarations */
+struct rte_pci_device;
+struct rte_pci_driver;
+
+/** List of PCI devices */
+TAILQ_HEAD(rte_pci_device_list, rte_pci_device);
+/** List of PCI drivers */
+TAILQ_HEAD(rte_pci_driver_list, rte_pci_driver);
+
+/* PCI Bus iterators */
+//遍历pci上所有设备
+#define FOREACH_DEVICE_ON_PCIBUS(p)	\
+		TAILQ_FOREACH(p, &(rte_pci_bus.device_list), next)
+
+#define FOREACH_DRIVER_ON_PCIBUS(p)	\
+		TAILQ_FOREACH(p, &(rte_pci_bus.driver_list), next)
+
+/**
+ * A structure describing an ID for a PCI driver. Each driver provides a
+ * table of these IDs for each device that it supports.
+ */
+struct rte_pci_id {
+	uint32_t class_id;            /**< Class ID (class, subclass, pi) or RTE_CLASS_ANY_ID. */ //设备类型
+	uint16_t vendor_id;           /**< Vendor ID or PCI_ANY_ID. */ //供货商
+	uint16_t device_id;           /**< Device ID or PCI_ANY_ID. */ //设备编号
+	uint16_t subsystem_vendor_id; /**< Subsystem vendor ID or PCI_ANY_ID. */ //细化供货商
+	uint16_t subsystem_device_id; /**< Subsystem device ID or PCI_ANY_ID. */ //细化设备编号
+};
+
+/**
+ * A structure describing the location of a PCI device.
+ */
+struct rte_pci_addr {
+	uint32_t domain;                /**< Device domain */
+	uint8_t bus;                    /**< Device bus */
+	uint8_t devid;                  /**< Device ID */
+	uint8_t function;               /**< Device function. */
+};
+
+struct rte_devargs;
+
+/**
+ * A structure describing a PCI device.
+ */
+//pci设备
+struct rte_pci_device {
+	TAILQ_ENTRY(rte_pci_device) next;       /**< Next probed PCI device. */
+	struct rte_device device;               /**< Inherit core device */
+	struct rte_pci_addr addr;               /**< PCI location. */ //pci地址
+	struct rte_pci_id id;                   /**< PCI ID. */ //pci id号
+	struct rte_mem_resource mem_resource[PCI_MAX_RESOURCE];//设备资源
+						/**< PCI Memory Resource */
+	struct rte_intr_handle intr_handle;     /**< Interrupt handle */
+	struct rte_pci_driver *driver;          /**< Associated driver */ //为此设备关联的驱动
+	uint16_t max_vfs;                       /**< sriov enable if not zero */
+	enum rte_kernel_driver kdrv;            /**< Kernel driver passthrough */ //驱动类型
+	char name[PCI_PRI_STR_SIZE+1];          /**< PCI location (ASCII) */ //pci地址
+};
+
+/**
+ * @internal
+ * Helper macro for drivers that need to convert to struct rte_pci_device.
+ */
+#define RTE_DEV_TO_PCI(ptr) container_of(ptr, struct rte_pci_device, device)
+
+/** Any PCI device identifier (vendor, device, ...) */
+#define PCI_ANY_ID (0xffff)
+#define RTE_CLASS_ANY_ID (0xffffff)
+
+#ifdef __cplusplus
+/** C++ macro used to help building up tables of device IDs */
+#define RTE_PCI_DEVICE(vend, dev) \
+	RTE_CLASS_ANY_ID,         \
+	(vend),                   \
+	(dev),                    \
+	PCI_ANY_ID,               \
+	PCI_ANY_ID
+#else
+/** Macro used to help building up tables of device IDs */
+#define RTE_PCI_DEVICE(vend, dev)          \
+	.class_id = RTE_CLASS_ANY_ID,      \
+	.vendor_id = (vend),               \
+	.device_id = (dev),                \
+	.subsystem_vendor_id = PCI_ANY_ID, \
+	.subsystem_device_id = PCI_ANY_ID
+#endif
+
+/**
+ * Initialisation function for the driver called during PCI probing.
+ */
+typedef int (pci_probe_t)(struct rte_pci_driver *, struct rte_pci_device *);
+
+/**
+ * Uninitialisation function for the driver called during hotplugging.
+ */
+typedef int (pci_remove_t)(struct rte_pci_device *);
+
+/**
+ * A structure describing a PCI driver.
+ */
+struct rte_pci_driver {
+	TAILQ_ENTRY(rte_pci_driver) next;       /**< Next in list. */
+	struct rte_driver driver;               /**< Inherit core driver. */
+	struct rte_pci_bus *bus;                /**< PCI bus reference. */
+	pci_probe_t *probe;                     /**< Device Probe function. */
+	pci_remove_t *remove;                   /**< Device Remove function. */
+	const struct rte_pci_id *id_table;	/**< ID table, NULL terminated. */ //通过id_table指明自已支持哪些设备
+	uint32_t drv_flags;                     /**< Flags contolling handling of device. */
+};
+
+/**
+ * Structure describing the PCI bus
+ */
+struct rte_pci_bus {
+	struct rte_bus bus;               /**< Inherit the generic class */
+	struct rte_pci_device_list device_list;  /**< List of PCI devices */ //串扫描到的pci设备（即总结上挂载的所有设备）
+	struct rte_pci_driver_list driver_list;  /**< List of PCI drivers */ //串注册的pci驱动
+};
+
+/** Device needs PCI BAR mapping (done with either IGB_UIO or VFIO) */
+#define RTE_PCI_DRV_NEED_MAPPING 0x0001
+/** Device driver supports link state interrupt */
+#define RTE_PCI_DRV_INTR_LSC	0x0008
+/** Device driver supports device removal interrupt */
+#define RTE_PCI_DRV_INTR_RMV 0x0010
+/** Device driver needs to keep mapped resources if unsupported dev detected */
+#define RTE_PCI_DRV_KEEP_MAPPED_RES 0x0020
+
+/**
+ * A structure describing a PCI mapping.
+ */
+struct pci_map {
+	void *addr;
+	char *path;
+	uint64_t offset;
+	uint64_t size;
+	uint64_t phaddr;
+};
+
+/**
+ * A structure describing a mapped PCI resource.
+ * For multi-process we need to reproduce all PCI mappings in secondary
+ * processes, so save them in a tailq.
+ */
+struct mapped_pci_resource {
+	TAILQ_ENTRY(mapped_pci_resource) next;
+
+	struct rte_pci_addr pci_addr;//设备pci地址
+	char path[PATH_MAX];//设备名称
+	int nb_maps;
+	struct pci_map maps[PCI_MAX_RESOURCE];
+};
+
+/** mapped pci device list */
+TAILQ_HEAD(mapped_pci_res_list, mapped_pci_resource);
+
+/**< Internal use only - Macro used by pci addr parsing functions **/
+#define GET_PCIADDR_FIELD(in, fd, lim, dlm)                   \
+do {                                                               \
+	unsigned long val;                                      \
+	char *end;                                              \
+	errno = 0;                                              \
+	val = strtoul((in), &end, 16);                          \
+	if (errno != 0 || end[0] != (dlm) || val > (lim))       \
+		return -EINVAL;                                 \
+	(fd) = (typeof (fd))val;                                \
+	(in) = end + 1;                                         \
+} while(0)
+
+/**
+ * Utility function to produce a PCI Bus-Device-Function value
+ * given a string representation. Assumes that the BDF is provided without
+ * a domain prefix (i.e. domain returned is always 0)
+ *
+ * @param input
+ *	The input string to be parsed. Should have the format XX:XX.X
+ * @param dev_addr
+ *	The PCI Bus-Device-Function address to be returned. Domain will always be
+ *	returned as 0
+ * @return
+ *  0 on success, negative on error.
+ */
+static inline int
+eal_parse_pci_BDF(const char *input, struct rte_pci_addr *dev_addr)
+{
+	dev_addr->domain = 0;
+	GET_PCIADDR_FIELD(input, dev_addr->bus, UINT8_MAX, ':');//只接受XX:XX.X格式
+	GET_PCIADDR_FIELD(input, dev_addr->devid, UINT8_MAX, '.');
+	GET_PCIADDR_FIELD(input, dev_addr->function, UINT8_MAX, 0);
+	return 0;
+}
+
+/**
+ * Utility function to produce a PCI Bus-Device-Function value
+ * given a string representation. Assumes that the BDF is provided including
+ * a domain prefix.
+ *
+ * @param input
+ *	The input string to be parsed. Should have the format XXXX:XX:XX.X
+ * @param dev_addr
+ *	The PCI Bus-Device-Function address to be returned
+ * @return
+ *  0 on success, negative on error.
+ */
+static inline int
+eal_parse_pci_DomBDF(const char *input, struct rte_pci_addr *dev_addr)
+{
+	GET_PCIADDR_FIELD(input, dev_addr->domain, UINT16_MAX, ':');
+	GET_PCIADDR_FIELD(input, dev_addr->bus, UINT8_MAX, ':');
+	GET_PCIADDR_FIELD(input, dev_addr->devid, UINT8_MAX, '.');
+	GET_PCIADDR_FIELD(input, dev_addr->function, UINT8_MAX, 0);
+	return 0;
+}
+#undef GET_PCIADDR_FIELD
+
+/**
+ * Utility function to write a pci device name, this device name can later be
+ * used to retrieve the corresponding rte_pci_addr using eal_parse_pci_*
+ * BDF helpers.
+ *
+ * @param addr
+ *	The PCI Bus-Device-Function address
+ * @param output
+ *	The output buffer string
+ * @param size
+ *	The output buffer size
+ */
+static inline void
+rte_pci_device_name(const struct rte_pci_addr *addr,
+		char *output, size_t size)
+{
+	RTE_VERIFY(size >= PCI_PRI_STR_SIZE);
+	RTE_VERIFY(snprintf(output, size, PCI_PRI_FMT,
+			    addr->domain, addr->bus,
+			    addr->devid, addr->function) >= 0);
+}
+
+/* Compare two PCI device addresses. */
+/**
+ * Utility function to compare two PCI device addresses.
+ *
+ * @param addr
+ *	The PCI Bus-Device-Function address to compare
+ * @param addr2
+ *	The PCI Bus-Device-Function address to compare
+ * @return
+ *	0 on equal PCI address.
+ *	Positive on addr is greater than addr2.
+ *	Negative on addr is less than addr2, or error.
+ */
+//比对两个pci地址
+static inline int
+rte_eal_compare_pci_addr(const struct rte_pci_addr *addr,
+			 const struct rte_pci_addr *addr2)
+{
+	uint64_t dev_addr, dev_addr2;
+
+	if ((addr == NULL) || (addr2 == NULL))
+		return -1;
+
+	dev_addr = ((uint64_t)addr->domain << 24) |
+		(addr->bus << 16) | (addr->devid << 8) | addr->function;
+	dev_addr2 = ((uint64_t)addr2->domain << 24) |
+		(addr2->bus << 16) | (addr2->devid << 8) | addr2->function;
+
+	if (dev_addr > dev_addr2)
+		return 1;
+	else if (dev_addr < dev_addr2)
+		return -1;
+	else
+		return 0;
+}
+
+/**
+ * Scan the content of the PCI bus, and the devices in the devices
+ * list
+ *
+ * @return
+ *  0 on success, negative on error
+ */
+int rte_pci_scan(void);
+
+/**
+ * Probe the PCI bus
+ *
+ * @return
+ *   - 0 on success.
+ *   - !0 on error.
+ */
+int
+rte_pci_probe(void);
+
+/**
+ * Map the PCI device resources in user space virtual memory address
+ *
+ * Note that driver should not call this function when flag
+ * RTE_PCI_DRV_NEED_MAPPING is set, as EAL will do that for
+ * you when it's on.
+ *
+ * @param dev
+ *   A pointer to a rte_pci_device structure describing the device
+ *   to use
+ *
+ * @return
+ *   0 on success, negative on error and positive if no driver
+ *   is found for the device.
+ */
+int rte_pci_map_device(struct rte_pci_device *dev);
+
+/**
+ * Unmap this device
+ *
+ * @param dev
+ *   A pointer to a rte_pci_device structure describing the device
+ *   to use
+ */
+void rte_pci_unmap_device(struct rte_pci_device *dev);
+
+/**
+ * @internal
+ * Map a particular resource from a file.
+ *
+ * @param requested_addr
+ *      The starting address for the new mapping range.
+ * @param fd
+ *      The file descriptor.
+ * @param offset
+ *      The offset for the mapping range.
+ * @param size
+ *      The size for the mapping range.
+ * @param additional_flags
+ *      The additional flags for the mapping range.
+ * @return
+ *   - On success, the function returns a pointer to the mapped area.
+ *   - On error, the value MAP_FAILED is returned.
+ */
+void *pci_map_resource(void *requested_addr, int fd, off_t offset,
+		size_t size, int additional_flags);
+
+/**
+ * @internal
+ * Unmap a particular resource.
+ *
+ * @param requested_addr
+ *      The address for the unmapping range.
+ * @param size
+ *      The size for the unmapping range.
+ */
+void pci_unmap_resource(void *requested_addr, size_t size);
+
+/**
+ * Probe the single PCI device.
+ *
+ * Scan the content of the PCI bus, and find the pci device specified by pci
+ * address, then call the probe() function for registered driver that has a
+ * matching entry in its id_table for discovered device.
+ *
+ * @param addr
+ *	The PCI Bus-Device-Function address to probe.
+ * @return
+ *   - 0 on success.
+ *   - Negative on error.
+ */
+int rte_pci_probe_one(const struct rte_pci_addr *addr);
+
+/**
+ * Close the single PCI device.
+ *
+ * Scan the content of the PCI bus, and find the pci device specified by pci
+ * address, then call the remove() function for registered driver that has a
+ * matching entry in its id_table for discovered device.
+ *
+ * @param addr
+ *	The PCI Bus-Device-Function address to close.
+ * @return
+ *   - 0 on success.
+ *   - Negative on error.
+ */
+int rte_pci_detach(const struct rte_pci_addr *addr);
+
+/**
+ * Dump the content of the PCI bus.
+ *
+ * @param f
+ *   A pointer to a file for output
+ */
+void rte_pci_dump(FILE *f);
+
+/**
+ * Register a PCI driver.
+ *
+ * @param driver
+ *   A pointer to a rte_pci_driver structure describing the driver
+ *   to be registered.
+ */
+void rte_pci_register(struct rte_pci_driver *driver);
+
+/** Helper for PCI device registration from driver (eth, crypto) instance */
+//注册pci驱动
+#define RTE_PMD_REGISTER_PCI(nm, pci_drv) \
+RTE_INIT(pciinitfn_ ##nm); \
+static void pciinitfn_ ##nm(void) \
+{\
+	(pci_drv).driver.name = RTE_STR(nm);\
+	rte_pci_register(&pci_drv); \
+} \
+RTE_PMD_EXPORT_NAME(nm, __COUNTER__)
+
+/**
+ * Unregister a PCI driver.
+ *
+ * @param driver
+ *   A pointer to a rte_pci_driver structure describing the driver
+ *   to be unregistered.
+ */
+void rte_pci_unregister(struct rte_pci_driver *driver);
+
+/**
+ * Read PCI config space.
+ *
+ * @param device
+ *   A pointer to a rte_pci_device structure describing the device
+ *   to use
+ * @param buf
+ *   A data buffer where the bytes should be read into
+ * @param len
+ *   The length of the data buffer.
+ * @param offset
+ *   The offset into PCI config space
+ */
+int rte_pci_read_config(const struct rte_pci_device *device,
+		void *buf, size_t len, off_t offset);
+
+/**
+ * Write PCI config space.
+ *
+ * @param device
+ *   A pointer to a rte_pci_device structure describing the device
+ *   to use
+ * @param buf
+ *   A data buffer containing the bytes should be written
+ * @param len
+ *   The length of the data buffer.
+ * @param offset
+ *   The offset into PCI config space
+ */
+int rte_pci_write_config(const struct rte_pci_device *device,
+		const void *buf, size_t len, off_t offset);
+
+/**
+ * A structure used to access io resources for a pci device.
+ * rte_pci_ioport is arch, os, driver specific, and should not be used outside
+ * of pci ioport api.
+ */
+struct rte_pci_ioport {
+	struct rte_pci_device *dev;
+	uint64_t base;
+	uint64_t len; /* only filled for memory mapped ports */
+};
+
+/**
+ * Initialize a rte_pci_ioport object for a pci device io resource.
+ *
+ * This object is then used to gain access to those io resources (see below).
+ *
+ * @param dev
+ *   A pointer to a rte_pci_device structure describing the device
+ *   to use.
+ * @param bar
+ *   Index of the io pci resource we want to access.
+ * @param p
+ *   The rte_pci_ioport object to be initialized.
+ * @return
+ *  0 on success, negative on error.
+ */
+int rte_pci_ioport_map(struct rte_pci_device *dev, int bar,
+		struct rte_pci_ioport *p);
+
+/**
+ * Release any resources used in a rte_pci_ioport object.
+ *
+ * @param p
+ *   The rte_pci_ioport object to be uninitialized.
+ * @return
+ *  0 on success, negative on error.
+ */
+int rte_pci_ioport_unmap(struct rte_pci_ioport *p);
+
+/**
+ * Read from a io pci resource.
+ *
+ * @param p
+ *   The rte_pci_ioport object from which we want to read.
+ * @param data
+ *   A data buffer where the bytes should be read into
+ * @param len
+ *   The length of the data buffer.
+ * @param offset
+ *   The offset into the pci io resource.
+ */
+void rte_pci_ioport_read(struct rte_pci_ioport *p,
+		void *data, size_t len, off_t offset);
+
+/**
+ * Write to a io pci resource.
+ *
+ * @param p
+ *   The rte_pci_ioport object to which we want to write.
+ * @param data
+ *   A data buffer where the bytes should be read into
+ * @param len
+ *   The length of the data buffer.
+ * @param offset
+ *   The offset into the pci io resource.
+ */
+void rte_pci_ioport_write(struct rte_pci_ioport *p,
+		const void *data, size_t len, off_t offset);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* _RTE_PCI_H_ */
diff --git a/lib/librte_eal/common/include/rte_random.h b/lib/librte_eal/common/include/rte_random.h
index 2b30ec85c..3c2ed21d0 100644
--- a/lib/librte_eal/common/include/rte_random.h
+++ b/lib/librte_eal/common/include/rte_random.h
@@ -46,6 +46,7 @@ rte_srand(uint64_t seedval);
  * @return
  *   A pseudo-random value between 0 and (1<<64)-1.
  */
+//合并为64bit的整数
 uint64_t
 rte_rand(void);
 
diff --git a/lib/librte_eal/common/malloc_elem.c b/lib/librte_eal/common/malloc_elem.c
index 658c9b5b7..8d77362d1 100644
--- a/lib/librte_eal/common/malloc_elem.c
+++ b/lib/librte_eal/common/malloc_elem.c
@@ -217,6 +217,7 @@ elem_check_phys_contig(const struct rte_memseg_list *msl,
  * and alignment would fit in the current element. If the data doesn't
  * fit, return NULL.
  */
+//检查是否可以分配，如果不能返回NULL，如果可以返回拆分点
 static void *
 elem_start_pt(struct malloc_elem *elem, size_t size, unsigned align,
 		size_t bound, bool contig)
@@ -363,6 +364,7 @@ prev_elem_is_adjacent(struct malloc_elem *elem)
  *   heap->free_head[2] - (2^10 ,2^12]
  *   heap->free_head[3] - (2^12, 2^14]
  *   heap->free_head[4] - (2^14, MAX_SIZE]
+ * 如上述，按照不同的size返回相应的索引号
  */
 size_t
 malloc_elem_free_list_index(size_t size)
@@ -439,6 +441,7 @@ malloc_elem_alloc(struct malloc_elem *elem, size_t size, unsigned align,
 			elem->heap->last = new_free_elem;
 	}
 
+	//剩余的块太小了，直接做为padding存在
 	if (old_elem_size < MALLOC_ELEM_OVERHEAD + MIN_DATA_SIZE) {
 		/* don't split it, pad the element instead */
 		elem->state = ELEM_BUSY;
diff --git a/lib/librte_eal/common/malloc_heap.c b/lib/librte_eal/common/malloc_heap.c
index 842eb9de7..805c561aa 100644
--- a/lib/librte_eal/common/malloc_heap.c
+++ b/lib/librte_eal/common/malloc_heap.c
@@ -157,12 +157,14 @@ find_suitable_element(struct malloc_heap *heap, size_t size,
 	size_t idx;
 	struct malloc_elem *elem, *alt_elem = NULL;
 
+	//依据不同的size返回起始的idx，当在当前的idx中无法找到合适的elem，则向上层的idx进行查找
 	for (idx = malloc_elem_free_list_index(size);
 			idx < RTE_HEAP_NUM_FREELISTS; idx++) {
 		for (elem = LIST_FIRST(&heap->free_head[idx]);
 				!!elem; elem = LIST_NEXT(elem, free_list)) {
 			if (malloc_elem_can_hold(elem, size, align, bound,
 					contig)) {
+				//这个elem可以用于分配，检查大页属性是否合适，如果合适返回此elem
 				if (check_hugepage_sz(flags,
 						elem->msl->page_sz))
 					return elem;
@@ -1315,6 +1317,7 @@ malloc_heap_destroy(struct malloc_heap *heap)
 	return 0;
 }
 
+//初始化各socket的堆
 int
 rte_eal_malloc_heap_init(void)
 {
diff --git a/lib/librte_eal/common/malloc_heap.h b/lib/librte_eal/common/malloc_heap.h
index 772736b53..ceae9f76a 100644
--- a/lib/librte_eal/common/malloc_heap.h
+++ b/lib/librte_eal/common/malloc_heap.h
@@ -40,8 +40,10 @@ extern "C" {
 static inline unsigned
 malloc_get_numa_socket(void)
 {
+	//尝试当前线程，所在的numa节点
 	unsigned socket_id = rte_socket_id();
 
+	//如果当前线程所在numa节点为any,使用0
 	if (socket_id == (unsigned)SOCKET_ID_ANY)
 		return 0;
 
diff --git a/lib/librte_eal/common/rte_malloc.c b/lib/librte_eal/common/rte_malloc.c
index 413e4aa00..d7a98fc4c 100644
--- a/lib/librte_eal/common/rte_malloc.c
+++ b/lib/librte_eal/common/rte_malloc.c
@@ -45,6 +45,7 @@ rte_malloc_socket(const char *type, size_t size, unsigned int align,
 		int socket_arg)
 {
 	/* return NULL if size is 0 or alignment is not power-of-2 */
+	//参数检查
 	if (size == 0 || (align && !rte_is_power_of_2(align)))
 		return NULL;
 
@@ -55,8 +56,10 @@ rte_malloc_socket(const char *type, size_t size, unsigned int align,
 	 */
 	if (rte_malloc_heap_socket_is_external(socket_arg) != 1 &&
 				!rte_eal_has_hugepages())
+		//没有配置大页内存时，强制变更为socket不相关申请
 		socket_arg = SOCKET_ID_ANY;
 
+	//指定在$socket堆上进行申请
 	return malloc_heap_alloc(type, size, socket_arg, 0,
 			align == 0 ? 1 : align, 0, false);
 }
@@ -76,6 +79,7 @@ rte_malloc(const char *type, size_t size, unsigned align)
 void *
 rte_zmalloc_socket(const char *type, size_t size, unsigned align, int socket)
 {
+	//当socket == SOCKET_ID_ANY时，与socket无关
 	void *ptr = rte_malloc_socket(type, size, align, socket);
 
 #ifdef RTE_MALLOC_DEBUG
@@ -96,6 +100,7 @@ rte_zmalloc_socket(const char *type, size_t size, unsigned align, int socket)
 void *
 rte_zmalloc(const char *type, size_t size, unsigned align)
 {
+	//申请内存，对内存所在的socket无要求
 	return rte_zmalloc_socket(type, size, align, SOCKET_ID_ANY);
 }
 
diff --git a/lib/librte_eal/freebsd/eal/eal.c b/lib/librte_eal/freebsd/eal/eal.c
index 6ae37e7e6..ed06eaad2 100644
--- a/lib/librte_eal/freebsd/eal/eal.c
+++ b/lib/librte_eal/freebsd/eal/eal.c
@@ -504,6 +504,7 @@ eal_log_level_parse(int argc, char **argv)
 		if (opt == '?')
 			break;
 
+		//只处理log-level
 		ret = (opt == OPT_LOG_LEVEL_NUM) ?
 			eal_parse_common_option(opt, optarg, &internal_config) : 0;
 
@@ -709,11 +710,13 @@ rte_eal_init(int argc, char **argv)
 
 	/* checks if the machine is adequate */
 	if (!rte_cpu_is_supported()) {
+		//如果cpu没有开启，则报错
 		rte_eal_init_alert("unsupported cpu type.");
 		rte_errno = ENOTSUP;
 		return -1;
 	}
 
+	//防止rte_eal_init函数被多次调用
 	if (!rte_atomic32_test_and_set(&run_once)) {
 		rte_eal_init_alert("already called initialization.");
 		rte_errno = EALREADY;
@@ -733,6 +736,7 @@ rte_eal_init(int argc, char **argv)
 		return -1;
 	}
 
+	//解析命令行
 	fctret = eal_parse_args(argc, argv);
 	if (fctret < 0) {
 		rte_eal_init_alert("Invalid 'command line' arguments.");
@@ -784,6 +788,7 @@ rte_eal_init(int argc, char **argv)
 		}
 	}
 
+	//使注册的所有bus进行扫描
 	if (rte_bus_scan()) {
 		rte_eal_init_alert("Cannot scan the buses for devices");
 		rte_errno = ENODEV;
diff --git a/lib/librte_eal/linux/eal/eal.c b/lib/librte_eal/linux/eal/eal.c
index 9e2d50cfb..a23bc1df4 100644
--- a/lib/librte_eal/linux/eal/eal.c
+++ b/lib/librte_eal/linux/eal/eal.c
@@ -90,6 +90,7 @@ static struct rte_config rte_config = {
 };
 
 /* internal configuration (per-core) */
+//各core配置
 struct lcore_config lcore_config[RTE_MAX_LCORE];
 
 /* internal configuration */
@@ -266,6 +267,7 @@ rte_eal_iova_mode(void)
 }
 
 /* parse a sysfs (or other) file containing one integer value */
+//读文件，并解析其包含的一行数据，将其解析为整数
 int
 eal_parse_sysfs_value(const char *filename, unsigned long *val)
 {
@@ -469,6 +471,7 @@ rte_eal_config_reattach(void)
 enum rte_proc_type_t
 eal_proc_type_detect(void)
 {
+	//默认是主进程
 	enum rte_proc_type_t ptype = RTE_PROC_PRIMARY;
 	const char *pathname = eal_runtime_config_path();
 
@@ -481,7 +484,7 @@ eal_proc_type_detect(void)
 		 */
 		if (((mem_cfg_fd = open(pathname, O_RDWR)) >= 0) &&
 				(fcntl(mem_cfg_fd, F_SETLK, &wr_lock) < 0))
-			ptype = RTE_PROC_SECONDARY;
+			ptype = RTE_PROC_SECONDARY;//如果无法锁住，则为从进程
 	}
 
 	RTE_LOG(INFO, EAL, "Auto-detected process type: %s\n",
@@ -497,12 +500,12 @@ rte_config_init(void)
 	rte_config.process_type = internal_config.process_type;
 
 	switch (rte_config.process_type){
-	case RTE_PROC_PRIMARY:
+	case RTE_PROC_PRIMARY://进程为主类型
 		if (rte_eal_config_create() < 0)
 			return -1;
 		eal_mcfg_update_from_internal();
 		break;
-	case RTE_PROC_SECONDARY:
+	case RTE_PROC_SECONDARY://进程为从类型
 		if (rte_eal_config_attach() < 0)
 			return -1;
 		eal_mcfg_wait_complete();
@@ -514,6 +517,7 @@ rte_config_init(void)
 			return -1;
 		eal_mcfg_update_internal();
 		break;
+		//此时必须已将auto,invalid实现了转换
 	case RTE_PROC_AUTO:
 	case RTE_PROC_INVALID:
 		RTE_LOG(ERR, EAL, "Invalid process type %d\n",
@@ -533,6 +537,7 @@ eal_hugedirs_unlock(void)
 	for (i = 0; i < MAX_HUGEPAGE_SIZES; i++)
 	{
 		/* skip uninitialized */
+		//如果有锁，则解锁
 		if (internal_config.hugepage_info[i].lock_descriptor < 0)
 			continue;
 		/* unlock hugepage file */
@@ -580,6 +585,7 @@ rte_set_application_usage_hook( rte_usage_hook_t usage_func )
 	return old_func;
 }
 
+//解析各socket上内存"1024,1024,1024"
 static int
 eal_parse_socket_arg(char *strval, volatile uint64_t *socket_arg)
 {
@@ -588,6 +594,7 @@ eal_parse_socket_arg(char *strval, volatile uint64_t *socket_arg)
 	int arg_num, i, len;
 	uint64_t total_mem = 0;
 
+	//长度限制
 	len = strnlen(strval, SOCKET_MEM_STRLEN);
 	if (len == SOCKET_MEM_STRLEN) {
 		RTE_LOG(ERR, EAL, "--socket-mem is too long\n");
@@ -595,6 +602,7 @@ eal_parse_socket_arg(char *strval, volatile uint64_t *socket_arg)
 	}
 
 	/* all other error cases will be caught later */
+	//最后一个字符非数字，报错
 	if (!isdigit(strval[len-1]))
 		return -1;
 
@@ -611,13 +619,16 @@ eal_parse_socket_arg(char *strval, volatile uint64_t *socket_arg)
 	for (i = 0; i < arg_num; i++) {
 		uint64_t val;
 		end = NULL;
+		//记录第i个socket上使用内存
 		val = strtoull(arg[i], &end, 10);
 
 		/* check for invalid input */
 		if ((errno != 0)  ||
 				(arg[i][0] == '\0') || (end == NULL) || (*end != '\0'))
 			return -1;
+		//换算成字节
 		val <<= 20;
+		//计录总内存
 		total_mem += val;
 		socket_arg[i] = val;
 	}
@@ -670,6 +681,7 @@ eal_log_level_parse(int argc, char **argv)
 		if (opt == '?')
 			break;
 
+		//--log-level
 		ret = (opt == OPT_LOG_LEVEL_NUM) ?
 			eal_parse_common_option(opt, optarg, &internal_config) : 0;
 
@@ -720,6 +732,7 @@ eal_parse_args(int argc, char **argv)
 		ret = eal_parse_common_option(opt, optarg, &internal_config);
 		/* common parser is not happy */
 		if (ret < 0) {
+			//失败处理
 			eal_usage(prgname);
 			ret = -1;
 			goto out;
@@ -759,6 +772,7 @@ eal_parse_args(int argc, char **argv)
 			}
 			break;
 		}
+		//socket内存设置
 		case OPT_SOCKET_MEM_NUM:
 			if (eal_parse_socket_arg(optarg,
 					internal_config.socket_mem) < 0) {
@@ -844,6 +858,7 @@ eal_parse_args(int argc, char **argv)
 		goto out;
 	}
 
+	//对core,进程类型等需要自动检测的，进行检测
 	if (eal_adjust_config(&internal_config) != 0) {
 		ret = -1;
 		goto out;
@@ -856,10 +871,12 @@ eal_parse_args(int argc, char **argv)
 		goto out;
 	}
 
+	//修改argv[optind-1]为进程名，将其它选项交给argv后面处理
 	if (optind >= 0)
 		argv[optind-1] = prgname;
 	ret = optind-1;
 
+	//还原getopt
 out:
 	/* restore getopt lib */
 	optind = old_optind;
@@ -970,17 +987,20 @@ rte_eal_init(int argc, char **argv)
 
 	/* checks if the machine is adequate */
 	if (!rte_cpu_is_supported()) {
+		//检查是否为支持的cpu类型
 		rte_eal_init_alert("unsupported cpu type.");
 		rte_errno = ENOTSUP;
 		return -1;
 	}
 
+	//仅能运行一次
 	if (!rte_atomic32_test_and_set(&run_once)) {
 		rte_eal_init_alert("already called initialization.");
 		rte_errno = EALREADY;
 		return -1;
 	}
 
+	//取可执行程序名称
 	p = strrchr(argv[0], '/');
 	strlcpy(logid, p ? p + 1 : argv[0], sizeof(logid));
 	thread_id = pthread_self();
@@ -988,14 +1008,17 @@ rte_eal_init(int argc, char **argv)
 	eal_reset_internal_config(&internal_config);
 
 	/* set log level as early as possible */
+	//处理log-level
 	eal_log_level_parse(argc, argv);
 
+	//cpu配置检测
 	if (rte_eal_cpu_init() < 0) {
 		rte_eal_init_alert("Cannot detect lcores.");
 		rte_errno = ENOTSUP;
 		return -1;
 	}
 
+	//参数解析
 	fctret = eal_parse_args(argc, argv);
 	if (fctret < 0) {
 		rte_eal_init_alert("Invalid 'command line' arguments.");
@@ -1004,6 +1027,7 @@ rte_eal_init(int argc, char **argv)
 		return -1;
 	}
 
+	//插件机制初始化
 	if (eal_plugins_init() < 0) {
 		rte_eal_init_alert("Cannot init plugins");
 		rte_errno = EINVAL;
@@ -1011,6 +1035,7 @@ rte_eal_init(int argc, char **argv)
 		return -1;
 	}
 
+	//遍历设备选项，生成设备参数挂接在设备参数链上
 	if (eal_option_device_parse()) {
 		rte_errno = ENODEV;
 		rte_atomic32_clear(&run_once);
@@ -1022,6 +1047,7 @@ rte_eal_init(int argc, char **argv)
 		return -1;
 	}
 
+	//中断线程初始化
 	if (rte_eal_intr_init() < 0) {
 		rte_eal_init_alert("Cannot init interrupt-handling thread");
 		return -1;
@@ -1036,6 +1062,7 @@ rte_eal_init(int argc, char **argv)
 	/* Put mp channel init before bus scan so that we can init the vdev
 	 * bus through mp channel in the secondary process before the bus scan.
 	 */
+	//多进程通道初始化
 	if (rte_mp_channel_init() < 0 && rte_errno != ENOTSUP) {
 		rte_eal_init_alert("failed to init mp channel");
 		if (rte_eal_process_type() == RTE_PROC_PRIMARY) {
@@ -1050,6 +1077,7 @@ rte_eal_init(int argc, char **argv)
 		return -1;
 	}
 
+	//各总线扫描自身设备（用于发现设备）
 	if (rte_bus_scan()) {
 		rte_eal_init_alert("Cannot scan the buses for devices");
 		rte_errno = ENODEV;
@@ -1112,6 +1140,7 @@ rte_eal_init(int argc, char **argv)
 	RTE_LOG(INFO, EAL, "Selected IOVA mode '%s'\n",
 		rte_eal_iova_mode() == RTE_IOVA_PA ? "PA" : "VA");
 
+	//初始化hugepage
 	if (internal_config.no_hugetlbfs == 0) {
 		/* rte_config isn't initialized yet */
 		ret = internal_config.process_type == RTE_PROC_PRIMARY ?
@@ -1141,6 +1170,7 @@ rte_eal_init(int argc, char **argv)
 #endif
 	}
 
+	//初始化syslog
 	if (rte_eal_log_init(logid, internal_config.syslog_facility) < 0) {
 		rte_eal_init_alert("Cannot init logging.");
 		rte_errno = ENOMEM;
@@ -1149,6 +1179,7 @@ rte_eal_init(int argc, char **argv)
 	}
 
 #ifdef VFIO_PRESENT
+	//使能vfio
 	if (rte_eal_vfio_setup() < 0) {
 		rte_eal_init_alert("Cannot init VFIO");
 		rte_errno = EAGAIN;
@@ -1160,6 +1191,7 @@ rte_eal_init(int argc, char **argv)
 	 * not present in primary processes, so to avoid any potential issues,
 	 * initialize memzones first.
 	 */
+	//初始化memzone
 	if (rte_eal_memzone_init() < 0) {
 		rte_eal_init_alert("Cannot init memzone");
 		rte_errno = ENODEV;
@@ -1175,6 +1207,7 @@ rte_eal_init(int argc, char **argv)
 	/* the directories are locked during eal_hugepage_info_init */
 	eal_hugedirs_unlock();
 
+	//初始化malloc
 	if (rte_eal_malloc_heap_init() < 0) {
 		rte_eal_init_alert("Cannot init malloc heap");
 		rte_errno = ENODEV;
@@ -1195,6 +1228,7 @@ rte_eal_init(int argc, char **argv)
 
 	eal_check_mem_on_local_socket();
 
+	//设置master线程
 	eal_thread_init_master(rte_config.master_lcore);
 
 	ret = eal_thread_dump_affinity(cpuset, sizeof(cpuset));
@@ -1203,19 +1237,22 @@ rte_eal_init(int argc, char **argv)
 		rte_config.master_lcore, (uintptr_t)thread_id, cpuset,
 		ret == 0 ? "" : "...");
 
+	//遍历所有slave core
 	RTE_LCORE_FOREACH_SLAVE(i) {
 
 		/*
 		 * create communication pipes between master thread
 		 * and children
 		 */
+		//建立主从间通信用的管道
 		if (pipe(lcore_config[i].pipe_master2slave) < 0)
 			rte_panic("Cannot create pipe\n");
 		if (pipe(lcore_config[i].pipe_slave2master) < 0)
 			rte_panic("Cannot create pipe\n");
 
-		lcore_config[i].state = WAIT;
+		lcore_config[i].state = WAIT;//将各slave core状态直为wait状态
 
+		//为slave创建线程（并阻塞，等待master安排任务）
 		/* create a thread for each lcore */
 		ret = pthread_create(&lcore_config[i].thread_id, NULL,
 				     eal_thread_loop, NULL);
@@ -1223,6 +1260,7 @@ rte_eal_init(int argc, char **argv)
 			rte_panic("Cannot create thread\n");
 
 		/* Set thread_name for aid in debugging. */
+		//设置slave线程名称
 		snprintf(thread_name, sizeof(thread_name),
 			"lcore-slave-%d", i);
 		ret = rte_thread_setname(lcore_config[i].thread_id,
@@ -1236,8 +1274,8 @@ rte_eal_init(int argc, char **argv)
 	 * Launch a dummy function on all slave lcores, so that master lcore
 	 * knows they are all ready when this function returns.
 	 */
-	rte_eal_mp_remote_launch(sync_func, NULL, SKIP_MASTER);
-	rte_eal_mp_wait_lcore();
+	rte_eal_mp_remote_launch(sync_func, NULL, SKIP_MASTER);//用于确保各slave均完成初始化并阻塞
+	rte_eal_mp_wait_lcore();//阻塞确保各slave完成工作
 
 	/* initialize services so vdevs register service during bus_probe. */
 	ret = rte_service_init();
@@ -1248,6 +1286,7 @@ rte_eal_init(int argc, char **argv)
 	}
 
 	/* Probe all the buses and devices/drivers on them */
+	//bus探测
 	if (rte_bus_probe()) {
 		rte_eal_init_alert("Cannot probe devices");
 		rte_errno = ENOTSUP;
@@ -1329,6 +1368,7 @@ rte_eal_process_type(void)
 	return rte_config.process_type;
 }
 
+//检查是否配置有大页内存
 int rte_eal_has_hugepages(void)
 {
 	return ! internal_config.no_hugetlbfs;
@@ -1350,6 +1390,7 @@ rte_eal_vfio_intr_mode(void)
 	return internal_config.vfio_intr_mode;
 }
 
+//检查module是否被加载
 int
 rte_eal_check_module(const char *module_name)
 {
diff --git a/lib/librte_eal/linux/eal/eal_debug.c b/lib/librte_eal/linux/eal/eal_debug.c
index 5d92500bf..358f2f44d 100644
--- a/lib/librte_eal/linux/eal/eal_debug.c
+++ b/lib/librte_eal/linux/eal/eal_debug.c
@@ -66,6 +66,7 @@ void __rte_panic(const char *funcname, const char *format, ...)
  * Like rte_panic this terminates the application. However, no traceback is
  * provided and no core-dump is generated.
  */
+//显示日志并退出，如果开启panic，则显示堆栈及寄存器
 void
 rte_exit(int exit_code, const char *format, ...)
 {
diff --git a/lib/librte_eal/linux/eal/eal_dev.c b/lib/librte_eal/linux/eal/eal_dev.c
index 83c9cd660..19fc7be57 100644
--- a/lib/librte_eal/linux/eal/eal_dev.c
+++ b/lib/librte_eal/linux/eal/eal_dev.c
@@ -107,6 +107,7 @@ dev_uev_socket_fd_create(void)
 	struct sockaddr_nl addr;
 	int ret;
 
+	//创建netlink消息，并注udev事件
 	intr_handle.fd = socket(PF_NETLINK, SOCK_RAW | SOCK_CLOEXEC |
 			SOCK_NONBLOCK,
 			NETLINK_KOBJECT_UEVENT);
@@ -133,6 +134,7 @@ dev_uev_socket_fd_create(void)
 	return ret;
 }
 
+//将buf解析成rte_dev_event
 static int
 dev_uev_parse(const char *buf, struct rte_dev_event *event, int length)
 {
@@ -222,6 +224,7 @@ dev_uev_handler(__rte_unused void *param)
 	memset(&uevent, 0, sizeof(struct rte_dev_event));
 	memset(buf, 0, EAL_UEV_MSG_LEN);
 
+	//自fd中读取uevent消息
 	ret = recv(intr_handle.fd, buf, EAL_UEV_MSG_LEN, MSG_DONTWAIT);
 	if (ret < 0 && errno == EAGAIN)
 		return;
@@ -232,6 +235,7 @@ dev_uev_handler(__rte_unused void *param)
 		return;
 	}
 
+	//解析uevent消息
 	ret = dev_uev_parse(buf, &uevent, EAL_UEV_MSG_LEN);
 	if (ret < 0) {
 		RTE_LOG(DEBUG, EAL, "It is not an valid event "
@@ -276,6 +280,7 @@ dev_uev_handler(__rte_unused void *param)
 			}
 			rte_spinlock_unlock(&failure_handle_lock);
 		}
+		//udevent消息处理，目前支持设备添加，设备移除两个事件
 		rte_dev_event_callback_process(uevent.devname, uevent.type);
 	}
 
@@ -291,14 +296,16 @@ rte_dev_event_monitor_start(void)
 	int ret;
 
 	if (monitor_started)
-		return 0;
+		return 0;//已开启时，直接返回
 
+	//创建netlink socket fd
 	ret = dev_uev_socket_fd_create();
 	if (ret) {
 		RTE_LOG(ERR, EAL, "error create device event fd.\n");
 		return -1;
 	}
 
+	//注册中断回调
 	intr_handle.type = RTE_INTR_HANDLE_DEV_EVENT;
 	ret = rte_intr_callback_register(&intr_handle, dev_uev_handler, NULL);
 
diff --git a/lib/librte_eal/linux/eal/eal_hugepage_info.c b/lib/librte_eal/linux/eal/eal_hugepage_info.c
index 91a4fede7..6ad7835a9 100644
--- a/lib/librte_eal/linux/eal/eal_hugepage_info.c
+++ b/lib/librte_eal/linux/eal/eal_hugepage_info.c
@@ -81,6 +81,7 @@ static int get_hp_sysfs_value(const char *subdir, const char *file, unsigned lon
 
 /* this function is only called from eal_hugepage_info_init which itself
  * is only called from a primary process */
+//获取此类型大页内存的空闲页数目
 static uint32_t
 get_num_hugepages(const char *subdir)
 {
@@ -91,9 +92,11 @@ get_num_hugepages(const char *subdir)
 	const char *nr_splus_file = "surplus_hugepages";
 
 	/* first, check how many reserved pages kernel reports */
+	//提取reserved的页大小
 	if (get_hp_sysfs_value(subdir, nr_rsvd_file, &resv_pages) < 0)
 		return 0;
 
+	//提取free的页大小
 	if (get_hp_sysfs_value(subdir, nr_hp_file, &num_pages) < 0)
 		return 0;
 
@@ -169,6 +172,7 @@ get_num_hugepages_on_node(const char *subdir, unsigned int socket)
 	return num_pages;
 }
 
+//取meminfo中的Hugepagesize字段量，即大页的页大小
 static uint64_t
 get_default_hp_size(void)
 {
@@ -178,10 +182,11 @@ get_default_hp_size(void)
 	char buffer[256];
 	unsigned long long size = 0;
 
-	FILE *fd = fopen(proc_meminfo, "r");
+	FILE *fd = fopen(proc_meminfo, "r");//打开meminfo
 	if (fd == NULL)
 		rte_panic("Cannot open %s\n", proc_meminfo);
 	while(fgets(buffer, sizeof(buffer), fd)){
+		//取Hugepagesize:字段
 		if (strncmp(buffer, str_hugepagesz, hugepagesz_len) == 0){
 			size = rte_str_to_size(&buffer[hugepagesz_len]);
 			break;
@@ -193,6 +198,7 @@ get_default_hp_size(void)
 	return size;
 }
 
+//解析指定的大页目录
 static int
 get_hugepage_dir(uint64_t hugepage_sz, char *hugedir, int len)
 {
@@ -214,13 +220,15 @@ get_hugepage_dir(uint64_t hugepage_sz, char *hugedir, int len)
 	char buf[BUFSIZ];
 	int retval = -1;
 
+	//打开kernel挂载文件
 	FILE *fd = fopen(proc_mounts, "r");
 	if (fd == NULL)
 		rte_panic("Cannot open %s\n", proc_mounts);
 
 	if (default_size == 0)
-		default_size = get_default_hp_size();
+		default_size = get_default_hp_size();//获取默认大页的页大小
 
+	//读取一行数据，然后按空隔拆分，需要最多拆分为4列，否则会报错
 	while (fgets(buf, sizeof(buf), fd)){
 		if (rte_strsplit(buf, sizeof(buf), splitstr, _FIELDNAME_MAX,
 				split_tok) != _FIELDNAME_MAX) {
@@ -229,14 +237,18 @@ get_hugepage_dir(uint64_t hugepage_sz, char *hugedir, int len)
 		}
 
 		/* we have a specified --huge-dir option, only examine that dir */
+		//如果通过参数指定了大页的目录，则仅检查参数指定的目录，其它目录将被忽略
 		if (internal_config.hugepage_dir != NULL &&
 				strcmp(splitstr[MOUNTPT], internal_config.hugepage_dir) != 0)
 			continue;
 
+		//仅要求文件类型为hugetlbfs类型
 		if (strncmp(splitstr[FSTYPE], hugetlbfs_str, htlbfs_str_len) == 0){
+			//尝试着在option中取pagesize选项
 			const char *pagesz_str = strstr(splitstr[OPTIONS], pagesize_opt);
 
 			/* if no explicit page size, the default page size is compared */
+			//如果没有指定页大小，则与默认页大小比对
 			if (pagesz_str == NULL){
 				if (hugepage_sz == default_size){
 					strlcpy(hugedir, splitstr[MOUNTPT], len);
@@ -245,6 +257,7 @@ get_hugepage_dir(uint64_t hugepage_sz, char *hugedir, int len)
 				}
 			}
 			/* there is an explicit page size, so check it */
+			//如果选项中指定了页大小，与页大小比对
 			else {
 				uint64_t pagesz = rte_str_to_size(&pagesz_str[pagesize_opt_len]);
 				if (pagesz == hugepage_sz) {
@@ -257,6 +270,7 @@ get_hugepage_dir(uint64_t hugepage_sz, char *hugedir, int len)
 	} /* end while fgets */
 
 	fclose(fd);
+	//返回此大页对应的挂载点
 	return retval;
 }
 
@@ -265,6 +279,7 @@ get_hugepage_dir(uint64_t hugepage_sz, char *hugedir, int len)
  * there are. Checks if the file is locked (i.e.
  * if it's in use by another DPDK process).
  */
+//删除掉hugepage上所有的hugepage文件（没有用的hugepage文件，会加锁尝试）
 static int
 clear_hugedir(const char * hugedir)
 {
@@ -280,7 +295,7 @@ clear_hugedir(const char * hugedir)
 				hugedir);
 		goto error;
 	}
-	dir_fd = dirfd(dir);
+	dir_fd = dirfd(dir);//将dir转为fd
 
 	dirent = readdir(dir);
 	if (!dirent) {
@@ -306,11 +321,13 @@ clear_hugedir(const char * hugedir)
 		}
 
 		/* non-blocking lock */
+		//尝试锁住此fd
 		lck_result = flock(fd, LOCK_EX | LOCK_NB);
 
 		/* if lock succeeds, remove the file */
 		if (lck_result != -1)
 			unlinkat(dir_fd, dirent->d_name, 0);
+		//如果没有锁成功，则不处理
 		close (fd);
 		dirent = readdir(dir);
 	}
@@ -383,6 +400,15 @@ hugepage_info_init(void)
 	DIR *dir;
 	struct dirent *dirent;
 
+	//打开大页目录
+	/*
+anlang@anlang:~/workspace/acc_be$ ls -al /sys/kernel/mm/hugepages/
+total 0
+drwxr-xr-x 4 root root 0 1月  16 15:15 .
+drwxr-xr-x 6 root root 0 1月  16 15:15 ..
+drwxr-xr-x 2 root root 0 1月  16 15:15 hugepages-1048576kB
+drwxr-xr-x 2 root root 0 1月  16 15:15 hugepages-2048kB
+	 */
 	dir = opendir(sys_dir_path);
 	if (dir == NULL) {
 		RTE_LOG(ERR, EAL,
@@ -394,22 +420,27 @@ hugepage_info_init(void)
 	for (dirent = readdir(dir); dirent != NULL; dirent = readdir(dir)) {
 		struct hugepage_info *hpi;
 
+		//如果不以$dirent_start_text开头，则忽略
 		if (strncmp(dirent->d_name, dirent_start_text,
 			    dirent_start_len) != 0)
 			continue;
 
+		//我们最多支持3种（2M，1G，4M）
 		if (num_sizes >= MAX_HUGEPAGE_SIZES)
 			break;
 
+		//获取大页类型
 		hpi = &internal_config.hugepage_info[num_sizes];
 		hpi->hugepage_sz =
-			rte_str_to_size(&dirent->d_name[dirent_start_len]);
+			rte_str_to_size(&dirent->d_name[dirent_start_len]);//页大小
+
 
 		/* first, check if we have a mountpoint */
 		if (get_hugepage_dir(hpi->hugepage_sz,
-			hpi->hugedir, sizeof(hpi->hugedir)) < 0) {
+			hpi->hugedir, sizeof(hpi->hugedir)) < 0) {//获取页大小为$hugepage_sz的挂载点
 			uint32_t num_pages;
 
+			//告警，有reserved，但没有挂载
 			num_pages = get_num_hugepages(dirent->d_name);
 			if (num_pages > 0)
 				RTE_LOG(NOTICE, EAL,
@@ -436,6 +467,7 @@ hugepage_info_init(void)
 			continue;
 		}
 
+		//锁住此大页目录，排除其它进程
 		/* try to obtain a writelock */
 		hpi->lock_descriptor = open(hpi->hugedir, O_RDONLY);
 
@@ -446,6 +478,7 @@ hugepage_info_init(void)
 			break;
 		}
 		/* clear out the hugepages dir from unused pages */
+		//删除掉此目录无用的页
 		if (clear_hugedir(hpi->hugedir) == -1)
 			break;
 
@@ -462,10 +495,12 @@ hugepage_info_init(void)
 	internal_config.num_hugepage_sizes = num_sizes;
 
 	/* sort the page directory entries by size, largest to smallest */
+	//按页大小进行排序（从最大size到最小size)
 	qsort(&internal_config.hugepage_info[0], num_sizes,
 	      sizeof(internal_config.hugepage_info[0]), compare_hpi);
 
 	/* now we have all info, check we have at least one valid size */
+	//至少要有一个有效的hugepage_info
 	for (i = 0; i < num_sizes; i++) {
 		/* pages may no longer all be on socket 0, so check all */
 		unsigned int j, num_pages = 0;
diff --git a/lib/librte_eal/linux/eal/eal_interrupts.c b/lib/librte_eal/linux/eal/eal_interrupts.c
index 1955324d3..d2d084049 100644
--- a/lib/librte_eal/linux/eal/eal_interrupts.c
+++ b/lib/librte_eal/linux/eal/eal_interrupts.c
@@ -49,11 +49,11 @@ static RTE_DEFINE_PER_LCORE(int, _epfd) = -1; /**< epoll fd per thread */
  */
 union intr_pipefds{
 	struct {
-		int pipefd[2];
+		int pipefd[2];//创建用
 	};
 	struct {
-		int readfd;
-		int writefd;
+		int readfd;//读fd
+		int writefd;//写fd
 	};
 };
 
@@ -97,7 +97,7 @@ static union intr_pipefds intr_pipe;
 static struct rte_intr_source_list intr_sources;
 
 /* interrupt handling thread */
-static pthread_t intr_thread;
+static pthread_t intr_thread;//中断线程
 
 /* VFIO interrupts */
 #ifdef VFIO_PRESENT
@@ -489,11 +489,13 @@ rte_intr_callback_register(const struct rte_intr_handle *intr_handle,
 	}
 
 	/* allocate a new interrupt callback entity */
+	//申请中断回调实体
 	callback = calloc(1, sizeof(*callback));
 	if (callback == NULL) {
 		RTE_LOG(ERR, EAL, "Can not allocate memory\n");
 		return -ENOMEM;
 	}
+	//设置回调及回调参数
 	callback->cb_fn = cb;
 	callback->cb_arg = cb_arg;
 	callback->pending_delete = 0;
@@ -508,6 +510,7 @@ rte_intr_callback_register(const struct rte_intr_handle *intr_handle,
 			if (TAILQ_EMPTY(&src->callbacks))
 				wake_thread = 1;
 
+			//将callback加入到尾部
 			TAILQ_INSERT_TAIL(&(src->callbacks), callback, next);
 			ret = 0;
 			break;
@@ -954,6 +957,7 @@ eal_intr_process_interrupts(struct epoll_event *events, int nfds)
 				rte_spinlock_unlock(&intr_lock);
 
 				/* call the actual callback */
+				//调用实际中的回调
 				active_cb.cb_fn(active_cb.cb_arg);
 
 				/*get the lock back. */
@@ -1013,6 +1017,7 @@ eal_intr_handle_interrupts(int pfd, unsigned totalfds)
 	int nfds = 0;
 
 	for(;;) {
+		//等待事件发生
 		nfds = epoll_wait(pfd, events, totalfds,
 			EAL_INTR_EPOLL_WAIT_FOREVER);
 		/* epoll_wait fail */
@@ -1042,6 +1047,7 @@ eal_intr_handle_interrupts(int pfd, unsigned totalfds)
  * @return
  *  never return;
  */
+//中断主线程
 static __attribute__((noreturn)) void *
 eal_intr_thread_main(__rte_unused void *arg)
 {
@@ -1097,7 +1103,7 @@ eal_intr_thread_main(__rte_unused void *arg)
 		}
 		rte_spinlock_unlock(&intr_lock);
 		/* serve the interrupt */
-		eal_intr_handle_interrupts(pfd, numfds);
+		eal_intr_handle_interrupts(pfd, numfds);//中断处理
 
 		/**
 		 * when we return, we need to rebuild the
@@ -1107,6 +1113,7 @@ eal_intr_thread_main(__rte_unused void *arg)
 	}
 }
 
+//中断线程初始化
 int
 rte_eal_intr_init(void)
 {
@@ -1126,7 +1133,7 @@ rte_eal_intr_init(void)
 
 	/* create the host thread to wait/handle the interrupt */
 	ret = rte_ctrl_thread_create(&intr_thread, "eal-intr-thread", NULL,
-			eal_intr_thread_main, NULL);
+			eal_intr_thread_main, NULL);//创建中断线程
 	if (ret != 0) {
 		rte_errno = -ret;
 		RTE_LOG(ERR, EAL,
diff --git a/lib/librte_eal/linux/eal/eal_lcore.c b/lib/librte_eal/linux/eal/eal_lcore.c
index bc8965844..455181642 100644
--- a/lib/librte_eal/linux/eal/eal_lcore.c
+++ b/lib/librte_eal/linux/eal/eal_lcore.c
@@ -23,6 +23,7 @@
 #define NUMA_NODE_PATH "/sys/devices/system/node"
 
 /* Check if a cpu is present by the presence of the cpu information for it */
+//看指定core是否存在
 int
 eal_cpu_detected(unsigned lcore_id)
 {
@@ -44,6 +45,7 @@ eal_cpu_detected(unsigned lcore_id)
  * lcore_id and returns the numa node where the lcore is found. If lcore is not
  * found on any numa node, returns zero.
  */
+//获取core对应的numa节点
 unsigned
 eal_cpu_socket_id(unsigned lcore_id)
 {
@@ -61,6 +63,7 @@ eal_cpu_socket_id(unsigned lcore_id)
 }
 
 /* Get the cpu core id value from the /sys/.../cpuX core_id value */
+//读给定lcore_id对应的物理core_id
 unsigned
 eal_cpu_core_id(unsigned lcore_id)
 {
diff --git a/lib/librte_eal/linux/eal/eal_log.c b/lib/librte_eal/linux/eal/eal_log.c
index 9d02dddbe..c955dc982 100644
--- a/lib/librte_eal/linux/eal/eal_log.c
+++ b/lib/librte_eal/linux/eal/eal_log.c
@@ -54,6 +54,7 @@ rte_eal_log_init(const char *id, int facility)
 	if (log_stream == NULL)
 		return -1;
 
+	//打开log
 	openlog(id, LOG_NDELAY | LOG_PID, facility);
 
 	eal_log_set_default(log_stream);
diff --git a/lib/librte_eal/linux/eal/eal_memory.c b/lib/librte_eal/linux/eal/eal_memory.c
index 43e4ffc75..c57347dc3 100644
--- a/lib/librte_eal/linux/eal/eal_memory.c
+++ b/lib/librte_eal/linux/eal/eal_memory.c
@@ -92,6 +92,7 @@ uint64_t eal_get_baseaddr(void)
 /*
  * Get physical address of any mapped virtual address in the current process.
  */
+//取virtaddr对应的物理地址
 phys_addr_t
 rte_mem_virt2phy(const void *virtaddr)
 {
@@ -105,6 +106,7 @@ rte_mem_virt2phy(const void *virtaddr)
 		return RTE_BAD_IOVA;
 
 	/* standard page size */
+	//通过/proc/self/pagemap文件读取指定页首地址对应的物理地址
 	page_size = getpagesize();
 
 	fd = open("/proc/self/pagemap", O_RDONLY);
@@ -114,15 +116,17 @@ rte_mem_virt2phy(const void *virtaddr)
 		return RTE_BAD_IOVA;
 	}
 
-	virt_pfn = (unsigned long)virtaddr / page_size;
-	offset = sizeof(uint64_t) * virt_pfn;
+	virt_pfn = (unsigned long)virtaddr / page_size;//虚拟地址对应的页号
+	offset = sizeof(uint64_t) * virt_pfn;//采用64位输出指针地址，故需要偏移sizeof(uint64_t)*virt_pfn
 	if (lseek(fd, offset, SEEK_SET) == (off_t) -1) {
+		//偏移到指定页
 		RTE_LOG(INFO, EAL, "%s(): seek error in /proc/self/pagemap: %s\n",
 				__func__, strerror(errno));
 		close(fd);
 		return RTE_BAD_IOVA;
 	}
 
+	//读取此页对应的物理地址（8字节）
 	retval = read(fd, &page, PFN_MASK_SIZE);
 	close(fd);
 	if (retval < 0) {
@@ -140,9 +144,11 @@ rte_mem_virt2phy(const void *virtaddr)
 	 * the pfn (page frame number) are bits 0-54 (see
 	 * pagemap.txt in linux Documentation)
 	 */
+	//有效地址在0-54位，如果0-54位为0，则说明地址有误
 	if ((page & 0x7fffffffffffffULL) == 0)
 		return RTE_BAD_IOVA;
 
+	//求出虚地址页首地址对应的物理机址，再由此算出具体一个虚地址对应的物理地址
 	physaddr = ((page & 0x7fffffffffffffULL) * page_size)
 		+ ((unsigned long)virtaddr % page_size);
 
@@ -153,8 +159,8 @@ rte_iova_t
 rte_mem_virt2iova(const void *virtaddr)
 {
 	if (rte_eal_iova_mode() == RTE_IOVA_VA)
-		return (uintptr_t)virtaddr;
-	return rte_mem_virt2phy(virtaddr);
+		return (uintptr_t)virtaddr;//如果dma采用虚拟地址，则直接返回
+	return rte_mem_virt2phy(virtaddr);//通过memmap获取物理地址
 }
 
 /*
@@ -1339,6 +1345,7 @@ eal_legacy_hugepage_init(void)
 	mcfg = rte_eal_get_configuration()->mem_config;
 
 	/* hugetlbfs can be disabled */
+	//当大页被禁用，此时memseg即使用addr
 	if (internal_config.no_hugetlbfs) {
 		struct rte_memseg_list *msl;
 		int n_segs, cur_seg, fd, flags;
@@ -1395,9 +1402,11 @@ eal_legacy_hugepage_init(void)
 			}
 		}
 #endif
+		//申请一块内存（大小为internal_config.memory)
 		addr = mmap(NULL, internal_config.memory, PROT_READ | PROT_WRITE,
 				flags, fd, 0);
 		if (addr == MAP_FAILED) {
+			//申请失败处理
 			RTE_LOG(ERR, EAL, "%s: mmap() failed: %s\n", __func__,
 					strerror(errno));
 			return -1;
@@ -1457,7 +1466,7 @@ eal_legacy_hugepage_init(void)
 		/* meanwhile, also initialize used_hp hugepage sizes in used_hp */
 		used_hp[i].hugepage_sz = internal_config.hugepage_info[i].hugepage_sz;
 
-		nr_hugepages += internal_config.hugepage_info[i].num_pages[0];
+		nr_hugepages += internal_config.hugepage_info[i].num_pages[0];//记录一共有多少大页
 	}
 
 	/*
@@ -1466,6 +1475,7 @@ eal_legacy_hugepage_init(void)
 	 * processing done on these pages, shared memory will be created
 	 * at a later stage.
 	 */
+	//为每个大页注册申请一个struct hugepage_file结构
 	tmp_hp = malloc(nr_hugepages * sizeof(struct hugepage_file));
 	if (tmp_hp == NULL)
 		goto fail;
@@ -2010,6 +2020,7 @@ rte_eal_hugepage_attach(void)
 			eal_hugepage_attach();
 }
 
+//检查当前系统是否可以获取到物理地址
 int
 rte_eal_using_phys_addrs(void)
 {
diff --git a/lib/librte_eal/linux/eal/eal_thread.c b/lib/librte_eal/linux/eal/eal_thread.c
index 379773b68..cb6bc60bd 100644
--- a/lib/librte_eal/linux/eal/eal_thread.c
+++ b/lib/librte_eal/linux/eal/eal_thread.c
@@ -33,6 +33,7 @@ RTE_DEFINE_PER_LCORE(rte_cpuset_t, _cpuset);
  * function f with argument arg. Once the execution is done, the
  * remote lcore switch in FINISHED state.
  */
+//为slave分配任务，并通知其运行，等待其开始运行后退出
 int
 rte_eal_remote_launch(int (*f)(void *), void *arg, unsigned slave_id)
 {
@@ -44,10 +45,12 @@ rte_eal_remote_launch(int (*f)(void *), void *arg, unsigned slave_id)
 	if (lcore_config[slave_id].state != WAIT)
 		return -EBUSY;
 
+	//分配任务给slave
 	lcore_config[slave_id].f = f;
 	lcore_config[slave_id].arg = arg;
 
 	/* send message */
+	//通知slave工作已安排
 	n = 0;
 	while (n == 0 || (n < 0 && errno == EINTR))
 		n = write(m2s, &c, 1);
@@ -55,6 +58,7 @@ rte_eal_remote_launch(int (*f)(void *), void *arg, unsigned slave_id)
 		rte_panic("cannot write on configuration pipe\n");
 
 	/* wait ack */
+	//等待此slave答复
 	do {
 		n = read(s2m, &c, 1);
 	} while (n < 0 && errno == EINTR);
@@ -84,6 +88,7 @@ void eal_thread_init_master(unsigned lcore_id)
 	RTE_PER_LCORE(_lcore_id) = lcore_id;
 
 	/* set CPU affinity */
+	//设置对应core的亲昵性
 	if (eal_thread_set_affinity() < 0)
 		rte_panic("cannot set affinity\n");
 }
@@ -102,10 +107,12 @@ eal_thread_loop(__attribute__((unused)) void *arg)
 	thread_id = pthread_self();
 
 	/* retrieve our lcore_id from the configuration structure */
+	//获取自已现在用哪个core进行服务
 	RTE_LCORE_FOREACH_SLAVE(lcore_id) {
 		if (thread_id == lcore_config[lcore_id].thread_id)
 			break;
 	}
+	//没有拿到core_id,挂掉
 	if (lcore_id == RTE_MAX_LCORE)
 		rte_panic("cannot retrieve lcore id\n");
 
@@ -113,6 +120,7 @@ eal_thread_loop(__attribute__((unused)) void *arg)
 	s2m = lcore_config[lcore_id].pipe_slave2master[1];
 
 	/* set the lcore ID in per-lcore memory area */
+	//设置自已的core_id
 	RTE_PER_LCORE(_lcore_id) = lcore_id;
 
 	/* set CPU affinity */
@@ -130,25 +138,33 @@ eal_thread_loop(__attribute__((unused)) void *arg)
 
 		/* wait command */
 		do {
+			//阻塞等待master发送工作通知
 			n = read(m2s, &c, 1);
 		} while (n < 0 && errno == EINTR);
 
+		//与master之间无法通信，挂掉
 		if (n <= 0)
 			rte_panic("cannot read on configuration pipe\n");
 
+		//收到工作通知，更改自已为running状态
 		lcore_config[lcore_id].state = RUNNING;
 
 		/* send ack */
+		//知会master自已已计划开始工作
 		n = 0;
 		while (n == 0 || (n < 0 && errno == EINTR))
 			n = write(s2m, &c, 1);
+
+		//无法知会master，挂掉
 		if (n < 0)
 			rte_panic("cannot write on configuration pipe\n");
 
+		//master深深的伤害了我，挂掉，它一定会知道自已错了
 		if (lcore_config[lcore_id].f == NULL)
 			rte_panic("NULL function pointer\n");
 
 		/* call the function and store the return value */
+		//处理master交待的任务
 		fct_arg = lcore_config[lcore_id].arg;
 		ret = lcore_config[lcore_id].f(fct_arg);
 		lcore_config[lcore_id].ret = ret;
@@ -160,7 +176,9 @@ eal_thread_loop(__attribute__((unused)) void *arg)
 		if (lcore_config[lcore_id].core_role == ROLE_SERVICE)
 			lcore_config[lcore_id].state = WAIT;
 		else
+			//工作完成，置自已为finish状态
 			lcore_config[lcore_id].state = FINISHED;
+		//让我们愉快的进行下一轮玩耍吧！
 	}
 
 	/* never reached */
@@ -174,6 +192,7 @@ int rte_sys_gettid(void)
 	return (int)syscall(SYS_gettid);
 }
 
+//设置线程名称
 int rte_thread_setname(pthread_t id, const char *name)
 {
 	int ret = ENOSYS;
diff --git a/lib/librte_eal/linux/eal/eal_vfio.c b/lib/librte_eal/linux/eal/eal_vfio.c
index 28e10cdc4..65ed2999a 100644
--- a/lib/librte_eal/linux/eal/eal_vfio.c
+++ b/lib/librte_eal/linux/eal/eal_vfio.c
@@ -981,12 +981,14 @@ rte_vfio_enable(const char *modname)
 
 	/* return error directly */
 	if (vfio_available == -1) {
+		//检测失败
 		RTE_LOG(INFO, EAL, "Could not get loaded module details!\n");
 		return -1;
 	}
 
 	/* return 0 if VFIO modules not loaded */
 	if (vfio_available == 0) {
+		//模块未被加载
 		RTE_LOG(DEBUG, EAL, "VFIO modules not loaded, "
 			"skipping VFIO support...\n");
 		return 0;
diff --git a/lib/librte_eal/linux/eal/include/rte_kni_common.h b/lib/librte_eal/linux/eal/include/rte_kni_common.h
index 46f75a710..d06b3f35c 100644
--- a/lib/librte_eal/linux/eal/include/rte_kni_common.h
+++ b/lib/librte_eal/linux/eal/include/rte_kni_common.h
@@ -61,12 +61,12 @@ struct rte_kni_fifo {
 	unsigned write;              /**< Next position to be written*/
 	unsigned read;               /**< Next position to be read */
 #else
-	volatile unsigned write;     /**< Next position to be written*/
-	volatile unsigned read;      /**< Next position to be read */
+	volatile unsigned write;     /**< Next position to be written*/ //写起始位置
+	volatile unsigned read;      /**< Next position to be read */ //读起始位置
 #endif
-	unsigned len;                /**< Circular buffer length */
-	unsigned elem_size;          /**< Pointer size - for 32/64 bit OS */
-	void *volatile buffer[];     /**< The buffer contains mbuf pointers */
+	unsigned len;                /**< Circular buffer length */ //循环队列长度，总为2的整数次方
+	unsigned elem_size;          /**< Pointer size - for 32/64 bit OS */ //元素大小
+	void *volatile buffer[];     /**< The buffer contains mbuf pointers */ //队列
 };
 
 /*
diff --git a/lib/librte_ethdev/rte_ethdev.c b/lib/librte_ethdev/rte_ethdev.c
index 7743205d3..c291a8b7c 100644
--- a/lib/librte_ethdev/rte_ethdev.c
+++ b/lib/librte_ethdev/rte_ethdev.c
@@ -47,6 +47,7 @@
 int rte_eth_dev_logtype;
 
 static const char *MZ_RTE_ETH_DEV_DATA = "rte_eth_dev_data";
+//网络设备对象（用于管理按口编号等）
 struct rte_eth_dev rte_eth_devices[RTE_MAX_ETHPORTS];
 
 /* spinlock for eth device callbacks */
@@ -386,6 +387,7 @@ rte_eth_find_next_sibling(uint16_t port_id, uint16_t ref_port_id)
 			rte_eth_devices[ref_port_id].device);
 }
 
+//分配eth_dev_data空间
 static void
 rte_eth_dev_shared_data_prepare(void)
 {
@@ -395,6 +397,7 @@ rte_eth_dev_shared_data_prepare(void)
 	rte_spinlock_lock(&rte_eth_shared_data_lock);
 
 	if (rte_eth_dev_shared_data == NULL) {
+		//如果rte_eth_dev_shared_data未初始化，则初始化
 		if (rte_eal_process_type() == RTE_PROC_PRIMARY) {
 			/* Allocate port data and ownership shared memory. */
 			mz = rte_memzone_reserve(MZ_RTE_ETH_DEV_DATA,
@@ -424,6 +427,7 @@ is_allocated(const struct rte_eth_dev *ethdev)
 	return ethdev->data->name[0] != '\0';
 }
 
+//给定eth设备名称，获取其对应的ret_eth-devices结构体
 static struct rte_eth_dev *
 _rte_eth_dev_allocated(const char *name)
 {
@@ -437,6 +441,7 @@ _rte_eth_dev_allocated(const char *name)
 	return NULL;
 }
 
+//查找$name名称对应的eth_dev是否存在
 struct rte_eth_dev *
 rte_eth_dev_allocated(const char *name)
 {
@@ -453,6 +458,7 @@ rte_eth_dev_allocated(const char *name)
 	return ethdev;
 }
 
+//分配空闲的eth-dev
 static uint16_t
 rte_eth_dev_find_free_port(void)
 {
@@ -479,6 +485,7 @@ eth_dev_get(uint16_t port_id)
 	return eth_dev;
 }
 
+//创建名称为name的eth_dev
 struct rte_eth_dev *
 rte_eth_dev_allocate(const char *name)
 {
@@ -503,14 +510,17 @@ rte_eth_dev_allocate(const char *name)
 	rte_spinlock_lock(&rte_eth_dev_shared_data->ownership_lock);
 
 	if (_rte_eth_dev_allocated(name) != NULL) {
+		//如果name已有对应的eth-dev结构，则报错
 		RTE_ETHDEV_LOG(ERR,
 			"Ethernet device with name %s already allocated\n",
 			name);
 		goto unlock;
 	}
 
+	//分配空间的端口号（对应的eth-dev对象）
 	port_id = rte_eth_dev_find_free_port();
 	if (port_id == RTE_MAX_ETHPORTS) {
+		//无空闲节点可分配
 		RTE_ETHDEV_LOG(ERR,
 			"Reached maximum number of Ethernet ports\n");
 		goto unlock;
@@ -560,6 +570,7 @@ rte_eth_dev_attach_secondary(const char *name)
 	return eth_dev;
 }
 
+//释放eth_dev设备
 int
 rte_eth_dev_release_port(struct rte_eth_dev *eth_dev)
 {
@@ -569,6 +580,7 @@ rte_eth_dev_release_port(struct rte_eth_dev *eth_dev)
 	rte_eth_dev_shared_data_prepare();
 
 	if (eth_dev->state != RTE_ETH_DEV_UNUSED)
+	//触发ETH_EVENT_DESTROY事件
 		_rte_eth_dev_callback_process(eth_dev,
 				RTE_ETH_EVENT_DESTROY, NULL);
 
@@ -590,6 +602,7 @@ rte_eth_dev_release_port(struct rte_eth_dev *eth_dev)
 	return 0;
 }
 
+//检查给定的port是否存在
 int
 rte_eth_dev_is_valid_port(uint16_t port_id)
 {
@@ -609,12 +622,15 @@ rte_eth_is_valid_owner_id(uint64_t owner_id)
 	return 1;
 }
 
+//从port_id端口开始，查找第一个非RTE_ETH_DEV_ATTACHED,
+//非RTE_ETH_DEV_REMOVED(即未使用的port)，owner.id不等于ower_id的port
 uint64_t
 rte_eth_find_next_owned_by(uint16_t port_id, const uint64_t owner_id)
 {
 	port_id = rte_eth_find_next(port_id);
 	while (port_id < RTE_MAX_ETHPORTS &&
 			rte_eth_devices[port_id].data->owner.id != owner_id)
+		//计算识别的port数
 		port_id = rte_eth_find_next(port_id + 1);
 
 	return port_id;
@@ -758,6 +774,7 @@ rte_eth_dev_owner_get(const uint16_t port_id, struct rte_eth_dev_owner *owner)
 	return ret;
 }
 
+//获取dev属于那个numa-node
 int
 rte_eth_dev_socket_id(uint16_t port_id)
 {
@@ -772,12 +789,14 @@ rte_eth_dev_get_sec_ctx(uint16_t port_id)
 	return rte_eth_devices[port_id].security_ctx;
 }
 
+//获取系统共识别出多少个设备
 uint16_t
 rte_eth_dev_count(void)
 {
 	return rte_eth_dev_count_avail();
 }
 
+//获取系统中有效设备数
 uint16_t
 rte_eth_dev_count_avail(void)
 {
@@ -822,6 +841,7 @@ rte_eth_dev_get_name_by_port(uint16_t port_id, char *name)
 	return 0;
 }
 
+//通过名称查找port_id
 int
 rte_eth_dev_get_port_by_name(const char *name, uint16_t *port_id)
 {
@@ -1019,6 +1039,9 @@ rte_eth_dev_tx_queue_stop(uint16_t port_id, uint16_t tx_queue_id)
 
 }
 
+//配置有多少个tx队列（此函数仅初始化tx_queues,按nb_queues要求生成相应的存放ring指定的空间）
+//也就是说，如果有10个队列，在64位机器上，我们仅申请80字节，用于存入10个队列指针
+//另外函数也支持缩减发送队列，会调用tx_queue_release函数将tx_queues中多余的队列进行释放
 static int
 rte_eth_dev_tx_queue_config(struct rte_eth_dev *dev, uint16_t nb_queues)
 {
@@ -1027,24 +1050,33 @@ rte_eth_dev_tx_queue_config(struct rte_eth_dev *dev, uint16_t nb_queues)
 	unsigned i;
 
 	if (dev->data->tx_queues == NULL && nb_queues != 0) { /* first time configuration */
+		//首次配置，发队列还为空，为其申请内存，并初始化为0
 		dev->data->tx_queues = rte_zmalloc("ethdev->tx_queues",
 						   sizeof(dev->data->tx_queues[0]) * nb_queues,
 						   RTE_CACHE_LINE_SIZE);
 		if (dev->data->tx_queues == NULL) {
+			//申请内存失败，报错
 			dev->data->nb_tx_queues = 0;
 			return -(ENOMEM);
 		}
 	} else if (dev->data->tx_queues != NULL && nb_queues != 0) { /* re-configure */
+		//之前存在tx_queues,现在需要重新配置
+		//检查是否容许重新配置（tx_queue_release实现是否提供），如果不容许报错，否则重新配置
 		RTE_FUNC_PTR_OR_ERR_RET(*dev->dev_ops->tx_queue_release, -ENOTSUP);
 
 		txq = dev->data->tx_queues;
 
+		//如果原有队列比我们目前要求的队列多，则释放多余队列（调用tx_queue_release)
 		for (i = nb_queues; i < old_nb_queues; i++)
 			(*dev->dev_ops->tx_queue_release)(txq[i]);
+
+		//在原有位置上realloc(完成旧的数据的copy)
 		txq = rte_realloc(txq, sizeof(txq[0]) * nb_queues,
 				  RTE_CACHE_LINE_SIZE);
 		if (txq == NULL)
-			return -ENOMEM;
+			return -ENOMEM;//失败，一般此时是进行扩大
+
+		//如果当前要求的队列比原有队列要多，则初始化增加的队列
 		if (nb_queues > old_nb_queues) {
 			uint16_t new_qs = nb_queues - old_nb_queues;
 
@@ -1055,6 +1087,8 @@ rte_eth_dev_tx_queue_config(struct rte_eth_dev *dev, uint16_t nb_queues)
 		dev->data->tx_queues = txq;
 
 	} else if (dev->data->tx_queues != NULL && nb_queues == 0) {
+		//之前存在队列，现在要求队列数变为0，这种情况相当于所有队列被释放
+		//tx_queues被置为NULL
 		RTE_FUNC_PTR_OR_ERR_RET(*dev->dev_ops->tx_queue_release, -ENOTSUP);
 
 		txq = dev->data->tx_queues;
@@ -1065,6 +1099,7 @@ rte_eth_dev_tx_queue_config(struct rte_eth_dev *dev, uint16_t nb_queues)
 		rte_free(dev->data->tx_queues);
 		dev->data->tx_queues = NULL;
 	}
+	//指出当前发队列
 	dev->data->nb_tx_queues = nb_queues;
 	return 0;
 }
@@ -1102,6 +1137,7 @@ rte_eth_speed_bitflag(uint32_t speed, int duplex)
 	}
 }
 
+//接口配置
 const char *
 rte_eth_dev_rx_offload_name(uint64_t offload)
 {
@@ -1134,6 +1170,7 @@ rte_eth_dev_tx_offload_name(uint64_t offload)
 	return name;
 }
 
+//接口配置(dpdk初始化完成后，需要针对每个扫描出来的接口进行configure)
 int
 rte_eth_dev_configure(uint16_t port_id, uint16_t nb_rx_q, uint16_t nb_tx_q,
 		      const struct rte_eth_conf *dev_conf)
@@ -1213,6 +1250,7 @@ rte_eth_dev_configure(uint16_t port_id, uint16_t nb_rx_q, uint16_t nb_tx_q,
 		goto rollback;
 	}
 
+	//要配置的tx队列超过设备容许的数据，报错
 	if (nb_tx_q > dev_info.max_tx_queues) {
 		RTE_ETHDEV_LOG(ERR, "Ethdev port_id=%u nb_tx_queues=%u > %u\n",
 			port_id, nb_tx_q, dev_info.max_tx_queues);
@@ -1241,6 +1279,7 @@ rte_eth_dev_configure(uint16_t port_id, uint16_t nb_rx_q, uint16_t nb_tx_q,
 	 * length is supported by the configured device.
 	 */
 	if (dev_conf->rxmode.offloads & DEV_RX_OFFLOAD_JUMBO_FRAME) {
+		//巨帧被开启了，检查是否max_rx_pkt_len
 		if (dev_conf->rxmode.max_rx_pkt_len > dev_info.max_rx_pktlen) {
 			RTE_ETHDEV_LOG(ERR,
 				"Ethdev port_id=%u max_rx_pkt_len %u > max valid value %u\n",
@@ -1306,6 +1345,7 @@ rte_eth_dev_configure(uint16_t port_id, uint16_t nb_rx_q, uint16_t nb_tx_q,
 	/*
 	 * Setup new number of RX/TX queues and reconfigure device.
 	 */
+	//rx队列配置
 	diag = rte_eth_dev_rx_queue_config(dev, nb_rx_q);
 	if (diag != 0) {
 		RTE_ETHDEV_LOG(ERR,
@@ -1315,8 +1355,10 @@ rte_eth_dev_configure(uint16_t port_id, uint16_t nb_rx_q, uint16_t nb_tx_q,
 		goto rollback;
 	}
 
+	//tx队列配置（准备存放对列指针的空间）
 	diag = rte_eth_dev_tx_queue_config(dev, nb_tx_q);
 	if (diag != 0) {
+		//释放rx队列
 		RTE_ETHDEV_LOG(ERR,
 			"Port%u rte_eth_dev_tx_queue_config = %d\n",
 			port_id, diag);
@@ -1473,6 +1515,7 @@ rte_eth_dev_config_restore(struct rte_eth_dev *dev,
 	return 0;
 }
 
+//启动网卡（1。先配置网卡，2再配置tx队列，rx队列;3启动网卡）
 int
 rte_eth_dev_start(uint16_t port_id)
 {
@@ -1524,6 +1567,7 @@ rte_eth_dev_start(uint16_t port_id)
 	return 0;
 }
 
+//停止网卡
 void
 rte_eth_dev_stop(uint16_t port_id)
 {
@@ -1601,6 +1645,7 @@ rte_eth_dev_close(uint16_t port_id)
 	dev->data->tx_queues = NULL;
 }
 
+//启动收队列
 int
 rte_eth_dev_reset(uint16_t port_id)
 {
@@ -1641,6 +1686,7 @@ rte_eth_dev_is_removed(uint16_t port_id)
 	return ret;
 }
 
+//构建收队列
 int
 rte_eth_rx_queue_setup(uint16_t port_id, uint16_t rx_queue_id,
 		       uint16_t nb_rx_desc, unsigned int socket_id,
@@ -1779,6 +1825,7 @@ rte_eth_rx_queue_setup(uint16_t port_id, uint16_t rx_queue_id,
 	return eth_err(port_id, ret);
 }
 
+//构造发队列
 int
 rte_eth_tx_queue_setup(uint16_t port_id, uint16_t tx_queue_id,
 		       uint16_t nb_tx_desc, unsigned int socket_id,
@@ -1794,6 +1841,7 @@ rte_eth_tx_queue_setup(uint16_t port_id, uint16_t tx_queue_id,
 
 	dev = &rte_eth_devices[port_id];
 	if (tx_queue_id >= dev->data->nb_tx_queues) {
+		//给定的tx_queue_id大于queue总数，调用有误
 		RTE_ETHDEV_LOG(ERR, "Invalid TX queue_id=%u\n", tx_queue_id);
 		return -EINVAL;
 	}
@@ -1814,6 +1862,7 @@ rte_eth_tx_queue_setup(uint16_t port_id, uint16_t tx_queue_id,
 	if (nb_tx_desc > dev_info.tx_desc_lim.nb_max ||
 	    nb_tx_desc < dev_info.tx_desc_lim.nb_min ||
 	    nb_tx_desc % dev_info.tx_desc_lim.nb_align != 0) {
+		//tx描述符取值过大或者过小或者与要求的数量不对齐
 		RTE_ETHDEV_LOG(ERR,
 			"Invalid value for nb_tx_desc(=%hu), should be: <= %hu, >= %hu, and a product of %hu\n",
 			nb_tx_desc, dev_info.tx_desc_lim.nb_max,
@@ -1834,6 +1883,7 @@ rte_eth_tx_queue_setup(uint16_t port_id, uint16_t tx_queue_id,
 
 	txq = dev->data->tx_queues;
 	if (txq[tx_queue_id]) {
+		//要设置的txq已有值，需要将原队列释放掉
 		RTE_FUNC_PTR_OR_ERR_RET(*dev->dev_ops->tx_queue_release,
 					-ENOTSUP);
 		(*dev->dev_ops->tx_queue_release)(txq[tx_queue_id]);
@@ -1841,6 +1891,7 @@ rte_eth_tx_queue_setup(uint16_t port_id, uint16_t tx_queue_id,
 	}
 
 	if (tx_conf == NULL)
+		//如果未指定，采用设备的默认tx配置
 		tx_conf = &dev_info.default_txconf;
 
 	local_conf = *tx_conf;
@@ -1874,6 +1925,7 @@ rte_eth_tx_queue_setup(uint16_t port_id, uint16_t tx_queue_id,
 		return -EINVAL;
 	}
 
+	//创建tx队列tx_queue_id,队列的发送描述符nb_tx_desc,内存位置socket_id
 	return eth_err(port_id, (*dev->dev_ops->tx_queue_setup)(dev,
 		       tx_queue_id, nb_tx_desc, socket_id, &local_conf));
 }
@@ -1943,6 +1995,7 @@ rte_eth_tx_done_cleanup(uint16_t port_id, uint16_t queue_id, uint32_t free_cnt)
 	return eth_err(port_id, ret);
 }
 
+//将网卡设置为混杂模式
 int
 rte_eth_promiscuous_enable(uint16_t port_id)
 {
@@ -1996,6 +2049,7 @@ rte_eth_promiscuous_get(uint16_t port_id)
 	return dev->data->promiscuous;
 }
 
+//开启组播
 int
 rte_eth_allmulticast_enable(uint16_t port_id)
 {
@@ -2047,13 +2101,14 @@ rte_eth_allmulticast_get(uint16_t port_id)
 	return dev->data->all_multicast;
 }
 
+//取链路状态
 int
 rte_eth_link_get(uint16_t port_id, struct rte_eth_link *eth_link)
 {
 	struct rte_eth_dev *dev;
 
 	RTE_ETH_VALID_PORTID_OR_ERR_RET(port_id, -ENODEV);
-	dev = &rte_eth_devices[port_id];
+	dev = &rte_eth_devices[port_id];//取出对应的设备
 
 	if (dev->data->dev_conf.intr_conf.lsc &&
 	    dev->data->dev_started)
@@ -2067,11 +2122,13 @@ rte_eth_link_get(uint16_t port_id, struct rte_eth_link *eth_link)
 	return 0;
 }
 
+//返回接口的link状态
 int
 rte_eth_link_get_nowait(uint16_t port_id, struct rte_eth_link *eth_link)
 {
 	struct rte_eth_dev *dev;
 
+	//检查给定的port_id是否有效的接口，返回对应的dev
 	RTE_ETH_VALID_PORTID_OR_ERR_RET(port_id, -ENODEV);
 	dev = &rte_eth_devices[port_id];
 
@@ -2080,6 +2137,7 @@ rte_eth_link_get_nowait(uint16_t port_id, struct rte_eth_link *eth_link)
 		rte_eth_linkstatus_get(dev, eth_link);
 	else {
 		RTE_FUNC_PTR_OR_ERR_RET(*dev->dev_ops->link_update, -ENOTSUP);
+		//调用link_update后，获取link结果
 		(*dev->dev_ops->link_update)(dev, 0);
 		*eth_link = dev->data->dev_link;
 	}
@@ -2663,6 +2721,7 @@ rte_eth_dev_fw_version_get(uint16_t port_id, char *fw_version, size_t fw_size)
 							fw_version, fw_size));
 }
 
+//通过port编号获取取设备信息（获得：收描述符最大值，发措述符最大值）
 int
 rte_eth_dev_info_get(uint16_t port_id, struct rte_eth_dev_info *dev_info)
 {
@@ -2734,6 +2793,7 @@ rte_eth_dev_get_supported_ptypes(uint16_t port_id, uint32_t ptype_mask,
 	return j;
 }
 
+//给定port id，取此port对应的mac地址
 int
 rte_eth_macaddr_get(uint16_t port_id, struct rte_ether_addr *mac_addr)
 {
@@ -3596,6 +3656,7 @@ rte_eth_dev_callback_register(uint16_t port_id,
 			if (user_cb->cb_fn == cb_fn &&
 				user_cb->cb_arg == cb_arg &&
 				user_cb->event == event) {
+				//重复注册情况
 				break;
 			}
 		}
@@ -3608,6 +3669,7 @@ rte_eth_dev_callback_register(uint16_t port_id,
 				user_cb->cb_fn = cb_fn;
 				user_cb->cb_arg = cb_arg;
 				user_cb->event = event;
+				//添加入链表，完成注册
 				TAILQ_INSERT_TAIL(&(dev->link_intr_cbs),
 						  user_cb, next);
 			} else {
@@ -3624,6 +3686,7 @@ rte_eth_dev_callback_register(uint16_t port_id,
 	return 0;
 }
 
+//解注册
 int
 rte_eth_dev_callback_unregister(uint16_t port_id,
 			enum rte_eth_event_type event,
@@ -3669,9 +3732,11 @@ rte_eth_dev_callback_unregister(uint16_t port_id,
 			 * then remove it.
 			 */
 			if (cb->active == 0) {
+				//未执行回调，可安全删除
 				TAILQ_REMOVE(&(dev->link_intr_cbs), cb, next);
 				rte_free(cb);
 			} else {
+				//已计划执行回调，需要稍后重试
 				ret = -EAGAIN;
 			}
 		}
@@ -3693,12 +3758,14 @@ _rte_eth_dev_callback_process(struct rte_eth_dev *dev,
 	TAILQ_FOREACH(cb_lst, &(dev->link_intr_cbs), next) {
 		if (cb_lst->cb_fn == NULL || cb_lst->event != event)
 			continue;
+		//找到合乎的回调
 		dev_cb = *cb_lst;
-		cb_lst->active = 1;
+		cb_lst->active = 1;//标记开始执行回调
 		if (ret_param != NULL)
 			dev_cb.ret_param = ret_param;
 
 		rte_spinlock_unlock(&rte_eth_dev_cb_lock);
+		//解锁执行回调
 		rc = dev_cb.cb_fn(dev->data->port_id, dev_cb.event,
 				dev_cb.cb_arg, dev_cb.ret_param);
 		rte_spinlock_lock(&rte_eth_dev_cb_lock);
@@ -3714,6 +3781,7 @@ rte_eth_dev_probing_finish(struct rte_eth_dev *dev)
 	if (dev == NULL)
 		return;
 
+	//触发设备new事件
 	_rte_eth_dev_callback_process(dev, RTE_ETH_EVENT_NEW, NULL);
 
 	dev->state = RTE_ETH_DEV_ATTACHED;
@@ -4041,6 +4109,7 @@ rte_eth_add_rx_callback(uint16_t port_id, uint16_t queue_id,
 	return cb;
 }
 
+//注册收包回调fn到rte_eth_devices[port_id].post_rx_burst_cbs[queue_id]
 const struct rte_eth_rxtx_callback *
 rte_eth_add_first_rx_callback(uint16_t port_id, uint16_t queue_id,
 		rte_rx_callback_fn fn, void *user_param)
diff --git a/lib/librte_ethdev/rte_ethdev.h b/lib/librte_ethdev/rte_ethdev.h
index c36c1b631..631fbdfca 100644
--- a/lib/librte_ethdev/rte_ethdev.h
+++ b/lib/librte_ethdev/rte_ethdev.h
@@ -456,20 +456,20 @@ struct rte_eth_rss_conf {
  */
 #define RTE_ETH_FLOW_UNKNOWN             0
 #define RTE_ETH_FLOW_RAW                 1
-#define RTE_ETH_FLOW_IPV4                2
-#define RTE_ETH_FLOW_FRAG_IPV4           3
+#define RTE_ETH_FLOW_IPV4                2 //ipv4报文 (*)
+#define RTE_ETH_FLOW_FRAG_IPV4           3 //ipv4分片
 #define RTE_ETH_FLOW_NONFRAG_IPV4_TCP    4
 #define RTE_ETH_FLOW_NONFRAG_IPV4_UDP    5
 #define RTE_ETH_FLOW_NONFRAG_IPV4_SCTP   6
-#define RTE_ETH_FLOW_NONFRAG_IPV4_OTHER  7
-#define RTE_ETH_FLOW_IPV6                8
-#define RTE_ETH_FLOW_FRAG_IPV6           9
+#define RTE_ETH_FLOW_NONFRAG_IPV4_OTHER  7 //非分片其它4层协议
+#define RTE_ETH_FLOW_IPV6                8 //ipv6 (*)
+#define RTE_ETH_FLOW_FRAG_IPV6           9 //ipv6分片
 #define RTE_ETH_FLOW_NONFRAG_IPV6_TCP   10
 #define RTE_ETH_FLOW_NONFRAG_IPV6_UDP   11
 #define RTE_ETH_FLOW_NONFRAG_IPV6_SCTP  12
-#define RTE_ETH_FLOW_NONFRAG_IPV6_OTHER 13
+#define RTE_ETH_FLOW_NONFRAG_IPV6_OTHER 13 //非ipv6其它协议
 #define RTE_ETH_FLOW_L2_PAYLOAD         14
-#define RTE_ETH_FLOW_IPV6_EX            15
+#define RTE_ETH_FLOW_IPV6_EX            15 //ipv6扩展
 #define RTE_ETH_FLOW_IPV6_TCP_EX        16
 #define RTE_ETH_FLOW_IPV6_UDP_EX        17
 #define RTE_ETH_FLOW_PORT               18
@@ -1345,6 +1345,7 @@ struct rte_eth_dcb_info {
 #define RTE_ETH_ALL RTE_MAX_ETHPORTS
 
 /* Macros to check for valid port */
+//检查给定的port_id是否为有效的port
 #define RTE_ETH_VALID_PORTID_OR_ERR_RET(port_id, retval) do { \
 	if (!rte_eth_dev_is_valid_port(port_id)) { \
 		RTE_ETHDEV_LOG(ERR, "Invalid port_id=%u\n", port_id); \
@@ -1352,6 +1353,7 @@ struct rte_eth_dcb_info {
 	} \
 } while (0)
 
+//检查给定的port_id是否为有效的port,如果不是有效port，则退出
 #define RTE_ETH_VALID_PORTID_OR_RET(port_id) do { \
 	if (!rte_eth_dev_is_valid_port(port_id)) { \
 		RTE_ETHDEV_LOG(ERR, "Invalid port_id=%u\n", port_id); \
@@ -4242,6 +4244,7 @@ rte_eth_dev_get_sec_ctx(uint16_t port_id);
  *   of pointers to *rte_mbuf* structures effectively supplied to the
  *   *rx_pkts* array.
  */
+//自接口port_id的指定队列queue_id收包，收到的报文存在rx_pkts中，最多收取nb_pkts个包
 static inline uint16_t
 rte_eth_rx_burst(uint16_t port_id, uint16_t queue_id,
 		 struct rte_mbuf **rx_pkts, const uint16_t nb_pkts)
@@ -4258,10 +4261,12 @@ rte_eth_rx_burst(uint16_t port_id, uint16_t queue_id,
 		return 0;
 	}
 #endif
+	//自队列queue_id处收取nb_pkts个报文，报文存储在rx_pkts数组内
 	nb_rx = (*dev->rx_pkt_burst)(dev->data->rx_queues[queue_id],
 				     rx_pkts, nb_pkts);
 
 #ifdef RTE_ETHDEV_RXTX_CALLBACKS
+	//如果存在收包回调，则逐个调用收包回调（例如实现个capture包文，什么的）
 	if (unlikely(dev->post_rx_burst_cbs[queue_id] != NULL)) {
 		struct rte_eth_rxtx_callback *cb =
 				dev->post_rx_burst_cbs[queue_id];
@@ -4274,6 +4279,7 @@ rte_eth_rx_burst(uint16_t port_id, uint16_t queue_id,
 	}
 #endif
 
+	//返回收到的报文数
 	return nb_rx;
 }
 
@@ -4509,6 +4515,7 @@ static inline int rte_eth_tx_descriptor_status(uint16_t port_id,
  *   the transmit ring. The return value can be less than the value of the
  *   *tx_pkts* parameter when the transmit ring is full or has been filled up.
  */
+//dpdk发包函数
 static inline uint16_t
 rte_eth_tx_burst(uint16_t port_id, uint16_t queue_id,
 		 struct rte_mbuf **tx_pkts, uint16_t nb_pkts)
@@ -4537,6 +4544,7 @@ rte_eth_tx_burst(uint16_t port_id, uint16_t queue_id,
 	}
 #endif
 
+	//调用dev的发包函数对设备的指定队列$queue_id发送nb_pkts个包
 	return (*dev->tx_pkt_burst)(dev->data->tx_queues[queue_id], tx_pkts, nb_pkts);
 }
 
diff --git a/lib/librte_ethdev/rte_ethdev_core.h b/lib/librte_ethdev/rte_ethdev_core.h
index 392aea8e6..6f118fe3d 100644
--- a/lib/librte_ethdev/rte_ethdev_core.h
+++ b/lib/librte_ethdev/rte_ethdev_core.h
@@ -671,7 +671,9 @@ struct rte_eth_rxtx_callback {
  * process, while the actual configuration data for the device is shared.
  */
 struct rte_eth_dev {
+	//收报文函数
 	eth_rx_burst_t rx_pkt_burst; /**< Pointer to PMD receive function. */
+	//发报文函数
 	eth_tx_burst_t tx_pkt_burst; /**< Pointer to PMD transmit function. */
 	eth_tx_prep_t tx_pkt_prepare; /**< Pointer to PMD transmit prepare function. */
 	/**
@@ -681,6 +683,7 @@ struct rte_eth_dev {
 	 */
 	struct rte_eth_dev_data *data;  /**< Pointer to device data. */
 	void *process_private; /**< Pointer to per-process device data. */
+	//设备操作函数集
 	const struct eth_dev_ops *dev_ops; /**< Functions exported by PMD */
 	struct rte_device *device; /**< Backing device */
 	struct rte_intr_handle *intr_handle; /**< Device interrupt handle */
@@ -715,8 +718,8 @@ struct rte_eth_dev_data {
 
 	void **rx_queues; /**< Array of pointers to RX queues. */
 	void **tx_queues; /**< Array of pointers to TX queues. */
-	uint16_t nb_rx_queues; /**< Number of RX queues. */
-	uint16_t nb_tx_queues; /**< Number of TX queues. */
+	uint16_t nb_rx_queues; /**< Number of RX queues. */ //收队列数
+	uint16_t nb_tx_queues; /**< Number of TX queues. */ //发队列数
 
 	struct rte_eth_dev_sriov sriov;    /**< SRIOV data */
 
diff --git a/lib/librte_ethdev/rte_ethdev_pci.h b/lib/librte_ethdev/rte_ethdev_pci.h
index ccdbb46ec..9e85a314e 100644
--- a/lib/librte_ethdev/rte_ethdev_pci.h
+++ b/lib/librte_ethdev/rte_ethdev_pci.h
@@ -113,9 +113,11 @@ rte_eth_dev_pci_allocate(struct rte_pci_device *dev, size_t private_data_size)
 			return NULL;
 
 		if (private_data_size) {
+			//为dev申请其所需要私有数据
 			eth_dev->data->dev_private = rte_zmalloc_socket(name,
 				private_data_size, RTE_CACHE_LINE_SIZE,
 				dev->device.numa_node);
+			//申请失败，释放port
 			if (!eth_dev->data->dev_private) {
 				rte_eth_dev_release_port(eth_dev);
 				return NULL;
@@ -156,13 +158,15 @@ rte_eth_dev_pci_generic_probe(struct rte_pci_device *pci_dev,
 	struct rte_eth_dev *eth_dev;
 	int ret;
 
+	//分配eth_dev
 	eth_dev = rte_eth_dev_pci_allocate(pci_dev, private_data_size);
 	if (!eth_dev)
 		return -ENOMEM;
 
-	RTE_FUNC_PTR_OR_ERR_RET(*dev_init, -EINVAL);
-	ret = dev_init(eth_dev);
+	RTE_FUNC_PTR_OR_ERR_RET(*dev_init, -EINVAL);//dev_init参数检查
+	ret = dev_init(eth_dev);//初始化设备
 	if (ret)
+		//初始化失败，释放port
 		rte_eth_dev_pci_release(eth_dev);
 	else
 		rte_eth_dev_probing_finish(eth_dev);
diff --git a/lib/librte_ethdev/rte_ethdev_vdev.h b/lib/librte_ethdev/rte_ethdev_vdev.h
index 259feda3f..648e5e841 100644
--- a/lib/librte_ethdev/rte_ethdev_vdev.h
+++ b/lib/librte_ethdev/rte_ethdev_vdev.h
@@ -53,6 +53,7 @@
  * @return
  *	A pointer to a rte_eth_dev or NULL if allocation failed.
  */
+//为vdev申请eth_dev
 static inline struct rte_eth_dev *
 rte_eth_vdev_allocate(struct rte_vdev_device *dev, size_t private_data_size)
 {
@@ -63,10 +64,12 @@ rte_eth_vdev_allocate(struct rte_vdev_device *dev, size_t private_data_size)
 	if (!eth_dev)
 		return NULL;
 
+	//如果有私有数据，则为私有数据申请空间
 	if (private_data_size) {
 		eth_dev->data->dev_private = rte_zmalloc_socket(name,
 			private_data_size, RTE_CACHE_LINE_SIZE,
 			dev->device.numa_node);
+		//申请私有数据失败，释放eth_dev
 		if (!eth_dev->data->dev_private) {
 			rte_eth_dev_release_port(eth_dev);
 			return NULL;
@@ -76,7 +79,7 @@ rte_eth_vdev_allocate(struct rte_vdev_device *dev, size_t private_data_size)
 	eth_dev->device = &dev->device;
 	eth_dev->intr_handle = NULL;
 
-	eth_dev->data->kdrv = RTE_KDRV_NONE;
+	eth_dev->data->kdrv = RTE_KDRV_NONE;//未绑定驱动
 	eth_dev->data->numa_node = dev->device.numa_node;
 	return eth_dev;
 }
diff --git a/lib/librte_ethdev/rte_flow.c b/lib/librte_ethdev/rte_flow.c
index ca0f68016..e940289f1 100644
--- a/lib/librte_ethdev/rte_flow.c
+++ b/lib/librte_ethdev/rte_flow.c
@@ -16,6 +16,12 @@
 #include "rte_flow_driver.h"
 #include "rte_flow.h"
 
+//rte_flow_ops定义了一组函数，本接口用于实现这组函数与port_id的直接关联
+//校验网卡是否支持此条规则
+//下发一条规则
+//删除下发的一条规则
+//删除下发的所有规则
+//查询是否下发了某条规则
 /**
  * Flow elements description tables.
  */
@@ -172,6 +178,7 @@ flow_err(uint16_t port_id, int ret, struct rte_flow_error *error)
 }
 
 /* Get generic flow operations structure from a port. */
+//给定port-id，取此port-id上对应的flow操作符（通过dev_ops->filter_ctrl拿到相应函数)
 const struct rte_flow_ops *
 rte_flow_ops_get(uint16_t port_id, struct rte_flow_error *error)
 {
@@ -179,6 +186,7 @@ rte_flow_ops_get(uint16_t port_id, struct rte_flow_error *error)
 	const struct rte_flow_ops *ops;
 	int code;
 
+	//接口不存在时，报错
 	if (unlikely(!rte_eth_dev_is_valid_port(port_id)))
 		code = ENODEV;
 	else if (unlikely(!dev->dev_ops->filter_ctrl ||
@@ -187,9 +195,11 @@ rte_flow_ops_get(uint16_t port_id, struct rte_flow_error *error)
 						    RTE_ETH_FILTER_GET,
 						    &ops) ||
 			  !ops))
+		//如果无filter_ctrl回调，或者filter_ctrl回调无法获得ops，则报错
 		code = ENOSYS;
 	else
 		return ops;
+	//返回错误
 	rte_flow_error_set(error, code, RTE_FLOW_ERROR_TYPE_UNSPECIFIED,
 			   NULL, rte_strerror(code));
 	return NULL;
@@ -203,6 +213,7 @@ rte_flow_validate(uint16_t port_id,
 		  const struct rte_flow_action actions[],
 		  struct rte_flow_error *error)
 {
+	//获取到flow_ops的操作集，然后用使用操作集中的validate进行校验
 	const struct rte_flow_ops *ops = rte_flow_ops_get(port_id, error);
 	struct rte_eth_dev *dev = &rte_eth_devices[port_id];
 
@@ -217,25 +228,31 @@ rte_flow_validate(uint16_t port_id,
 }
 
 /* Create a flow rule on a given port. */
+//创建一条流规则
 struct rte_flow *
-rte_flow_create(uint16_t port_id,
-		const struct rte_flow_attr *attr,
-		const struct rte_flow_item pattern[],
-		const struct rte_flow_action actions[],
-		struct rte_flow_error *error)
+rte_flow_create(uint16_t port_id,//作用于哪个接口
+		const struct rte_flow_attr *attr,//定义流规则属性
+		const struct rte_flow_item pattern[],//流规则匹配项
+		const struct rte_flow_action actions[],//流匹配后执行哪种action
+		struct rte_flow_error *error)//下发出错处理
 {
 	struct rte_eth_dev *dev = &rte_eth_devices[port_id];
 	struct rte_flow *flow;
+	//取设备对应的flow_ops
 	const struct rte_flow_ops *ops = rte_flow_ops_get(port_id, error);
 
 	if (unlikely(!ops))
 		return NULL;
+
+	//执行规则创建
 	if (likely(!!ops->create)) {
 		flow = ops->create(dev, attr, pattern, actions, error);
 		if (flow == NULL)
 			flow_err(port_id, -rte_errno, error);
 		return flow;
 	}
+
+	//如果创建失败或者无create接口，则返回失败详情
 	rte_flow_error_set(error, ENOSYS, RTE_FLOW_ERROR_TYPE_UNSPECIFIED,
 			   NULL, rte_strerror(ENOSYS));
 	return NULL;
@@ -252,6 +269,8 @@ rte_flow_destroy(uint16_t port_id,
 
 	if (unlikely(!ops))
 		return -rte_errno;
+
+	//规则移除
 	if (likely(!!ops->destroy))
 		return flow_err(port_id, ops->destroy(dev, flow, error),
 				error);
@@ -270,6 +289,7 @@ rte_flow_flush(uint16_t port_id,
 
 	if (unlikely(!ops))
 		return -rte_errno;
+	//移除掉所有flow
 	if (likely(!!ops->flush))
 		return flow_err(port_id, ops->flush(dev, error), error);
 	return rte_flow_error_set(error, ENOSYS,
@@ -304,12 +324,14 @@ rte_flow_isolate(uint16_t port_id,
 		 int set,
 		 struct rte_flow_error *error)
 {
+	//取对应netdev的flow ops操作
 	struct rte_eth_dev *dev = &rte_eth_devices[port_id];
 	const struct rte_flow_ops *ops = rte_flow_ops_get(port_id, error);
 
 	if (!ops)
 		return -rte_errno;
 	if (likely(!!ops->isolate))
+		//调用对应的isolate回调
 		return flow_err(port_id, ops->isolate(dev, set, error), error);
 	return rte_flow_error_set(error, ENOSYS,
 				  RTE_FLOW_ERROR_TYPE_UNSPECIFIED,
diff --git a/lib/librte_ethdev/rte_flow.h b/lib/librte_ethdev/rte_flow.h
index 4fee10559..798997a17 100644
--- a/lib/librte_ethdev/rte_flow.h
+++ b/lib/librte_ethdev/rte_flow.h
@@ -70,9 +70,11 @@ extern "C" {
  * Specifying both directions at once for a given rule is not recommended
  * but may be valid in a few cases (e.g. shared counter).
  */
+//流规则属性
 struct rte_flow_attr {
 	uint32_t group; /**< Priority group. */
 	uint32_t priority; /**< Rule priority level within group. */
+	//可以同时两个方向生效
 	uint32_t ingress:1; /**< Rule applies to ingress traffic. */
 	uint32_t egress:1; /**< Rule applies to egress traffic. */
 	/**
@@ -113,6 +115,7 @@ struct rte_flow_attr {
  * See the description of individual types for more information. Those
  * marked with [META] fall into the second category.
  */
+//匹配模式
 enum rte_flow_item_type {
 	/**
 	 * [META]
@@ -122,7 +125,7 @@ enum rte_flow_item_type {
 	 *
 	 * No associated specification structure.
 	 */
-	RTE_FLOW_ITEM_TYPE_END,
+	RTE_FLOW_ITEM_TYPE_END,//标明flow-item结束
 
 	/**
 	 * [META]
@@ -132,7 +135,7 @@ enum rte_flow_item_type {
 	 *
 	 * No associated specification structure.
 	 */
-	RTE_FLOW_ITEM_TYPE_VOID,
+	RTE_FLOW_ITEM_TYPE_VOID,//占位用
 
 	/**
 	 * [META]
@@ -142,7 +145,7 @@ enum rte_flow_item_type {
 	 *
 	 * No associated specification structure.
 	 */
-	RTE_FLOW_ITEM_TYPE_INVERT,
+	RTE_FLOW_ITEM_TYPE_INVERT,//反向匹配，例如处理未命中的报文
 
 	/**
 	 * Matches any protocol in place of the current layer, a single ANY
@@ -150,7 +153,7 @@ enum rte_flow_item_type {
 	 *
 	 * See struct rte_flow_item_any.
 	 */
-	RTE_FLOW_ITEM_TYPE_ANY,
+	RTE_FLOW_ITEM_TYPE_ANY,//全匹配（相当于通配符*)
 
 	/**
 	 * [META]
@@ -160,7 +163,7 @@ enum rte_flow_item_type {
 	 *
 	 * No associated specification structure.
 	 */
-	RTE_FLOW_ITEM_TYPE_PF,
+	RTE_FLOW_ITEM_TYPE_PF,//区配输入输出至pf
 
 	/**
 	 * [META]
@@ -170,7 +173,7 @@ enum rte_flow_item_type {
 	 *
 	 * See struct rte_flow_item_vf.
 	 */
-	RTE_FLOW_ITEM_TYPE_VF,
+	RTE_FLOW_ITEM_TYPE_VF,//匹配输入输出至vf
 
 	/**
 	 * [META]
@@ -190,7 +193,7 @@ enum rte_flow_item_type {
 	 *
 	 * See struct rte_flow_item_port_id.
 	 */
-	RTE_FLOW_ITEM_TYPE_PORT_ID,
+	RTE_FLOW_ITEM_TYPE_PORT_ID,//匹配in_port
 
 	/**
 	 * Matches a byte string of a given length at a given offset.
@@ -204,7 +207,7 @@ enum rte_flow_item_type {
 	 *
 	 * See struct rte_flow_item_eth.
 	 */
-	RTE_FLOW_ITEM_TYPE_ETH,
+	RTE_FLOW_ITEM_TYPE_ETH,//匹配以太头
 
 	/**
 	 * Matches an 802.1Q/ad VLAN tag.
@@ -648,6 +651,7 @@ static const struct rte_flow_item_port_id rte_flow_item_port_id_mask = {
  *
  * This type does not support ranges (struct rte_flow_item.last).
  */
+//比较原始的匹配方式，将报文看做是字节流，(offset,length,data)
 struct rte_flow_item_raw {
 	uint32_t relative:1; /**< Look for pattern after the previous item. */
 	uint32_t search:1; /**< Search pattern from offset (see also limit). */
@@ -682,6 +686,7 @@ static const struct rte_flow_item_raw rte_flow_item_raw_mask = {
  * inner EtherType/TPID provided by the subsequent pattern item. This is the
  * same order as on the wire.
  */
+//以太头匹配参数
 struct rte_flow_item_eth {
 	struct rte_ether_addr dst; /**< Destination MAC. */
 	struct rte_ether_addr src; /**< Source MAC. */
@@ -1471,8 +1476,10 @@ static const struct rte_flow_item_ah rte_flow_item_ah_mask = {
  */
 struct rte_flow_item {
 	enum rte_flow_item_type type; /**< Item type. */
+	//指向type类型对应的值，例如匹配以太头时，其对应的为struct rte_flow_item_eth结构体
 	const void *spec; /**< Pointer to item specification structure. */
 	const void *last; /**< Defines an inclusive range (spec to last). */
+	//指向type类型对应的掩码，例如匹配以太头时，其对应的是struct rte_flow_item_eth结构体
 	const void *mask; /**< Bit-mask applied to spec and last. */
 };
 
@@ -1509,7 +1516,7 @@ enum rte_flow_action_type {
 	 *
 	 * No associated configuration structure.
 	 */
-	RTE_FLOW_ACTION_TYPE_END,
+	RTE_FLOW_ACTION_TYPE_END,//action处理结束
 
 	/**
 	 * Used as a placeholder for convenience. It is ignored and simply
@@ -1517,7 +1524,7 @@ enum rte_flow_action_type {
 	 *
 	 * No associated configuration structure.
 	 */
-	RTE_FLOW_ACTION_TYPE_VOID,
+	RTE_FLOW_ACTION_TYPE_VOID,//action占位
 
 	/**
 	 * Leaves traffic up for additional processing by subsequent flow
@@ -1542,7 +1549,7 @@ enum rte_flow_action_type {
 	 *
 	 * See struct rte_flow_action_mark.
 	 */
-	RTE_FLOW_ACTION_TYPE_MARK,
+	RTE_FLOW_ACTION_TYPE_MARK,//在报文上附加一个interger值。
 
 	/**
 	 * Flags packets. Similar to MARK without a specific value; only
@@ -1550,14 +1557,14 @@ enum rte_flow_action_type {
 	 *
 	 * No associated configuration structure.
 	 */
-	RTE_FLOW_ACTION_TYPE_FLAG,
+	RTE_FLOW_ACTION_TYPE_FLAG,//设置报文的标记位
 
 	/**
 	 * Assigns packets to a given queue index.
 	 *
 	 * See struct rte_flow_action_queue.
 	 */
-	RTE_FLOW_ACTION_TYPE_QUEUE,
+	RTE_FLOW_ACTION_TYPE_QUEUE,//设置报文定向到某个队列
 
 	/**
 	 * Drops packets.
@@ -1566,7 +1573,7 @@ enum rte_flow_action_type {
 	 *
 	 * No associated configuration structure.
 	 */
-	RTE_FLOW_ACTION_TYPE_DROP,
+	RTE_FLOW_ACTION_TYPE_DROP,//丢包
 
 	/**
 	 * Enables counters for this flow rule.
@@ -1576,7 +1583,7 @@ enum rte_flow_action_type {
 	 *
 	 * See struct rte_flow_action_count.
 	 */
-	RTE_FLOW_ACTION_TYPE_COUNT,
+	RTE_FLOW_ACTION_TYPE_COUNT,//对此规则进行计数
 
 	/**
 	 * Similar to QUEUE, except RSS is additionally performed on packets
@@ -1593,7 +1600,7 @@ enum rte_flow_action_type {
 	 *
 	 * No associated configuration structure.
 	 */
-	RTE_FLOW_ACTION_TYPE_PF,
+	RTE_FLOW_ACTION_TYPE_PF,//输出到pf
 
 	/**
 	 * Directs matching traffic to a given virtual function of the
@@ -1601,7 +1608,7 @@ enum rte_flow_action_type {
 	 *
 	 * See struct rte_flow_action_vf.
 	 */
-	RTE_FLOW_ACTION_TYPE_VF,
+	RTE_FLOW_ACTION_TYPE_VF,//输出到vf
 
 	/**
 	 * Directs packets to a given physical port index of the underlying
@@ -2437,7 +2444,9 @@ struct rte_flow_action_set_mac {
  * For simple actions without a configuration object, conf remains NULL.
  */
 struct rte_flow_action {
+	//action类型
 	enum rte_flow_action_type type; /**< Action type. */
+	//指向action的配置信息（由各action规定相应的配置结构体）
 	const void *conf; /**< Pointer to action configuration object. */
 };
 
@@ -2740,6 +2749,7 @@ rte_flow_validate(uint16_t port_id,
  *   to the positive version of one of the error codes defined for
  *   rte_flow_validate().
  */
+//创建规则
 struct rte_flow *
 rte_flow_create(uint16_t port_id,
 		const struct rte_flow_attr *attr,
diff --git a/lib/librte_ethdev/rte_flow_driver.h b/lib/librte_ethdev/rte_flow_driver.h
index a0359853e..b0d5f2adc 100644
--- a/lib/librte_ethdev/rte_flow_driver.h
+++ b/lib/librte_ethdev/rte_flow_driver.h
@@ -60,6 +60,7 @@ extern "C" {
  * callbacks otherwise only differ by their first argument (with port ID
  * already resolved to a pointer to struct rte_eth_dev).
  */
+//flow操作操作回调，由各卡商驱动层提供
 struct rte_flow_ops {
 	/** See rte_flow_validate(). */
 	int (*validate)
@@ -69,6 +70,7 @@ struct rte_flow_ops {
 		 const struct rte_flow_action [],
 		 struct rte_flow_error *);
 	/** See rte_flow_create(). */
+	//驱动完成flow规则创建
 	struct rte_flow *(*create)
 		(struct rte_eth_dev *,
 		 const struct rte_flow_attr *,
@@ -76,11 +78,13 @@ struct rte_flow_ops {
 		 const struct rte_flow_action [],
 		 struct rte_flow_error *);
 	/** See rte_flow_destroy(). */
+	//驱动完成flow规则的销毁
 	int (*destroy)
 		(struct rte_eth_dev *,
 		 struct rte_flow *,
 		 struct rte_flow_error *);
 	/** See rte_flow_flush(). */
+	//驱动完成所有flow移除
 	int (*flush)
 		(struct rte_eth_dev *,
 		 struct rte_flow_error *);
diff --git a/lib/librte_gro/gro_tcp4.c b/lib/librte_gro/gro_tcp4.c
index feb585514..f433b8196 100644
--- a/lib/librte_gro/gro_tcp4.c
+++ b/lib/librte_gro/gro_tcp4.c
@@ -270,6 +270,7 @@ gro_tcp4_reassemble(struct rte_mbuf *pkt,
 	 * packet into the flow.
 	 */
 	if (find == 0) {
+		//没有找到，缓存新的item
 		item_idx = insert_new_item(tbl, pkt, start_time,
 				INVALID_ARRAY_INDEX, sent_seq, ip_id,
 				is_atomic);
diff --git a/lib/librte_gro/rte_gro.c b/lib/librte_gro/rte_gro.c
index 6618f4d32..346e70650 100644
--- a/lib/librte_gro/rte_gro.c
+++ b/lib/librte_gro/rte_gro.c
@@ -185,6 +185,7 @@ rte_gro_reassemble_burst(struct rte_mbuf **pkts,
 				unprocess_pkts[unprocess_num++] = pkts[i];
 		} else if (IS_IPV4_TCP_PKT(pkts[i]->packet_type) &&
 				do_tcp4_gro) {
+			//tcp重组
 			ret = gro_tcp4_reassemble(pkts[i], &tcp_tbl, 0);
 			if (ret > 0)
 				/* merge successfully */
diff --git a/lib/librte_hash/rte_cuckoo_hash.c b/lib/librte_hash/rte_cuckoo_hash.c
index 87a4c01f2..e9156ea0e 100644
--- a/lib/librte_hash/rte_cuckoo_hash.c
+++ b/lib/librte_hash/rte_cuckoo_hash.c
@@ -121,6 +121,7 @@ get_alt_bucket_index(const struct rte_hash *h,
 	return (cur_bkt_idx ^ sig) & h->bucket_bitmask;
 }
 
+//创建hash表
 struct rte_hash *
 rte_hash_create(const struct rte_hash_parameters *params)
 {
@@ -644,6 +645,7 @@ search_and_update(const struct rte_hash *h, void *data, const void *key,
 
 	for (i = 0; i < RTE_HASH_BUCKET_ENTRIES; i++) {
 		if (bkt->sig_current[i] == sig) {
+			//取出key
 			k = (struct rte_hash_key *) ((char *)keys +
 					bkt->key_idx[i] * h->key_entry_size);
 			if (rte_hash_cmp_eq(key, k->key, h) == 0) {
@@ -690,11 +692,13 @@ rte_hash_cuckoo_insert_mw(const struct rte_hash *h,
 	 */
 	ret = search_and_update(h, data, key, prim_bkt, sig);
 	if (ret != -1) {
+		//已存在，更新
 		__hash_rw_writer_unlock(h);
 		*ret_val = ret;
 		return 1;
 	}
 
+	//防止其它cpu添加进从桶
 	FOR_EACH_BUCKET(cur_bkt, sec_bkt) {
 		ret = search_and_update(h, data, key, cur_bkt, sig);
 		if (ret != -1) {
@@ -707,6 +711,7 @@ rte_hash_cuckoo_insert_mw(const struct rte_hash *h,
 	/* Insert new entry if there is room in the primary
 	 * bucket.
 	 */
+	//自主桶中找出一个空闲的key_idx位置
 	for (i = 0; i < RTE_HASH_BUCKET_ENTRIES; i++) {
 		/* Check if slot is available */
 		if (likely(prim_bkt->key_idx[i] == EMPTY_SLOT)) {
@@ -724,6 +729,7 @@ rte_hash_cuckoo_insert_mw(const struct rte_hash *h,
 	}
 	__hash_rw_writer_unlock(h);
 
+	//检查是否有主桶中找到了合适的位置
 	if (i != RTE_HASH_BUCKET_ENTRIES)
 		return 0;
 
@@ -770,6 +776,7 @@ rte_hash_cuckoo_move_insert_mw(const struct rte_hash *h,
 		return 1;
 	}
 
+	//防其它cpu,将其加入到从
 	FOR_EACH_BUCKET(cur_bkt, alt_bkt) {
 		ret = search_and_update(h, data, key, cur_bkt, sig);
 		if (ret != -1) {
@@ -779,6 +786,7 @@ rte_hash_cuckoo_move_insert_mw(const struct rte_hash *h,
 		}
 	}
 
+	//把堆放在队列里的那些元素，按出堆顺序，加入到应的slot里
 	while (likely(curr_node->prev != NULL)) {
 		prev_node = curr_node->prev;
 		prev_bkt = prev_node->bkt;
@@ -864,10 +872,10 @@ rte_hash_cuckoo_move_insert_mw(const struct rte_hash *h,
  */
 static inline int
 rte_hash_cuckoo_make_space_mw(const struct rte_hash *h,
-			struct rte_hash_bucket *bkt,
-			struct rte_hash_bucket *sec_bkt,
+			struct rte_hash_bucket *bkt/*主桶*/,
+			struct rte_hash_bucket *sec_bkt/*从桶*/,
 			const struct rte_hash_key *key, void *data,
-			uint16_t sig, uint32_t bucket_idx,
+			uint16_t sig, uint32_t bucket_idx/*主桶index*/,
 			uint32_t new_idx, int32_t *ret_val)
 {
 	unsigned int i;
@@ -876,10 +884,12 @@ rte_hash_cuckoo_make_space_mw(const struct rte_hash *h,
 	struct rte_hash_bucket *curr_bkt, *alt_bkt;
 	uint32_t cur_idx, alt_idx;
 
+	/*队尾*/
 	tail = queue;
+	/*队头*/
 	head = queue + 1;
 	tail->bkt = bkt;
-	tail->prev = NULL;
+	tail->prev = NULL;/*尾部没有更多备选*/
 	tail->prev_slot = -1;
 	tail->cur_bkt_idx = bucket_idx;
 
@@ -891,11 +901,13 @@ rte_hash_cuckoo_make_space_mw(const struct rte_hash *h,
 		cur_idx = tail->cur_bkt_idx;
 		for (i = 0; i < RTE_HASH_BUCKET_ENTRIES; i++) {
 			if (curr_bkt->key_idx[i] == EMPTY_SLOT) {
+				//当前桶上的key_idx为空，可以insert到此位置
 				int32_t ret = rte_hash_cuckoo_move_insert_mw(h,
 						bkt, sec_bkt, key, data,
 						tail, i, sig,
 						new_idx, ret_val);
 				if (likely(ret != -1))
+					/*添加成功*/
 					return ret;
 			}
 
@@ -917,7 +929,7 @@ rte_hash_cuckoo_make_space_mw(const struct rte_hash *h,
 
 static inline int32_t
 __rte_hash_add_key_with_hash(const struct rte_hash *h, const void *key,
-						hash_sig_t sig, void *data)
+						hash_sig_t sig/*指代hashcode*/, void *data)
 {
 	uint16_t short_sig;
 	uint32_t prim_bucket_idx, sec_bucket_idx;
@@ -934,9 +946,13 @@ __rte_hash_add_key_with_hash(const struct rte_hash *h, const void *key,
 	int32_t ret_val;
 	struct rte_hash_bucket *last;
 
+	//第二个hash
 	short_sig = get_short_sig(sig);
+	//对应的桶index
 	prim_bucket_idx = get_prim_bucket_index(h, sig);
+	//第二个桶的index
 	sec_bucket_idx = get_alt_bucket_index(h, prim_bucket_idx, short_sig);
+	//取主备两个桶头指针
 	prim_bkt = &h->buckets[prim_bucket_idx];
 	sec_bkt = &h->buckets[sec_bucket_idx];
 	rte_prefetch0(prim_bkt);
@@ -944,13 +960,16 @@ __rte_hash_add_key_with_hash(const struct rte_hash *h, const void *key,
 
 	/* Check if key is already inserted in primary location */
 	__hash_rw_writer_lock(h);
-	ret = search_and_update(h, data, key, prim_bkt, short_sig);
+	//先查有没有，如果有就更新
+	ret = search_and_update(h, data, key, prim_bkt/*主桶*/, short_sig);
 	if (ret != -1) {
+		//查找，并已更新，直接返回
 		__hash_rw_writer_unlock(h);
 		return ret;
 	}
 
 	/* Check if key is already inserted in secondary location */
+	//查询从索引
 	FOR_EACH_BUCKET(cur_bkt, sec_bkt) {
 		ret = search_and_update(h, data, key, cur_bkt, short_sig);
 		if (ret != -1) {
@@ -967,6 +986,7 @@ __rte_hash_add_key_with_hash(const struct rte_hash *h, const void *key,
 		cached_free_slots = &h->local_free_slots[lcore_id];
 		/* Try to get a free slot from the local cache */
 		if (cached_free_slots->len == 0) {
+			//如果不存在空闲的slots,就自h->free_slots中出一组objs出来
 			/* Need to get another burst of free slots from global ring */
 			n_slots = rte_ring_mc_dequeue_burst(h->free_slots,
 					cached_free_slots->objs,
@@ -982,11 +1002,13 @@ __rte_hash_add_key_with_hash(const struct rte_hash *h, const void *key,
 		cached_free_slots->len--;
 		slot_id = cached_free_slots->objs[cached_free_slots->len];
 	} else {
+		//有，直接出队分配
 		if (rte_ring_sc_dequeue(h->free_slots, &slot_id) != 0) {
 			return -ENOSPC;
 		}
 	}
 
+	//分配出key内存，并填充key,data到new_k
 	new_k = RTE_PTR_ADD(keys, (uintptr_t)slot_id * h->key_entry_size);
 	new_idx = (uint32_t)((uintptr_t) slot_id);
 	/* The store to application data (by the application) at *data should
@@ -1001,39 +1023,46 @@ __rte_hash_add_key_with_hash(const struct rte_hash *h, const void *key,
 
 	/* Find an empty slot and insert */
 	ret = rte_hash_cuckoo_insert_mw(h, prim_bkt, sec_bkt, key, data,
-					short_sig, new_idx, &ret_val);
+					short_sig, new_idx/*找到的空slot id*/, &ret_val);
 	if (ret == 0)
+		//当前cpu已成功insert,返回
 		return new_idx - 1;
 	else if (ret == 1) {
+		//其它cpu已添加，退还申请的slot，并返回
 		enqueue_slot_back(h, cached_free_slots, slot_id);
 		return ret_val;
 	}
 
 	/* Primary bucket full, need to make space for new entry */
+	//通过搬家，给key找一个主桶insert进去
 	ret = rte_hash_cuckoo_make_space_mw(h, prim_bkt, sec_bkt, key, data,
-				short_sig, prim_bucket_idx, new_idx, &ret_val);
+				short_sig, prim_bucket_idx/*主桶的index*/, new_idx/*空闲的slot id*/, &ret_val);
 	if (ret == 0)
 		return new_idx - 1;
 	else if (ret == 1) {
+		/*其它cpu已帮助加入，返回*/
 		enqueue_slot_back(h, cached_free_slots, slot_id);
 		return ret_val;
 	}
 
+	//搬家也解决不了，在从上尝试搬家
 	/* Also search secondary bucket to get better occupancy */
-	ret = rte_hash_cuckoo_make_space_mw(h, sec_bkt, prim_bkt, key, data,
+	ret = rte_hash_cuckoo_make_space_mw(h, sec_bkt/*从桶*/, prim_bkt, key, data,
 				short_sig, sec_bucket_idx, new_idx, &ret_val);
 
 	if (ret == 0)
 		return new_idx - 1;
 	else if (ret == 1) {
+		//其它cpu已帮助加入，返回
 		enqueue_slot_back(h, cached_free_slots, slot_id);
 		return ret_val;
 	}
 
+	//从通过搬家也加不进去，尝试扩展表
 	/* if ext table not enabled, we failed the insertion */
 	if (!h->ext_table_support) {
 		enqueue_slot_back(h, cached_free_slots, slot_id);
-		return ret;
+		return ret;//返回-1
 	}
 
 	/* Now we need to go through the extendable bucket. Protection is needed
@@ -1056,6 +1085,7 @@ __rte_hash_add_key_with_hash(const struct rte_hash *h, const void *key,
 	}
 
 	/* Search sec and ext buckets to find an empty entry to insert. */
+	//防止此期间有删除操作，检查下是否有空闲slot
 	FOR_EACH_BUCKET(cur_bkt, sec_bkt) {
 		for (i = 0; i < RTE_HASH_BUCKET_ENTRIES; i++) {
 			/* Check if slot is available */
@@ -1078,6 +1108,7 @@ __rte_hash_add_key_with_hash(const struct rte_hash *h, const void *key,
 	/* Failed to get an empty entry from extendable buckets. Link a new
 	 * extendable bucket. We first get a free bucket from ring.
 	 */
+	//终于可以从ext buckets中出一个，并将其加入
 	if (rte_ring_sc_dequeue(h->free_ext_bkts, &ext_bkt_id) != 0) {
 		ret = -ENOSPC;
 		goto failure;
@@ -1342,7 +1373,9 @@ rte_hash_lookup_with_hash(const struct rte_hash *h,
 int32_t
 rte_hash_lookup(const struct rte_hash *h, const void *key)
 {
+	//不容许key＝＝NULL，H＝＝NULL
 	RETURN_IF_TRUE(((h == NULL) || (key == NULL)), -EINVAL);
+	//给出key,给出hashcode查表h
 	return __rte_hash_lookup_with_hash(h, key, rte_hash_hash(h, key), NULL);
 }
 
diff --git a/lib/librte_hash/rte_cuckoo_hash.h b/lib/librte_hash/rte_cuckoo_hash.h
index fb19bb27d..4e22ca435 100644
--- a/lib/librte_hash/rte_cuckoo_hash.h
+++ b/lib/librte_hash/rte_cuckoo_hash.h
@@ -149,6 +149,7 @@ enum rte_hash_sig_compare_function {
 struct rte_hash_bucket {
 	uint16_t sig_current[RTE_HASH_BUCKET_ENTRIES];
 
+	//每个key的针对key_store的offset
 	uint32_t key_idx[RTE_HASH_BUCKET_ENTRIES];
 
 	uint8_t flag[RTE_HASH_BUCKET_ENTRIES];
@@ -191,6 +192,7 @@ struct rte_hash {
 	/**< If read-write concurrency lock free support is enabled */
 	uint8_t writer_takes_lock;
 	/**< Indicates if the writer threads need to take lock */
+	//hash函数
 	rte_hash_function hash_func;    /**< Function used to calculate hash. */
 	uint32_t hash_func_init_val;    /**< Init value used by hash_func. */
 	rte_hash_cmp_eq_t rte_hash_custom_cmp_eq;
diff --git a/lib/librte_ip_frag/ip_frag_common.h b/lib/librte_ip_frag/ip_frag_common.h
index a17a74076..4cd426c2a 100644
--- a/lib/librte_ip_frag/ip_frag_common.h
+++ b/lib/librte_ip_frag/ip_frag_common.h
@@ -55,6 +55,7 @@ struct rte_mbuf *ipv6_frag_reassemble(struct ip_frag_pkt *fp);
  */
 
 /* check if key is empty */
+//检查key是否为空
 static inline int
 ip_frag_key_is_empty(const struct ip_frag_key * key)
 {
@@ -74,9 +75,9 @@ ip_frag_key_cmp(const struct ip_frag_key * k1, const struct ip_frag_key * k2)
 {
 	uint32_t i;
 	uint64_t val;
-	val = k1->id_key_len ^ k2->id_key_len;
+	val = k1->id_key_len ^ k2->id_key_len;//key长度相等
 	for (i = 0; i < k1->key_len; i++)
-		val |= k1->src_dst[i] ^ k2->src_dst[i];
+		val |= k1->src_dst[i] ^ k2->src_dst[i];//且key长度内所有值相等时，认为匹配
 	return val;
 }
 
diff --git a/lib/librte_ip_frag/ip_frag_internal.c b/lib/librte_ip_frag/ip_frag_internal.c
index 97470a872..3501b05c6 100644
--- a/lib/librte_ip_frag/ip_frag_internal.c
+++ b/lib/librte_ip_frag/ip_frag_internal.c
@@ -14,6 +14,8 @@
 #define	IP_FRAG_TBL_POS(tbl, sig)	\
 	((tbl)->pkt + ((sig) & (tbl)->entry_mask))
 
+
+//将key填充到fp项中
 static inline void
 ip_frag_tbl_add(struct rte_ip_frag_tbl *tbl,  struct ip_frag_pkt *fp,
 	const struct ip_frag_key *key, uint64_t tms)
@@ -97,11 +99,13 @@ ip_frag_process(struct ip_frag_pkt *fp, struct rte_ip_frag_death_row *dr,
 
 	/* this is the first fragment. */
 	if (ofs == 0) {
+		//首片
 		idx = (fp->frags[IP_FIRST_FRAG_IDX].mb == NULL) ?
 				IP_FIRST_FRAG_IDX : UINT32_MAX;
 
 	/* this is the last fragment. */
 	} else if (more_frags == 0) {
+		//最后一片
 		fp->total_size = ofs + len;
 		idx = (fp->frags[IP_LAST_FRAG_IDX].mb == NULL) ?
 				IP_LAST_FRAG_IDX : UINT32_MAX;
@@ -109,6 +113,7 @@ ip_frag_process(struct ip_frag_pkt *fp, struct rte_ip_frag_death_row *dr,
 	/* this is the intermediate fragment. */
 	} else if ((idx = fp->last_idx) <
 		sizeof (fp->frags) / sizeof (fp->frags[0])) {
+		//其它片
 		fp->last_idx++;
 	}
 
@@ -154,6 +159,7 @@ ip_frag_process(struct ip_frag_pkt *fp, struct rte_ip_frag_death_row *dr,
 		return NULL;
 	}
 
+	//存入收到的分片
 	fp->frags[idx].ofs = ofs;
 	fp->frags[idx].len = len;
 	fp->frags[idx].mb = mb;
@@ -162,11 +168,12 @@ ip_frag_process(struct ip_frag_pkt *fp, struct rte_ip_frag_death_row *dr,
 
 	/* not all fragments are collected yet. */
 	if (likely (fp->frag_size < fp->total_size)) {
-		return mb;
+		return mb;//分片没有收全，返回NULL
 
 	/* if we collected all fragments, then try to reassemble. */
 	} else if (fp->frag_size == fp->total_size &&
 			fp->frags[IP_FIRST_FRAG_IDX].mb != NULL) {
+		//分片收集完成，进行分片重组
 		if (fp->key.key_len == IPV4_KEYLEN)
 			mb = ipv4_frag_reassemble(fp);
 		else
@@ -239,9 +246,10 @@ ip_frag_find(struct rte_ip_frag_tbl *tbl, struct rte_ip_frag_death_row *dr,
 	if ((pkt = ip_frag_lookup(tbl, key, tms, &free, &stale)) == NULL) {
 
 		/*timed-out entry, free and invalidate it*/
+		//移除掉过期的
 		if (stale != NULL) {
 			ip_frag_tbl_del(tbl, dr, stale);
-			free = stale;
+			free = stale;//优先用过期的
 
 		/*
 		 * we found a free entry, check if we can use it.
@@ -261,6 +269,7 @@ ip_frag_find(struct rte_ip_frag_tbl *tbl, struct rte_ip_frag_death_row *dr,
 		}
 
 		/* found a free entry to reuse. */
+		//在分片表中没有找到，恰好有free项，使用free项填充key
 		if (free != NULL) {
 			ip_frag_tbl_add(tbl,  free, key, tms);
 			pkt = free;
@@ -272,6 +281,7 @@ ip_frag_find(struct rte_ip_frag_tbl *tbl, struct rte_ip_frag_death_row *dr,
 	 * and reuse it.
 	 */
 	} else if (max_cycles + pkt->start < tms) {
+		//找到了，但已过期
 		ip_frag_tbl_reuse(tbl, dr, pkt, tms);
 	}
 
@@ -281,6 +291,7 @@ ip_frag_find(struct rte_ip_frag_tbl *tbl, struct rte_ip_frag_death_row *dr,
 	return pkt;
 }
 
+//查找key对应的分片列表，free记录空闲分片，stale记录过期的
 struct ip_frag_pkt *
 ip_frag_lookup(struct rte_ip_frag_tbl *tbl,
 	const struct ip_frag_key *key, uint64_t tms,
@@ -297,6 +308,7 @@ ip_frag_lookup(struct rte_ip_frag_tbl *tbl,
 	max_cycles = tbl->max_cycles;
 	assoc = tbl->bucket_entries;
 
+	//cache查询
 	if (tbl->last != NULL && ip_frag_key_cmp(key, &tbl->last->key) == 0)
 		return tbl->last;
 
@@ -330,10 +342,12 @@ ip_frag_lookup(struct rte_ip_frag_tbl *tbl,
 			IPv6_KEY_BYTES(p1[i].key.src_dst), p1[i].key.id, p1[i].start);
 
 		if (ip_frag_key_cmp(key, &p1[i].key) == 0)
-			return p1 + i;
+			return p1 + i;//与第i个匹配，直接返回
 		else if (ip_frag_key_is_empty(&p1[i].key))
+			//遇到空闲的，记录空闲位置
 			empty = (empty == NULL) ? (p1 + i) : empty;
 		else if (max_cycles + p1[i].start < tms)
+			//遇到过期的，记录过期的位置
 			old = (old == NULL) ? (p1 + i) : old;
 
 		if (p2->key.key_len == IPV4_KEYLEN)
@@ -356,10 +370,12 @@ ip_frag_lookup(struct rte_ip_frag_tbl *tbl,
 			IPv6_KEY_BYTES(p2[i].key.src_dst), p2[i].key.id, p2[i].start);
 
 		if (ip_frag_key_cmp(key, &p2[i].key) == 0)
-			return p2 + i;
+			return p2 + i;//在p2中找到，直接返回
 		else if (ip_frag_key_is_empty(&p2[i].key))
+			//如果之前没有记录空闲位置，记录空闲位置
 			empty = (empty == NULL) ?( p2 + i) : empty;
 		else if (max_cycles + p2[i].start < tms)
+			//如果之前没有记录过期位置，记录过期位置
 			old = (old == NULL) ? (p2 + i) : old;
 	}
 
diff --git a/lib/librte_ip_frag/rte_ipv4_fragmentation.c b/lib/librte_ip_frag/rte_ipv4_fragmentation.c
index 9e9f986cc..6b21383f8 100644
--- a/lib/librte_ip_frag/rte_ipv4_fragmentation.c
+++ b/lib/librte_ip_frag/rte_ipv4_fragmentation.c
@@ -61,6 +61,7 @@ static inline void __free_fragments(struct rte_mbuf *mb[], uint32_t num)
  *   in the pkts_out array.
  *   Otherwise - (-1) * <errno>.
  */
+//实现报文pkt_in按mtu_size进行分片
 int32_t
 rte_ipv4_fragment_packet(struct rte_mbuf *pkt_in,
 	struct rte_mbuf **pkts_out,
@@ -80,17 +81,22 @@ rte_ipv4_fragment_packet(struct rte_mbuf *pkt_in,
 	 * Ensure the IP payload length of all fragments is aligned to a
 	 * multiple of 8 bytes as per RFC791 section 2.3.
 	 */
+	//确保ip负载长度均以8字节对齐
 	frag_size = RTE_ALIGN_FLOOR((mtu_size - sizeof(struct rte_ipv4_hdr)),
 				    IPV4_HDR_FO_ALIGN);
 
+	//取ip头
 	in_hdr = rte_pktmbuf_mtod(pkt_in, struct rte_ipv4_hdr *);
+	//取分片offset
 	flag_offset = rte_cpu_to_be_16(in_hdr->fragment_offset);
 
 	/* If Don't Fragment flag is set */
+	//是不设置了不容许分片标记
 	if (unlikely ((flag_offset & IPV4_HDR_DF_MASK) != 0))
 		return -ENOTSUP;
 
 	/* Check that pkts_out is big enough to hold all fragments */
+	//如果分片了，nb_pkts_out是否足够容纳
 	if (unlikely(frag_size * nb_pkts_out <
 	    (uint16_t)(pkt_in->pkt_len - sizeof(struct rte_ipv4_hdr))))
 		return -EINVAL;
@@ -107,8 +113,10 @@ rte_ipv4_fragment_packet(struct rte_mbuf *pkt_in,
 		struct rte_ipv4_hdr *out_hdr;
 
 		/* Allocate direct buffer */
+		//自pool_direct中申请mbuf
 		out_pkt = rte_pktmbuf_alloc(pool_direct);
 		if (unlikely(out_pkt == NULL)) {
+			//申请失败，释放掉已完成的分片
 			__free_fragments(pkts_out, out_pkt_pos);
 			return -ENOMEM;
 		}
@@ -166,6 +174,7 @@ rte_ipv4_fragment_packet(struct rte_mbuf *pkt_in,
 
 		out_hdr = rte_pktmbuf_mtod(out_pkt, struct rte_ipv4_hdr *);
 
+		//完成分片数据填充
 		__fill_ipv4hdr_frag(out_hdr, in_hdr,
 		    (uint16_t)out_pkt->pkt_len,
 		    flag_offset, fragment_offset, more_in_segs);
diff --git a/lib/librte_ip_frag/rte_ipv4_reassembly.c b/lib/librte_ip_frag/rte_ipv4_reassembly.c
index 1dda8aca0..373c5378c 100644
--- a/lib/librte_ip_frag/rte_ipv4_reassembly.c
+++ b/lib/librte_ip_frag/rte_ipv4_reassembly.c
@@ -11,6 +11,7 @@
 /*
  * Reassemble fragments into one packet.
  */
+//将收集全的分片进行重组
 struct rte_mbuf *
 ipv4_frag_reassemble(struct ip_frag_pkt *fp)
 {
@@ -94,6 +95,7 @@ ipv4_frag_reassemble(struct ip_frag_pkt *fp)
  *   - an error occurred.
  *   - not all fragments of the packet are collected yet.
  */
+//ipv4分片重组
 struct rte_mbuf *
 rte_ipv4_frag_reassemble_packet(struct rte_ip_frag_tbl *tbl,
 	struct rte_ip_frag_death_row *dr, struct rte_mbuf *mb, uint64_t tms,
@@ -111,11 +113,12 @@ rte_ipv4_frag_reassemble_packet(struct rte_ip_frag_tbl *tbl,
 
 	psd = (unaligned_uint64_t *)&ip_hdr->src_addr;
 	/* use first 8 bytes only */
-	key.src_dst[0] = psd[0];
-	key.id = ip_hdr->packet_id;
+	key.src_dst[0] = psd[0];//填充src-dst ip地址
+	key.id = ip_hdr->packet_id;//ip层id号
 	key.key_len = IPV4_KEYLEN;
 
-	ip_ofs *= RTE_IPV4_HDR_OFFSET_UNITS;
+	ip_ofs *= RTE_IPV4_HDR_OFFSET_UNITS;//展开为真实的offset
+	//获得分片负载的长度（mb->l3_len指出ip头长度）
 	ip_len = rte_be_to_cpu_16(ip_hdr->total_length) - mb->l3_len;
 
 	IP_FRAG_LOG(DEBUG, "%s:%d:\n"
@@ -135,6 +138,7 @@ rte_ipv4_frag_reassemble_packet(struct rte_ip_frag_tbl *tbl,
 	}
 
 	/* try to find/add entry into the fragment's table. */
+	//找本包对应的分片列表
 	if ((fp = ip_frag_find(tbl, dr, &key, tms)) == NULL) {
 		IP_FRAG_MBUF2DR(dr, mb);
 		return NULL;
diff --git a/lib/librte_kni/rte_kni.c b/lib/librte_kni/rte_kni.c
index 7fbcf2201..10fd66a70 100644
--- a/lib/librte_kni/rte_kni.c
+++ b/lib/librte_kni/rte_kni.c
@@ -199,6 +199,7 @@ kni_release_mz(struct rte_kni *kni)
 	rte_memzone_free(kni->m_sync_addr);
 }
 
+//创建必要的参数，通过ioctl完成kni接口创建
 struct rte_kni *
 rte_kni_alloc(struct rte_mempool *pktmbuf_pool,
 	      const struct rte_kni_conf *conf,
@@ -302,6 +303,7 @@ rte_kni_alloc(struct rte_mempool *pktmbuf_pool,
 	kni->group_id = conf->group_id;
 	kni->mbuf_size = conf->mbuf_size;
 
+	//调用ioctl触发kernel创建kni接口
 	ret = ioctl(kni_fd, RTE_KNI_IOCTL_CREATE, &dev_info);
 	if (ret < 0)
 		goto ioctl_fail;
@@ -608,6 +610,7 @@ rte_kni_tx_burst(struct rte_kni *kni, struct rte_mbuf **mbufs, unsigned int num)
 	return ret;
 }
 
+//自tx_q队列中取num个mbuf
 unsigned
 rte_kni_rx_burst(struct rte_kni *kni, struct rte_mbuf **mbufs, unsigned int num)
 {
@@ -620,6 +623,7 @@ rte_kni_rx_burst(struct rte_kni *kni, struct rte_mbuf **mbufs, unsigned int num)
 	return ret;
 }
 
+//自free_q中取出报文，并逐个释放
 static void
 kni_free_mbufs(struct rte_kni *kni)
 {
@@ -689,6 +693,7 @@ kni_allocate_mbufs(struct rte_kni *kni)
 	}
 }
 
+//通过名称查kni结构体
 struct rte_kni *
 rte_kni_get(const char *name)
 {
diff --git a/lib/librte_kni/rte_kni_fifo.h b/lib/librte_kni/rte_kni_fifo.h
index d2ec82fe8..3c5a68c37 100644
--- a/lib/librte_kni/rte_kni_fifo.h
+++ b/lib/librte_kni/rte_kni_fifo.h
@@ -64,12 +64,14 @@ kni_fifo_put(struct rte_kni_fifo *fifo, void **data, unsigned num)
 	for (i = 0; i < num; i++) {
 		new_write = (new_write + 1) & (fifo->len - 1);
 
+		//写位置与读位置相等，跳出
 		if (new_write == fifo_read)
 			break;
+		//设置放入的元素
 		fifo->buffer[fifo_write] = data[i];
 		fifo_write = new_write;
 	}
-	__KNI_STORE_RELEASE(&fifo->write, fifo_write);
+	__KNI_STORE_RELEASE(&fifo->write, fifo_write);//更新写位置
 	return i;
 }
 
@@ -85,12 +87,12 @@ kni_fifo_get(struct rte_kni_fifo *fifo, void **data, unsigned num)
 
 	for (i = 0; i < num; i++) {
 		if (new_read == fifo_write)
-			break;
+			break;//到达写起始位置，所有元素均已读完，跳出
 
 		data[i] = fifo->buffer[new_read];
 		new_read = (new_read + 1) & (fifo->len - 1);
 	}
-	__KNI_STORE_RELEASE(&fifo->read, new_read);
+	__KNI_STORE_RELEASE(&fifo->read, new_read);//更新读位置
 	return i;
 }
 
@@ -100,6 +102,7 @@ kni_fifo_get(struct rte_kni_fifo *fifo, void **data, unsigned num)
 static inline uint32_t
 kni_fifo_count(struct rte_kni_fifo *fifo)
 {
+	//取可读取的元素数
 	unsigned fifo_write = __KNI_LOAD_ACQUIRE(&fifo->write);
 	unsigned fifo_read = __KNI_LOAD_ACQUIRE(&fifo->read);
 	return (fifo->len + fifo_write - fifo_read) & (fifo->len - 1);
diff --git a/lib/librte_kvargs/rte_kvargs.c b/lib/librte_kvargs/rte_kvargs.c
index d39332999..a4ebbc322 100644
--- a/lib/librte_kvargs/rte_kvargs.c
+++ b/lib/librte_kvargs/rte_kvargs.c
@@ -15,6 +15,7 @@
  * key=value,key=value,... and insert them into the list.
  * strtok() is used so the params string will be copied to be modified.
  */
+//将abc=33,def=vaue,dafdas=ab解析成一个数组，存放在kvlist中
 static int
 rte_kvargs_tokenize(struct rte_kvargs *kvlist, const char *params)
 {
@@ -31,6 +32,7 @@ rte_kvargs_tokenize(struct rte_kvargs *kvlist, const char *params)
 		return -1;
 
 	/* browse each key/value pair and add it in kvlist */
+	//类似这样的格式abc=1,dfer=45,dds=87
 	str = kvlist->str;
 	while ((str = strtok_r(str, RTE_KVARGS_PAIRS_DELIM, &ctx1)) != NULL) {
 
@@ -69,6 +71,7 @@ rte_kvargs_tokenize(struct rte_kvargs *kvlist, const char *params)
  * Determine whether a key is valid or not by looking
  * into a list of valid keys.
  */
+//检查key_match是否在valid数组之内
 static int
 is_valid_key(const char * const valid[], const char *key_match)
 {
@@ -85,6 +88,7 @@ is_valid_key(const char * const valid[], const char *key_match)
  * Determine whether all keys are valid or not by looking
  * into a list of valid keys.
  */
+//检查kvlist是否包含有非法的keys,如果有，返回-1,否则近回0
 static int
 check_for_valid_keys(struct rte_kvargs *kvlist,
 		const char * const valid[])
@@ -96,6 +100,7 @@ check_for_valid_keys(struct rte_kvargs *kvlist,
 		pair = &kvlist->pairs[i];
 		ret = is_valid_key(valid, pair->key);
 		if (!ret)
+			//我们看到了一个未知的参数，报错
 			return -1;
 	}
 	return 0;
@@ -106,6 +111,7 @@ check_for_valid_keys(struct rte_kvargs *kvlist,
  * E.g. given a list = { rx = 0, rx = 1, tx = 2 } the number of args for
  * arg "rx" will be 2.
  */
+//计算key-match在kvlist中出现的次数
 unsigned
 rte_kvargs_count(const struct rte_kvargs *kvlist, const char *key_match)
 {
@@ -125,6 +131,8 @@ rte_kvargs_count(const struct rte_kvargs *kvlist, const char *key_match)
 /*
  * For each matching key, call the given handler function.
  */
+//查找参数key-match,取其对应的值，并执行handler
+//如果key-match为NULL，则kvlist中所有参数需要挨个执行
 int
 rte_kvargs_process(const struct rte_kvargs *kvlist,
 		const char *key_match,
@@ -137,6 +145,7 @@ rte_kvargs_process(const struct rte_kvargs *kvlist,
 	if (kvlist == NULL)
 		return 0;
 
+	//key_match与kvlist发生匹配，则执行对应回调
 	for (i = 0; i < kvlist->count; i++) {
 		pair = &kvlist->pairs[i];
 		if (key_match == NULL || strcmp(pair->key, key_match) == 0) {
@@ -163,6 +172,7 @@ rte_kvargs_free(struct rte_kvargs *kvlist)
  * an allocated structure that contains a key/value list. Also
  * check if only valid keys were used.
  */
+//解析参数，检查是否为有效参数，有效时返回kvlist,否则为NULL
 struct rte_kvargs *
 rte_kvargs_parse(const char *args, const char * const valid_keys[])
 {
@@ -173,11 +183,14 @@ rte_kvargs_parse(const char *args, const char * const valid_keys[])
 		return NULL;
 	memset(kvlist, 0, sizeof(*kvlist));
 
+	//将args解析为key,values（逗号划分）
 	if (rte_kvargs_tokenize(kvlist, args) < 0) {
 		rte_kvargs_free(kvlist);
 		return NULL;
 	}
 
+	//如果指定了valid_keys，则检查kvlist中的key是否包含于valid_keys
+	//如果不包含在valid_keys中，则释放kvlist,返回NULL
 	if (valid_keys != NULL && check_for_valid_keys(kvlist, valid_keys) < 0) {
 		rte_kvargs_free(kvlist);
 		return NULL;
diff --git a/lib/librte_mbuf/rte_mbuf.c b/lib/librte_mbuf/rte_mbuf.c
index 8c51dc1a0..af202487b 100644
--- a/lib/librte_mbuf/rte_mbuf.c
+++ b/lib/librte_mbuf/rte_mbuf.c
@@ -37,6 +37,7 @@
  * rte_mempool_create(), or called directly if using
  * rte_mempool_create_empty()/rte_mempool_populate()
  */
+//通过opaque_arg设置pktmbuf　pool私有数据
 void
 rte_pktmbuf_pool_init(struct rte_mempool *mp, void *opaque_arg)
 {
@@ -47,6 +48,7 @@ rte_pktmbuf_pool_init(struct rte_mempool *mp, void *opaque_arg)
 	RTE_ASSERT(mp->elt_size >= sizeof(struct rte_mbuf));
 
 	/* if no structure is provided, assume no mbuf private area */
+	//如果未提供opaque_arg,则默认计算mbuf空闲空间大小
 	user_mbp_priv = opaque_arg;
 	if (user_mbp_priv == NULL) {
 		default_mbp_priv.mbuf_priv_size = 0;
@@ -58,10 +60,12 @@ rte_pktmbuf_pool_init(struct rte_mempool *mp, void *opaque_arg)
 		user_mbp_priv = &default_mbp_priv;
 	}
 
+	//预留的空间大小校验
 	RTE_ASSERT(mp->elt_size >= sizeof(struct rte_mbuf) +
 		user_mbp_priv->mbuf_data_room_size +
 		user_mbp_priv->mbuf_priv_size);
 
+	//采用用户传入参数填充mempool的私有数据
 	mbp_priv = rte_mempool_get_priv(mp);
 	memcpy(mbp_priv, user_mbp_priv, sizeof(*mbp_priv));
 }
@@ -71,6 +75,7 @@ rte_pktmbuf_pool_init(struct rte_mempool *mp, void *opaque_arg)
  * rte_mempool_obj_iter() or rte_mempool_create().
  * Set the fields of a packet mbuf to their default values.
  */
+//mbuf初始化函数，用于初始化mbuf pool中的所各个mbuf
 void
 rte_pktmbuf_init(struct rte_mempool *mp,
 		 __attribute__((unused)) void *opaque_arg,
@@ -91,11 +96,12 @@ rte_pktmbuf_init(struct rte_mempool *mp,
 	memset(m, 0, mbuf_size);
 	/* start of buffer is after mbuf structure and priv data */
 	m->priv_size = priv_size;
-	m->buf_addr = (char *)m + mbuf_size;
+	m->buf_addr = (char *)m + mbuf_size;//将mbuf结构体放在m的后面，并空开一个priv_size
 	m->buf_iova = rte_mempool_virt2iova(m) + mbuf_size;
 	m->buf_len = (uint16_t)buf_len;
 
 	/* keep some headroom between start of buffer and data */
+	//自buf_addr后再空开一个HEADROOM放置报文
 	m->data_off = RTE_MIN(RTE_PKTMBUF_HEADROOM, (uint16_t)m->buf_len);
 
 	/* init some constant fields */
@@ -118,12 +124,14 @@ rte_pktmbuf_pool_create_by_ops(const char *name, unsigned int n,
 	unsigned elt_size;
 	int ret;
 
+	//priv_size必须按RTE_MBUF_PRIV_ALIGN对齐
 	if (RTE_ALIGN(priv_size, RTE_MBUF_PRIV_ALIGN) != priv_size) {
 		RTE_LOG(ERR, MBUF, "mbuf priv_size=%u is not aligned\n",
 			priv_size);
 		rte_errno = EINVAL;
 		return NULL;
 	}
+	//每个mbuf的大小为，rte_mbuf结构体大小＋用户私有数据大小＋缓冲区大小
 	elt_size = sizeof(struct rte_mbuf) + (unsigned)priv_size +
 		(unsigned)data_room_size;
 	mbp_priv.mbuf_data_room_size = data_room_size;
@@ -158,6 +166,13 @@ rte_pktmbuf_pool_create_by_ops(const char *name, unsigned int n,
 }
 
 /* helper to create a mbuf pool */
+//创建一个mbuf pool
+//@name 创建的池名称
+//@n 创建多少个buf
+//@cache_size cacheline大小
+//@priv_size 用户私有数据大小
+//@data_room_size 缓冲区大小（用于存放数据）
+
 struct rte_mempool *
 rte_pktmbuf_pool_create(const char *name, unsigned int n,
 	unsigned int cache_size, uint16_t priv_size, uint16_t data_room_size,
diff --git a/lib/librte_mbuf/rte_mbuf.h b/lib/librte_mbuf/rte_mbuf.h
index 92d81972a..765d42811 100644
--- a/lib/librte_mbuf/rte_mbuf.h
+++ b/lib/librte_mbuf/rte_mbuf.h
@@ -47,7 +47,6 @@
 #ifdef __cplusplus
 extern "C" {
 #endif
-
 /**
  * Get the name of a RX offload flag
  *
@@ -302,6 +301,7 @@ rte_mbuf_to_priv(struct rte_mbuf *m)
  */
 struct rte_pktmbuf_pool_private {
 	uint16_t mbuf_data_room_size; /**< Size of data space in each mbuf. */
+	//mbuf的私有数据
 	uint16_t mbuf_priv_size;      /**< Size of private area in each mbuf. */
 };
 
@@ -1413,18 +1413,19 @@ static inline struct rte_mbuf *rte_pktmbuf_lastseg(struct rte_mbuf *m)
  *   A pointer to the start of the newly prepended data, or
  *   NULL if there is not enough headroom space in the first segment
  */
+//尝试在m的headroom中分配一个len长度的空间用于存放报文，如果无法分配返回NULL
 static inline char *rte_pktmbuf_prepend(struct rte_mbuf *m,
 					uint16_t len)
 {
 	__rte_mbuf_sanity_check(m, 1);
 
 	if (unlikely(len > rte_pktmbuf_headroom(m)))
-		return NULL;
+		return NULL;//超过headroom，返回NULL
 
 	/* NB: elaborating the subtraction like this instead of using
 	 *     -= allows us to ensure the result type is uint16_t
 	 *     avoiding compiler warnings on gcc 8.1 at least */
-	m->data_off = (uint16_t)(m->data_off - len);
+	m->data_off = (uint16_t)(m->data_off - len);//data_off前移
 	m->data_len = (uint16_t)(m->data_len + len);
 	m->pkt_len  = (m->pkt_len + len);
 
diff --git a/lib/librte_mbuf/rte_mbuf_core.h b/lib/librte_mbuf/rte_mbuf_core.h
index 302270146..d69aa81b6 100644
--- a/lib/librte_mbuf/rte_mbuf_core.h
+++ b/lib/librte_mbuf/rte_mbuf_core.h
@@ -287,7 +287,7 @@ extern "C" {
  *  - if it's IPv4, set the PKT_TX_IP_CKSUM flag
  *  - fill the mbuf offload information: l2_len, l3_len, l4_len, tso_segsz
  */
-#define PKT_TX_TCP_SEG       (1ULL << 50)
+#define PKT_TX_TCP_SEG       (1ULL << 50) //tso功能标记
 
 /** TX IEEE1588 packet to timestamp. */
 #define PKT_TX_IEEE1588_TMST (1ULL << 51)
@@ -312,6 +312,7 @@ extern "C" {
 #define PKT_TX_UDP_CKSUM     (3ULL << 52)
 
 /** Mask for L4 cksum offload request. */
+//硬件计算4层checksum　mask
 #define PKT_TX_L4_MASK       (3ULL << 52)
 
 /**
@@ -320,7 +321,7 @@ extern "C" {
  * PKT_TX_IP_CKSUM.
  *  - fill the mbuf offload information: l2_len, l3_len
  */
-#define PKT_TX_IP_CKSUM      (1ULL << 54)
+#define PKT_TX_IP_CKSUM      (1ULL << 54) //ip层checksum
 
 /**
  * Packet is IPv4. This flag must be set when using any offload feature
@@ -328,7 +329,7 @@ extern "C" {
  * packet. If the packet is a tunneled packet, this flag is related to
  * the inner headers.
  */
-#define PKT_TX_IPV4          (1ULL << 55)
+#define PKT_TX_IPV4          (1ULL << 55) //指明报文为ipv4,为了使用tso需要告知网卡报文为ipv4
 
 /**
  * Packet is IPv6. This flag must be set when using an offload feature
@@ -484,8 +485,9 @@ enum {
  * The generic rte_mbuf, containing a packet mbuf.
  */
 struct rte_mbuf {
-	MARKER cacheline0;
+	MARKER cacheline0;// 这种写法很帅气
 
+	//mbuf的数据缓冲起始地址
 	void *buf_addr;           /**< Virtual address of segment buffer. */
 	/**
 	 * Physical address of segment buffer.
@@ -495,13 +497,13 @@ struct rte_mbuf {
 	 */
 	RTE_STD_C11
 	union {
-		rte_iova_t buf_iova;
+		rte_iova_t buf_iova;//buf_addr的物理地址
 		rte_iova_t buf_physaddr; /**< deprecated */
 	} __rte_aligned(sizeof(rte_iova_t));
 
 	/* next 8 bytes are initialised on RX descriptor rearm */
 	MARKER64 rearm_data;
-	uint16_t data_off;
+	uint16_t data_off;//真实存放数据的位置（自buf_addr偏移data_off)
 
 	/**
 	 * Reference counter. Its size should at least equal to the size
@@ -525,6 +527,7 @@ struct rte_mbuf {
 	 */
 	uint16_t port;
 
+	//mbuf的offload功能标记（dpdk定义了一组功能，然后由驱动对这组功能进行映射，并对应到各自硬件特性上）
 	uint64_t ol_flags;        /**< Offload features. */
 
 	/* remaining bytes are set on RX when pulling packet from descriptor */
@@ -564,9 +567,12 @@ struct rte_mbuf {
 		};
 	};
 
+	//总的报文长度(可能有多个seg,用next串起来)
 	uint32_t pkt_len;         /**< Total pkt len: sum of all segments. */
+	//mbuf中的数据长度
 	uint16_t data_len;        /**< Amount of data in segment buffer. */
 	/** VLAN TCI (CPU order), valid if PKT_RX_VLAN is set. */
+	//为了让网卡来插入vlan,这里将vlan放在此种（cpu序），等报文发送时，网卡将报文插入
 	uint16_t vlan_tci;
 
 	RTE_STD_C11
@@ -644,14 +650,21 @@ struct rte_mbuf {
 		uint64_t tx_offload;       /**< combined for easy fetch */
 		__extension__
 		struct {
+		    //2层头长度
 			uint64_t l2_len:RTE_MBUF_L2_LEN_BITS;
 			/**< L2 (MAC) Header Length for non-tunneling pkt.
 			 * Outer_L4_len + ... + Inner_L2_len for tunneling pkt.
 			 */
-			uint64_t l3_len:RTE_MBUF_L3_LEN_BITS;
+			uint64_t l3_len:RTE_MBUF_L3_LEN_BITS;//3层头部的长度
 			/**< L3 (IP) Header Length. */
-			uint64_t l4_len:RTE_MBUF_L4_LEN_BITS;
+			uint64_t l4_len:RTE_MBUF_L4_LEN_BITS;//3层头部的长度
 			/**< L4 (TCP/UDP) Header Length. */
+            //在不支持TSO的网卡上，TCP层向IP层发送数据会考虑mss，使得TCP向下发送的数据可以包含在一个IP分组中而不会造成分片，
+            //mss是在TCP初始建立连接时由网卡MTU确定并和对端协商的，所以在一个MTU＝1500的网卡上，TCP向下发送的数据不会大于
+            //min(mss_local, mss_remote)-ip头-tcp头。
+            //网卡支持TSO时，TCP层会逐渐增大mss（总是整数倍数增加），当TCP层向下发送大块数据时，仅仅计算TCP头，网卡接到到了
+            //IP层传下的大数 据包后自己重新分成若干个IP数据包，添加IP头，复制TCP头并且重新计算校验和等相关数据，这样就把一部
+            //分CPU相关的处理工作转移到由网卡来处理。
 			uint64_t tso_segsz:RTE_MBUF_TSO_SEGSZ_BITS;
 			/**< TCP TSO segment size */
 
@@ -679,6 +692,8 @@ struct rte_mbuf {
 	/** Size of the application private data. In case of an indirect
 	 * mbuf, it stores the direct mbuf private data size.
 	 */
+    //mbuf缓冲的头部存放mbuf的控制信息（rte_mbuf_t结构内容），接着在后面有一个
+    //priv_size用于存放用户定制的信息，例如crypto中存放rte_crypto_op
 	uint16_t priv_size;
 
 	/** Timesync flags for use with IEEE1588. */
@@ -753,6 +768,7 @@ struct rte_mbuf_ext_shared_info {
  * @param t
  *   The type to cast the result into.
  */
+//buf_addr + data_off可以找到真正数据的起始位置，在这个位置处再进行o的偏移，以便扩大或缩小headroom的空间
 #define rte_pktmbuf_mtod_offset(m, t, o)	\
 	((t)((char *)(m)->buf_addr + (m)->data_off + (o)))
 
@@ -768,6 +784,8 @@ struct rte_mbuf_ext_shared_info {
  * @param t
  *   The type to cast the result into.
  */
+//mtod ＝ move to data
+//返回指向数据区起始位置的指针
 #define rte_pktmbuf_mtod(m, t) rte_pktmbuf_mtod_offset(m, t, 0)
 
 /**
diff --git a/lib/librte_mempool/rte_mempool.c b/lib/librte_mempool/rte_mempool.c
index 40cae3eb6..70502c321 100644
--- a/lib/librte_mempool/rte_mempool.c
+++ b/lib/librte_mempool/rte_mempool.c
@@ -36,6 +36,7 @@
 
 TAILQ_HEAD(rte_mempool_list, rte_tailq_entry);
 
+//用于串连系统中所有mempool
 static struct rte_tailq_elem rte_mempool_tailq = {
 	.name = "RTE_MEMPOOL",
 };
@@ -163,6 +164,7 @@ mempool_add_elem(struct rte_mempool *mp, __rte_unused void *opaque,
 }
 
 /* call obj_cb() for each mempool element */
+//针对每个mp对象调用obj_cb回调
 uint32_t
 rte_mempool_obj_iter(struct rte_mempool *mp,
 	rte_mempool_obj_cb_t *obj_cb, void *obj_cb_arg)
@@ -197,6 +199,7 @@ rte_mempool_mem_iter(struct rte_mempool *mp,
 }
 
 /* get the header, trailer and total size of a mempool element. */
+//确定实体总大小
 uint32_t
 rte_mempool_calc_obj_size(uint32_t elt_size, uint32_t flags,
 	struct rte_mempool_objsz *sz)
@@ -205,20 +208,24 @@ rte_mempool_calc_obj_size(uint32_t elt_size, uint32_t flags,
 
 	sz = (sz != NULL) ? sz : &lsz;
 
+	//确定header size的大小
 	sz->header_size = sizeof(struct rte_mempool_objhdr);
 	if ((flags & MEMPOOL_F_NO_CACHE_ALIGN) == 0)
 		sz->header_size = RTE_ALIGN_CEIL(sz->header_size,
 			RTE_MEMPOOL_ALIGN);
 
+	//确定trailer size预期大小
 #ifdef RTE_LIBRTE_MEMPOOL_DEBUG
 	sz->trailer_size = sizeof(struct rte_mempool_objtlr);
 #else
 	sz->trailer_size = 0;
 #endif
 
+	//确定实体大小
 	/* element size is 8 bytes-aligned at least */
 	sz->elt_size = RTE_ALIGN_CEIL(elt_size, sizeof(uint64_t));
 
+	//确定实体大小，尾部大小（对齐后）
 	/* expand trailer to next cache line */
 	if ((flags & MEMPOOL_F_NO_CACHE_ALIGN) == 0) {
 		sz->total_size = sz->header_size + sz->elt_size +
@@ -278,6 +285,7 @@ rte_mempool_free_memchunks(struct rte_mempool *mp)
 	}
 }
 
+//调用一次ops_alloc
 static int
 mempool_ops_alloc_once(struct rte_mempool *mp)
 {
@@ -705,7 +713,7 @@ static void
 mempool_cache_init(struct rte_mempool_cache *cache, uint32_t size)
 {
 	cache->size = size;
-	cache->flushthresh = CALC_CACHE_FLUSHTHRESH(size);
+	cache->flushthresh = CALC_CACHE_FLUSHTHRESH(size);//1.5倍的size
 	cache->len = 0;
 }
 
@@ -750,9 +758,9 @@ rte_mempool_cache_free(struct rte_mempool_cache *cache)
 
 /* create an empty mempool */
 struct rte_mempool *
-rte_mempool_create_empty(const char *name, unsigned n, unsigned elt_size,
-	unsigned cache_size, unsigned private_data_size,
-	int socket_id, unsigned flags)
+rte_mempool_create_empty(const char *name/*名称*/, unsigned n/*元素数*/, unsigned elt_size/*实体大小*/,
+	unsigned cache_size/*缓存大小*/, unsigned private_data_size/*私有数据大小*/,
+	int socket_id/*节点编号*/, unsigned flags)
 {
 	char mz_name[RTE_MEMZONE_NAMESIZE];
 	struct rte_mempool_list *mempool_list;
@@ -766,8 +774,10 @@ rte_mempool_create_empty(const char *name, unsigned n, unsigned elt_size,
 	int ret;
 
 	/* compilation-time checks */
+	//结构体大小必须cacheline对齐
 	RTE_BUILD_BUG_ON((sizeof(struct rte_mempool) &
 			  RTE_CACHE_LINE_MASK) != 0);
+	//cache结构体大小必须cacheline对齐
 	RTE_BUILD_BUG_ON((sizeof(struct rte_mempool_cache) &
 			  RTE_CACHE_LINE_MASK) != 0);
 #ifdef RTE_LIBRTE_MEMPOOL_DEBUG
@@ -780,6 +790,7 @@ rte_mempool_create_empty(const char *name, unsigned n, unsigned elt_size,
 	mempool_list = RTE_TAILQ_CAST(rte_mempool_tailq.head, rte_mempool_list);
 
 	/* asked for zero items */
+	//不支持创建0个元素
 	if (n == 0) {
 		rte_errno = EINVAL;
 		return NULL;
@@ -797,6 +808,7 @@ rte_mempool_create_empty(const char *name, unsigned n, unsigned elt_size,
 		flags |= MEMPOOL_F_NO_SPREAD;
 
 	/* calculate mempool object sizes. */
+	//计算mempool元素大小
 	if (!rte_mempool_calc_obj_size(elt_size, flags, &objsz)) {
 		rte_errno = EINVAL;
 		return NULL;
@@ -808,6 +820,7 @@ rte_mempool_create_empty(const char *name, unsigned n, unsigned elt_size,
 	 * reserve a memory zone for this mempool: private data is
 	 * cache-aligned
 	 */
+	//记算私有数据大小（mempool私有数据)
 	private_data_size = (private_data_size +
 			     RTE_MEMPOOL_ALIGN_MASK) & (~RTE_MEMPOOL_ALIGN_MASK);
 
@@ -819,16 +832,19 @@ rte_mempool_create_empty(const char *name, unsigned n, unsigned elt_size,
 		goto exit_unlock;
 	}
 
+	//计算mempool结构体大小
 	mempool_size = MEMPOOL_HEADER_SIZE(mp, cache_size);
 	mempool_size += private_data_size;
 	mempool_size = RTE_ALIGN_CEIL(mempool_size, RTE_MEMPOOL_ALIGN);
 
+	//设置memzone名称
 	ret = snprintf(mz_name, sizeof(mz_name), RTE_MEMPOOL_MZ_FORMAT, name);
 	if (ret < 0 || ret >= (int)sizeof(mz_name)) {
 		rte_errno = ENAMETOOLONG;
 		goto exit_unlock;
 	}
 
+	//创建memzone
 	mz = rte_memzone_reserve(mz_name, mempool_size, socket_id, mz_flags);
 	if (mz == NULL)
 		goto exit_unlock;
@@ -862,15 +878,18 @@ rte_mempool_create_empty(const char *name, unsigned n, unsigned elt_size,
 		RTE_PTR_ADD(mp, MEMPOOL_HEADER_SIZE(mp, 0));
 
 	/* Init all default caches. */
+	//要求开启cache时，初始化cache
 	if (cache_size != 0) {
 		for (lcore_id = 0; lcore_id < RTE_MAX_LCORE; lcore_id++)
 			mempool_cache_init(&mp->local_cache[lcore_id],
 					   cache_size);
 	}
 
+	//使te指出mbuf
 	te->data = mp;
 
 	rte_mcfg_tailq_write_lock();
+	//将此mbuf挂载在mempool_list上
 	TAILQ_INSERT_TAIL(mempool_list, te, next);
 	rte_mcfg_tailq_write_unlock();
 	rte_mcfg_mempool_write_unlock();
@@ -895,6 +914,7 @@ rte_mempool_create(const char *name, unsigned n, unsigned elt_size,
 	int ret;
 	struct rte_mempool *mp;
 
+	//创建空的mempool(不含实体,及ring)
 	mp = rte_mempool_create_empty(name, n, elt_size, cache_size,
 		private_data_size, socket_id, flags);
 	if (mp == NULL)
@@ -1251,6 +1271,7 @@ rte_mempool_list_dump(FILE *f)
 }
 
 /* search a mempool from its name */
+//查询mempool
 struct rte_mempool *
 rte_mempool_lookup(const char *name)
 {
diff --git a/lib/librte_mempool/rte_mempool.h b/lib/librte_mempool/rte_mempool.h
index f81152af9..ad2cc324e 100644
--- a/lib/librte_mempool/rte_mempool.h
+++ b/lib/librte_mempool/rte_mempool.h
@@ -81,23 +81,32 @@ struct rte_mempool_debug_stats {
  * A structure that stores a per-core object cache.
  */
 struct rte_mempool_cache {
+	//缓存的最大数目
 	uint32_t size;	      /**< Size of the cache */
+	//flush门限
 	uint32_t flushthresh; /**< Threshold before we flush excess elements */
+	//当前缓存数目
 	uint32_t len;	      /**< Current cache count */
 	/*
 	 * Cache is allocated to this size to allow it to overflow in certain
 	 * cases to avoid needless emptying of cache.
 	 */
+	//存放obj
 	void *objs[RTE_MEMPOOL_CACHE_MAX_SIZE * 3]; /**< Cache objects */
 } __rte_cache_aligned;
 
 /**
  * A structure that stores the size of mempool elements.
  */
+//mempool元素大小
 struct rte_mempool_objsz {
+    //元素大小
 	uint32_t elt_size;     /**< Size of an element. */
+	//header的大小
 	uint32_t header_size;  /**< Size of header (before elt). */
+	//尾部大小
 	uint32_t trailer_size; /**< Size of trailer (after elt). */
+	//总大小
 	uint32_t total_size;
 	/**< Total size of an object (header + elt + trailer). */
 };
@@ -135,6 +144,7 @@ struct rte_mempool_objsz {
  */
 struct rte_mempool_objhdr {
 	STAILQ_ENTRY(rte_mempool_objhdr) next; /**< Next in list. */
+	//元素属于那个mempool
 	struct rte_mempool *mp;          /**< The mempool owning the object. */
 	RTE_STD_C11
 	union {
@@ -226,17 +236,24 @@ struct rte_mempool {
 		uint64_t pool_id;        /**< External mempool identifier. */
 	};
 	void *pool_config;               /**< optional args for ops alloc. */
+	//所属的memzone
 	const struct rte_memzone *mz;    /**< Memzone where pool is alloc'd. */
 	unsigned int flags;              /**< Flags of the mempool. */
 	int socket_id;                   /**< Socket id passed at create. */
+	//元素数
 	uint32_t size;                   /**< Max size of the mempool. */
+	//mempool　cache大小
 	uint32_t cache_size;
 	/**< Size of per-lcore default local cache. */
 
+	//实体大小
 	uint32_t elt_size;               /**< Size of an element. */
+	//实体头部大小
 	uint32_t header_size;            /**< Size of header (before elt). */
+	//实体尾部大小
 	uint32_t trailer_size;           /**< Size of trailer (after elt). */
 
+	//mpool私有数据大小
 	unsigned private_data_size;      /**< Size of private data. */
 	/**
 	 * Index into rte_mempool_ops_table array of mempool ops
@@ -245,11 +262,13 @@ struct rte_mempool {
 	 * to facilitate any secondary processes that may want to use
 	 * this mempool.
 	 */
-	int32_t ops_index;
+	int32_t ops_index;//操作集索引
 
+	//mp针对每个core的cache
 	struct rte_mempool_cache *local_cache; /**< Per-lcore local cache */
 
 	uint32_t populated_size;         /**< Number of populated objects. */
+	//将元素采用链表串起来
 	struct rte_mempool_objhdr_list elt_list; /**< List of objects in pool */
 	uint32_t nb_mem_chunks;          /**< Number of memory chunks */
 	struct rte_mempool_memhdr_list mem_list; /**< List of memory chunks */
@@ -639,6 +658,7 @@ typedef int (*rte_mempool_get_info_t)(const struct rte_mempool *mp,
 /** Structure defining mempool operations structure */
 struct rte_mempool_ops {
 	char name[RTE_MEMPOOL_OPS_NAMESIZE]; /**< Name of mempool ops struct. */
+	//初始化mempool的私有数据
 	rte_mempool_alloc_t alloc;       /**< Allocate private data. */
 	rte_mempool_free_t free;         /**< Free the external pool. */
 	rte_mempool_enqueue_t enqueue;   /**< Enqueue an object. */
@@ -1309,9 +1329,10 @@ __mempool_generic_put(struct rte_mempool *mp, void * const *obj_table,
 
 	/* No cache provided or if put would overflow mem allocated for cache */
 	if (unlikely(cache == NULL || n > RTE_MEMPOOL_CACHE_MAX_SIZE))
+		//需要cache的object过多，采用直接入队方式
 		goto ring_enqueue;
 
-	cache_objs = &cache->objs[cache->len];
+	cache_objs = &cache->objs[cache->len];//指向当前最后一个cache位置
 
 	/*
 	 * The cache follows the following algorithm
@@ -1321,11 +1342,13 @@ __mempool_generic_put(struct rte_mempool *mp, void * const *obj_table,
 	 */
 
 	/* Add elements back into the cache */
+	//将要放入cache的元素直接copy到cache
 	rte_memcpy(&cache_objs[0], obj_table, sizeof(void *) * n);
 
 	cache->len += n;
 
 	if (cache->len >= cache->flushthresh) {
+		//缓存的cache过多，将过多的cache入队到mp
 		rte_mempool_ops_enqueue_bulk(mp, &cache->objs[cache->size],
 				cache->len - cache->size);
 		cache->len = cache->size;
diff --git a/lib/librte_mempool/rte_mempool_ops.c b/lib/librte_mempool/rte_mempool_ops.c
index 22c5251eb..ce6f200a8 100644
--- a/lib/librte_mempool/rte_mempool_ops.c
+++ b/lib/librte_mempool/rte_mempool_ops.c
@@ -18,6 +18,7 @@ struct rte_mempool_ops_table rte_mempool_ops_table = {
 };
 
 /* add a new ops struct in rte_mempool_ops_table, return its index. */
+//操作集注册
 int
 rte_mempool_register_ops(const struct rte_mempool_ops *h)
 {
@@ -154,6 +155,7 @@ rte_mempool_ops_get_info(const struct rte_mempool *mp,
 
 
 /* sets mempool ops previously registered by rte_mempool_register_ops. */
+//通过名称查找操作集，并设置ops
 int
 rte_mempool_set_ops_byname(struct rte_mempool *mp, const char *name,
 	void *pool_config)
diff --git a/lib/librte_meter/rte_meter.h b/lib/librte_meter/rte_meter.h
index d69b11849..9bbb57013 100644
--- a/lib/librte_meter/rte_meter.h
+++ b/lib/librte_meter/rte_meter.h
@@ -414,6 +414,8 @@ struct rte_meter_trtcm_rfc4115 {
 	/**< Number of bytes currently available in the excess(E) token bucket */
 };
 
+//报文qos处理
+//可以参考此网页：http://www.cnblogs.com/yhp-smarthome/p/6901317.html （我没有细看）
 static inline enum rte_color
 rte_meter_srtcm_color_blind_check(struct rte_meter_srtcm *m,
 	struct rte_meter_srtcm_profile *p,
diff --git a/lib/librte_net/rte_arp.c b/lib/librte_net/rte_arp.c
index 784b7f48f..56e7e18da 100644
--- a/lib/librte_net/rte_arp.c
+++ b/lib/librte_net/rte_arp.c
@@ -7,6 +7,7 @@
 #include <rte_arp.h>
 
 #define RARP_PKT_SIZE	64
+//构造反向地址解析协议rarp报文
 struct rte_mbuf *
 rte_net_make_rarp_packet(struct rte_mempool *mpool,
 		const struct rte_ether_addr *mac)
@@ -30,8 +31,10 @@ rte_net_make_rarp_packet(struct rte_mempool *mpool,
 	}
 
 	/* Ethernet header. */
+	//设置广播mac
 	memset(eth_hdr->d_addr.addr_bytes, 0xff, RTE_ETHER_ADDR_LEN);
 	rte_ether_addr_copy(mac, &eth_hdr->s_addr);
+	//协议号为rarp协议
 	eth_hdr->ether_type = htons(RTE_ETHER_TYPE_RARP);
 
 	/* RARP header. */
@@ -40,12 +43,14 @@ rte_net_make_rarp_packet(struct rte_mempool *mpool,
 	rarp->arp_protocol = htons(RTE_ETHER_TYPE_IPV4);
 	rarp->arp_hlen = RTE_ETHER_ADDR_LEN;
 	rarp->arp_plen = 4;
+	//指明进行反向请求
 	rarp->arp_opcode  = htons(RTE_ARP_OP_REVREQUEST);
 
 	rte_ether_addr_copy(mac, &rarp->arp_data.arp_sha);
+	//设置目的方mac地址
 	rte_ether_addr_copy(mac, &rarp->arp_data.arp_tha);
 	memset(&rarp->arp_data.arp_sip, 0x00, 4);
-	memset(&rarp->arp_data.arp_tip, 0x00, 4);
+	memset(&rarp->arp_data.arp_tip, 0x00, 4);//填充0号ip
 
 	return mbuf;
 }
diff --git a/lib/librte_net/rte_ether.h b/lib/librte_net/rte_ether.h
index e069dc7fe..0d08f2c01 100644
--- a/lib/librte_net/rte_ether.h
+++ b/lib/librte_net/rte_ether.h
@@ -316,6 +316,7 @@ struct rte_vlan_hdr {
  *   - 0: Success
  *   - 1: not a vlan packet
  */
+//处理vlan strip操作
 static inline int rte_vlan_strip(struct rte_mbuf *m)
 {
 	struct rte_ether_hdr *eh
@@ -323,8 +324,9 @@ static inline int rte_vlan_strip(struct rte_mbuf *m)
 	struct rte_vlan_hdr *vh;
 
 	if (eh->ether_type != rte_cpu_to_be_16(RTE_ETHER_TYPE_VLAN))
-		return -1;
+		return -1;//非vlan报文，直接返回
 
+	//解析vlan头
 	vh = (struct rte_vlan_hdr *)(eh + 1);
 	m->ol_flags |= PKT_RX_VLAN | PKT_RX_VLAN_STRIPPED;
 	m->vlan_tci = rte_be_to_cpu_16(vh->vlan_tci);
@@ -348,12 +350,14 @@ static inline int rte_vlan_strip(struct rte_mbuf *m)
  *   -EPERM: mbuf is is shared overwriting would be unsafe
  *   -ENOSPC: not enough headroom in mbuf
  */
+//软件实现vlan头的插入
 static inline int rte_vlan_insert(struct rte_mbuf **m)
 {
 	struct rte_ether_hdr *oh, *nh;
 	struct rte_vlan_hdr *vh;
 
 	/* Can't insert header if mbuf is shared */
+	//针对共享的mbuf，直接clone一份再来修改
 	if (!RTE_MBUF_DIRECT(*m) || rte_mbuf_refcnt_read(*m) > 1)
 		return -EINVAL;
 
@@ -363,12 +367,17 @@ static inline int rte_vlan_insert(struct rte_mbuf **m)
 	if (nh == NULL)
 		return -ENOSPC;
 
+	//旧的以太头中目的mac,源mac前移到vlan头前
 	memmove(nh, oh, 2 * RTE_ETHER_ADDR_LEN);
+	//通过0x8100指定为vlan头
 	nh->ether_type = rte_cpu_to_be_16(RTE_ETHER_TYPE_VLAN);
 
+	//经过移动vlan头的合理位置为nh+1
 	vh = (struct rte_vlan_hdr *) (nh + 1);
+	//设置vlan id
 	vh->vlan_tci = rte_cpu_to_be_16((*m)->vlan_tci);
 
+	//软件已实现offload功能，清除相应标记
 	(*m)->ol_flags &= ~(PKT_RX_VLAN_STRIPPED | PKT_TX_VLAN);
 
 	if ((*m)->ol_flags & PKT_TX_TUNNEL_MASK)
diff --git a/lib/librte_net/rte_ip.h b/lib/librte_net/rte_ip.h
index 731ee4f21..c0c00ff1b 100644
--- a/lib/librte_net/rte_ip.h
+++ b/lib/librte_net/rte_ip.h
@@ -288,6 +288,7 @@ rte_ipv4_cksum(const struct rte_ipv4_hdr *ipv4_hdr)
  * @return
  *   The non-complemented checksum to set in the L4 header.
  */
+//计算ipv4假头部checksum
 static inline uint16_t
 rte_ipv4_phdr_cksum(const struct rte_ipv4_hdr *ipv4_hdr, uint64_t ol_flags)
 {
@@ -303,6 +304,7 @@ rte_ipv4_phdr_cksum(const struct rte_ipv4_hdr *ipv4_hdr, uint64_t ol_flags)
 	psd_hdr.dst_addr = ipv4_hdr->dst_addr;
 	psd_hdr.zero = 0;
 	psd_hdr.proto = ipv4_hdr->next_proto_id;
+	//注意，针对tso功能，不在假头checksum计算中包含len字段
 	if (ol_flags & PKT_TX_TCP_SEG) {
 		psd_hdr.len = 0;
 	} else {
diff --git a/lib/librte_net/rte_net.c b/lib/librte_net/rte_net.c
index 6f45b1339..9ce335170 100644
--- a/lib/librte_net/rte_net.c
+++ b/lib/librte_net/rte_net.c
@@ -124,6 +124,7 @@ ptype_inner_l4(uint8_t proto)
 }
 
 /* get the tunnel packet type if any, update proto and off. */
+//隧道类型解析
 static uint32_t
 ptype_tunnel(uint16_t *proto, const struct rte_mbuf *m,
 	uint32_t *off)
@@ -225,13 +226,14 @@ rte_net_skip_ip6_ext(uint16_t proto, const struct rte_mbuf *m, uint32_t *off,
 }
 
 /* parse mbuf data to get packet type */
+//解析报文获取报文的类型
 uint32_t rte_net_get_ptype(const struct rte_mbuf *m,
 	struct rte_net_hdr_lens *hdr_lens, uint32_t layers)
 {
 	struct rte_net_hdr_lens local_hdr_lens;
 	const struct rte_ether_hdr *eh;
 	struct rte_ether_hdr eh_copy;
-	uint32_t pkt_type = RTE_PTYPE_L2_ETHER;
+	uint32_t pkt_type = RTE_PTYPE_L2_ETHER;//以太包
 	uint32_t off = 0;
 	uint16_t proto;
 	int ret;
@@ -239,6 +241,7 @@ uint32_t rte_net_get_ptype(const struct rte_mbuf *m,
 	if (hdr_lens == NULL)
 		hdr_lens = &local_hdr_lens;
 
+	//定位到以太头
 	eh = rte_pktmbuf_read(m, off, sizeof(*eh), &eh_copy);
 	if (unlikely(eh == NULL))
 		return 0;
@@ -246,35 +249,39 @@ uint32_t rte_net_get_ptype(const struct rte_mbuf *m,
 	off = sizeof(*eh);
 	hdr_lens->l2_len = off;
 
+	//不关注二层，直接返回0
 	if ((layers & RTE_PTYPE_L2_MASK) == 0)
 		return 0;
 
+	//以太头协议指明负载为ipv4
 	if (proto == rte_cpu_to_be_16(RTE_ETHER_TYPE_IPV4))
 		goto l3; /* fast path if packet is IPv4 */
 
+	//以太头协议指明负载为vlan
 	if (proto == rte_cpu_to_be_16(RTE_ETHER_TYPE_VLAN)) {
 		const struct rte_vlan_hdr *vh;
 		struct rte_vlan_hdr vh_copy;
 
-		pkt_type = RTE_PTYPE_L2_ETHER_VLAN;
+		pkt_type = RTE_PTYPE_L2_ETHER_VLAN;//vlan包
 		vh = rte_pktmbuf_read(m, off, sizeof(*vh), &vh_copy);
 		if (unlikely(vh == NULL))
 			return pkt_type;
 		off += sizeof(*vh);
 		hdr_lens->l2_len += sizeof(*vh);
-		proto = vh->eth_proto;
+		proto = vh->eth_proto;//解vlan负载
 	} else if (proto == rte_cpu_to_be_16(RTE_ETHER_TYPE_QINQ)) {
+		//以太负载指明双vlan
 		const struct rte_vlan_hdr *vh;
 		struct rte_vlan_hdr vh_copy;
 
-		pkt_type = RTE_PTYPE_L2_ETHER_QINQ;
+		pkt_type = RTE_PTYPE_L2_ETHER_QINQ;//双q包
 		vh = rte_pktmbuf_read(m, off + sizeof(*vh), sizeof(*vh),
 			&vh_copy);
 		if (unlikely(vh == NULL))
 			return pkt_type;
-		off += 2 * sizeof(*vh);
+		off += 2 * sizeof(*vh);//跳过两个vlan头
 		hdr_lens->l2_len += 2 * sizeof(*vh);
-		proto = vh->eth_proto;
+		proto = vh->eth_proto;//解双vlan负载
 	} else if ((proto == rte_cpu_to_be_16(RTE_ETHER_TYPE_MPLS)) ||
 		(proto == rte_cpu_to_be_16(RTE_ETHER_TYPE_MPLSM))) {
 		unsigned int i;
@@ -321,7 +328,7 @@ uint32_t rte_net_get_ptype(const struct rte_mbuf *m,
 			return pkt_type;
 		}
 		proto = ip4h->next_proto_id;
-		pkt_type |= ptype_l4(proto);
+		pkt_type |= ptype_l4(proto);//ipv4 l4层协议
 	} else if (proto == rte_cpu_to_be_16(RTE_ETHER_TYPE_IPV6)) {
 		const struct rte_ipv6_hdr *ip6h;
 		struct rte_ipv6_hdr ip6h_copy;
@@ -331,7 +338,7 @@ uint32_t rte_net_get_ptype(const struct rte_mbuf *m,
 		if (unlikely(ip6h == NULL))
 			return pkt_type;
 
-		proto = ip6h->proto;
+		proto = ip6h->proto;//ipv6 l4层协议
 		hdr_lens->l3_len = sizeof(*ip6h);
 		off += hdr_lens->l3_len;
 		pkt_type |= ptype_l3_ip6(proto);
@@ -367,12 +374,14 @@ uint32_t rte_net_get_ptype(const struct rte_mbuf *m,
 		if (unlikely(th == NULL))
 			return pkt_type & (RTE_PTYPE_L2_MASK |
 				RTE_PTYPE_L3_MASK);
-		hdr_lens->l4_len = (th->data_off & 0xf0) >> 2;
+		hdr_lens->l4_len = (th->data_off & 0xf0) >> 2;//4层头长度
 		return pkt_type;
 	} else if ((pkt_type & RTE_PTYPE_L4_MASK) == RTE_PTYPE_L4_SCTP) {
+		//l4层负载为sctp
 		hdr_lens->l4_len = sizeof(struct rte_sctp_hdr);
 		return pkt_type;
 	} else {
+		//隧道协议解析
 		uint32_t prev_off = off;
 
 		hdr_lens->l4_len = 0;
@@ -392,6 +401,7 @@ uint32_t rte_net_get_ptype(const struct rte_mbuf *m,
 
 	hdr_lens->inner_l2_len = 0;
 	if (proto == rte_cpu_to_be_16(RTE_ETHER_TYPE_TEB)) {
+		//负载为以太包
 		eh = rte_pktmbuf_read(m, off, sizeof(*eh), &eh_copy);
 		if (unlikely(eh == NULL))
 			return pkt_type;
@@ -402,6 +412,7 @@ uint32_t rte_net_get_ptype(const struct rte_mbuf *m,
 	}
 
 	if (proto == rte_cpu_to_be_16(RTE_ETHER_TYPE_VLAN)) {
+		//负载为vlan
 		const struct rte_vlan_hdr *vh;
 		struct rte_vlan_hdr vh_copy;
 
@@ -414,6 +425,7 @@ uint32_t rte_net_get_ptype(const struct rte_mbuf *m,
 		hdr_lens->inner_l2_len += sizeof(*vh);
 		proto = vh->eth_proto;
 	} else if (proto == rte_cpu_to_be_16(RTE_ETHER_TYPE_QINQ)) {
+		//负载为双q
 		const struct rte_vlan_hdr *vh;
 		struct rte_vlan_hdr vh_copy;
 
@@ -431,6 +443,7 @@ uint32_t rte_net_get_ptype(const struct rte_mbuf *m,
 	if ((layers & RTE_PTYPE_INNER_L3_MASK) == 0)
 		return pkt_type;
 
+	//隧道内层解析
 	if (proto == rte_cpu_to_be_16(RTE_ETHER_TYPE_IPV4)) {
 		const struct rte_ipv4_hdr *ip4h;
 		struct rte_ipv4_hdr ip4h_copy;
diff --git a/lib/librte_net/rte_net.h b/lib/librte_net/rte_net.h
index 1560ecfa4..c3afb8696 100644
--- a/lib/librte_net/rte_net.h
+++ b/lib/librte_net/rte_net.h
@@ -110,6 +110,8 @@ uint32_t rte_net_get_ptype(const struct rte_mbuf *m,
  * @return
  *   0 if checksum is initialized properly
  */
+//为mbuf的内层ipv4,１，使ipv4头部checksum为０;2.udp,tcp计算假头部的checksum
+//注意，针对tso功能，不在假头checksum计算中包含len字段
 static inline int
 rte_net_intel_cksum_flags_prepare(struct rte_mbuf *m, uint64_t ol_flags)
 {
@@ -118,6 +120,7 @@ rte_net_intel_cksum_flags_prepare(struct rte_mbuf *m, uint64_t ol_flags)
 	struct rte_ipv6_hdr *ipv6_hdr;
 	struct rte_tcp_hdr *tcp_hdr;
 	struct rte_udp_hdr *udp_hdr;
+	//到内层l3层的offset
 	uint64_t inner_l3_offset = m->l2_len;
 
 #ifdef RTE_LIBRTE_ETHDEV_DEBUG
@@ -130,6 +133,7 @@ rte_net_intel_cksum_flags_prepare(struct rte_mbuf *m, uint64_t ol_flags)
 		return 0;
 #endif
 
+	//如果有outer_ip标记，则加上外层l2,l3头
 	if (ol_flags & (PKT_TX_OUTER_IPV4 | PKT_TX_OUTER_IPV6))
 		inner_l3_offset += m->outer_l2_len + m->outer_l3_len;
 
@@ -144,18 +148,23 @@ rte_net_intel_cksum_flags_prepare(struct rte_mbuf *m, uint64_t ol_flags)
 		return -ENOTSUP;
 #endif
 
+	//ipv4报文时，取内层ipv4头部
 	if (ol_flags & PKT_TX_IPV4) {
 		ipv4_hdr = rte_pktmbuf_mtod_offset(m, struct rte_ipv4_hdr *,
 				inner_l3_offset);
 
+		//如果需要offload checksum,则将ip头部置为０
 		if (ol_flags & PKT_TX_IP_CKSUM)
 			ipv4_hdr->hdr_checksum = 0;
 	}
 
+	//如果需要做udp checksum
 	if ((ol_flags & PKT_TX_L4_MASK) == PKT_TX_UDP_CKSUM) {
 		if (ol_flags & PKT_TX_IPV4) {
+		    	//取出udp头部
 			udp_hdr = (struct rte_udp_hdr *)((char *)ipv4_hdr +
 					m->l3_len);
+			//为udp填充ipv4层的假头部checksum
 			udp_hdr->dgram_cksum = rte_ipv4_phdr_cksum(ipv4_hdr,
 					ol_flags);
 		} else {
@@ -174,6 +183,7 @@ rte_net_intel_cksum_flags_prepare(struct rte_mbuf *m, uint64_t ol_flags)
 			/* non-TSO tcp or TSO */
 			tcp_hdr = (struct rte_tcp_hdr *)((char *)ipv4_hdr +
 					m->l3_len);
+			//为tcp计算假头部
 			tcp_hdr->cksum = rte_ipv4_phdr_cksum(ipv4_hdr,
 					ol_flags);
 		} else {
diff --git a/lib/librte_net/rte_tcp.h b/lib/librte_net/rte_tcp.h
index 06f623da8..6697b2640 100644
--- a/lib/librte_net/rte_tcp.h
+++ b/lib/librte_net/rte_tcp.h
@@ -30,8 +30,8 @@ struct rte_tcp_hdr {
 	rte_be16_t dst_port; /**< TCP destination port. */
 	rte_be32_t sent_seq; /**< TX data sequence number. */
 	rte_be32_t recv_ack; /**< RX data acknowledgment sequence number. */
-	uint8_t  data_off;   /**< Data offset. */
-	uint8_t  tcp_flags;  /**< TCP flags */
+	uint8_t  data_off;   /**< Data offset. */ //高位有四个预留位
+	uint8_t  tcp_flags;  /**< TCP flags */ //高位有两个预留位（6位有效，自低向高，依次为fin,syn,rst,push,ack,urg)
 	rte_be16_t rx_win;   /**< RX flow control window. */
 	rte_be16_t cksum;    /**< TCP checksum. */
 	rte_be16_t tcp_urp;  /**< TCP urgent pointer, if any. */
diff --git a/lib/librte_pci/rte_pci.c b/lib/librte_pci/rte_pci.c
index a753cf3ec..81d022388 100644
--- a/lib/librte_pci/rte_pci.c
+++ b/lib/librte_pci/rte_pci.c
@@ -98,6 +98,7 @@ rte_pci_device_name(const struct rte_pci_addr *addr,
 }
 
 int
+//检查两个pci地址是否一致
 rte_pci_addr_cmp(const struct rte_pci_addr *addr,
 	     const struct rte_pci_addr *addr2)
 {
@@ -130,6 +131,7 @@ rte_pci_addr_parse(const char *str, struct rte_pci_addr *addr)
 
 
 /* map a particular resource from a file */
+//实现对文件的映射
 void *
 pci_map_resource(void *requested_addr, int fd, off_t offset, size_t size,
 		 int additional_flags)
diff --git a/lib/librte_pdump/rte_pdump.c b/lib/librte_pdump/rte_pdump.c
index ac94fea93..191308246 100644
--- a/lib/librte_pdump/rte_pdump.c
+++ b/lib/librte_pdump/rte_pdump.c
@@ -57,8 +57,8 @@ struct pdump_response {
 };
 
 static struct pdump_rxtx_cbs {
-	struct rte_ring *ring;
-	struct rte_mempool *mp;
+	struct rte_ring *ring;//将copy好的mbuf入队到哪个ring中
+	struct rte_mempool *mp;//自哪个mbuf pool中申请mbuf
 	const struct rte_eth_rxtx_callback *cb;
 	void *filter;
 } rx_cbs[RTE_MAX_ETHPORTS][RTE_MAX_QUEUES_PER_PORT],
@@ -132,6 +132,7 @@ pdump_pktmbuf_copy(struct rte_mbuf *m, struct rte_mempool *mp)
 	return m_dup;
 }
 
+//将报文入队到cbs->ring中
 static inline void
 pdump_copy(struct rte_mbuf **pkts, uint16_t nb_pkts, void *user_params)
 {
@@ -147,14 +148,17 @@ pdump_copy(struct rte_mbuf **pkts, uint16_t nb_pkts, void *user_params)
 	cbs  = user_params;
 	ring = cbs->ring;
 	mp = cbs->mp;
+	//自mp中申请mbuf,并实现报文copy到pkts,再将pkts中报文统一入队到ring中
 	for (i = 0; i < nb_pkts; i++) {
 		p = pdump_pktmbuf_copy(pkts[i], mp);
 		if (p)
 			dup_bufs[d_pkts++] = p;
 	}
 
+	//将复制的报文统一入队
 	ring_enq = rte_ring_enqueue_burst(ring, (void *)dup_bufs, d_pkts, NULL);
 	if (unlikely(ring_enq < d_pkts)) {
+		//入队失败的报文统一丢弃
 		RTE_LOG(DEBUG, PDUMP,
 			"only %d of packets enqueued to ring\n", ring_enq);
 		do {
@@ -163,6 +167,7 @@ pdump_copy(struct rte_mbuf **pkts, uint16_t nb_pkts, void *user_params)
 	}
 }
 
+//capture收包回调
 static uint16_t
 pdump_rx(uint16_t port __rte_unused, uint16_t qidx __rte_unused,
 	struct rte_mbuf **pkts, uint16_t nb_pkts,
@@ -203,7 +208,7 @@ pdump_register_rx_callbacks(uint16_t end_q, uint16_t port, uint16_t queue,
 			cbs->ring = ring;
 			cbs->mp = mp;
 			cbs->cb = rte_eth_add_first_rx_callback(port, qid,
-								pdump_rx, cbs);
+								pdump_rx, cbs);//注册port的qid在收包时执行回调pdump_rx
 			if (cbs->cb == NULL) {
 				RTE_LOG(ERR, PDUMP,
 					"failed to add rx callback, errno=%d\n",
@@ -304,6 +309,7 @@ set_pdump_rxtx_cbs(const struct pdump_request *p)
 	flags = p->flags;
 	operation = p->op;
 	if (operation == ENABLE) {
+		//找到设备对应的port id
 		ret = rte_eth_dev_get_port_by_name(p->data.en_v1.device,
 				&port);
 		if (ret < 0) {
@@ -331,6 +337,7 @@ set_pdump_rxtx_cbs(const struct pdump_request *p)
 
 	/* validation if packet capture is for all queues */
 	if (queue == RTE_PDUMP_ALL_QUEUES) {
+		//capture所有的queues,则获取此port有多少个queue
 		struct rte_eth_dev_info dev_info;
 
 		ret = rte_eth_dev_info_get(port, &dev_info);
@@ -344,6 +351,7 @@ set_pdump_rxtx_cbs(const struct pdump_request *p)
 		nb_rx_q = dev_info.nb_rx_queues;
 		nb_tx_q = dev_info.nb_tx_queues;
 		if (nb_rx_q == 0 && flags & RTE_PDUMP_FLAG_RX) {
+			//rx queue为0，报错
 			RTE_LOG(ERR, PDUMP,
 				"number of rx queues cannot be 0\n");
 			return -EINVAL;
@@ -361,6 +369,7 @@ set_pdump_rxtx_cbs(const struct pdump_request *p)
 		}
 	}
 
+	//分别注册rx,tx的回调
 	/* register RX callback */
 	if (flags & RTE_PDUMP_FLAG_RX) {
 		end_q = (queue == RTE_PDUMP_ALL_QUEUES) ? nb_rx_q : queue + 1;
@@ -382,6 +391,9 @@ set_pdump_rxtx_cbs(const struct pdump_request *p)
 	return ret;
 }
 
+//此函数实现方式：对端通过unix socket发送相应的ring,
+//及mp地址给本端，本端自相应的mp中申请mbuf并copy，然后将copy后的mbuf入队
+//思路没问题，但要求发送端与接收端理解mbuf结构，理解ring结构，这种要求太高了
 static int
 pdump_server(const struct rte_mp_msg *mp_msg, const void *peer)
 {
@@ -390,6 +402,8 @@ pdump_server(const struct rte_mp_msg *mp_msg, const void *peer)
 	struct pdump_response *resp = (struct pdump_response *)&mp_resp.param;
 
 	/* recv client requests */
+	//这太难用了，对端需要指出设备将报文发送到哪个ring,这种在多进程间使用还行，capture到普通进程
+	//时，普通进程这么传地址肯定不行
 	if (mp_msg->len_param != sizeof(*cli_req)) {
 		RTE_LOG(ERR, PDUMP, "failed to recv from client\n");
 		resp->err_value = -EINVAL;
@@ -415,6 +429,7 @@ pdump_server(const struct rte_mp_msg *mp_msg, const void *peer)
 int
 rte_pdump_init(void)
 {
+	//注册action entry,收到PDUMP_MP类消息，将由pdump_server进行处理
 	int ret = rte_mp_action_register(PDUMP_MP, pdump_server);
 	if (ret && rte_errno != ENOTSUP)
 		return -1;
diff --git a/lib/librte_pipeline/rte_pipeline.c b/lib/librte_pipeline/rte_pipeline.c
index f5f397d29..749b508b5 100644
--- a/lib/librte_pipeline/rte_pipeline.c
+++ b/lib/librte_pipeline/rte_pipeline.c
@@ -47,19 +47,19 @@
 
 struct rte_port_in {
 	/* Input parameters */
-	struct rte_port_in_ops ops;
-	rte_pipeline_port_in_action_handler f_action;
-	void *arg_ah;
+	struct rte_port_in_ops ops;//h_port的创建，释放操作函数
+	rte_pipeline_port_in_action_handler f_action;//报文input-callback
+	void *arg_ah;//input-callback参数
 	uint32_t burst_size;
 
 	/* The table to which this port is connected */
-	uint32_t table_id;
+	uint32_t table_id;//此in-port连接到那个表
 
 	/* Handle to low-level port */
-	void *h_port;
+	void *h_port;//port对应的底层句柄
 
 	/* List of enabled ports */
-	struct rte_port_in *next;
+	struct rte_port_in *next;//将开启的port串成一串
 
 	/* Statistics */
 	uint64_t n_pkts_dropped_by_ah;
@@ -80,15 +80,15 @@ struct rte_port_out {
 
 struct rte_table {
 	/* Input parameters */
-	struct rte_table_ops ops;
-	rte_pipeline_table_action_handler_hit f_action_hit;
-	rte_pipeline_table_action_handler_miss f_action_miss;
-	void *arg_ah;
-	struct rte_pipeline_table_entry *default_entry;
-	uint32_t entry_size;
+	struct rte_table_ops ops;//表操作函数
+	rte_pipeline_table_action_handler_hit f_action_hit;//表命中时action
+	rte_pipeline_table_action_handler_miss f_action_miss;//表missing时action
+	void *arg_ah;//hit,miss回调需要的参数
+	struct rte_pipeline_table_entry *default_entry;//默认表项
+	uint32_t entry_size;//表项大小
 
-	uint32_t table_next_id;
-	uint32_t table_next_id_valid;
+	uint32_t table_next_id;//下一个表，用于在本表处理完成后，交给其它表处理
+	uint32_t table_next_id_valid;//table_next_id是否包含有效值
 
 	/* Handle to the low-level table object */
 	void *h_table;
@@ -104,30 +104,30 @@ struct rte_table {
 
 struct rte_pipeline {
 	/* Input parameters */
-	char name[RTE_PIPELINE_MAX_NAME_SZ];
-	int socket_id;
-	uint32_t offset_port_id;
+	char name[RTE_PIPELINE_MAX_NAME_SZ];//pipeline名称
+	int socket_id;//内存位于哪个socket
+	uint32_t offset_port_id;//port_id自mbuf中哪个偏移量中得出
 
 	/* Internal tables */
-	struct rte_port_in ports_in[RTE_PIPELINE_PORT_IN_MAX];
+	struct rte_port_in ports_in[RTE_PIPELINE_PORT_IN_MAX];//保存in-port
 	struct rte_port_out ports_out[RTE_PIPELINE_PORT_OUT_MAX];
-	struct rte_table tables[RTE_PIPELINE_TABLE_MAX];
+	struct rte_table tables[RTE_PIPELINE_TABLE_MAX];//按表编号存放不同的表
 
 	/* Occupancy of internal tables */
 	uint32_t num_ports_in;
 	uint32_t num_ports_out;
-	uint32_t num_tables;
+	uint32_t num_tables;//有多少表（也用于分配表编号）
 
 	/* List of enabled ports */
-	uint64_t enabled_port_in_mask;
+	uint64_t enabled_port_in_mask;//开启的port（用一个bit表示一个port)
 	struct rte_port_in *port_in_next;
 
 	/* Pipeline run structures */
-	struct rte_mbuf *pkts[RTE_PORT_IN_BURST_SIZE_MAX];
-	struct rte_pipeline_table_entry *entries[RTE_PORT_IN_BURST_SIZE_MAX];
-	uint64_t action_mask0[RTE_PIPELINE_ACTIONS];
+	struct rte_mbuf *pkts[RTE_PORT_IN_BURST_SIZE_MAX];//用来缓存需要处理的报文
+	struct rte_pipeline_table_entry *entries[RTE_PORT_IN_BURST_SIZE_MAX];//用于缓存被命中的表项
+	uint64_t action_mask0[RTE_PIPELINE_ACTIONS];//指出某个报文执行某种action,与pkts_mask相对应
 	uint64_t action_mask1[RTE_PIPELINE_ACTIONS];
-	uint64_t pkts_mask;
+	uint64_t pkts_mask;//每个bit对应一个pkt
 	uint64_t n_pkts_ah_drop;
 	uint64_t pkts_drop_mask;
 } __rte_cache_aligned;
@@ -137,14 +137,20 @@ rte_mask_get_next(uint64_t mask, uint32_t pos)
 {
 	uint64_t mask_rot = (mask << ((63 - pos) & 0x3F)) |
 			(mask >> ((pos + 1) & 0x3F));
+	//记算后继0的数目，与clzll相对。
 	return (__builtin_ctzll(mask_rot) - (63 - pos)) & 0x3F;
 }
 
 static inline uint32_t
 rte_mask_get_prev(uint64_t mask, uint32_t pos)
 {
+	//pos最大到64，故采用ox3f进行与。
+	//右移pos位后，mask的0－pos位的数据将被清除
+	//左移 64-pos位后，mask的64-pos位数据将被清除
+	//两者相或，则相当于实现了循环移位。
 	uint64_t mask_rot = (mask >> (pos & 0x3F)) |
 			(mask << ((64 - pos) & 0x3F));
+	//记算前导0的数目，并算上pos偏移，即可知道下一个'1'在那个位置
 	return ((63 - __builtin_clzll(mask_rot)) + pos) & 0x3F;
 }
 
@@ -161,6 +167,7 @@ rte_pipeline_port_out_free(struct rte_port_out *port);
  * Pipeline
  *
  */
+//检查pipeline参数
 static int
 rte_pipeline_check_params(struct rte_pipeline_params *params)
 {
@@ -188,6 +195,7 @@ rte_pipeline_check_params(struct rte_pipeline_params *params)
 	return 0;
 }
 
+//创建一个rte pipeline对象
 struct rte_pipeline *
 rte_pipeline_create(struct rte_pipeline_params *params)
 {
@@ -197,6 +205,7 @@ rte_pipeline_create(struct rte_pipeline_params *params)
 	/* Check input parameters */
 	status = rte_pipeline_check_params(params);
 	if (status != 0) {
+		//参数有误，创建失败
 		RTE_LOG(ERR, PIPELINE,
 			"%s: Pipeline params check failed (%d)\n",
 			__func__, status);
@@ -204,10 +213,12 @@ rte_pipeline_create(struct rte_pipeline_params *params)
 	}
 
 	/* Allocate memory for the pipeline on requested socket */
+	//在指定socket上申请内存
 	p = rte_zmalloc_socket("PIPELINE", sizeof(struct rte_pipeline),
 			RTE_CACHE_LINE_SIZE, params->socket_id);
 
 	if (p == NULL) {
+		//申请内存失败
 		RTE_LOG(ERR, PIPELINE,
 			"%s: Pipeline memory allocation failed\n", __func__);
 		return NULL;
@@ -273,6 +284,7 @@ rte_pipeline_free(struct rte_pipeline *p)
  * Table
  *
  */
+//表创建参数检查
 static int
 rte_table_check_params(struct rte_pipeline *p,
 		struct rte_pipeline_table_params *params,
@@ -324,6 +336,7 @@ rte_table_check_params(struct rte_pipeline *p,
 	return 0;
 }
 
+//pipeline创建table
 int
 rte_pipeline_table_create(struct rte_pipeline *p,
 		struct rte_pipeline_table_params *params,
@@ -340,10 +353,12 @@ rte_pipeline_table_create(struct rte_pipeline *p,
 	if (status != 0)
 		return status;
 
+	//取出当前表编号
 	id = p->num_tables;
 	table = &p->tables[id];
 
 	/* Allocate space for the default table entry */
+	//表项大小
 	entry_size = sizeof(struct rte_pipeline_table_entry) +
 		params->action_data_size;
 	default_entry = rte_zmalloc_socket(
@@ -357,14 +372,14 @@ rte_pipeline_table_create(struct rte_pipeline *p,
 	/* Create the table */
 	h_table = params->ops->f_create(params->arg_create, p->socket_id,
 		entry_size);
-	if (h_table == NULL) {
+	if (h_table == NULL) {//创建表失败
 		rte_free(default_entry);
 		RTE_LOG(ERR, PIPELINE, "%s: Table creation failed\n", __func__);
 		return -EINVAL;
 	}
 
 	/* Commit current table to the pipeline */
-	p->num_tables++;
+	p->num_tables++;//表编号增加
 	*table_id = id;
 
 	/* Save input parameters */
@@ -375,8 +390,8 @@ rte_pipeline_table_create(struct rte_pipeline *p,
 	table->entry_size = entry_size;
 
 	/* Clear the lookup miss actions (to be set later through API) */
-	table->default_entry = default_entry;
-	table->default_entry->action = RTE_PIPELINE_ACTION_DROP;
+	table->default_entry = default_entry;//设置默认表项的空间
+	table->default_entry->action = RTE_PIPELINE_ACTION_DROP;//默认是drop
 
 	/* Initialize table internal data structure */
 	table->h_table = h_table;
@@ -395,6 +410,7 @@ rte_pipeline_table_free(struct rte_table *table)
 	rte_free(table->default_entry);
 }
 
+//添加默认表项
 int
 rte_pipeline_table_default_entry_add(struct rte_pipeline *p,
 	uint32_t table_id,
@@ -417,6 +433,7 @@ rte_pipeline_table_default_entry_add(struct rte_pipeline *p,
 	}
 
 	if (table_id >= p->num_tables) {
+		//表索引检查
 		RTE_LOG(ERR, PIPELINE,
 			"%s: table_id %d out of range\n", __func__, table_id);
 		return -EINVAL;
@@ -427,24 +444,28 @@ rte_pipeline_table_default_entry_add(struct rte_pipeline *p,
 	if ((default_entry->action == RTE_PIPELINE_ACTION_TABLE) &&
 		table->table_next_id_valid &&
 		(default_entry->table_id != table->table_next_id)) {
+		//下一跳表编号，无效
 		RTE_LOG(ERR, PIPELINE,
 			"%s: Tree-like topologies not allowed\n", __func__);
 		return -EINVAL;
 	}
 
 	/* Set the lookup miss actions */
+	//如果default的行为是跳转至某表，则更新table_next_id为default表项值
 	if ((default_entry->action == RTE_PIPELINE_ACTION_TABLE) &&
 		(table->table_next_id_valid == 0)) {
 		table->table_next_id = default_entry->table_id;
 		table->table_next_id_valid = 1;
 	}
 
+	//设置表的失配项
 	memcpy(table->default_entry, default_entry, table->entry_size);
 
 	*default_entry_ptr = table->default_entry;
 	return 0;
 }
 
+//删除默认表项
 int
 rte_pipeline_table_default_entry_delete(struct rte_pipeline *p,
 		uint32_t table_id,
@@ -472,12 +493,14 @@ rte_pipeline_table_default_entry_delete(struct rte_pipeline *p,
 		memcpy(entry, table->default_entry, table->entry_size);
 
 	/* Clear the lookup miss actions */
+	//当默认项不存在时，默认是drop动作
 	memset(table->default_entry, 0, table->entry_size);
 	table->default_entry->action = RTE_PIPELINE_ACTION_DROP;
 
 	return 0;
 }
 
+//添加表项
 int
 rte_pipeline_table_entry_add(struct rte_pipeline *p,
 		uint32_t table_id,
@@ -512,7 +535,7 @@ rte_pipeline_table_entry_add(struct rte_pipeline *p,
 		return -EINVAL;
 	}
 
-	table = &p->tables[table_id];
+	table = &p->tables[table_id];//取表
 
 	if (table->ops.f_add == NULL) {
 		RTE_LOG(ERR, PIPELINE, "%s: f_add function pointer NULL\n",
@@ -522,23 +545,26 @@ rte_pipeline_table_entry_add(struct rte_pipeline *p,
 
 	if ((entry->action == RTE_PIPELINE_ACTION_TABLE) &&
 		table->table_next_id_valid &&
-		(entry->table_id != table->table_next_id)) {
+		(entry->table_id != table->table_next_id)) {//跳向指定表时，表必须与next_id相等
 		RTE_LOG(ERR, PIPELINE,
 			"%s: Tree-like topologies not allowed\n", __func__);
 		return -EINVAL;
 	}
 
 	/* Add entry */
+	//如果表项的action是跳表，则next_id与entry的table_id相等
 	if ((entry->action == RTE_PIPELINE_ACTION_TABLE) &&
 		(table->table_next_id_valid == 0)) {
 		table->table_next_id = entry->table_id;
 		table->table_next_id_valid = 1;
 	}
 
+	//加入表项
 	return (table->ops.f_add)(table->h_table, key, (void *) entry,
 		key_found, (void **) entry_ptr);
 }
 
+//表项删除
 int
 rte_pipeline_table_entry_delete(struct rte_pipeline *p,
 		uint32_t table_id,
@@ -578,6 +604,7 @@ rte_pipeline_table_entry_delete(struct rte_pipeline *p,
 	return (table->ops.f_delete)(table->h_table, key, key_found, entry);
 }
 
+//批量表项添加
 int rte_pipeline_table_entry_add_bulk(struct rte_pipeline *p,
 	uint32_t table_id,
 	void **keys,
@@ -613,7 +640,7 @@ int rte_pipeline_table_entry_add_bulk(struct rte_pipeline *p,
 		return -EINVAL;
 	}
 
-	table = &p->tables[table_id];
+	table = &p->tables[table_id];//按索引取表
 
 	if (table->ops.f_add_bulk == NULL) {
 		RTE_LOG(ERR, PIPELINE, "%s: f_add_bulk function pointer NULL\n",
@@ -640,10 +667,12 @@ int rte_pipeline_table_entry_add_bulk(struct rte_pipeline *p,
 		}
 	}
 
+	//调用add_bulk批量加入entries
 	return (table->ops.f_add_bulk)(table->h_table, keys, (void **) entries,
 		n_keys, key_found, (void **) entries_ptr);
 }
 
+//批量删除
 int rte_pipeline_table_entry_delete_bulk(struct rte_pipeline *p,
 	uint32_t table_id,
 	void **keys,
@@ -728,6 +757,7 @@ rte_pipeline_port_in_check_params(struct rte_pipeline *p,
 	}
 
 	/* burst_size */
+	//burst大小需要在0到64以内
 	if ((params->burst_size == 0) ||
 		(params->burst_size > RTE_PORT_IN_BURST_SIZE_MAX)) {
 		RTE_LOG(ERR, PIPELINE, "%s: invalid value for burst_size\n",
@@ -745,6 +775,7 @@ rte_pipeline_port_in_check_params(struct rte_pipeline *p,
 	return 0;
 }
 
+//out-port参数检查
 static int
 rte_pipeline_port_out_check_params(struct rte_pipeline *p,
 		struct rte_pipeline_port_out_params *params,
@@ -802,6 +833,7 @@ rte_pipeline_port_out_check_params(struct rte_pipeline *p,
 	return 0;
 }
 
+//创建port-in
 int
 rte_pipeline_port_in_create(struct rte_pipeline *p,
 		struct rte_pipeline_port_in_params *params,
@@ -821,6 +853,7 @@ rte_pipeline_port_in_create(struct rte_pipeline *p,
 	port = &p->ports_in[id];
 
 	/* Create the port */
+	//创建in-port
 	h_port = params->ops->f_create(params->arg_create, p->socket_id);
 	if (h_port == NULL) {
 		RTE_LOG(ERR, PIPELINE, "%s: Port creation failed\n", __func__);
@@ -828,7 +861,7 @@ rte_pipeline_port_in_create(struct rte_pipeline *p,
 	}
 
 	/* Commit current table to the pipeline */
-	p->num_ports_in++;
+	p->num_ports_in++;//in-port数增加了
 	*port_id = id;
 
 	/* Save input parameters */
@@ -852,6 +885,7 @@ rte_pipeline_port_in_free(struct rte_port_in *port)
 		port->ops.f_free(port->h_port);
 }
 
+//创建port-out
 int
 rte_pipeline_port_out_create(struct rte_pipeline *p,
 		struct rte_pipeline_port_out_params *params,
@@ -899,6 +933,7 @@ rte_pipeline_port_out_free(struct rte_port_out *port)
 		port->ops.f_free(port->h_port);
 }
 
+//设置port_id in方向从属于table_id
 int
 rte_pipeline_port_in_connect_to_table(struct rte_pipeline *p,
 		uint32_t port_id,
@@ -933,6 +968,7 @@ rte_pipeline_port_in_connect_to_table(struct rte_pipeline *p,
 	return 0;
 }
 
+//在pipeline中使能port_in
 int
 rte_pipeline_port_in_enable(struct rte_pipeline *p, uint32_t port_id)
 {
@@ -959,17 +995,19 @@ rte_pipeline_port_in_enable(struct rte_pipeline *p, uint32_t port_id)
 	/* Return if current input port is already enabled */
 	port_mask = 1LLU << port_id;
 	if (p->enabled_port_in_mask & port_mask)
-		return 0;
+		return 0;//已开启，则不再设置
 
 	p->enabled_port_in_mask |= port_mask;
 
 	/* Add current input port to the pipeline chain of enabled ports */
+	//port_id前后被使能的两个port
 	port_prev_id = rte_mask_get_prev(p->enabled_port_in_mask, port_id);
 	port_next_id = rte_mask_get_next(p->enabled_port_in_mask, port_id);
 
 	port_prev = &p->ports_in[port_prev_id];
 	port_next = &p->ports_in[port_next_id];
 
+	//按顺序组成一个链表
 	port_prev->next = port;
 	port->next = port_next;
 
@@ -1036,6 +1074,7 @@ rte_pipeline_port_in_disable(struct rte_pipeline *p, uint32_t port_id)
  * Pipeline run-time
  *
  */
+//参数校查及所有in-port关联table检查
 int
 rte_pipeline_check(struct rte_pipeline *p)
 {
@@ -1067,6 +1106,7 @@ rte_pipeline_check(struct rte_pipeline *p)
 	}
 
 	/* Check that all input ports are connected */
+	//确保所有的port_in均被关联到table
 	for (port_in_id = 0; port_in_id < p->num_ports_in; port_in_id++) {
 		struct rte_port_in *port_in = &p->ports_in[port_in_id];
 
@@ -1081,6 +1121,7 @@ rte_pipeline_check(struct rte_pipeline *p)
 	return 0;
 }
 
+//将pkts_mask对应的报文，分区到不同的action掩码上去
 static inline void
 rte_pipeline_compute_masks(struct rte_pipeline *p, uint64_t pkts_mask)
 {
@@ -1090,6 +1131,8 @@ rte_pipeline_compute_masks(struct rte_pipeline *p, uint64_t pkts_mask)
 	p->action_mask1[RTE_PIPELINE_ACTION_TABLE] = 0;
 
 	if ((pkts_mask & (pkts_mask + 1)) == 0) {
+		//pkts_mask是一个2**n-1的形式
+		//GCC有一个叫做__builtin_popcount的内建函数，它可以精确的计算1的个数。
 		uint64_t n_pkts = __builtin_popcountll(pkts_mask);
 		uint32_t i;
 
@@ -1100,6 +1143,7 @@ rte_pipeline_compute_masks(struct rte_pipeline *p, uint64_t pkts_mask)
 			p->action_mask1[pos] |= pkt_mask;
 		}
 	} else {
+		//非（2**n）-1的形式
 		uint32_t i;
 
 		for (i = 0; i < RTE_PORT_IN_BURST_SIZE_MAX; i++) {
@@ -1115,6 +1159,7 @@ rte_pipeline_compute_masks(struct rte_pipeline *p, uint64_t pkts_mask)
 	}
 }
 
+//批量输出到接口port_id
 static inline void
 rte_pipeline_action_handler_port_bulk(struct rte_pipeline *p,
 	uint64_t pkts_mask, uint32_t port_id)
@@ -1124,6 +1169,7 @@ rte_pipeline_action_handler_port_bulk(struct rte_pipeline *p,
 	p->pkts_mask = pkts_mask;
 
 	/* Output port user actions */
+	//如果output port有用户定义的回调，则执行回调
 	if (port_out->f_action != NULL) {
 		port_out->f_action(p, p->pkts, pkts_mask, port_out->arg_ah);
 
@@ -1132,12 +1178,15 @@ rte_pipeline_action_handler_port_bulk(struct rte_pipeline *p,
 	}
 
 	/* Output port TX */
+	//输出至指定port
 	if (p->pkts_mask != 0)
 		port_out->ops.f_tx_bulk(port_out->h_port,
 			p->pkts,
 			p->pkts_mask);
 }
 
+//执行action,输出到port
+//在输出到port前，用户可定义f_action进行提前处理
 static inline void
 rte_pipeline_action_handler_port(struct rte_pipeline *p, uint64_t pkts_mask)
 {
@@ -1151,12 +1200,13 @@ rte_pipeline_action_handler_port(struct rte_pipeline *p, uint64_t pkts_mask)
 			struct rte_mbuf *pkt = p->pkts[i];
 			uint32_t port_out_id = p->entries[i]->port_id;
 			struct rte_port_out *port_out =
-				&p->ports_out[port_out_id];
+				&p->ports_out[port_out_id];//需要输出到port_out
 
 			/* Output port user actions */
 			if (port_out->f_action == NULL) /* Output port TX */
 				port_out->ops.f_tx(port_out->h_port, pkt);
 			else {
+				//用户定义了f_action,则先调用action,再调用f_tx进行输出
 				uint64_t pkt_mask = 1LLU << i;
 
 				port_out->f_action(p,
@@ -1193,6 +1243,7 @@ rte_pipeline_action_handler_port(struct rte_pipeline *p, uint64_t pkts_mask)
 			if (port_out->f_action == NULL) /* Output port TX */
 				port_out->ops.f_tx(port_out->h_port, pkt);
 			else {
+				//用户定义了f_action,则先调用action,再调用f_tx进行输出
 				port_out->f_action(p,
 					p->pkts,
 					pkt_mask,
@@ -1210,6 +1261,7 @@ rte_pipeline_action_handler_port(struct rte_pipeline *p, uint64_t pkts_mask)
 	}
 }
 
+//action处理，port_meta，通过p->offset_port_id自mbuf得出port_id
 static inline void
 rte_pipeline_action_handler_port_meta(struct rte_pipeline *p,
 	uint64_t pkts_mask)
@@ -1226,12 +1278,13 @@ rte_pipeline_action_handler_port_meta(struct rte_pipeline *p,
 				RTE_MBUF_METADATA_UINT32(pkt,
 					p->offset_port_id);
 			struct rte_port_out *port_out = &p->ports_out[
-				port_out_id];
+				port_out_id];//输出port
 
 			/* Output port user actions */
 			if (port_out->f_action == NULL) /* Output port TX */
 				port_out->ops.f_tx(port_out->h_port, pkt);
 			else {
+				//用户定义了f_action,则先调用action,再调用f_tx进行输出
 				uint64_t pkt_mask = 1LLU << i;
 
 				port_out->f_action(p,
@@ -1286,6 +1339,7 @@ rte_pipeline_action_handler_port_meta(struct rte_pipeline *p,
 	}
 }
 
+//action处理,丢包
 static inline void
 rte_pipeline_action_handler_drop(struct rte_pipeline *p, uint64_t pkts_mask)
 {
@@ -1297,7 +1351,7 @@ rte_pipeline_action_handler_drop(struct rte_pipeline *p, uint64_t pkts_mask)
 			rte_pktmbuf_free(p->pkts[i]);
 	} else {
 		uint32_t i;
-
+		//非连续的1，按位计算，并进行释放
 		for (i = 0; i < RTE_PORT_IN_BURST_SIZE_MAX; i++) {
 			uint64_t pkt_mask = 1LLU << i;
 
@@ -1309,9 +1363,21 @@ rte_pipeline_action_handler_drop(struct rte_pipeline *p, uint64_t pkts_mask)
 	}
 }
 
+//定义报文处理基本流程：
+//1.通过port_in.ops.f_rx进行收取
+//2.执行port_in.f_action
+//3.查port_in对应的表
+//3.1 如果miss,有table.f_action_miss ，则执行;并按table.default_entry的action进行执行，见4
+//3.2 如果hit,有table.f_action_hit,则执行，并接对应action进行执行，见4
+//4.执行action
+//	4.1 drop动作，直接丢
+//  4.2 outport动作，按动作要求的port,进行输出，先执行port_out.f_action,再执行port_out.ops.f_tx进行发送
+//  4.3 port-meta动作，自mbuf中按动作定义的offset得到输出port,先执行port_out.f_action,再执行port_out.ops.f_tx进行发送
+//  4.4 table动作，取出新的表id,table.table_next_id 将其定义为port_in对应的表，跳到（3）步开始执行
 int
 rte_pipeline_run(struct rte_pipeline *p)
 {
+	//取出当前需要收取的port
 	struct rte_port_in *port_in = p->port_in_next;
 	uint32_t n_pkts, table_id;
 
@@ -1319,20 +1385,25 @@ rte_pipeline_run(struct rte_pipeline *p)
 		return 0;
 
 	/* Input port RX */
+	//自port_in收取报文(依据接口的不同类型，调用不同的函数进行处理）
 	n_pkts = port_in->ops.f_rx(port_in->h_port, p->pkts,
 		port_in->burst_size);
 	if (n_pkts == 0) {
+		//未收取到报文，切换下次收取的port,本次结束
 		p->port_in_next = port_in->next;
 		return 0;
 	}
 
+	//收取到N个报文，则设置N位的bit位（低位开始）
 	p->pkts_mask = RTE_LEN2MASK(n_pkts, uint64_t);
+	//各action标位清0
 	p->action_mask0[RTE_PIPELINE_ACTION_DROP] = 0;
 	p->action_mask0[RTE_PIPELINE_ACTION_PORT] = 0;
 	p->action_mask0[RTE_PIPELINE_ACTION_PORT_META] = 0;
 	p->action_mask0[RTE_PIPELINE_ACTION_TABLE] = 0;
 
 	/* Input port user actions */
+	//port收到后执行用户定义action
 	if (port_in->f_action != NULL) {
 		port_in->f_action(p, p->pkts, n_pkts, port_in->arg_ah);
 
@@ -1341,24 +1412,29 @@ rte_pipeline_run(struct rte_pipeline *p)
 	}
 
 	/* Table */
+	//查询此port_in对应的table
 	for (table_id = port_in->table_id; p->pkts_mask != 0; ) {
 		struct rte_table *table;
 		uint64_t lookup_hit_mask, lookup_miss_mask;
 
 		/* Lookup */
+		//针对具体表进行查询
 		table = &p->tables[table_id];
 		table->ops.f_lookup(table->h_table, p->pkts, p->pkts_mask,
 			&lookup_hit_mask, (void **) p->entries);
+		//lookup_hit_mask记录的是命中表项的报文掩码，lookup_miss_mask即为未命中表项的报文掩码
 		lookup_miss_mask = p->pkts_mask & (~lookup_hit_mask);
 
 		/* Lookup miss */
 		if (lookup_miss_mask != 0) {
+			//有未查询到表项的
 			struct rte_pipeline_table_entry *default_entry =
 				table->default_entry;
 
 			p->pkts_mask = lookup_miss_mask;
 
 			/* Table user actions */
+			//如果有用户定义的miss处理，则进行处理
 			if (table->f_action_miss != NULL) {
 				table->f_action_miss(p,
 					p->pkts,
@@ -1373,10 +1449,13 @@ rte_pipeline_run(struct rte_pipeline *p)
 			/* Table reserved actions */
 			if ((default_entry->action == RTE_PIPELINE_ACTION_PORT) &&
 				(p->pkts_mask != 0))
+				//如果默认表项需要输出到port，则批量输出到port
 				rte_pipeline_action_handler_port_bulk(p,
 					p->pkts_mask,
 					default_entry->port_id);
 			else {
+				//要么drop,要么port-meta,要么next-table
+				//这里将其先分类到不同的action_mask0上去
 				uint32_t pos = default_entry->action;
 
 				RTE_PIPELINE_STATS_TABLE_DROP0(p);
@@ -1390,9 +1469,11 @@ rte_pipeline_run(struct rte_pipeline *p)
 
 		/* Lookup hit */
 		if (lookup_hit_mask != 0) {
+			//有命中表项的报文
 			p->pkts_mask = lookup_hit_mask;
 
 			/* Table user actions */
+			//如果有hit的自定义处理，则调用
 			if (table->f_action_hit != NULL) {
 				table->f_action_hit(p,
 					p->pkts,
@@ -1405,6 +1486,7 @@ rte_pipeline_run(struct rte_pipeline *p)
 			}
 
 			/* Table reserved actions */
+			//对于命中的报文，计算其对应的各action掩码
 			RTE_PIPELINE_STATS_TABLE_DROP0(p);
 			rte_pipeline_compute_masks(p, p->pkts_mask);
 			p->action_mask0[RTE_PIPELINE_ACTION_DROP] |=
@@ -1425,11 +1507,13 @@ rte_pipeline_run(struct rte_pipeline *p)
 		}
 
 		/* Prepare for next iteration */
+		//对于需要交付给其它表处理的报文，交给其它表处理，继续循环
 		p->pkts_mask = p->action_mask0[RTE_PIPELINE_ACTION_TABLE];
 		table_id = table->table_next_id;
 		p->action_mask0[RTE_PIPELINE_ACTION_TABLE] = 0;
 	}
 
+	//按action处理
 	/* Table reserved action PORT */
 	rte_pipeline_action_handler_port(p,
 		p->action_mask0[RTE_PIPELINE_ACTION_PORT]);
@@ -1443,7 +1527,7 @@ rte_pipeline_run(struct rte_pipeline *p)
 		p->action_mask0[RTE_PIPELINE_ACTION_DROP]);
 
 	/* Pick candidate for next port IN to serve */
-	p->port_in_next = port_in->next;
+	p->port_in_next = port_in->next;//切换到下一个port
 
 	return (int) n_pkts;
 }
@@ -1460,6 +1544,7 @@ rte_pipeline_flush(struct rte_pipeline *p)
 		return -EINVAL;
 	}
 
+	//对所有output_port进行flush
 	for (port_id = 0; port_id < p->num_ports_out; port_id++) {
 		struct rte_port_out *port = &p->ports_out[port_id];
 
diff --git a/lib/librte_pipeline/rte_pipeline.h b/lib/librte_pipeline/rte_pipeline.h
index 3cfb6868f..5bd699506 100644
--- a/lib/librte_pipeline/rte_pipeline.h
+++ b/lib/librte_pipeline/rte_pipeline.h
@@ -17,6 +17,8 @@ extern "C" {
  * a standard methodology (logically similar to OpenFlow) for rapid development
  * of complex packet processing pipelines out of ports, tables and actions.
  *
+ * 这个工具是DPDK包框架工具套件的一部分，提供了一个标准的方法论（逻辑同OpenFlow)
+ *
  * <B>Basic operation.</B> A pipeline is constructed by connecting its input
  * ports to its output ports through a chain of lookup tables. As result of
  * lookup operation into the current table, one of the table entries (or the
@@ -26,6 +28,8 @@ extern "C" {
  * table action handler, while the reserved actions define the next hop for the
  * current packet (either another table, an output port or packet drop) and are
  * handled transparently by the framework.
+ * pipeline 将input ports和output port之间通过一组查询连接起来
+ * 查询的结果会提供actions
  *
  * <B>Initialization and run-time flows.</B> Once all the pipeline elements
  * (input ports, tables, output ports) have been created, input ports connected
@@ -76,7 +80,7 @@ struct rte_pipeline_params {
 
 	/** CPU socket ID where memory for the pipeline and its elements (ports
 	and tables) should be allocated */
-	int socket_id;
+	int socket_id;//采用那个socket上的内存
 
 	/** Offset within packet meta-data to port_id to be used by action
 	"Send packet to output port read from packet meta-data". Has to be
@@ -180,16 +184,16 @@ int rte_pipeline_flush(struct rte_pipeline *p);
 /** Reserved actions */
 enum rte_pipeline_action {
 	/** Drop the packet */
-	RTE_PIPELINE_ACTION_DROP = 0,
+	RTE_PIPELINE_ACTION_DROP = 0,//丢包
 
 	/** Send packet to output port */
-	RTE_PIPELINE_ACTION_PORT,
+	RTE_PIPELINE_ACTION_PORT,//输出到指定port
 
 	/** Send packet to output port read from packet meta-data */
-	RTE_PIPELINE_ACTION_PORT_META,
+	RTE_PIPELINE_ACTION_PORT_META,//自报文中某个metadata位置取出出接口，然后发送
 
 	/** Send packet to table */
-	RTE_PIPELINE_ACTION_TABLE,
+	RTE_PIPELINE_ACTION_TABLE,//跳转到表x
 
 	/** Number of reserved actions */
 	RTE_PIPELINE_ACTIONS
@@ -212,6 +216,7 @@ enum rte_pipeline_action {
  * associated meta-data. As all the currently defined reserved actions are
  * mutually exclusive, only one reserved action can be set per table entry.
  */
+//pipeline表实体头
 struct rte_pipeline_table_entry {
 	/** Reserved action */
 	enum rte_pipeline_action action;
@@ -305,23 +310,23 @@ typedef int (*rte_pipeline_table_action_handler_miss)(
     NULL). */
 struct rte_pipeline_table_params {
 	/** Table operations (specific to each table type) */
-	struct rte_table_ops *ops;
+	struct rte_table_ops *ops;//表操作函数
 	/** Opaque param to be passed to the table create operation when
 	invoked */
 	void *arg_create;
 	/** Callback function to execute the user actions on input packets in
 	case of lookup hit */
-	rte_pipeline_table_action_handler_hit f_action_hit;
+	rte_pipeline_table_action_handler_hit f_action_hit;//表项命中时
 	/** Callback function to execute the user actions on input packets in
 	case of lookup miss */
-	rte_pipeline_table_action_handler_miss f_action_miss;
+	rte_pipeline_table_action_handler_miss f_action_miss;//表项不命中时
 
 	/** Opaque parameter to be passed to lookup hit and/or lookup miss
 	action handlers when invoked */
-	void *arg_ah;
+	void *arg_ah;//hit,miss回调对应的参数
 	/** Memory size to be reserved per table entry for storing the user
 	actions and their meta-data */
-	uint32_t action_data_size;
+	uint32_t action_data_size;//用于存放表项内容的数据大小（不含实际使用时的表项头）
 };
 
 /**
@@ -569,7 +574,7 @@ struct rte_pipeline_port_in_params {
 	/** Input port operations (specific to each table type) */
 	struct rte_port_in_ops *ops;
 	/** Opaque parameter to be passed to create operation when invoked */
-	void *arg_create;
+	void *arg_create;//创建时参数
 
 	/** Callback function to execute the user actions on input packets.
 		Disabled if set to NULL. */
diff --git a/lib/librte_port/rte_port.h b/lib/librte_port/rte_port.h
index 7f156ef47..f19a079e6 100644
--- a/lib/librte_port/rte_port.h
+++ b/lib/librte_port/rte_port.h
@@ -50,6 +50,7 @@ extern "C" {
  */
 /** Maximum number of packets read from any input port in a single burst.
 Cannot be changed. */
+//这个值目前基本没办法修改
 #define RTE_PORT_IN_BURST_SIZE_MAX                         64
 
 /** Input port statistics */
diff --git a/lib/librte_port/rte_port_ethdev.c b/lib/librte_port/rte_port_ethdev.c
index 0da789026..f714f839e 100644
--- a/lib/librte_port/rte_port_ethdev.c
+++ b/lib/librte_port/rte_port_ethdev.c
@@ -34,6 +34,7 @@ struct rte_port_ethdev_reader {
 	uint16_t port_id;
 };
 
+//创建逻辑port(对应的是某port的某队列）
 static void *
 rte_port_ethdev_reader_create(void *params, int socket_id)
 {
@@ -55,6 +56,7 @@ rte_port_ethdev_reader_create(void *params, int socket_id)
 		return NULL;
 	}
 
+	//设置port_id,queue_id
 	/* Initialization */
 	port->port_id = conf->port_id;
 	port->queue_id = conf->queue_id;
@@ -62,6 +64,7 @@ rte_port_ethdev_reader_create(void *params, int socket_id)
 	return port;
 }
 
+//自指定port的指定队列收取报文（最大收取n_pkts个）
 static int
 rte_port_ethdev_reader_rx(void *port, struct rte_mbuf **pkts, uint32_t n_pkts)
 {
@@ -70,10 +73,11 @@ rte_port_ethdev_reader_rx(void *port, struct rte_mbuf **pkts, uint32_t n_pkts)
 	uint16_t rx_pkt_cnt;
 
 	rx_pkt_cnt = rte_eth_rx_burst(p->port_id, p->queue_id, pkts, n_pkts);
-	RTE_PORT_ETHDEV_READER_STATS_PKTS_IN_ADD(p, rx_pkt_cnt);
+	RTE_PORT_ETHDEV_READER_STATS_PKTS_IN_ADD(p, rx_pkt_cnt);//增加统计计数
 	return rx_pkt_cnt;
 }
 
+//port资源释放
 static int
 rte_port_ethdev_reader_free(void *port)
 {
@@ -87,6 +91,7 @@ rte_port_ethdev_reader_free(void *port)
 	return 0;
 }
 
+//读取状态计数
 static int rte_port_ethdev_reader_stats_read(void *port,
 		struct rte_port_in_stats *stats, int clear)
 {
@@ -94,9 +99,11 @@ static int rte_port_ethdev_reader_stats_read(void *port,
 			port;
 
 	if (stats != NULL)
+		//copy计数
 		memcpy(stats, &p->stats, sizeof(p->stats));
 
 	if (clear)
+		//如果需要clear
 		memset(&p->stats, 0, sizeof(p->stats));
 
 	return 0;
@@ -123,13 +130,14 @@ struct rte_port_ethdev_writer {
 	struct rte_port_out_stats stats;
 
 	struct rte_mbuf *tx_buf[2 * RTE_PORT_IN_BURST_SIZE_MAX];
-	uint32_t tx_burst_sz;
-	uint16_t tx_buf_count;
-	uint64_t bsz_mask;
+	uint32_t tx_burst_sz;//burst大小
+	uint16_t tx_buf_count;//当前发送缓存中含有多少buf
+	uint64_t bsz_mask;//tx_buf_count为2的n次方,mask为其减1
 	uint16_t queue_id;
 	uint16_t port_id;
 };
 
+//创建write对象
 static void *
 rte_port_ethdev_writer_create(void *params, int socket_id)
 {
@@ -157,7 +165,7 @@ rte_port_ethdev_writer_create(void *params, int socket_id)
 	/* Initialization */
 	port->port_id = conf->port_id;
 	port->queue_id = conf->queue_id;
-	port->tx_burst_sz = conf->tx_burst_sz;
+	port->tx_burst_sz = conf->tx_burst_sz;//多大进行发送
 	port->tx_buf_count = 0;
 	port->bsz_mask = 1LLU << (conf->tx_burst_sz - 1);
 
@@ -169,9 +177,10 @@ send_burst(struct rte_port_ethdev_writer *p)
 {
 	uint32_t nb_tx;
 
+	//将tx_buf中的报文，自port_id的queue_id队列中发出
 	nb_tx = rte_eth_tx_burst(p->port_id, p->queue_id,
 			 p->tx_buf, p->tx_buf_count);
-
+	//没有发送成功的包，将被丢弃
 	RTE_PORT_ETHDEV_WRITER_STATS_PKTS_DROP_ADD(p, p->tx_buf_count - nb_tx);
 	for ( ; nb_tx < p->tx_buf_count; nb_tx++)
 		rte_pktmbuf_free(p->tx_buf[nb_tx]);
@@ -179,6 +188,7 @@ send_burst(struct rte_port_ethdev_writer *p)
 	p->tx_buf_count = 0;
 }
 
+//向硬件队列中发包。
 static int
 rte_port_ethdev_writer_tx(void *port, struct rte_mbuf *pkt)
 {
@@ -187,6 +197,7 @@ rte_port_ethdev_writer_tx(void *port, struct rte_mbuf *pkt)
 
 	p->tx_buf[p->tx_buf_count++] = pkt;
 	RTE_PORT_ETHDEV_WRITER_STATS_PKTS_IN_ADD(p, 1);
+	//如果够发送，就对外发送
 	if (p->tx_buf_count >= p->tx_burst_sz)
 		send_burst(p);
 
@@ -209,13 +220,16 @@ rte_port_ethdev_writer_tx_bulk(void *port,
 		uint64_t n_pkts = __builtin_popcountll(pkts_mask);
 		uint32_t n_pkts_ok;
 
+		//如果port中已缓存有报文，则先将其发送
 		if (tx_buf_count)
 			send_burst(p);
 
+		//再发送传入的
 		RTE_PORT_ETHDEV_WRITER_STATS_PKTS_IN_ADD(p, n_pkts);
 		n_pkts_ok = rte_eth_tx_burst(p->port_id, p->queue_id, pkts,
 			n_pkts);
 
+		//发送失败，刚丢包
 		RTE_PORT_ETHDEV_WRITER_STATS_PKTS_DROP_ADD(p, n_pkts - n_pkts_ok);
 		for ( ; n_pkts_ok < n_pkts; n_pkts_ok++) {
 			struct rte_mbuf *pkt = pkts[n_pkts_ok];
@@ -261,6 +275,7 @@ rte_port_ethdev_writer_free(void *port)
 		return -EINVAL;
 	}
 
+	//如果缓存了一些报文，则将这些报文发送出去
 	rte_port_ethdev_writer_flush(port);
 	rte_free(port);
 
@@ -306,7 +321,7 @@ struct rte_port_ethdev_writer_nodrop {
 	uint32_t tx_burst_sz;
 	uint16_t tx_buf_count;
 	uint64_t bsz_mask;
-	uint64_t n_retries;
+	uint64_t n_retries;//尝试发送多少次
 	uint16_t queue_id;
 	uint16_t port_id;
 };
@@ -352,15 +367,18 @@ rte_port_ethdev_writer_nodrop_create(void *params, int socket_id)
 	return port;
 }
 
+//如果暂时无法成功发送，则进行有限次尝试，尝试后仍不能成功发送，则释放mbuf
 static inline void
 send_burst_nodrop(struct rte_port_ethdev_writer_nodrop *p)
 {
 	uint32_t nb_tx = 0, i;
 
+	//先发送buf
 	nb_tx = rte_eth_tx_burst(p->port_id, p->queue_id, p->tx_buf,
 			p->tx_buf_count);
 
 	/* We sent all the packets in a first try */
+	//所有报文均已成功发送，则返回
 	if (nb_tx >= p->tx_buf_count) {
 		p->tx_buf_count = 0;
 		return;
@@ -371,6 +389,7 @@ send_burst_nodrop(struct rte_port_ethdev_writer_nodrop *p)
 							 p->tx_buf + nb_tx, p->tx_buf_count - nb_tx);
 
 		/* We sent all the packets in more than one try */
+		//在本次尝试中，所有报文，均发成发送，则返回
 		if (nb_tx >= p->tx_buf_count) {
 			p->tx_buf_count = 0;
 			return;
@@ -378,6 +397,7 @@ send_burst_nodrop(struct rte_port_ethdev_writer_nodrop *p)
 	}
 
 	/* We didn't send the packets in maximum allowed attempts */
+	//在进行了n_retries次尝试后，报文未成功发送，释放未成功发送的报文。
 	RTE_PORT_ETHDEV_WRITER_NODROP_STATS_PKTS_DROP_ADD(p, p->tx_buf_count - nb_tx);
 	for ( ; nb_tx < p->tx_buf_count; nb_tx++)
 		rte_pktmbuf_free(p->tx_buf[nb_tx]);
@@ -498,6 +518,7 @@ static int rte_port_ethdev_writer_nodrop_stats_read(void *port,
 /*
  * Summary of port operations
  */
+//自物理网卡的rx中读取报文
 struct rte_port_in_ops rte_port_ethdev_reader_ops = {
 	.f_create = rte_port_ethdev_reader_create,
 	.f_free = rte_port_ethdev_reader_free,
@@ -505,15 +526,17 @@ struct rte_port_in_ops rte_port_ethdev_reader_ops = {
 	.f_stats = rte_port_ethdev_reader_stats_read,
 };
 
+//向硬件网卡的tx中写入报文
 struct rte_port_out_ops rte_port_ethdev_writer_ops = {
 	.f_create = rte_port_ethdev_writer_create,
 	.f_free = rte_port_ethdev_writer_free,
 	.f_tx = rte_port_ethdev_writer_tx,
-	.f_tx_bulk = rte_port_ethdev_writer_tx_bulk,
-	.f_flush = rte_port_ethdev_writer_flush,
+	.f_tx_bulk = rte_port_ethdev_writer_tx_bulk,//支持一次发多个
+	.f_flush = rte_port_ethdev_writer_flush,//将缓存的清空
 	.f_stats = rte_port_ethdev_writer_stats_read,
 };
 
+//如尽最大可能进行报文发送
 struct rte_port_out_ops rte_port_ethdev_writer_nodrop_ops = {
 	.f_create = rte_port_ethdev_writer_nodrop_create,
 	.f_free = rte_port_ethdev_writer_nodrop_free,
diff --git a/lib/librte_port/rte_port_frag.c b/lib/librte_port/rte_port_frag.c
index 8a803fda1..9bdb8fc92 100644
--- a/lib/librte_port/rte_port_frag.c
+++ b/lib/librte_port/rte_port_frag.c
@@ -40,17 +40,17 @@ struct rte_port_ring_reader_frag {
 	/* Input parameters */
 	struct rte_ring *ring;
 	uint32_t mtu;
-	uint32_t metadata_size;
+	uint32_t metadata_size;//mbuf中的一些数据，每个分片都需要和源报文保持一致的数据
 	struct rte_mempool *pool_direct;
 	struct rte_mempool *pool_indirect;
 
 	/* Internal buffers */
-	struct rte_mbuf *pkts[RTE_PORT_IN_BURST_SIZE_MAX];
+	struct rte_mbuf *pkts[RTE_PORT_IN_BURST_SIZE_MAX];//从ring中读取的报文，还未分片
 	struct rte_mbuf *frags[RTE_PORT_FRAG_MAX_FRAGS_PER_PACKET];
 	uint32_t n_pkts;
 	uint32_t pos_pkts;
-	uint32_t n_frags;
-	uint32_t pos_frags;
+	uint32_t n_frags;//当前分片数目
+	uint32_t pos_frags;//当前缓存读取位置
 
 	frag_op f_frag;
 } __rte_cache_aligned;
@@ -106,24 +106,28 @@ rte_port_ring_reader_frag_create(void *params, int socket_id, int is_ipv4)
 	port->n_frags = 0;
 	port->pos_frags = 0;
 
+	//挂载分片函数
 	port->f_frag = (is_ipv4) ?
 			rte_ipv4_fragment_packet : rte_ipv6_fragment_packet;
 
 	return port;
 }
 
+//ipv4 分片 port
 static void *
 rte_port_ring_reader_ipv4_frag_create(void *params, int socket_id)
 {
 	return rte_port_ring_reader_frag_create(params, socket_id, 1);
 }
 
+//ipv6 分片port
 static void *
 rte_port_ring_reader_ipv6_frag_create(void *params, int socket_id)
 {
 	return rte_port_ring_reader_frag_create(params, socket_id, 0);
 }
 
+//对端将需要分片的报文，入了ring,我们自ring中提取报文并分片，抽象出来的port完成分片
 static int
 rte_port_ring_reader_frag_rx(void *port,
 		struct rte_mbuf **pkts,
@@ -137,6 +141,7 @@ rte_port_ring_reader_frag_rx(void *port,
 
 	/* Get packets from the "frag" buffer */
 	if (p->n_frags >= n_pkts) {
+		//缓存的frags比要收取的pkts要多，直接传递缓存的
 		memcpy(pkts, &p->frags[p->pos_frags], n_pkts * sizeof(void *));
 		p->pos_frags += n_pkts;
 		p->n_frags -= n_pkts;
@@ -144,9 +149,10 @@ rte_port_ring_reader_frag_rx(void *port,
 		return n_pkts;
 	}
 
+	//先将剩余的写入
 	memcpy(pkts, &p->frags[p->pos_frags], p->n_frags * sizeof(void *));
-	n_pkts_out = p->n_frags;
-	p->n_frags = 0;
+	n_pkts_out = p->n_frags;//已写入x个报文
+	p->n_frags = 0;//缓存数为0
 
 	/* Look to "pkts" buffer to get more packets */
 	for ( ; ; ) {
@@ -156,31 +162,35 @@ rte_port_ring_reader_frag_rx(void *port,
 
 		/* If "pkts" buffer is empty, read packet burst from ring */
 		if (p->n_pkts == 0) {
+			//自ring中出一个报文
 			p->n_pkts = rte_ring_sc_dequeue_burst(p->ring,
 				(void **) p->pkts, RTE_PORT_IN_BURST_SIZE_MAX,
 				NULL);
 			RTE_PORT_RING_READER_FRAG_STATS_PKTS_IN_ADD(p, p->n_pkts);
 			if (p->n_pkts == 0)
-				return n_pkts_out;
+				return n_pkts_out;//ring中无报文，返回
 			p->pos_pkts = 0;
 		}
 
 		/* Read next packet from "pkts" buffer */
+		//提取一个报文
 		pkt = p->pkts[p->pos_pkts++];
 		p->n_pkts--;
 
 		/* If not jumbo, pass current packet to output */
+		//不需要分片，直接传入到output
 		if (pkt->pkt_len <= p->mtu) {
 			pkts[n_pkts_out++] = pkt;
 
 			n_pkts_to_provide = n_pkts - n_pkts_out;
 			if (n_pkts_to_provide == 0)
-				return n_pkts;
+				return n_pkts;//已达到需要的，返回
 
 			continue;
 		}
 
 		/* Fragment current packet into the "frags" buffer */
+		//对此报文进行分片，并将分好的片，缓存在p->frags中
 		status = p->f_frag(
 			pkt,
 			p->frags,
@@ -190,16 +200,19 @@ rte_port_ring_reader_frag_rx(void *port,
 			p->pool_indirect
 		);
 
+		//分片失败
 		if (status < 0) {
 			rte_pktmbuf_free(pkt);
 			RTE_PORT_RING_READER_FRAG_STATS_PKTS_DROP_ADD(p, 1);
 			continue;
 		}
 
+		//记录分了多少片
 		p->n_frags = (uint32_t) status;
 		p->pos_frags = 0;
 
 		/* Copy meta-data from input jumbo packet to its fragments */
+		//从src copy 元数据到每个分片
 		for (i = 0; i < p->n_frags; i++) {
 			uint8_t *src =
 			  RTE_MBUF_METADATA_UINT8_PTR(pkt, sizeof(struct rte_mbuf));
@@ -230,6 +243,7 @@ rte_port_ring_reader_frag_rx(void *port,
 	}
 }
 
+//分片port资源释放
 static int
 rte_port_ring_reader_frag_free(void *port)
 {
@@ -243,6 +257,7 @@ rte_port_ring_reader_frag_free(void *port)
 	return 0;
 }
 
+//分片状态读取
 static int
 rte_port_frag_reader_stats_read(void *port,
 		struct rte_port_in_stats *stats, int clear)
@@ -262,6 +277,7 @@ rte_port_frag_reader_stats_read(void *port,
 /*
  * Summary of port operations
  */
+//分片功能抽象成port
 struct rte_port_in_ops rte_port_ring_reader_ipv4_frag_ops = {
 	.f_create = rte_port_ring_reader_ipv4_frag_create,
 	.f_free = rte_port_ring_reader_frag_free,
diff --git a/lib/librte_port/rte_port_ras.c b/lib/librte_port/rte_port_ras.c
index 403028f8d..771705673 100644
--- a/lib/librte_port/rte_port_ras.c
+++ b/lib/librte_port/rte_port_ras.c
@@ -50,16 +50,17 @@ process_ipv6(struct rte_port_ring_writer_ras *p, struct rte_mbuf *pkt);
 struct rte_port_ring_writer_ras {
 	struct rte_port_out_stats stats;
 
-	struct rte_mbuf *tx_buf[RTE_PORT_IN_BURST_SIZE_MAX];
+	struct rte_mbuf *tx_buf[RTE_PORT_IN_BURST_SIZE_MAX];//缓存收到的报文
 	struct rte_ring *ring;
 	uint32_t tx_burst_sz;
 	uint32_t tx_buf_count;
-	struct rte_ip_frag_tbl *frag_tbl;
+	struct rte_ip_frag_tbl *frag_tbl;//分片表
 	struct rte_ip_frag_death_row death_row;
 
 	ras_op f_ras;
 };
 
+//分片重组port
 static void *
 rte_port_ring_writer_ras_create(void *params, int socket_id, int is_ipv4)
 {
@@ -151,18 +152,21 @@ static void
 process_ipv4(struct rte_port_ring_writer_ras *p, struct rte_mbuf *pkt)
 {
 	/* Assume there is no ethernet header */
+	//取ipv4头
 	struct rte_ipv4_hdr *pkt_hdr =
 		rte_pktmbuf_mtod(pkt, struct rte_ipv4_hdr *);
 
 	/* Get "More fragments" flag and fragment offset */
-	uint16_t frag_field = rte_be_to_cpu_16(pkt_hdr->fragment_offset);
-	uint16_t frag_offset = (uint16_t)(frag_field & RTE_IPV4_HDR_OFFSET_MASK);
-	uint16_t frag_flag = (uint16_t)(frag_field & RTE_IPV4_HDR_MF_FLAG);
+	uint16_t frag_field = rte_be_to_cpu_16(pkt_hdr->fragment_offset);//分片字段取值
+	uint16_t frag_offset = (uint16_t)(frag_field & RTE_IPV4_HDR_OFFSET_MASK);//分片偏移量
+	uint16_t frag_flag = (uint16_t)(frag_field & RTE_IPV4_HDR_MF_FLAG);//是否有more标记
 
 	/* If it is a fragmented packet, then try to reassemble */
+	//非分片报文
 	if ((frag_flag == 0) && (frag_offset == 0))
 		p->tx_buf[p->tx_buf_count++] = pkt;
 	else {
+		//分片报文处理
 		struct rte_mbuf *mo;
 		struct rte_ip_frag_tbl *tbl = p->frag_tbl;
 		struct rte_ip_frag_death_row *dr = &p->death_row;
@@ -170,10 +174,11 @@ process_ipv4(struct rte_port_ring_writer_ras *p, struct rte_mbuf *pkt)
 		pkt->l3_len = sizeof(*pkt_hdr);
 
 		/* Process this fragment */
+		//执行分片重组
 		mo = rte_ipv4_frag_reassemble_packet(tbl, dr, pkt, rte_rdtsc(),
 				pkt_hdr);
 		if (mo != NULL)
-			p->tx_buf[p->tx_buf_count++] = mo;
+			p->tx_buf[p->tx_buf_count++] = mo;//合并重组后的分片
 
 		rte_ip_frag_free_death_row(&p->death_row, 3);
 	}
@@ -313,6 +318,7 @@ rte_port_ras_writer_stats_read(void *port,
 /*
  * Summary of port operations
  */
+//实现ipv4分片重组功能
 struct rte_port_out_ops rte_port_ring_writer_ipv4_ras_ops = {
 	.f_create = rte_port_ring_writer_ipv4_ras_create,
 	.f_free = rte_port_ring_writer_ras_free,
diff --git a/lib/librte_port/rte_port_ring.c b/lib/librte_port/rte_port_ring.c
index 47fcdd06a..58e91187a 100644
--- a/lib/librte_port/rte_port_ring.c
+++ b/lib/librte_port/rte_port_ring.c
@@ -51,6 +51,7 @@ rte_port_ring_reader_create_internal(void *params, int socket_id,
 	}
 
 	/* Memory allocation */
+	//在socket_id上申请port
 	port = rte_zmalloc_socket("PORT", sizeof(*port),
 			RTE_CACHE_LINE_SIZE, socket_id);
 	if (port == NULL) {
@@ -59,29 +60,36 @@ rte_port_ring_reader_create_internal(void *params, int socket_id,
 	}
 
 	/* Initialization */
+	//指向对应的ring
 	port->ring = conf->ring;
 
 	return port;
 }
 
+//创建ring对应的port
 static void *
 rte_port_ring_reader_create(void *params, int socket_id)
 {
+	//单独者
 	return rte_port_ring_reader_create_internal(params, socket_id, 0);
 }
 
+//创建多读者对应的port
 static void *
 rte_port_ring_multi_reader_create(void *params, int socket_id)
 {
+	//多读者
 	return rte_port_ring_reader_create_internal(params, socket_id, 1);
 }
 
+//自ring对应的port中收取报文
 static int
 rte_port_ring_reader_rx(void *port, struct rte_mbuf **pkts, uint32_t n_pkts)
 {
 	struct rte_port_ring_reader *p = port;
 	uint32_t nb_rx;
 
+	//出队，并统计计数(单消费者）
 	nb_rx = rte_ring_sc_dequeue_burst(p->ring, (void **) pkts,
 			n_pkts, NULL);
 	RTE_PORT_RING_READER_STATS_PKTS_IN_ADD(p, nb_rx);
@@ -89,6 +97,7 @@ rte_port_ring_reader_rx(void *port, struct rte_mbuf **pkts, uint32_t n_pkts)
 	return nb_rx;
 }
 
+//自ring对应的port中收取报文
 static int
 rte_port_ring_multi_reader_rx(void *port, struct rte_mbuf **pkts,
 	uint32_t n_pkts)
@@ -96,6 +105,7 @@ rte_port_ring_multi_reader_rx(void *port, struct rte_mbuf **pkts,
 	struct rte_port_ring_reader *p = port;
 	uint32_t nb_rx;
 
+	//出队，并统计计数(多消费者）
 	nb_rx = rte_ring_mc_dequeue_burst(p->ring, (void **) pkts,
 			n_pkts, NULL);
 	RTE_PORT_RING_READER_STATS_PKTS_IN_ADD(p, nb_rx);
@@ -103,6 +113,7 @@ rte_port_ring_multi_reader_rx(void *port, struct rte_mbuf **pkts,
 	return nb_rx;
 }
 
+//释放资源
 static int
 rte_port_ring_reader_free(void *port)
 {
@@ -116,6 +127,7 @@ rte_port_ring_reader_free(void *port)
 	return 0;
 }
 
+//读取ring port对应的统计计数
 static int
 rte_port_ring_reader_stats_read(void *port,
 		struct rte_port_in_stats *stats, int clear)
@@ -208,6 +220,7 @@ rte_port_ring_multi_writer_create(void *params, int socket_id)
 	return rte_port_ring_writer_create_internal(params, socket_id, 1);
 }
 
+//将报文，按单生产者方式入队
 static inline void
 send_burst(struct rte_port_ring_writer *p)
 {
@@ -216,6 +229,7 @@ send_burst(struct rte_port_ring_writer *p)
 	nb_tx = rte_ring_sp_enqueue_burst(p->ring, (void **)p->tx_buf,
 			p->tx_buf_count, NULL);
 
+	//如果入队失败，则释放报文
 	RTE_PORT_RING_WRITER_STATS_PKTS_DROP_ADD(p, p->tx_buf_count - nb_tx);
 	for ( ; nb_tx < p->tx_buf_count; nb_tx++)
 		rte_pktmbuf_free(p->tx_buf[nb_tx]);
@@ -243,9 +257,11 @@ rte_port_ring_writer_tx(void *port, struct rte_mbuf *pkt)
 {
 	struct rte_port_ring_writer *p = port;
 
+	//向tx_buf中存放报文
 	p->tx_buf[p->tx_buf_count++] = pkt;
 	RTE_PORT_RING_WRITER_STATS_PKTS_IN_ADD(p, 1);
 	if (p->tx_buf_count >= p->tx_burst_sz)
+		//如果报文数超限，则进行发送
 		send_burst(p);
 
 	return 0;
@@ -552,6 +568,7 @@ send_burst_mp_nodrop(struct rte_port_ring_writer_nodrop *p)
 	p->tx_buf_count = 0;
 }
 
+//尽可能的不丢包来进行入队（单生产者方式）
 static int
 rte_port_ring_writer_nodrop_tx(void *port, struct rte_mbuf *pkt)
 {
@@ -736,6 +753,7 @@ rte_port_ring_writer_nodrop_stats_read(void *port,
 /*
  * Summary of port operations
  */
+//ring抽象的port操作,出队
 struct rte_port_in_ops rte_port_ring_reader_ops = {
 	.f_create = rte_port_ring_reader_create,
 	.f_free = rte_port_ring_reader_free,
@@ -743,6 +761,7 @@ struct rte_port_in_ops rte_port_ring_reader_ops = {
 	.f_stats = rte_port_ring_reader_stats_read,
 };
 
+//单生产者方式入队
 struct rte_port_out_ops rte_port_ring_writer_ops = {
 	.f_create = rte_port_ring_writer_create,
 	.f_free = rte_port_ring_writer_free,
@@ -752,6 +771,7 @@ struct rte_port_out_ops rte_port_ring_writer_ops = {
 	.f_stats = rte_port_ring_writer_stats_read,
 };
 
+//采用尽可能不丢包的方式进行入队（单生产者模式）
 struct rte_port_out_ops rte_port_ring_writer_nodrop_ops = {
 	.f_create = rte_port_ring_writer_nodrop_create,
 	.f_free = rte_port_ring_writer_nodrop_free,
@@ -768,6 +788,7 @@ struct rte_port_in_ops rte_port_ring_multi_reader_ops = {
 	.f_stats = rte_port_ring_reader_stats_read,
 };
 
+//多生产者方式入队
 struct rte_port_out_ops rte_port_ring_multi_writer_ops = {
 	.f_create = rte_port_ring_multi_writer_create,
 	.f_free = rte_port_ring_writer_free,
@@ -777,6 +798,7 @@ struct rte_port_out_ops rte_port_ring_multi_writer_ops = {
 	.f_stats = rte_port_ring_writer_stats_read,
 };
 
+//多生产者方式入队（最大可能不丢包）
 struct rte_port_out_ops rte_port_ring_multi_writer_nodrop_ops = {
 	.f_create = rte_port_ring_multi_writer_nodrop_create,
 	.f_free = rte_port_ring_writer_nodrop_free,
diff --git a/lib/librte_port/rte_port_source_sink.c b/lib/librte_port/rte_port_source_sink.c
index 74b7385a2..7b1a5de09 100644
--- a/lib/librte_port/rte_port_source_sink.c
+++ b/lib/librte_port/rte_port_source_sink.c
@@ -220,6 +220,8 @@ rte_port_source_create(void *params, int socket_id)
 	port->mempool = (struct rte_mempool *) p->mempool;
 
 	if (p->file_name) {
+		//读取pcap文件，并将pcap文件指定的报文加载在内存中，后续直接
+		//自此port收取报文时，将自pcap中收取报文
 		int status = PCAP_SOURCE_LOAD(port, p->file_name,
 			p->n_bytes_per_pkt, socket_id);
 
@@ -254,6 +256,7 @@ rte_port_source_free(void *port)
 	return 0;
 }
 
+//自pcap方式的port中读取报文
 static int
 rte_port_source_rx(void *port, struct rte_mbuf **pkts, uint32_t n_pkts)
 {
@@ -605,14 +608,14 @@ rte_port_sink_stats_read(void *port, struct rte_port_out_stats *stats,
 struct rte_port_in_ops rte_port_source_ops = {
 	.f_create = rte_port_source_create,
 	.f_free = rte_port_source_free,
-	.f_rx = rte_port_source_rx,
+	.f_rx = rte_port_source_rx,//自pcap文件中收取报文
 	.f_stats = rte_port_source_stats_read,
 };
 
 struct rte_port_out_ops rte_port_sink_ops = {
 	.f_create = rte_port_sink_create,
 	.f_free = rte_port_sink_free,
-	.f_tx = rte_port_sink_tx,
+	.f_tx = rte_port_sink_tx,//向pcap文件中写报文
 	.f_tx_bulk = rte_port_sink_tx_bulk,
 	.f_flush = rte_port_sink_flush,
 	.f_stats = rte_port_sink_stats_read,
diff --git a/lib/librte_ring/rte_ring.c b/lib/librte_ring/rte_ring.c
index d9b308036..84d7ac052 100644
--- a/lib/librte_ring/rte_ring.c
+++ b/lib/librte_ring/rte_ring.c
@@ -115,6 +115,7 @@ rte_ring_init(struct rte_ring *r, const char *name, unsigned count,
 }
 
 /* create the ring */
+//创建ring
 struct rte_ring *
 rte_ring_create(const char *name, unsigned count, int socket_id,
 		unsigned flags)
diff --git a/lib/librte_ring/rte_ring.h b/lib/librte_ring/rte_ring.h
index 2a9f768a1..9ffdf5a4c 100644
--- a/lib/librte_ring/rte_ring.h
+++ b/lib/librte_ring/rte_ring.h
@@ -640,6 +640,7 @@ rte_ring_mc_dequeue(struct rte_ring *r, void **obj_p)
  *   - -ENOENT: Not enough entries in the ring to dequeue, no object is
  *     dequeued.
  */
+//出队，单消费者
 static __rte_always_inline int
 rte_ring_sc_dequeue(struct rte_ring *r, void **obj_p)
 {
diff --git a/lib/librte_table/rte_table.h b/lib/librte_table/rte_table.h
index cccded1a1..eed012cd7 100644
--- a/lib/librte_table/rte_table.h
+++ b/lib/librte_table/rte_table.h
@@ -229,11 +229,11 @@ typedef int (*rte_table_op_entry_delete_bulk)(
  *   0 on success, error code otherwise
  */
 typedef int (*rte_table_op_lookup)(
-	void *table,
-	struct rte_mbuf **pkts,
-	uint64_t pkts_mask,
-	uint64_t *lookup_hit_mask,
-	void **entries);
+	void *table,//需要查的表
+	struct rte_mbuf **pkts,//需要查表的pkts
+	uint64_t pkts_mask,//通过掩码指出有那些pkts
+	uint64_t *lookup_hit_mask,//命中的掩码
+	void **entries);//命中的表项指针
 
 /**
  * Lookup table stats read
@@ -254,6 +254,7 @@ typedef int (*rte_table_op_stats_read)(
 	int clear);
 
 /** Lookup table interface defining the lookup table operation */
+//表项的操作集（表创建，表释放，表项添加，表项删除，表项查询）
 struct rte_table_ops {
 	rte_table_op_create f_create;                 /**< Create */
 	rte_table_op_free f_free;                     /**< Free */
diff --git a/lib/librte_timer/rte_timer.c b/lib/librte_timer/rte_timer.c
index 381a9f43f..bd19f522e 100644
--- a/lib/librte_timer/rte_timer.c
+++ b/lib/librte_timer/rte_timer.c
@@ -35,7 +35,7 @@
  */
 struct priv_timer {
 	struct rte_timer pending_head;  /**< dummy timer instance to head up list */
-	rte_spinlock_t list_lock;       /**< lock to protect list access */
+	rte_spinlock_t list_lock;       /**< lock to protect list access */ //链操作时需要加的锁
 
 	/** per-core variable that true if a timer was updated on this
 	 *  core since last reset of the variable */
@@ -44,10 +44,11 @@ struct priv_timer {
 	/** track the current depth of the skiplist */
 	unsigned curr_skiplist_depth;
 
+	//用来做轮循，记录上一次我们在此timer上使用了哪个core
 	unsigned prev_lcore;              /**< used for lcore round robin */
 
 	/** running timer on this lcore now */
-	struct rte_timer *running_tim;
+	struct rte_timer *running_tim;//记录正在本core上运行的timer
 
 #ifdef RTE_LIBRTE_TIMER_DEBUG
 	/** per-lcore statistics */
@@ -132,6 +133,7 @@ rte_timer_data_dealloc(uint32_t id)
 }
 
 void __vsym
+//timer库初始化
 rte_timer_subsystem_init_v20(void)
 {
 	unsigned lcore_id;
@@ -140,6 +142,7 @@ rte_timer_subsystem_init_v20(void)
 	/* since priv_timer is static, it's zeroed by default, so only init some
 	 * fields.
 	 */
+	//每core一个
 	for (lcore_id = 0; lcore_id < RTE_MAX_LCORE; lcore_id ++) {
 		rte_spinlock_init(&priv_timer[lcore_id].list_lock);
 		priv_timer[lcore_id].prev_lcore = lcore_id;
@@ -245,6 +248,7 @@ rte_timer_init(struct rte_timer *tim)
  * us), mark timer as configuring, and on success return the previous
  * status of the timer
  */
+//采用原子方式设置状态
 static int
 timer_set_config_state(struct rte_timer *tim,
 		       union rte_timer_status *ret_prev_status,
@@ -289,6 +293,7 @@ timer_set_config_state(struct rte_timer *tim,
 /*
  * if timer is pending, mark timer as running
  */
+//为了防止两个线程同时在操作一个timer,故由原子赋值来进行变更
 static int
 timer_set_running_state(struct rte_timer *tim)
 {
@@ -303,15 +308,15 @@ timer_set_running_state(struct rte_timer *tim)
 
 		/* timer is not pending anymore */
 		if (prev_status.state != RTE_TIMER_PENDING)
-			return -1;
+			return -1;//如果timer未变为pending状态，则timer可能加入还未完成，不处理
 
 		/* here, we know that timer is stopped or pending,
 		 * mark it atomically as being configured */
-		status.state = RTE_TIMER_RUNNING;
-		status.owner = (int16_t)lcore_id;
+		status.state = RTE_TIMER_RUNNING;//标记为running状态
+		status.owner = (int16_t)lcore_id;//标记lcore_id现在处理此timer
 		success = rte_atomic32_cmpset(&tim->status.u32,
 					      prev_status.u32,
-					      status.u32);
+					      status.u32);//原子写状态
 	}
 
 	return 0;
@@ -358,6 +363,7 @@ timer_get_skiplist_level(unsigned curr_depth)
  * For a given time value, get the entries at each level which
  * are <= that time value.
  */
+//在tim_locre core上查找time_val时间应存放的位置
 static void
 timer_get_prev_entries(uint64_t time_val, unsigned tim_lcore,
 		       struct rte_timer **prev, struct priv_timer *priv_timer)
@@ -367,6 +373,7 @@ timer_get_prev_entries(uint64_t time_val, unsigned tim_lcore,
 	while(lvl != 0) {
 		lvl--;
 		prev[lvl] = prev[lvl+1];
+		//有下一个元素，且下一个元素的过期时间小于time_val,则需要继续后移
 		while (prev[lvl]->sl_next[lvl] &&
 				prev[lvl]->sl_next[lvl]->expire <= time_val)
 			prev[lvl] = prev[lvl]->sl_next[lvl];
@@ -401,6 +408,7 @@ timer_get_prev_entries_for_node(struct rte_timer *tim, unsigned tim_lcore,
  * timer must be in config state
  * timer must not be in a list
  */
+//将timer加入到tim_lcore上，按跳表方式插入
 static void
 timer_add(struct rte_timer *tim, unsigned int tim_lcore,
 	  struct priv_timer *priv_timer)
@@ -478,11 +486,12 @@ timer_del(struct rte_timer *tim, union rte_timer_status prev_status,
 }
 
 /* Reset and start the timer associated with the timer handle (private func) */
+//重新设置定义器tim
 static int
-__rte_timer_reset(struct rte_timer *tim, uint64_t expire,
-		  uint64_t period, unsigned tim_lcore,
-		  rte_timer_cb_t fct, void *arg,
-		  int local_is_locked,
+__rte_timer_reset(struct rte_timer *tim, uint64_t expire,//timer及过期时间
+		  uint64_t period, unsigned tim_lcore,//周期数，分配到那个timer上
+		  rte_timer_cb_t fct, void *arg,//回调函数，及其参数
+		  int local_is_locked,//是否已加锁
 		  struct rte_timer_data *timer_data)
 {
 	union rte_timer_status prev_status, status;
@@ -491,7 +500,9 @@ __rte_timer_reset(struct rte_timer *tim, uint64_t expire,
 	struct priv_timer *priv_timer = timer_data->priv_timer;
 
 	/* round robin for tim_lcore */
+	//如果未指定core，则选择一个core,按robin的方式
 	if (tim_lcore == (unsigned)LCORE_ID_ANY) {
+		//容许在任意core上出现，轮循出一个core
 		if (lcore_id < RTE_MAX_LCORE) {
 			/* EAL thread with valid lcore_id */
 			tim_lcore = rte_get_next_lcore(
@@ -506,6 +517,7 @@ __rte_timer_reset(struct rte_timer *tim, uint64_t expire,
 
 	/* wait that the timer is in correct status before update,
 	 * and mark it as being configured */
+	//将timer置为config模式,并返回上一个状态
 	ret = timer_set_config_state(tim, &prev_status, priv_timer);
 	if (ret < 0)
 		return -1;
@@ -522,6 +534,7 @@ __rte_timer_reset(struct rte_timer *tim, uint64_t expire,
 		__TIMER_STAT_ADD(priv_timer, pending, -1);
 	}
 
+	//设置tim
 	tim->period = period;
 	tim->expire = expire;
 	tim->f = fct;
@@ -540,7 +553,7 @@ __rte_timer_reset(struct rte_timer *tim, uint64_t expire,
 	/* update state: as we are in CONFIG state, only us can modify
 	 * the state so we don't need to use cmpset() here */
 	rte_wmb();
-	status.state = RTE_TIMER_PENDING;
+	status.state = RTE_TIMER_PENDING;//修改为pending状态
 	status.owner = (int16_t)tim_lcore;
 	tim->status.u32 = status.u32;
 
@@ -551,6 +564,8 @@ __rte_timer_reset(struct rte_timer *tim, uint64_t expire,
 }
 
 /* Reset and start the timer associated with the timer handle tim */
+//重置timer,将其过期时间定在ticks之后，定时器类型，定时器位于那个core,定时器过期回调
+//定时器过期回调参数
 int __vsym
 rte_timer_reset_v20(struct rte_timer *tim, uint64_t ticks,
 		    enum rte_timer_type type, unsigned int tim_lcore,
@@ -562,8 +577,9 @@ rte_timer_reset_v20(struct rte_timer *tim, uint64_t ticks,
 	if (unlikely((tim_lcore != (unsigned)LCORE_ID_ANY) &&
 			!(rte_lcore_is_enabled(tim_lcore) ||
 			  rte_lcore_has_role(tim_lcore, ROLE_SERVICE))))
-		return -1;
+		return -1;//参数校验
 
+	//检查是否为周期性定时器
 	if (type == PERIODICAL)
 		period = ticks;
 	else
@@ -718,7 +734,7 @@ __rte_timer_manage(struct rte_timer_data *timer_data)
 	/* optimize for the case where per-cpu list is empty */
 	if (priv_timer[lcore_id].pending_head.sl_next[0] == NULL)
 		return;
-	cur_time = rte_get_timer_cycles();
+	cur_time = rte_get_timer_cycles();//当前时间
 
 #ifdef RTE_ARCH_64
 	/* on 64-bit the value cached in the pending_head.expired will be
@@ -735,7 +751,7 @@ __rte_timer_manage(struct rte_timer_data *timer_data)
 	if (priv_timer[lcore_id].pending_head.sl_next[0] == NULL ||
 	    priv_timer[lcore_id].pending_head.sl_next[0]->expire > cur_time) {
 		rte_spinlock_unlock(&priv_timer[lcore_id].list_lock);
-		return;
+		return;//无过期的timer或者无timer,解锁后，立即返回
 	}
 
 	/* save start of list of expired timers */
@@ -760,14 +776,14 @@ __rte_timer_manage(struct rte_timer_data *timer_data)
 	for ( ; tim != NULL; tim = next_tim) {
 		next_tim = tim->sl_next[0];
 
-		ret = timer_set_running_state(tim);
+		ret = timer_set_running_state(tim);//将timer置为run状态
 		if (likely(ret == 0)) {
 			pprev = &tim->sl_next[0];
 		} else {
 			/* another core is trying to re-config this one,
 			 * remove it from local expired list
 			 */
-			*pprev = next_tim;
+			*pprev = next_tim;//将此timer自expired list中移除
 		}
 	}
 
@@ -779,13 +795,14 @@ __rte_timer_manage(struct rte_timer_data *timer_data)
 	rte_spinlock_unlock(&priv_timer[lcore_id].list_lock);
 
 	/* now scan expired list and call callbacks */
+	//遍历过期的timer
 	for (tim = run_first_tim; tim != NULL; tim = next_tim) {
 		next_tim = tim->sl_next[0];
 		priv_timer[lcore_id].updated = 0;
-		priv_timer[lcore_id].running_tim = tim;
+		priv_timer[lcore_id].running_tim = tim;//记录正在本core上运行的timer
 
 		/* execute callback function with list unlocked */
-		tim->f(tim, tim->arg);
+		tim->f(tim, tim->arg);//执行timer回调
 
 		__TIMER_STAT_ADD(priv_timer, pending, -1);
 		/* the timer was stopped or reloaded by the callback
@@ -794,6 +811,7 @@ __rte_timer_manage(struct rte_timer_data *timer_data)
 			continue;
 
 		if (tim->period == 0) {
+			//单次定时器处理
 			/* remove from done list and mark timer as stopped */
 			status.state = RTE_TIMER_STOP;
 			status.owner = RTE_TIMER_NO_OWNER;
@@ -801,6 +819,7 @@ __rte_timer_manage(struct rte_timer_data *timer_data)
 			tim->status.u32 = status.u32;
 		}
 		else {
+			//周期性定时器，重新加入
 			/* keep it in list and mark timer as pending */
 			rte_spinlock_lock(&priv_timer[lcore_id].list_lock);
 			status.state = RTE_TIMER_PENDING;
diff --git a/lib/librte_timer/rte_timer.h b/lib/librte_timer/rte_timer.h
index 05d287d8f..97a560232 100644
--- a/lib/librte_timer/rte_timer.h
+++ b/lib/librte_timer/rte_timer.h
@@ -99,10 +99,10 @@ typedef void (*rte_timer_cb_t)(struct rte_timer *, void *);
  */
 struct rte_timer
 {
-	uint64_t expire;       /**< Time when timer expire. */
+	uint64_t expire;       /**< Time when timer expire. */ //过期时间
 	struct rte_timer *sl_next[MAX_SKIPLIST_DEPTH];
 	volatile union rte_timer_status status; /**< Status of timer. */
-	uint64_t period;       /**< Period of timer (0 if not periodic). */
+	uint64_t period;       /**< Period of timer (0 if not periodic). */ //是否为周期性定时器
 	rte_timer_cb_t f;      /**< Callback function. */
 	void *arg;             /**< Argument to callback function. */
 };
diff --git a/lib/librte_vhost/fd_man.c b/lib/librte_vhost/fd_man.c
index 55d4856f9..bda27b914 100644
--- a/lib/librte_vhost/fd_man.c
+++ b/lib/librte_vhost/fd_man.c
@@ -21,6 +21,8 @@
 
 #define FDPOLLERR (POLLERR | POLLHUP | POLLNVAL)
 
+//自last_valid_idx开始，向前（回退到0）找一个有效的fd
+//即找到最后一个有效的fd
 static int
 get_last_valid_idx(struct fdset *pfdset, int last_valid_idx)
 {
@@ -29,9 +31,10 @@ get_last_valid_idx(struct fdset *pfdset, int last_valid_idx)
 	for (i = last_valid_idx; i >= 0 && pfdset->fd[i].fd == -1; i--)
 		;
 
-	return i;
+	return i;//如果此位置>=0,则此位置拥有有效的fd
 }
 
+//移动src位置的fdset到dst位置
 static void
 fdset_move(struct fdset *pfdset, int dst, int src)
 {
@@ -39,25 +42,33 @@ fdset_move(struct fdset *pfdset, int dst, int src)
 	pfdset->rwfds[dst] = pfdset->rwfds[src];
 }
 
+//将无效的fd移除，收紧pfdset
 static void
 fdset_shrink_nolock(struct fdset *pfdset)
 {
 	int i;
 	int last_valid_idx = get_last_valid_idx(pfdset, pfdset->num - 1);
 
+	//最多需要从0,检查到last_valid_idx位置
 	for (i = 0; i < last_valid_idx; i++) {
+		//如果是有效的，则继续
 		if (pfdset->fd[i].fd != -1)
 			continue;
 
+		//i位置fd是无效的，我们将last_valid_idx位置的fd移动到此位置（防止整个数组移动）
 		fdset_move(pfdset, i, last_valid_idx);
+
+		//刚刚我们把最后一个向前移动了，所以我们需要从last_valid_idx-1位置开始向前再找一个有效的fd
 		last_valid_idx = get_last_valid_idx(pfdset, last_valid_idx - 1);
 	}
+
 	pfdset->num = last_valid_idx + 1;
 }
 
 /*
  * Find deleted fd entries and remove them
  */
+//移除掉无效的fd,保持fd数组连续
 static void
 fdset_shrink(struct fdset *pfdset)
 {
@@ -71,6 +82,7 @@ fdset_shrink(struct fdset *pfdset)
  * @return
  *   index for the fd, or -1 if fd isn't in the fdset.
  */
+//用fd找下标
 static int
 fdset_find_fd(struct fdset *pfdset, int fd)
 {
@@ -82,6 +94,7 @@ fdset_find_fd(struct fdset *pfdset, int fd)
 	return i == pfdset->num ? -1 : i;
 }
 
+//注册fd及其回调（使用idx位置处的空间，其读回调rcb,写回计wcb,dat为回调时参数）
 static void
 fdset_add_fd(struct fdset *pfdset, int idx, int fd,
 	fd_cb rcb, fd_cb wcb, void *dat)
@@ -89,17 +102,20 @@ fdset_add_fd(struct fdset *pfdset, int idx, int fd,
 	struct fdentry *pfdentry = &pfdset->fd[idx];
 	struct pollfd *pfd = &pfdset->rwfds[idx];
 
+	//设置pfdentry
 	pfdentry->fd  = fd;
 	pfdentry->rcb = rcb;
 	pfdentry->wcb = wcb;
 	pfdentry->dat = dat;
 
+	//poll参数初始化
 	pfd->fd = fd;
-	pfd->events  = rcb ? POLLIN : 0;
-	pfd->events |= wcb ? POLLOUT : 0;
+	pfd->events  = rcb ? POLLIN : 0;//有读回调，关注in事件
+	pfd->events |= wcb ? POLLOUT : 0;//有写回调，关注out事件
 	pfd->revents = 0;
 }
 
+//fdset初始化
 void
 fdset_init(struct fdset *pfdset)
 {
@@ -118,6 +134,7 @@ fdset_init(struct fdset *pfdset)
 /**
  * Register the fd in the fdset with read/write handler and context.
  */
+//向fdset中注册一个fd及其回调
 int
 fdset_add(struct fdset *pfdset, int fd, fd_cb rcb, fd_cb wcb, void *dat)
 {
@@ -127,6 +144,8 @@ fdset_add(struct fdset *pfdset, int fd, fd_cb rcb, fd_cb wcb, void *dat)
 		return -1;
 
 	pthread_mutex_lock(&pfdset->fd_mutex);
+	//找一个空闲的位置，默认按序分配，如果达到最后一个，则检查是否可以检测是否可以释放
+	//尽量给分配一个空闲的位置
 	i = pfdset->num < MAX_FDS ? pfdset->num++ : -1;
 	if (i == -1) {
 		pthread_mutex_lock(&pfdset->fd_pooling_mutex);
@@ -134,11 +153,13 @@ fdset_add(struct fdset *pfdset, int fd, fd_cb rcb, fd_cb wcb, void *dat)
 		pthread_mutex_unlock(&pfdset->fd_pooling_mutex);
 		i = pfdset->num < MAX_FDS ? pfdset->num++ : -1;
 		if (i == -1) {
+			//没有空闲的位置了，失败返回
 			pthread_mutex_unlock(&pfdset->fd_mutex);
 			return -2;
 		}
 	}
 
+	//加入到i位置
 	fdset_add_fd(pfdset, i, fd, rcb, wcb, dat);
 	pthread_mutex_unlock(&pfdset->fd_mutex);
 
@@ -149,6 +170,7 @@ fdset_add(struct fdset *pfdset, int fd, fd_cb rcb, fd_cb wcb, void *dat)
  *  Unregister the fd from the fdset.
  *  Returns context of a given fd or NULL.
  */
+//移除指定fd
 void *
 fdset_del(struct fdset *pfdset, int fd)
 {
@@ -161,6 +183,8 @@ fdset_del(struct fdset *pfdset, int fd)
 	do {
 		pthread_mutex_lock(&pfdset->fd_mutex);
 
+		//找到这个fd,如果这个fd存在，并且不是busy状态，就将其标记为删除（不收缩，由dispatch线程负责，收缩）
+		//如果处于busy状态（dispatch线程正在执行回调），则解锁并重试。（加锁是防止将置为0的fd再次置为1）
 		i = fdset_find_fd(pfdset, fd);
 		if (i != -1 && pfdset->fd[i].busy == 0) {
 			/* busy indicates r/wcb is executing! */
@@ -171,7 +195,7 @@ fdset_del(struct fdset *pfdset, int fd)
 			i = -1;
 		}
 		pthread_mutex_unlock(&pfdset->fd_mutex);
-	} while (i != -1);
+	} while (i != -1);//如果没有移除，则再次尝试
 
 	return dat;
 }
@@ -218,6 +242,7 @@ fdset_try_del(struct fdset *pfdset, int fd)
  * will wait until the flag is reset to zero(which indicates the callback is
  * finished), then it could free the context after fdset_del.
  */
+//poll相关的fd,并对依据事件调用不同的回调函数
 void *
 fdset_event_dispatch(void *arg)
 {
@@ -262,12 +287,14 @@ fdset_event_dispatch(void *arg)
 			fd = pfdentry->fd;
 			pfd = &pfdset->rwfds[i];
 
+			//fd无效，置可收缩
 			if (fd < 0) {
 				need_shrink = 1;
 				pthread_mutex_unlock(&pfdset->fd_mutex);
 				continue;
 			}
 
+			//此fd无事件
 			if (!pfd->revents) {
 				pthread_mutex_unlock(&pfdset->fd_mutex);
 				continue;
@@ -282,10 +309,15 @@ fdset_event_dispatch(void *arg)
 
 			pthread_mutex_unlock(&pfdset->fd_mutex);
 
+			//如果关心读回调，且有读事件或者出错
 			if (rcb && pfd->revents & (POLLIN | FDPOLLERR))
+				//调用读回调
 				rcb(fd, dat, &remove1);
+
+			//如果关心写事件，且有写事件或者出错，则调用写回调
 			if (wcb && pfd->revents & (POLLOUT | FDPOLLERR))
 				wcb(fd, dat, &remove2);
+
 			pfdentry->busy = 0;
 			/*
 			 * fdset_del needs to check busy flag.
@@ -299,12 +331,14 @@ fdset_event_dispatch(void *arg)
 			 * listen fd in another thread, we couldn't call
 			 * fdset_del.
 			 */
+			//如果需要移除fd，标记需要收缩
 			if (remove1 || remove2) {
 				pfdentry->fd = -1;
 				need_shrink = 1;
 			}
 		}
 
+		//如果需要收缩，则将fd数组紧缩
 		if (need_shrink)
 			fdset_shrink(pfdset);
 	}
@@ -312,6 +346,7 @@ fdset_event_dispatch(void *arg)
 	return NULL;
 }
 
+//自fd中读取内容，（读取的数据将被丢弃），默认回调
 static void
 fdset_pipe_read_cb(int readfd, void *dat __rte_unused,
 		   int *remove __rte_unused)
@@ -339,12 +374,14 @@ fdset_pipe_init(struct fdset *fdset)
 {
 	int ret;
 
+	//制作管道
 	if (pipe(fdset->u.pipefd) < 0) {
 		RTE_LOG(ERR, VHOST_FDMAN,
 			"failed to create pipe for vhost fdset\n");
 		return -1;
 	}
 
+	//自fd中读（读内容丢）
 	ret = fdset_add(fdset, fdset->u.readfd,
 			fdset_pipe_read_cb, NULL, NULL);
 
@@ -360,6 +397,7 @@ fdset_pipe_init(struct fdset *fdset)
 	return 0;
 }
 
+//向writefd中写入1
 void
 fdset_pipe_notify(struct fdset *fdset)
 {
diff --git a/lib/librte_vhost/fd_man.h b/lib/librte_vhost/fd_man.h
index 3ab5cfdd6..b10a5b247 100644
--- a/lib/librte_vhost/fd_man.h
+++ b/lib/librte_vhost/fd_man.h
@@ -13,18 +13,19 @@
 typedef void (*fd_cb)(int fd, void *dat, int *remove);
 
 struct fdentry {
-	int fd;		/* -1 indicates this entry is empty */
-	fd_cb rcb;	/* callback when this fd is readable. */
-	fd_cb wcb;	/* callback when this fd is writeable.*/
-	void *dat;	/* fd context */
-	int busy;	/* whether this entry is being used in cb. */
+	int fd;		/* -1 indicates this entry is empty */ //fd
+	fd_cb rcb;	/* callback when this fd is readable. */ //读回调
+	fd_cb wcb;	/* callback when this fd is writeable.*/ //写回调
+	void *dat;	/* fd context */ //回调参数
+	int busy;	/* whether this entry is being used in cb. */ //指明entry是否正在被用
 };
 
 struct fdset {
-	struct pollfd rwfds[MAX_FDS];
-	struct fdentry fd[MAX_FDS];
-	pthread_mutex_t fd_mutex;
+	struct pollfd rwfds[MAX_FDS];//poll参数
+	struct fdentry fd[MAX_FDS];//注册所有fd用于轮询
+	pthread_mutex_t fd_mutex;//保护rwfds,fd数组
 	pthread_mutex_t fd_pooling_mutex;
+    //rwfds,fd数组的有效长度
 	int num;	/* current fd number of this fdset */
 
 	union pipefds {
diff --git a/lib/librte_vhost/iotlb.c b/lib/librte_vhost/iotlb.c
index 4a1d8c125..878869214 100644
--- a/lib/librte_vhost/iotlb.c
+++ b/lib/librte_vhost/iotlb.c
@@ -25,6 +25,7 @@ struct vhost_iotlb_entry {
 static void
 vhost_user_iotlb_cache_random_evict(struct vhost_virtqueue *vq);
 
+//移除掉pending的tlb
 static void
 vhost_user_iotlb_pending_remove_all(struct vhost_virtqueue *vq)
 {
@@ -40,6 +41,7 @@ vhost_user_iotlb_pending_remove_all(struct vhost_virtqueue *vq)
 	rte_rwlock_write_unlock(&vq->iotlb_pending_lock);
 }
 
+//是否pending miss?
 bool
 vhost_user_iotlb_pending_miss(struct vhost_virtqueue *vq, uint64_t iova,
 				uint8_t perm)
@@ -61,6 +63,7 @@ vhost_user_iotlb_pending_miss(struct vhost_virtqueue *vq, uint64_t iova,
 	return found;
 }
 
+//向pending插入tlb
 void
 vhost_user_iotlb_pending_insert(struct vhost_virtqueue *vq,
 				uint64_t iova, uint8_t perm)
@@ -74,6 +77,7 @@ vhost_user_iotlb_pending_insert(struct vhost_virtqueue *vq,
 		if (!TAILQ_EMPTY(&vq->iotlb_pending_list))
 			vhost_user_iotlb_pending_remove_all(vq);
 		else
+			//空间不够，驱逐
 			vhost_user_iotlb_cache_random_evict(vq);
 		ret = rte_mempool_get(vq->iotlb_pool, (void **)&node);
 		if (ret) {
@@ -114,6 +118,7 @@ vhost_user_iotlb_pending_remove(struct vhost_virtqueue *vq,
 	rte_rwlock_write_unlock(&vq->iotlb_pending_lock);
 }
 
+//移除掉所有cache
 static void
 vhost_user_iotlb_cache_remove_all(struct vhost_virtqueue *vq)
 {
@@ -131,6 +136,7 @@ vhost_user_iotlb_cache_remove_all(struct vhost_virtqueue *vq)
 	rte_rwlock_write_unlock(&vq->iotlb_lock);
 }
 
+//cache随机驱逐
 static void
 vhost_user_iotlb_cache_random_evict(struct vhost_virtqueue *vq)
 {
@@ -154,6 +160,7 @@ vhost_user_iotlb_cache_random_evict(struct vhost_virtqueue *vq)
 	rte_rwlock_write_unlock(&vq->iotlb_lock);
 }
 
+//插入tlb cache
 void
 vhost_user_iotlb_cache_insert(struct vhost_virtqueue *vq, uint64_t iova,
 				uint64_t uaddr, uint64_t size, uint8_t perm)
@@ -163,11 +170,13 @@ vhost_user_iotlb_cache_insert(struct vhost_virtqueue *vq, uint64_t iova,
 
 	ret = rte_mempool_get(vq->iotlb_pool, (void **)&new_node);
 	if (ret) {
+		//获取节点失败，清理后再获取节点
 		RTE_LOG(DEBUG, VHOST_CONFIG, "IOTLB pool empty, clear entries\n");
 		if (!TAILQ_EMPTY(&vq->iotlb_list))
 			vhost_user_iotlb_cache_random_evict(vq);
 		else
 			vhost_user_iotlb_pending_remove_all(vq);
+		//再次获取节点
 		ret = rte_mempool_get(vq->iotlb_pool, (void **)&new_node);
 		if (ret) {
 			RTE_LOG(ERR, VHOST_CONFIG, "IOTLB pool still empty, failure\n");
@@ -175,6 +184,7 @@ vhost_user_iotlb_cache_insert(struct vhost_virtqueue *vq, uint64_t iova,
 		}
 	}
 
+	//设置tlb缓存
 	new_node->iova = iova;
 	new_node->uaddr = uaddr;
 	new_node->size = size;
@@ -188,15 +198,18 @@ vhost_user_iotlb_cache_insert(struct vhost_virtqueue *vq, uint64_t iova,
 		 * So if iova already in list, assume identical.
 		 */
 		if (node->iova == new_node->iova) {
+			//已存在，归还new_node
 			rte_mempool_put(vq->iotlb_pool, new_node);
 			goto unlock;
 		} else if (node->iova > new_node->iova) {
+			//加入到tlb中
 			TAILQ_INSERT_BEFORE(node, new_node, next);
 			vq->iotlb_cache_nr++;
 			goto unlock;
 		}
 	}
 
+	//未找到，直接加入
 	TAILQ_INSERT_TAIL(&vq->iotlb_list, new_node, next);
 	vq->iotlb_cache_nr++;
 
@@ -207,6 +220,7 @@ vhost_user_iotlb_cache_insert(struct vhost_virtqueue *vq, uint64_t iova,
 
 }
 
+//cache段移除
 void
 vhost_user_iotlb_cache_remove(struct vhost_virtqueue *vq,
 					uint64_t iova, uint64_t size)
@@ -224,6 +238,7 @@ vhost_user_iotlb_cache_remove(struct vhost_virtqueue *vq,
 			break;
 
 		if (iova < node->iova + node->size) {
+			//找到node，并移除
 			TAILQ_REMOVE(&vq->iotlb_list, node, next);
 			rte_mempool_put(vq->iotlb_pool, node);
 			vq->iotlb_cache_nr--;
@@ -233,6 +248,7 @@ vhost_user_iotlb_cache_remove(struct vhost_virtqueue *vq,
 	rte_rwlock_write_unlock(&vq->iotlb_lock);
 }
 
+//cache查找
 uint64_t
 vhost_user_iotlb_cache_find(struct vhost_virtqueue *vq, uint64_t iova,
 						uint64_t *size, uint8_t perm)
@@ -243,14 +259,16 @@ vhost_user_iotlb_cache_find(struct vhost_virtqueue *vq, uint64_t iova,
 	if (unlikely(!*size))
 		goto out;
 
+	//遍历iotlb
 	TAILQ_FOREACH(node, &vq->iotlb_list, next) {
 		/* List sorted by iova */
 		if (unlikely(iova < node->iova))
-			break;
+			break;//iova不存在cache中
 
 		if (iova >= node->iova + node->size)
-			continue;
+			continue;//iova不在此段内
 
+		//在此段内，但perm不相等
 		if (unlikely((perm & node->perm) != perm)) {
 			vva = 0;
 			break;
@@ -282,6 +300,7 @@ vhost_user_iotlb_flush_all(struct vhost_virtqueue *vq)
 	vhost_user_iotlb_pending_remove_all(vq);
 }
 
+//tlb池创建
 int
 vhost_user_iotlb_init(struct virtio_net *dev, int vq_index)
 {
@@ -290,6 +309,7 @@ vhost_user_iotlb_init(struct virtio_net *dev, int vq_index)
 	int socket = 0;
 
 	if (vq->iotlb_pool) {
+		//初始化iotlb_pool
 		/*
 		 * The cache has already been initialized,
 		 * just drop all cached and pending entries.
@@ -308,14 +328,17 @@ vhost_user_iotlb_init(struct virtio_net *dev, int vq_index)
 	TAILQ_INIT(&vq->iotlb_list);
 	TAILQ_INIT(&vq->iotlb_pending_list);
 
+	//找对应的iotlb池
 	snprintf(pool_name, sizeof(pool_name), "iotlb_cache_%d_%d",
 			dev->vid, vq_index);
 
 	/* If already created, free it and recreate */
+	//如果有旧的，则移除后重新创建
 	vq->iotlb_pool = rte_mempool_lookup(pool_name);
 	if (vq->iotlb_pool)
 		rte_mempool_free(vq->iotlb_pool);
 
+	//创建iotlb_pool
 	vq->iotlb_pool = rte_mempool_create(pool_name,
 			IOTLB_CACHE_SIZE, sizeof(struct vhost_iotlb_entry), 0,
 			0, 0, NULL, NULL, NULL, socket,
diff --git a/lib/librte_vhost/rte_vdpa.h b/lib/librte_vhost/rte_vdpa.h
index 9a3deb31d..9b04d7d8b 100644
--- a/lib/librte_vhost/rte_vdpa.h
+++ b/lib/librte_vhost/rte_vdpa.h
@@ -82,9 +82,9 @@ struct rte_vdpa_dev_ops {
  */
 struct rte_vdpa_device {
 	/** vdpa device address */
-	struct rte_vdpa_dev_addr addr;
+	struct rte_vdpa_dev_addr addr;//设备pci地址
 	/** vdpa device operations */
-	struct rte_vdpa_dev_ops *ops;
+	struct rte_vdpa_dev_ops *ops;//设备操作集
 } __rte_cache_aligned;
 
 /**
diff --git a/lib/librte_vhost/rte_vhost.h b/lib/librte_vhost/rte_vhost.h
index 7b5dc87c2..1e795ded9 100644
--- a/lib/librte_vhost/rte_vhost.h
+++ b/lib/librte_vhost/rte_vhost.h
@@ -90,20 +90,20 @@ extern "C" {
  * addresses in QEMUs memory file.
  */
 struct rte_vhost_mem_region {
-	uint64_t guest_phys_addr;
-	uint64_t guest_user_addr;
-	uint64_t host_user_addr;
-	uint64_t size;
-	void	 *mmap_addr;
-	uint64_t mmap_size;
-	int fd;
+	uint64_t guest_phys_addr;//对端进程物理机址
+	uint64_t guest_user_addr;//对端进程虚拟地址起始地址
+	uint64_t host_user_addr; //本端进程虚拟地址起始地址（由mmap_addr加偏移量获得）
+	uint64_t size;//可访问的内存大小
+	void	 *mmap_addr;//mmap返回的起始地址（本端进程虚拟地址）
+	uint64_t mmap_size;//mmap映射的实际内存大小（>= size)
+	int fd;//映射自哪个文件
 };
 
 /**
  * Memory structure includes region and mapping information.
  */
 struct rte_vhost_memory {
-	uint32_t nregions;
+	uint32_t nregions;//regions数组大小
 	struct rte_vhost_mem_region regions[];
 };
 
@@ -235,7 +235,9 @@ struct rte_vhost_user_extern_ops {
  * Device and vring operations.
  */
 struct vhost_device_ops {
+	//设备新建通知
 	int (*new_device)(int vid);		/**< Add device. */
+	//设备销毁通知
 	void (*destroy_device)(int vid);	/**< Remove device. */
 
 	int (*vring_state_changed)(int vid, uint16_t queue_id, int enable);	/**< triggered when a vring is enabled or disabled */
@@ -246,9 +248,11 @@ struct vhost_device_ops {
 	 * start/end of live migration, respectively. This callback
 	 * is used to inform the application on such change.
 	 */
-	int (*features_changed)(int vid, uint64_t features);
+	int (*features_changed)(int vid, uint64_t features);//功能发生变更时生效
 
+	//连接新建通知
 	int (*new_connection)(int vid);
+	//连接稍毁通知
 	void (*destroy_connection)(int vid);
 
 	/**
@@ -286,8 +290,10 @@ rte_vhost_gpa_to_vva(struct rte_vhost_memory *mem, uint64_t gpa)
 
 	for (i = 0; i < mem->nregions; i++) {
 		reg = &mem->regions[i];
+		//找到gpa属于那段地址
 		if (gpa >= reg->guest_phys_addr &&
 		    gpa <  reg->guest_phys_addr + reg->size) {
+			//转换为vva地址（对端物理地址相对于段首地址的偏移量＋自已与对端物理地址对应的虚地址）
 			return gpa - reg->guest_phys_addr +
 			       reg->host_user_addr;
 		}
@@ -313,6 +319,7 @@ rte_vhost_gpa_to_vva(struct rte_vhost_memory *mem, uint64_t gpa)
  *  the host virtual address on success, 0 on failure
  */
 __rte_experimental
+//将qemu的物理地址转换为本端的虚拟地址
 static __rte_always_inline uint64_t
 rte_vhost_va_from_guest_pa(struct rte_vhost_memory *mem,
 						   uint64_t gpa, uint64_t *len)
@@ -324,10 +331,13 @@ rte_vhost_va_from_guest_pa(struct rte_vhost_memory *mem,
 		r = &mem->regions[i];
 		if (gpa >= r->guest_phys_addr &&
 		    gpa <  r->guest_phys_addr + r->size) {
+			//gpa在r的内存段内
 
+			//可读取的最大长度
 			if (unlikely(*len > r->guest_phys_addr + r->size - gpa))
 				*len = r->guest_phys_addr + r->size - gpa;
 
+			//转换后地址
 			return gpa - r->guest_phys_addr +
 			       r->host_user_addr;
 		}
diff --git a/lib/librte_vhost/socket.c b/lib/librte_vhost/socket.c
index a34bc7f9a..f30e1cd0d 100644
--- a/lib/librte_vhost/socket.c
+++ b/lib/librte_vhost/socket.c
@@ -30,16 +30,16 @@ TAILQ_HEAD(vhost_user_connection_list, vhost_user_connection);
  * vhost_user_socket struct will be created.
  */
 struct vhost_user_socket {
-	struct vhost_user_connection_list conn_list;
+	struct vhost_user_connection_list conn_list;//有这个socket上有多少个连接
 	pthread_mutex_t conn_mutex;
-	char *path;
-	int socket_fd;
-	struct sockaddr_un un;
-	bool is_server;
-	bool reconnect;
-	bool dequeue_zero_copy;
+	char *path;//server unix socket地址
+	int socket_fd;//unix socket对应的fd
+	struct sockaddr_un un;//采用path构造的unix地址
+	bool is_server;//是否为server模式
+	bool reconnect;//是否开启重连接
+	bool dequeue_zero_copy;//是否开启出队zero copy
 	bool iommu_support;
-	bool use_builtin_virtio_net;
+	bool use_builtin_virtio_net;//是否使用内建的virtio_net
 	bool extbuf;
 	bool linearbuf;
 
@@ -50,8 +50,8 @@ struct vhost_user_socket {
 	 * It is also the final feature bits used for vhost-user
 	 * features negotiation.
 	 */
-	uint64_t supported_features;
-	uint64_t features;
+	uint64_t supported_features;//支持的功能
+	uint64_t features;//生效的功能
 
 	uint64_t protocol_features;
 
@@ -62,21 +62,22 @@ struct vhost_user_socket {
 	 */
 	int vdpa_dev_id;
 
-	struct vhost_device_ops const *notify_ops;
+	struct vhost_device_ops const *notify_ops;//通知操作集
 };
 
 struct vhost_user_connection {
-	struct vhost_user_socket *vsocket;
-	int connfd;
-	int vid;
+	struct vhost_user_socket *vsocket;//连接对应的vsocket
+	int connfd;//连接的fd
+	int vid;//vhost设备id号
 
 	TAILQ_ENTRY(vhost_user_connection) next;
 };
 
 #define MAX_VHOST_SOCKET 1024
 struct vhost_user {
+	//用于存放所有已创建的socket,采用顺序表格式，当移除时，需要将后面的前移
 	struct vhost_user_socket *vsockets[MAX_VHOST_SOCKET];
-	struct fdset fdset;
+	struct fdset fdset;//用于注册所有fd
 	int vsocket_cnt;
 	pthread_mutex_t mutex;
 };
@@ -88,6 +89,7 @@ static void vhost_user_read_cb(int fd, void *dat, int *remove);
 static int create_unix_socket(struct vhost_user_socket *vsocket);
 static int vhost_user_start_client(struct vhost_user_socket *vsocket);
 
+//全局变量，用于维护所有vhost_user的控制fd
 static struct vhost_user vhost_user = {
 	.fdset = {
 		.fd = { [0 ... MAX_FDS - 1] = {-1, NULL, NULL, NULL, 0} },
@@ -103,6 +105,7 @@ static struct vhost_user vhost_user = {
  * return bytes# of read on success or negative val on failure. Update fdnum
  * with number of fds read.
  */
+//控制消息读取
 int
 read_fd_message(int sockfd, char *buf, int buflen, int *fds, int max_fds,
 		int *fd_num)
@@ -125,12 +128,14 @@ read_fd_message(int sockfd, char *buf, int buflen, int *fds, int max_fds,
 	msgh.msg_control = control;
 	msgh.msg_controllen = sizeof(control);
 
+	//自socket fd中读取消息
 	ret = recvmsg(sockfd, &msgh, 0);
 	if (ret <= 0) {
 		RTE_LOG(ERR, VHOST_CONFIG, "recvmsg failed\n");
 		return ret;
 	}
 
+	//消息被截断，收取失败
 	if (msgh.msg_flags & (MSG_TRUNC | MSG_CTRUNC)) {
 		RTE_LOG(ERR, VHOST_CONFIG, "truncted msg\n");
 		return -1;
@@ -142,18 +147,21 @@ read_fd_message(int sockfd, char *buf, int buflen, int *fds, int max_fds,
 			(cmsg->cmsg_type == SCM_RIGHTS)) {
 			got_fds = (cmsg->cmsg_len - CMSG_LEN(0)) / sizeof(int);
 			*fd_num = got_fds;
+			//收到发送过来的文件描述符，将其保存在fds中
 			memcpy(fds, CMSG_DATA(cmsg), got_fds * sizeof(int));
 			break;
 		}
 	}
 
 	/* Clear out unused file descriptors */
+	//通过将fds中不足got_fds的元素置为－1来表明数组长度
 	while (got_fds < max_fds)
 		fds[got_fds++] = -1;
 
 	return ret;
 }
 
+//向对端响应消息
 int
 send_fd_message(int sockfd, char *buf, int buflen, int *fds, int fd_num)
 {
@@ -202,6 +210,7 @@ send_fd_message(int sockfd, char *buf, int buflen, int *fds, int fd_num)
 	return ret;
 }
 
+//加入连接，注册fd读事件
 static void
 vhost_user_add_connection(int fd, struct vhost_user_socket *vsocket)
 {
@@ -219,18 +228,20 @@ vhost_user_add_connection(int fd, struct vhost_user_socket *vsocket)
 		return;
 	}
 
+	//申请一个vhost
 	vid = vhost_new_device();
 	if (vid == -1) {
 		goto err;
 	}
 
 	size = strnlen(vsocket->path, PATH_MAX);
-	vhost_set_ifname(vid, vsocket->path, size);
+	vhost_set_ifname(vid, vsocket->path, size);//设置设备名称
 
 	vhost_set_builtin_virtio_net(vid, vsocket->use_builtin_virtio_net);
 
 	vhost_attach_vdpa_device(vid, vsocket->vdpa_dev_id);
 
+	//如果vsocket开启了入队zero copy,则设置在dev上
 	if (vsocket->dequeue_zero_copy)
 		vhost_enable_dequeue_zero_copy(vid);
 
@@ -242,6 +253,7 @@ vhost_user_add_connection(int fd, struct vhost_user_socket *vsocket)
 
 	RTE_LOG(INFO, VHOST_CONFIG, "new device, handle is %d\n", vid);
 
+	//如果有通知回调，则通知出现一个新连接
 	if (vsocket->notify_ops->new_connection) {
 		ret = vsocket->notify_ops->new_connection(vid);
 		if (ret < 0) {
@@ -252,9 +264,13 @@ vhost_user_add_connection(int fd, struct vhost_user_socket *vsocket)
 		}
 	}
 
+	//初始化连接，注册fd进行读取
 	conn->connfd = fd;
 	conn->vsocket = vsocket;
 	conn->vid = vid;
+
+	//将与server间连接的fd注册到fdset中，如果有read事件发生，则调用vhost_user_read_cb
+	//此read方法，主要用于处理控制消息
 	ret = fdset_add(&vhost_user.fdset, fd, vhost_user_read_cb,
 			NULL, conn);
 	if (ret < 0) {
@@ -262,6 +278,7 @@ vhost_user_add_connection(int fd, struct vhost_user_socket *vsocket)
 			"failed to add fd %d into vhost server fdset\n",
 			fd);
 
+		//发起连接销毁通知
 		if (vsocket->notify_ops->destroy_connection)
 			vsocket->notify_ops->destroy_connection(conn->vid);
 
@@ -272,6 +289,7 @@ vhost_user_add_connection(int fd, struct vhost_user_socket *vsocket)
 	TAILQ_INSERT_TAIL(&vsocket->conn_list, conn, next);
 	pthread_mutex_unlock(&vsocket->conn_mutex);
 
+	//这个通知发起，但对端只是read,并不处理
 	fdset_pipe_notify(&vhost_user.fdset);
 	return;
 
@@ -283,6 +301,8 @@ vhost_user_add_connection(int fd, struct vhost_user_socket *vsocket)
 }
 
 /* call back when there is new vhost-user connection from client  */
+//vhost_user 服务端读取函数，监听新连接，当接入新连接时，仅仅是注册fd的read事件
+//与client模式处理方式相同（即dpdk仅仅总是协商的非发起方）
 static void
 vhost_user_server_new_connection(int fd, void *dat, int *remove __rte_unused)
 {
@@ -293,9 +313,12 @@ vhost_user_server_new_connection(int fd, void *dat, int *remove __rte_unused)
 		return;
 
 	RTE_LOG(INFO, VHOST_CONFIG, "new vhost user connection is %d\n", fd);
+	//收到新的连接，加入读事件
 	vhost_user_add_connection(fd, vsocket);
 }
 
+//vhost_user客户端读取处理(控制消息处理）
+//从这里可以看出，一旦连接上server,server就会向对端通知消息
 static void
 vhost_user_read_cb(int connfd, void *dat, int *remove)
 {
@@ -303,10 +326,12 @@ vhost_user_read_cb(int connfd, void *dat, int *remove)
 	struct vhost_user_socket *vsocket = conn->vsocket;
 	int ret;
 
+	//读取并处理控制消息
 	ret = vhost_user_msg_handler(conn->vid, connfd);
 	if (ret < 0) {
 		struct virtio_net *dev = get_device(conn->vid);
 
+		//处理控制消息失败后，重新连接
 		close(connfd);
 		*remove = 1;
 
@@ -324,6 +349,7 @@ vhost_user_read_cb(int connfd, void *dat, int *remove)
 
 		free(conn);
 
+		//如果支持重连，则重新创建，并连接
 		if (vsocket->reconnect) {
 			create_unix_socket(vsocket);
 			vhost_user_start_client(vsocket);
@@ -331,18 +357,21 @@ vhost_user_read_cb(int connfd, void *dat, int *remove)
 	}
 }
 
+//创建vhost user对应的unix socket
 static int
 create_unix_socket(struct vhost_user_socket *vsocket)
 {
 	int fd;
 	struct sockaddr_un *un = &vsocket->un;
 
+	//创建对应的socket
 	fd = socket(AF_UNIX, SOCK_STREAM, 0);
 	if (fd < 0)
 		return -1;
 	RTE_LOG(INFO, VHOST_CONFIG, "vhost-user %s: socket created, fd: %d\n",
 		vsocket->is_server ? "server" : "client", fd);
 
+	//如果不是server,则将fd置为非阻塞
 	if (!vsocket->is_server && fcntl(fd, F_SETFL, O_NONBLOCK)) {
 		RTE_LOG(ERR, VHOST_CONFIG,
 			"vhost-user: can't set nonblocking mode for socket, fd: "
@@ -353,13 +382,14 @@ create_unix_socket(struct vhost_user_socket *vsocket)
 
 	memset(un, 0, sizeof(*un));
 	un->sun_family = AF_UNIX;
-	strncpy(un->sun_path, vsocket->path, sizeof(un->sun_path));
+	strncpy(un->sun_path, vsocket->path, sizeof(un->sun_path));//使用path进行监听
 	un->sun_path[sizeof(un->sun_path) - 1] = '\0';
 
 	vsocket->socket_fd = fd;
 	return 0;
 }
 
+//启动vhost user server
 static int
 vhost_user_start_server(struct vhost_user_socket *vsocket)
 {
@@ -377,6 +407,7 @@ vhost_user_start_server(struct vhost_user_socket *vsocket)
 	 * The user must ensure that the socket does not exist before
 	 * registering the vhost driver in server mode.
 	 */
+	//bind到地址un
 	ret = bind(fd, (struct sockaddr *)&vsocket->un, sizeof(vsocket->un));
 	if (ret < 0) {
 		RTE_LOG(ERR, VHOST_CONFIG,
@@ -386,10 +417,12 @@ vhost_user_start_server(struct vhost_user_socket *vsocket)
 	}
 	RTE_LOG(INFO, VHOST_CONFIG, "bind to %s\n", path);
 
+	//开始监听地址
 	ret = listen(fd, MAX_VIRTIO_BACKLOG);
 	if (ret < 0)
 		goto err;
 
+	//监听读事件，接入新的client
 	ret = fdset_add(&vhost_user.fdset, fd, vhost_user_server_new_connection,
 		  NULL, vsocket);
 	if (ret < 0) {
@@ -417,17 +450,19 @@ struct vhost_user_reconnect {
 TAILQ_HEAD(vhost_user_reconnect_tailq_list, vhost_user_reconnect);
 struct vhost_user_reconnect_list {
 	struct vhost_user_reconnect_tailq_list head;
-	pthread_mutex_t mutex;
+	pthread_mutex_t mutex;//重连保护锁
 };
 
-static struct vhost_user_reconnect_list reconn_list;
-static pthread_t reconn_tid;
+static struct vhost_user_reconnect_list reconn_list;//重连链表
+static pthread_t reconn_tid;//重连线程id
 
+//连接到un地址，并设置为非阻塞
 static int
 vhost_user_connect_nonblock(int fd, struct sockaddr *un, size_t sz)
 {
 	int ret, flags;
 
+	//连接到un
 	ret = connect(fd, un, sz);
 	if (ret < 0 && errno != EISCONN)
 		return -1;
@@ -436,16 +471,21 @@ vhost_user_connect_nonblock(int fd, struct sockaddr *un, size_t sz)
 	if (flags < 0) {
 		RTE_LOG(ERR, VHOST_CONFIG,
 			"can't get flags for connfd %d\n", fd);
+		//fd有问题
 		return -2;
 	}
+
+	//将socket 置为非阻塞
 	if ((flags & O_NONBLOCK) && fcntl(fd, F_SETFL, flags & ~O_NONBLOCK)) {
 		RTE_LOG(ERR, VHOST_CONFIG,
 				"can't disable nonblocking on fd %d\n", fd);
+		//fd有问题
 		return -2;
 	}
 	return 0;
 }
 
+//周期性处理reconn_list链接，执行连接操作
 static void *
 vhost_user_client_reconnect(void *arg __rte_unused)
 {
@@ -463,10 +503,12 @@ vhost_user_client_reconnect(void *arg __rte_unused)
 		     reconn != NULL; reconn = next) {
 			next = TAILQ_NEXT(reconn, next);
 
+			//实现重连
 			ret = vhost_user_connect_nonblock(reconn->fd,
 						(struct sockaddr *)&reconn->un,
 						sizeof(reconn->un));
 			if (ret == -2) {
+				//重连失败，主要是fd的问题，不再尝试
 				close(reconn->fd);
 				RTE_LOG(ERR, VHOST_CONFIG,
 					"reconnection for fd %d failed\n",
@@ -474,23 +516,28 @@ vhost_user_client_reconnect(void *arg __rte_unused)
 				goto remove_fd;
 			}
 			if (ret == -1)
+				//重链失败，后续尝试
 				continue;
 
+			//重连成功，加入connect
 			RTE_LOG(INFO, VHOST_CONFIG,
 				"%s: connected\n", reconn->vsocket->path);
 			vhost_user_add_connection(reconn->fd, reconn->vsocket);
+
 remove_fd:
+			//重连成功，不需要再保存在reconnect链表了
 			TAILQ_REMOVE(&reconn_list.head, reconn, next);
 			free(reconn);
 		}
 
 		pthread_mutex_unlock(&reconn_list.mutex);
-		sleep(1);
+		sleep(1);//应搞个信号量来处理此情况，而不是简单的sleep(1)
 	}
 
 	return NULL;
 }
 
+//初始化重连线程
 static int
 vhost_user_reconnect_init(void)
 {
@@ -503,6 +550,7 @@ vhost_user_reconnect_init(void)
 	}
 	TAILQ_INIT(&reconn_list.head);
 
+	//初始化重连线程
 	ret = rte_ctrl_thread_create(&reconn_tid, "vhost_reconn", NULL,
 			     vhost_user_client_reconnect, NULL);
 	if (ret != 0) {
@@ -516,6 +564,7 @@ vhost_user_reconnect_init(void)
 	return ret;
 }
 
+//连接到vsocket.un，如果连接成功，则监测，并注册read处理函数
 static int
 vhost_user_start_client(struct vhost_user_socket *vsocket)
 {
@@ -527,19 +576,23 @@ vhost_user_start_client(struct vhost_user_socket *vsocket)
 	ret = vhost_user_connect_nonblock(fd, (struct sockaddr *)&vsocket->un,
 					  sizeof(vsocket->un));
 	if (ret == 0) {
+		//连接成功，将此fd加入到fdset中，以便开始协商
 		vhost_user_add_connection(fd, vsocket);
 		return 0;
 	}
 
+	//连接失败
 	RTE_LOG(WARNING, VHOST_CONFIG,
 		"failed to connect to %s: %s\n",
 		path, strerror(errno));
 
 	if (ret == -2 || !vsocket->reconnect) {
+		//无重连，直接失败
 		close(fd);
 		return -1;
 	}
 
+	//有重连，申请reconn结构，并挂在reconn-list上，重连线程会进行尝试
 	RTE_LOG(INFO, VHOST_CONFIG, "%s: reconnecting...\n", path);
 	reconn = malloc(sizeof(*reconn));
 	if (reconn == NULL) {
@@ -552,12 +605,14 @@ vhost_user_start_client(struct vhost_user_socket *vsocket)
 	reconn->fd = fd;
 	reconn->vsocket = vsocket;
 	pthread_mutex_lock(&reconn_list.mutex);
+	//加入到重连链表
 	TAILQ_INSERT_TAIL(&reconn_list.head, reconn, next);
 	pthread_mutex_unlock(&reconn_list.mutex);
 
 	return 0;
 }
 
+//通过path找到vsocket
 static struct vhost_user_socket *
 find_vhost_user_socket(const char *path)
 {
@@ -576,6 +631,7 @@ find_vhost_user_socket(const char *path)
 	return NULL;
 }
 
+//vsocket关掉某此功能
 int
 rte_vhost_driver_attach_vdpa_device(const char *path, int did)
 {
@@ -642,6 +698,7 @@ rte_vhost_driver_disable_features(const char *path, uint64_t features)
 	return vsocket ? 0 : -1;
 }
 
+//vsocket开启指定功能
 int
 rte_vhost_driver_enable_features(const char *path, uint64_t features)
 {
@@ -650,6 +707,7 @@ rte_vhost_driver_enable_features(const char *path, uint64_t features)
 	pthread_mutex_lock(&vhost_user.mutex);
 	vsocket = find_vhost_user_socket(path);
 	if (vsocket) {
+		//要开启的功能vsocket不支持时，报错
 		if ((vsocket->supported_features & features) != features) {
 			/*
 			 * trying to enable features the driver doesn't
@@ -658,6 +716,7 @@ rte_vhost_driver_enable_features(const char *path, uint64_t features)
 			pthread_mutex_unlock(&vhost_user.mutex);
 			return -1;
 		}
+		//开启对应功能
 		vsocket->features |= features;
 	}
 	pthread_mutex_unlock(&vhost_user.mutex);
@@ -665,6 +724,7 @@ rte_vhost_driver_enable_features(const char *path, uint64_t features)
 	return vsocket ? 0 : -1;
 }
 
+//设置设备支持的功能，及当前功能
 int
 rte_vhost_driver_set_features(const char *path, uint64_t features)
 {
@@ -679,6 +739,7 @@ rte_vhost_driver_set_features(const char *path, uint64_t features)
 		/* Anyone setting feature bits is implementing their own vhost
 		 * device backend.
 		 */
+		//功能位被重置，设置不用使用内建的virtio_net
 		vsocket->use_builtin_virtio_net = false;
 	}
 	pthread_mutex_unlock(&vhost_user.mutex);
@@ -686,6 +747,7 @@ rte_vhost_driver_set_features(const char *path, uint64_t features)
 	return vsocket ? 0 : -1;
 }
 
+//获得的当前vsocket上生效的功能
 int
 rte_vhost_driver_get_features(const char *path, uint64_t *features)
 {
@@ -726,6 +788,7 @@ rte_vhost_driver_get_features(const char *path, uint64_t *features)
 	return ret;
 }
 
+//获取当前vsocket上生效的协议功能
 int
 rte_vhost_driver_set_protocol_features(const char *path,
 		uint64_t protocol_features)
@@ -753,6 +816,7 @@ rte_vhost_driver_get_protocol_features(const char *path,
 	pthread_mutex_lock(&vhost_user.mutex);
 	vsocket = find_vhost_user_socket(path);
 	if (!vsocket) {
+		//无对应的socket，报错
 		RTE_LOG(ERR, VHOST_CONFIG,
 			"socket file %s is not registered yet.\n", path);
 		ret = -1;
@@ -775,6 +839,7 @@ rte_vhost_driver_get_protocol_features(const char *path,
 		goto unlock_exit;
 	}
 
+	//与得到的vdpa_dev协议取与，得到共同支持的
 	*protocol_features = vsocket->protocol_features
 		& vdpa_protocol_features;
 
@@ -816,6 +881,7 @@ rte_vhost_driver_get_queue_num(const char *path, uint32_t *queue_num)
 		goto unlock_exit;
 	}
 
+	//默认最大支持0x80个队列
 	*queue_num = RTE_MIN((uint32_t)VHOST_MAX_QUEUE_PAIRS, vdpa_queue_num);
 
 unlock_exit:
@@ -842,6 +908,7 @@ vhost_user_socket_mem_free(struct vhost_user_socket *vsocket)
  * (the default case), or client (when RTE_VHOST_USER_CLIENT) flag
  * is set.
  */
+//vhost-user socket注册
 int
 rte_vhost_driver_register(const char *path, uint64_t flags)
 {
@@ -853,12 +920,14 @@ rte_vhost_driver_register(const char *path, uint64_t flags)
 
 	pthread_mutex_lock(&vhost_user.mutex);
 
+	//vsocket数量已超限，报错
 	if (vhost_user.vsocket_cnt == MAX_VHOST_SOCKET) {
 		RTE_LOG(ERR, VHOST_CONFIG,
 			"error: the number of vhost sockets reaches maximum\n");
 		goto out;
 	}
 
+	//申请vsocket对象
 	vsocket = malloc(sizeof(struct vhost_user_socket));
 	if (!vsocket)
 		goto out;
@@ -877,6 +946,8 @@ rte_vhost_driver_register(const char *path, uint64_t flags)
 			"error: failed to init connection mutex\n");
 		goto out_free;
 	}
+
+	//是否开启出队zero copy
 	vsocket->dequeue_zero_copy = flags & RTE_VHOST_USER_DEQUEUE_ZERO_COPY;
 	vsocket->extbuf = flags & RTE_VHOST_USER_EXTBUF_SUPPORT;
 	vsocket->linearbuf = flags & RTE_VHOST_USER_LINEARBUF_SUPPORT;
@@ -901,7 +972,9 @@ rte_vhost_driver_register(const char *path, uint64_t flags)
 	 * rte_vhost_driver_set_features(), which will overwrite following
 	 * two values.
 	 */
+	//默认使用内建的virtio_net
 	vsocket->use_builtin_virtio_net = true;
+	//标记支持的功能
 	vsocket->supported_features = VIRTIO_NET_SUPPORTED_FEATURES;
 	vsocket->features           = VIRTIO_NET_SUPPORTED_FEATURES;
 	vsocket->protocol_features  = VHOST_USER_PROTOCOL_FEATURES;
@@ -952,6 +1025,7 @@ rte_vhost_driver_register(const char *path, uint64_t flags)
 	}
 
 	if (!(flags & RTE_VHOST_USER_IOMMU_SUPPORT)) {
+		//如果未开启iommu支持时，取掉对应标记位
 		vsocket->supported_features &= ~(1ULL << VIRTIO_F_IOMMU_PLATFORM);
 		vsocket->features &= ~(1ULL << VIRTIO_F_IOMMU_PLATFORM);
 	}
@@ -968,9 +1042,11 @@ rte_vhost_driver_register(const char *path, uint64_t flags)
 #endif
 	}
 
+	//是否指明为vhost user client
 	if ((flags & RTE_VHOST_USER_CLIENT) != 0) {
 		vsocket->reconnect = !(flags & RTE_VHOST_USER_NO_RECONNECT);
 		if (vsocket->reconnect && reconn_tid == 0) {
+			//如果需要支持重连，且重连线程还未初始化，则初始化重连线程
 			if (vhost_user_reconnect_init() != 0)
 				goto out_mutex;
 		}
@@ -982,6 +1058,7 @@ rte_vhost_driver_register(const char *path, uint64_t flags)
 		goto out_mutex;
 	}
 
+	//注册刚创建的vsocket
 	vhost_user.vsockets[vhost_user.vsocket_cnt++] = vsocket;
 
 	pthread_mutex_unlock(&vhost_user.mutex);
@@ -1027,6 +1104,7 @@ vhost_user_remove_reconnect(struct vhost_user_socket *vsocket)
 /**
  * Unregister the specified vhost socket
  */
+//关闭指定的vsocket
 int
 rte_vhost_driver_unregister(const char *path)
 {
@@ -1040,6 +1118,7 @@ rte_vhost_driver_unregister(const char *path)
 again:
 	pthread_mutex_lock(&vhost_user.mutex);
 
+	//先找到path对应的vsocket
 	for (i = 0; i < vhost_user.vsocket_cnt; i++) {
 		struct vhost_user_socket *vsocket = vhost_user.vsockets[i];
 
@@ -1073,21 +1152,26 @@ rte_vhost_driver_unregister(const char *path)
 			}
 			pthread_mutex_unlock(&vsocket->conn_mutex);
 
+			//要解注册的就是这个vsocket
 			if (vsocket->is_server) {
+				//它是服务端，移除掉这个fd,不再管理它
 				fdset_del(&vhost_user.fdset,
 						vsocket->socket_fd);
 				close(vsocket->socket_fd);
 				unlink(path);
 			} else if (vsocket->reconnect) {
+				//如果它处于重连队列，将其移除
 				vhost_user_remove_reconnect(vsocket);
 			}
 
+			//关闭掉此socket上的所有连接
 			pthread_mutex_destroy(&vsocket->conn_mutex);
 			vhost_user_socket_mem_free(vsocket);
 
+			//将最后一个设备的指针移动到此处，填补空缺
 			count = --vhost_user.vsocket_cnt;
 			vhost_user.vsockets[i] = vhost_user.vsockets[count];
-			vhost_user.vsockets[count] = NULL;
+			vhost_user.vsockets[count] = NULL;//最后一个置空
 			pthread_mutex_unlock(&vhost_user.mutex);
 
 			return 0;
@@ -1101,6 +1185,7 @@ rte_vhost_driver_unregister(const char *path)
 /*
  * Register ops so that we can add/remove device to data core.
  */
+//为指定path(通过path,可找到vsocket)注册notify_ops
 int
 rte_vhost_driver_callback_register(const char *path,
 	struct vhost_device_ops const * const ops)
@@ -1116,6 +1201,7 @@ rte_vhost_driver_callback_register(const char *path,
 	return vsocket ? 0 : -1;
 }
 
+//通过path查找vhost设备的操作集
 struct vhost_device_ops const *
 vhost_driver_callback_get(const char *path)
 {
@@ -1128,11 +1214,13 @@ vhost_driver_callback_get(const char *path)
 	return vsocket ? vsocket->notify_ops : NULL;
 }
 
+//处理此path对应的socket(或监听，或连接到服务器）
+//内部实现：采用一个线程读取fd来触发
 int
 rte_vhost_driver_start(const char *path)
 {
 	struct vhost_user_socket *vsocket;
-	static pthread_t fdset_tid;
+	static pthread_t fdset_tid;//全局的fd维护线程
 
 	pthread_mutex_lock(&vhost_user.mutex);
 	vsocket = find_vhost_user_socket(path);
@@ -1141,6 +1229,7 @@ rte_vhost_driver_start(const char *path)
 	if (!vsocket)
 		return -1;
 
+	//如果fdset dispatch线程还没有创建，则创建
 	if (fdset_tid == 0) {
 		/**
 		 * create a pipe which will be waited by poll and notified to
@@ -1152,6 +1241,7 @@ rte_vhost_driver_start(const char *path)
 			return -1;
 		}
 
+		//启动线程，处理vhost_user的读写事件
 		int ret = rte_ctrl_thread_create(&fdset_tid,
 			"vhost-events", NULL, fdset_event_dispatch,
 			&vhost_user.fdset);
@@ -1165,7 +1255,9 @@ rte_vhost_driver_start(const char *path)
 	}
 
 	if (vsocket->is_server)
+		//监听地址，接受客户端连接
 		return vhost_user_start_server(vsocket);
 	else
+		//连接服务器，读取服务器输出
 		return vhost_user_start_client(vsocket);
 }
diff --git a/lib/librte_vhost/vdpa.c b/lib/librte_vhost/vdpa.c
index 2b8670873..ac9c5c311 100644
--- a/lib/librte_vhost/vdpa.c
+++ b/lib/librte_vhost/vdpa.c
@@ -14,9 +14,11 @@
 #include "rte_vdpa.h"
 #include "vhost.h"
 
+//记录vdpa设备
 static struct rte_vdpa_device *vdpa_devices[MAX_VHOST_DEVICE];
 static uint32_t vdpa_device_num;
 
+//比对两个vdap设备地址是否相等
 static bool
 is_same_vdpa_device(struct rte_vdpa_dev_addr *a,
 		struct rte_vdpa_dev_addr *b)
@@ -41,6 +43,7 @@ is_same_vdpa_device(struct rte_vdpa_dev_addr *a,
 	return ret;
 }
 
+//vdpa设备注册
 int
 rte_vdpa_register_device(struct rte_vdpa_dev_addr *addr,
 		struct rte_vdpa_dev_ops *ops)
@@ -55,9 +58,10 @@ rte_vdpa_register_device(struct rte_vdpa_dev_addr *addr,
 	for (i = 0; i < MAX_VHOST_DEVICE; i++) {
 		dev = vdpa_devices[i];
 		if (dev && is_same_vdpa_device(&dev->addr, addr))
-			return -1;
+			return -1;//设备已注册
 	}
 
+	//找空闲节点
 	for (i = 0; i < MAX_VHOST_DEVICE; i++) {
 		if (vdpa_devices[i] == NULL)
 			break;
@@ -66,6 +70,7 @@ rte_vdpa_register_device(struct rte_vdpa_dev_addr *addr,
 	if (i == MAX_VHOST_DEVICE)
 		return -1;
 
+	//设置vdap设备
 	snprintf(device_name, sizeof(device_name), "vdpa-dev-%d", i);
 	dev = rte_zmalloc(device_name, sizeof(struct rte_vdpa_device),
 			RTE_CACHE_LINE_SIZE);
@@ -80,6 +85,7 @@ rte_vdpa_register_device(struct rte_vdpa_dev_addr *addr,
 	return i;
 }
 
+//vdpa设备解注册
 int
 rte_vdpa_unregister_device(int did)
 {
@@ -93,6 +99,7 @@ rte_vdpa_unregister_device(int did)
 	return did;
 }
 
+//通过vdap设备地址查找vdpa设备
 int
 rte_vdpa_find_device_id(struct rte_vdpa_dev_addr *addr)
 {
@@ -111,6 +118,7 @@ rte_vdpa_find_device_id(struct rte_vdpa_dev_addr *addr)
 	return -1;
 }
 
+//获取vdpa设备
 struct rte_vdpa_device *
 rte_vdpa_get_device(int did)
 {
diff --git a/lib/librte_vhost/vhost.c b/lib/librte_vhost/vhost.c
index 1cbe948f7..7ce68b94f 100644
--- a/lib/librte_vhost/vhost.c
+++ b/lib/librte_vhost/vhost.c
@@ -43,8 +43,10 @@ __vhost_iova_to_vva(struct virtio_net *dev, struct vhost_virtqueue *vq,
 	if (tmp_size == *size)
 		return vva;
 
+	//未找到，设置pending miss
 	iova += tmp_size;
 
+	//查询是否已在pending链上，尝试获取了
 	if (!vhost_user_iotlb_pending_miss(vq, iova, perm)) {
 		/*
 		 * iotlb_lock is read-locked for a full burst,
@@ -57,6 +59,7 @@ __vhost_iova_to_vva(struct virtio_net *dev, struct vhost_virtqueue *vq,
 
 		vhost_user_iotlb_pending_insert(vq, iova, perm);
 		if (vhost_user_iotlb_miss(dev, iova, perm)) {
+			//向对端请求
 			RTE_LOG(ERR, VHOST_CONFIG,
 				"IOTLB miss req failed for IOVA 0x%" PRIx64 "\n",
 				iova);
@@ -66,6 +69,7 @@ __vhost_iova_to_vva(struct virtio_net *dev, struct vhost_virtqueue *vq,
 		vhost_user_iotlb_rd_lock(vq);
 	}
 
+	//已加入pending,直接返回
 	return 0;
 }
 
@@ -318,6 +322,7 @@ cleanup_device(struct virtio_net *dev, int destroy)
 
 	vhost_backend_cleanup(dev);
 
+	//销毁队列
 	for (i = 0; i < dev->nr_vring; i++) {
 		cleanup_vq(dev->virtqueue[i], destroy);
 		cleanup_vq_inflight(dev, dev->virtqueue[i]);
@@ -350,11 +355,13 @@ free_device(struct virtio_net *dev)
 	rte_free(dev);
 }
 
+//实现vq->desc,vq->avali,vq->used地址翻译
 static int
 vring_translate_split(struct virtio_net *dev, struct vhost_virtqueue *vq)
 {
 	uint64_t req_size, size;
 
+	//转换desc
 	req_size = sizeof(struct vring_desc) * vq->size;
 	size = req_size;
 	vq->desc = (struct vring_desc *)(uintptr_t)vhost_iova_to_vva(dev, vq,
@@ -363,6 +370,7 @@ vring_translate_split(struct virtio_net *dev, struct vhost_virtqueue *vq)
 	if (!vq->desc || size != req_size)
 		return -1;
 
+	//转换avail
 	req_size = sizeof(struct vring_avail);
 	req_size += sizeof(uint16_t) * vq->size;
 	if (dev->features & (1ULL << VIRTIO_RING_F_EVENT_IDX))
@@ -374,6 +382,7 @@ vring_translate_split(struct virtio_net *dev, struct vhost_virtqueue *vq)
 	if (!vq->avail || size != req_size)
 		return -1;
 
+	//转换used
 	req_size = sizeof(struct vring_used);
 	req_size += sizeof(struct vring_used_elem) * vq->size;
 	if (dev->features & (1ULL << VIRTIO_RING_F_EVENT_IDX))
@@ -439,6 +448,7 @@ vring_translate(struct virtio_net *dev, struct vhost_virtqueue *vq)
 	return 0;
 }
 
+//将vring置为无效
 void
 vring_invalidate(struct virtio_net *dev, struct vhost_virtqueue *vq)
 {
@@ -455,11 +465,13 @@ vring_invalidate(struct virtio_net *dev, struct vhost_virtqueue *vq)
 		vhost_user_iotlb_wr_unlock(vq);
 }
 
+//初始化vq
 static void
 init_vring_queue(struct virtio_net *dev, uint32_t vring_idx)
 {
 	struct vhost_virtqueue *vq;
 
+	//数量超限
 	if (vring_idx >= VHOST_MAX_VRING) {
 		RTE_LOG(ERR, VHOST_CONFIG,
 				"Failed not init vring, out of bound (%d)\n",
@@ -500,6 +512,7 @@ reset_vring_queue(struct virtio_net *dev, uint32_t vring_idx)
 	vq->callfd = callfd;
 }
 
+//申请编号为vring_idx的virtqueue
 int
 alloc_vring_queue(struct virtio_net *dev, uint32_t vring_idx)
 {
@@ -512,6 +525,7 @@ alloc_vring_queue(struct virtio_net *dev, uint32_t vring_idx)
 		return -1;
 	}
 
+	//初始化vring_idx队列
 	dev->virtqueue[vring_idx] = vq;
 	init_vring_queue(dev, vring_idx);
 	rte_spinlock_init(&vq->access_lock);
@@ -519,7 +533,7 @@ alloc_vring_queue(struct virtio_net *dev, uint32_t vring_idx)
 	vq->used_wrap_counter = 1;
 	vq->signalled_used_valid = false;
 
-	dev->nr_vring += 1;
+	dev->nr_vring += 1;//设备vring数量增加
 
 	return 0;
 }
@@ -546,17 +560,20 @@ reset_device(struct virtio_net *dev)
  * Invoked when there is a new vhost-user connection established (when
  * there is a new virtio device being attached).
  */
+//申请一个vhost设备，返回vhost设备对应的id号
 int
 vhost_new_device(void)
 {
 	struct virtio_net *dev;
 	int i;
 
+	//分配一个vhost_id(通过找空闲的vhost_devices来确定）
 	for (i = 0; i < MAX_VHOST_DEVICE; i++) {
 		if (vhost_devices[i] == NULL)
 			break;
 	}
 
+	//vhost_device达到上限，报错
 	if (i == MAX_VHOST_DEVICE) {
 		RTE_LOG(ERR, VHOST_CONFIG,
 			"Failed to find a free slot for new device.\n");
@@ -570,6 +587,7 @@ vhost_new_device(void)
 		return -1;
 	}
 
+	//填充virtio设备
 	vhost_devices[i] = dev;
 	dev->vid = i;
 	dev->flags = VIRTIO_DEV_BUILTIN_VIRTIO_NET;
@@ -601,6 +619,7 @@ vhost_destroy_device_notify(struct virtio_net *dev)
  * Invoked when there is the vhost-user connection is broken (when
  * the virtio device is being detached).
  */
+//销毁指定vdev
 void
 vhost_destroy_device(int vid)
 {
@@ -609,6 +628,7 @@ vhost_destroy_device(int vid)
 	if (dev == NULL)
 		return;
 
+	//销毁dev
 	vhost_destroy_device_notify(dev);
 
 	cleanup_device(dev, 1);
@@ -631,6 +651,7 @@ vhost_attach_vdpa_device(int vid, int did)
 	dev->vdpa_dev_id = did;
 }
 
+//设置virtio_net设备名称
 void
 vhost_set_ifname(int vid, const char *if_name, unsigned int if_len)
 {
@@ -648,6 +669,7 @@ vhost_set_ifname(int vid, const char *if_name, unsigned int if_len)
 	dev->ifname[sizeof(dev->ifname) - 1] = '\0';
 }
 
+//开启zero copy
 void
 vhost_enable_dequeue_zero_copy(int vid)
 {
@@ -670,6 +692,7 @@ vhost_set_builtin_virtio_net(int vid, bool enable)
 	if (enable)
 		dev->flags |= VIRTIO_DEV_BUILTIN_VIRTIO_NET;
 	else
+		//如果不支持内建的virtio_net，则清楚掉标记
 		dev->flags &= ~VIRTIO_DEV_BUILTIN_VIRTIO_NET;
 }
 
@@ -741,6 +764,7 @@ rte_vhost_get_numa_node(int vid)
 #endif
 }
 
+//队列数向上不分收发队列，故除以2
 uint32_t
 rte_vhost_get_queue_num(int vid)
 {
@@ -752,6 +776,7 @@ rte_vhost_get_queue_num(int vid)
 	return dev->nr_vring / 2;
 }
 
+//ring数目
 uint16_t
 rte_vhost_get_vring_num(int vid)
 {
@@ -763,6 +788,7 @@ rte_vhost_get_vring_num(int vid)
 	return dev->nr_vring;
 }
 
+//获取virtio对应的接口名称
 int
 rte_vhost_get_ifname(int vid, char *buf, size_t len)
 {
@@ -1393,6 +1419,7 @@ int rte_vhost_get_vring_base(int vid, uint16_t queue_id,
 	return 0;
 }
 
+//设置ring的有效位置
 int rte_vhost_set_vring_base(int vid, uint16_t queue_id,
 		uint16_t last_avail_idx, uint16_t last_used_idx)
 {
diff --git a/lib/librte_vhost/vhost.h b/lib/librte_vhost/vhost.h
index 9f11b28a3..0266ea81f 100644
--- a/lib/librte_vhost/vhost.h
+++ b/lib/librte_vhost/vhost.h
@@ -76,9 +76,9 @@
  * from vring to do scatter RX.
  */
 struct buf_vector {
-	uint64_t buf_iova;
-	uint64_t buf_addr;
-	uint32_t buf_len;
+	uint64_t buf_iova;//buffer的对端地址
+	uint64_t buf_addr;//buffer的本端虚拟地址
+	uint32_t buf_len;//buffer长度
 	uint32_t desc_idx;
 };
 
@@ -126,8 +126,8 @@ struct vring_used_elem_packed {
  */
 struct vhost_virtqueue {
 	union {
-		struct vring_desc	*desc;
-		struct vring_packed_desc   *desc_packed;
+		struct vring_desc	*desc;//存放描述信息
+		struct vring_packed_desc   *desc_packed;//存放packet方式的描述符
 	};
 	union {
 		struct vring_avail	*avail;
@@ -137,10 +137,10 @@ struct vhost_virtqueue {
 		struct vring_used	*used;
 		struct vring_packed_desc_event *device_event;
 	};
-	uint32_t		size;
+	uint32_t		size;//队列大小（必须为2的N次方)
 
-	uint16_t		last_avail_idx;
-	uint16_t		last_used_idx;
+	uint16_t		last_avail_idx;//队列待收取位置
+	uint16_t		last_used_idx;//队列上一次发送位置
 	/* Last used index we notify to front end. */
 	uint16_t		signalled_used;
 	bool			signalled_used_valid;
@@ -149,14 +149,14 @@ struct vhost_virtqueue {
 
 	/* Backend value to determine if device should started/stopped */
 	int			backend;
-	int			enabled;
-	int			access_ok;
-	rte_spinlock_t		access_lock;
+	int			enabled;//指示此ring是否被使能（如未使能，则不能进行收发报文）
+	int			access_ok;//指示ring是否可访问
+	rte_spinlock_t		access_lock;//队列保护锁（防多线程写，读）
 
 	/* Used to notify the guest (trigger interrupt) */
 	int			callfd;
 	/* Currently unused as polling mode is enabled */
-	int			kickfd;
+	int			kickfd;//此queue对应的kick fd
 
 	/* Physical address of used ring, for logging */
 	uint64_t		log_guest_addr;
@@ -184,9 +184,9 @@ struct vhost_virtqueue {
 	uint16_t		shadow_aligned_idx;
 	/* Record packed ring first dequeue desc index */
 	uint16_t		shadow_last_used_idx;
-	struct vhost_vring_addr ring_addrs;
+	struct vhost_vring_addr ring_addrs;//设备的ring地址
 
-	struct batch_copy_elem	*batch_copy_elems;
+	struct batch_copy_elem	*batch_copy_elems;//记录需要batch copy的元素（实现mbuf copy)
 	uint16_t		batch_copy_nb_elems;
 	bool			used_wrap_counter;
 	bool			avail_wrap_counter;
@@ -194,11 +194,12 @@ struct vhost_virtqueue {
 	struct log_cache_entry log_cache[VHOST_LOG_CACHE_NR];
 	uint16_t log_cache_nb_elem;
 
+	//tlb相关
 	rte_rwlock_t	iotlb_lock;
 	rte_rwlock_t	iotlb_pending_lock;
 	struct rte_mempool *iotlb_pool;
 	TAILQ_HEAD(, vhost_iotlb_entry) iotlb_list;
-	int				iotlb_cache_nr;
+	int				iotlb_cache_nr;//tlb缓冲数目
 	TAILQ_HEAD(, vhost_iotlb_entry) iotlb_pending_list;
 } __rte_cache_aligned;
 
@@ -207,6 +208,7 @@ struct vhost_virtqueue {
  #define VIRTIO_NET_F_GUEST_ANNOUNCE 21
 #endif
 
+//如果无此标记位，则virtio-net仅支持单队列
 #ifndef VIRTIO_NET_F_MQ
  #define VIRTIO_NET_F_MQ		22
 #endif
@@ -261,10 +263,12 @@ struct vhost_msg {
 #endif
 
 /* Declare packed ring related bits for older kernels */
+//virtio 1.0未定义此字段
 #ifndef VIRTIO_F_RING_PACKED
 
 #define VIRTIO_F_RING_PACKED 34
 
+//packed方式描述符
 struct vring_packed_desc {
 	uint64_t addr;
 	uint32_t len;
@@ -343,29 +347,29 @@ struct inflight_mem_info {
  */
 struct virtio_net {
 	/* Frontend (QEMU) memory and memory region information */
-	struct rte_vhost_memory	*mem;
+	struct rte_vhost_memory	*mem;//内存信息（由消息发送过来，我们map)
 	uint64_t		features;
-	uint64_t		protocol_features;
-	int			vid;
+	uint64_t		protocol_features;//协议功能
+	int			vid;//virtio编号，属于vhost_devices的下标
 	uint32_t		flags;
 	uint16_t		vhost_hlen;
 	/* to tell if we need broadcast rarp packet */
-	rte_atomic16_t		broadcast_rarp;
-	uint32_t		nr_vring;
-	int			dequeue_zero_copy;
+	rte_atomic16_t		broadcast_rarp;//标记是否需要发送rarp报文（将由发包流程触发）
+	uint32_t		nr_vring;//队列数（收＋发）
+	int			dequeue_zero_copy;//是否入队是zero copy
 	int			extbuf;
 	int			linearbuf;
-	struct vhost_virtqueue	*virtqueue[VHOST_MAX_QUEUE_PAIRS * 2];
+	struct vhost_virtqueue	*virtqueue[VHOST_MAX_QUEUE_PAIRS * 2];//设备的所有vhost队列（收＋发）
 	struct inflight_mem_info *inflight_info;
 #define IF_NAME_SZ (PATH_MAX > IFNAMSIZ ? PATH_MAX : IFNAMSIZ)
-	char			ifname[IF_NAME_SZ];
-	uint64_t		log_size;
-	uint64_t		log_base;
-	uint64_t		log_addr;
+	char			ifname[IF_NAME_SZ];//设备名称
+	uint64_t		log_size;//map的内存大小
+	uint64_t		log_base;//log map的内存起始地址
+	uint64_t		log_addr;//map的可用于log的起始地址
 	struct rte_ether_addr	mac;
-	uint16_t		mtu;
+	uint16_t		mtu;//设备的mtu
 
-	struct vhost_device_ops const *notify_ops;
+	struct vhost_device_ops const *notify_ops;//此设备的操作集(来源于vsocket)
 
 	uint32_t		nr_guest_pages;
 	uint32_t		max_guest_pages;
@@ -572,6 +576,7 @@ hva_to_gpa(struct virtio_net *dev, uint64_t vva, uint64_t len)
 	return 0;
 }
 
+//取编号为vid的virtio设备
 static __rte_always_inline struct virtio_net *
 get_device(int vid)
 {
@@ -622,6 +627,7 @@ void *vhost_alloc_copy_ind_table(struct virtio_net *dev,
 int vring_translate(struct virtio_net *dev, struct vhost_virtqueue *vq);
 void vring_invalidate(struct virtio_net *dev, struct vhost_virtqueue *vq);
 
+//将iova转为虚拟地址
 static __rte_always_inline uint64_t
 vhost_iova_to_vva(struct virtio_net *dev, struct vhost_virtqueue *vq,
 			uint64_t iova, uint64_t *len, uint8_t perm)
@@ -672,6 +678,7 @@ vhost_vring_call_split(struct virtio_net *dev, struct vhost_virtqueue *vq)
 		if ((vhost_need_event(vhost_used_event(vq), new, old) &&
 					(vq->callfd >= 0)) ||
 				unlikely(!signalled_used_valid)) {
+			//通过callfd知会对端
 			eventfd_write(vq->callfd, (eventfd_t) 1);
 			if (dev->notify_ops->guest_notified)
 				dev->notify_ops->guest_notified(dev->vid);
diff --git a/lib/librte_vhost/vhost_user.c b/lib/librte_vhost/vhost_user.c
index 2a9fa7c6c..8a0134643 100644
--- a/lib/librte_vhost/vhost_user.c
+++ b/lib/librte_vhost/vhost_user.c
@@ -56,6 +56,7 @@
 #define INFLIGHT_ALIGNMENT	64
 #define INFLIGHT_VERSION	0x1
 
+//消息类型与字符串映射表
 static const char *vhost_message_str[VHOST_USER_MAX] = {
 	[VHOST_USER_NONE] = "VHOST_USER_NONE",
 	[VHOST_USER_GET_FEATURES] = "VHOST_USER_GET_FEATURES",
@@ -125,6 +126,7 @@ drain_zmbuf_list(struct vhost_virtqueue *vq)
 	}
 }
 
+//解掉对内存reg->mmap_addr的去映射
 static void
 free_mem_region(struct virtio_net *dev)
 {
@@ -152,6 +154,7 @@ free_mem_region(struct virtio_net *dev)
 	}
 }
 
+//移除映射的内存
 void
 vhost_backend_cleanup(struct virtio_net *dev)
 {
@@ -210,6 +213,7 @@ vhost_user_set_owner(struct virtio_net **pdev __rte_unused,
 	return RTE_VHOST_MSG_RESULT_OK;
 }
 
+//设备将被置为down,且队列将被重建
 static int
 vhost_user_reset_owner(struct virtio_net **pdev,
 			struct VhostUserMsg *msg __rte_unused,
@@ -218,6 +222,7 @@ vhost_user_reset_owner(struct virtio_net **pdev,
 	struct virtio_net *dev = *pdev;
 	vhost_destroy_device_notify(dev);
 
+	//设置置为reset状态
 	cleanup_device(dev, 0);
 	reset_device(dev);
 	return RTE_VHOST_MSG_RESULT_OK;
@@ -226,6 +231,7 @@ vhost_user_reset_owner(struct virtio_net **pdev,
 /*
  * The features that we support are requested.
  */
+//获取dev当前指定的功能
 static int
 vhost_user_get_features(struct virtio_net **pdev, struct VhostUserMsg *msg,
 			int main_fd __rte_unused)
@@ -245,6 +251,7 @@ vhost_user_get_features(struct virtio_net **pdev, struct VhostUserMsg *msg,
 /*
  * The queue number that we support are requested.
  */
+//返回设备支持的队列数
 static int
 vhost_user_get_queue_num(struct virtio_net **pdev, struct VhostUserMsg *msg,
 			int main_fd __rte_unused)
@@ -264,6 +271,7 @@ vhost_user_get_queue_num(struct virtio_net **pdev, struct VhostUserMsg *msg,
 /*
  * We receive the negotiated features supported by us and the virtio device.
  */
+//功能协商
 static int
 vhost_user_set_features(struct virtio_net **pdev, struct VhostUserMsg *msg,
 			int main_fd __rte_unused)
@@ -279,9 +287,11 @@ vhost_user_set_features(struct virtio_net **pdev, struct VhostUserMsg *msg,
 		RTE_LOG(ERR, VHOST_CONFIG,
 			"(%d) received invalid negotiated features.\n",
 			dev->vid);
+        //设置的功能是我们当前没有的功能，返失败
 		return RTE_VHOST_MSG_RESULT_ERR;
 	}
 
+	//设备已开始运行，但功能将发生变化，通知功能变化
 	if (dev->flags & VIRTIO_DEV_RUNNING) {
 		if (dev->features == features)
 			return RTE_VHOST_MSG_RESULT_OK;
@@ -299,12 +309,15 @@ vhost_user_set_features(struct virtio_net **pdev, struct VhostUserMsg *msg,
 		}
 
 		if (dev->notify_ops->features_changed)
+			//通知功能发生变化
 			dev->notify_ops->features_changed(dev->vid, features);
 	}
 
+	//依据功能变更vhost_hlen
 	dev->features = features;
 	if (dev->features &
 		((1 << VIRTIO_NET_F_MRG_RXBUF) | (1ULL << VIRTIO_F_VERSION_1))) {
+		//支持mrg_rxbuf时头部采用virtio_net_hdr_mrg_rxbuf
 		dev->vhost_hlen = sizeof(struct virtio_net_hdr_mrg_rxbuf);
 	} else {
 		dev->vhost_hlen = sizeof(struct virtio_net_hdr);
@@ -349,6 +362,7 @@ vhost_user_set_features(struct virtio_net **pdev, struct VhostUserMsg *msg,
 /*
  * The virtio device sends us the size of the descriptor ring.
  */
+//执行vring number的设置
 static int
 vhost_user_set_vring_num(struct virtio_net **pdev,
 			struct VhostUserMsg *msg,
@@ -357,6 +371,7 @@ vhost_user_set_vring_num(struct virtio_net **pdev,
 	struct virtio_net *dev = *pdev;
 	struct vhost_virtqueue *vq = dev->virtqueue[msg->payload.state.index];
 
+	//设置ring数目
 	vq->size = msg->payload.state.num;
 
 	/* VIRTIO 1.0, 2.4 Virtqueues says:
@@ -397,6 +412,7 @@ vhost_user_set_vring_num(struct virtio_net **pdev,
 		}
 
 	} else {
+		//按消息创建shadow_used_ring，有vq->size个struct vring_used_elem
 		vq->shadow_used_split = rte_malloc(NULL,
 				vq->size * sizeof(struct vring_used_elem),
 				RTE_CACHE_LINE_SIZE);
@@ -407,6 +423,7 @@ vhost_user_set_vring_num(struct virtio_net **pdev,
 		}
 	}
 
+	//按消息创建batch_copy_elems
 	vq->batch_copy_elems = rte_malloc(NULL,
 				vq->size * sizeof(struct batch_copy_elem),
 				RTE_CACHE_LINE_SIZE);
@@ -545,6 +562,7 @@ numa_realloc(struct virtio_net *dev, int index __rte_unused)
 }
 #endif
 
+//转换qemu虚地址到vhost虚地址
 /* Converts QEMU virtual address to Vhost virtual address. */
 static uint64_t
 qva_to_vva(struct virtio_net *dev, uint64_t qva, uint64_t *len)
@@ -561,10 +579,15 @@ qva_to_vva(struct virtio_net *dev, uint64_t qva, uint64_t *len)
 
 		if (qva >= r->guest_user_addr &&
 		    qva <  r->guest_user_addr + r->size) {
+			//qemu虚拟地址在此region范围内
 
+			//更新qva指向的地址有效长度
 			if (unlikely(*len > r->guest_user_addr + r->size - qva))
 				*len = r->guest_user_addr + r->size - qva;
 
+		    //如果qva在此范围以内，转换为本端地址（转换方法：
+			//qemu虚拟地址减去此region中的qemu的起始地址，获得相对于起始地址的偏移量
+			//然后加上本端此region虚地址的起始量，即完成转换）
 			return qva - r->guest_user_addr +
 			       r->host_user_addr;
 		}
@@ -581,6 +604,10 @@ qva_to_vva(struct virtio_net *dev, uint64_t qva, uint64_t *len)
  * If IOMMU is enabled, the ring address is a guest IO virtual address,
  * else it is a QEMU virtual address.
  */
+//将ra地址转换为host的虚拟地址
+//vq 对应虚队列
+//ra ring地址起始位置
+//size ra地址范围大小
 static uint64_t
 ring_addr_to_vva(struct virtio_net *dev, struct vhost_virtqueue *vq,
 		uint64_t ra, uint64_t *size)
@@ -589,9 +616,11 @@ ring_addr_to_vva(struct virtio_net *dev, struct vhost_virtqueue *vq,
 		uint64_t vva;
 		uint64_t req_size = *size;
 
+		//在cache中查找
 		vva = vhost_user_iotlb_cache_find(vq, ra,
 					size, VHOST_ACCESS_RW);
 		if (req_size != *size)
+			//cache中未命中，发送消息向对端请求
 			vhost_user_iotlb_miss(dev, (ra + *size),
 					      VHOST_ACCESS_RW);
 
@@ -634,10 +663,13 @@ translate_log_addr(struct virtio_net *dev, struct vhost_virtqueue *vq,
 		return log_addr;
 }
 
+//实现设备dev的第vq_index的地址转换
 static struct virtio_net *
 translate_ring_addresses(struct virtio_net *dev, int vq_index)
 {
+	//取对应虚队列
 	struct vhost_virtqueue *vq = dev->virtqueue[vq_index];
+	//取此虚对队对应的ring地址
 	struct vhost_vring_addr *addr = &vq->ring_addrs;
 	uint64_t len, expected_len;
 
@@ -691,6 +723,7 @@ translate_ring_addresses(struct virtio_net *dev, int vq_index)
 	if (vq->desc && vq->avail && vq->used)
 		return dev;
 
+	//转换desc的地址（转换为本端的虚拟地址）
 	len = sizeof(struct vring_desc) * vq->size;
 	vq->desc = (struct vring_desc *)(uintptr_t)ring_addr_to_vva(dev,
 			vq, addr->desc_user_addr, &len);
@@ -705,6 +738,7 @@ translate_ring_addresses(struct virtio_net *dev, int vq_index)
 	vq = dev->virtqueue[vq_index];
 	addr = &vq->ring_addrs;
 
+	//转换avail的地址
 	len = sizeof(struct vring_avail) + sizeof(uint16_t) * vq->size;
 	if (dev->features & (1ULL << VIRTIO_RING_F_EVENT_IDX))
 		len += sizeof(uint16_t);
@@ -718,6 +752,7 @@ translate_ring_addresses(struct virtio_net *dev, int vq_index)
 		return dev;
 	}
 
+	//转换used地址
 	len = sizeof(struct vring_used) +
 		sizeof(struct vring_used_elem) * vq->size;
 	if (dev->features & (1ULL << VIRTIO_RING_F_EVENT_IDX))
@@ -732,6 +767,7 @@ translate_ring_addresses(struct virtio_net *dev, int vq_index)
 		return dev;
 	}
 
+	//????
 	if (vq->last_used_idx != vq->used->idx) {
 		RTE_LOG(WARNING, VHOST_CONFIG,
 			"last_used_idx (%u) and vq->used->idx (%u) mismatches; "
@@ -767,6 +803,7 @@ translate_ring_addresses(struct virtio_net *dev, int vq_index)
  * The virtio device sends us the desc, used and avail ring addresses.
  * This function then converts these to our address space.
  */
+//设置virtio设备的ring地址
 static int
 vhost_user_set_vring_addr(struct virtio_net **pdev, struct VhostUserMsg *msg,
 			int main_fd __rte_unused)
@@ -788,6 +825,7 @@ vhost_user_set_vring_addr(struct virtio_net **pdev, struct VhostUserMsg *msg,
 	 * Rings addresses should not be interpreted as long as the ring is not
 	 * started and enabled
 	 */
+	//设置设备的ring地址
 	memcpy(&vq->ring_addrs, addr, sizeof(*addr));
 
 	vring_invalidate(dev, vq);
@@ -939,6 +977,7 @@ dump_guest_pages(struct virtio_net *dev)
 #define dump_guest_pages(dev)
 #endif
 
+//检查是否有必要更新memory
 static bool
 vhost_memory_changed(struct VhostUserMemory *new,
 		     struct rte_vhost_memory *old)
@@ -946,8 +985,10 @@ vhost_memory_changed(struct VhostUserMemory *new,
 	uint32_t i;
 
 	if (new->nregions != old->nregions)
-		return true;
+		return true;//region不同，可更新
 
+	//相同region时，如果guest_phys_addr,memory_size,userspace_addr不同，则需要更新
+	//否则完全相同，则没必要更新
 	for (i = 0; i < new->nregions; ++i) {
 		VhostUserMemoryRegion *new_r = &new->regions[i];
 		struct rte_vhost_mem_region *old_r = &old->regions[i];
@@ -978,22 +1019,26 @@ vhost_user_set_mem_table(struct virtio_net **pdev, struct VhostUserMsg *msg,
 	int populate;
 	int fd;
 
+	//region数不能过大
 	if (memory->nregions > VHOST_MEMORY_MAX_NREGIONS) {
 		RTE_LOG(ERR, VHOST_CONFIG,
 			"too many memory regions (%u)\n", memory->nregions);
 		return RTE_VHOST_MSG_RESULT_ERR;
 	}
 
+	//如果之前已设置过，则进行更新
 	if (dev->mem && !vhost_memory_changed(memory, dev->mem)) {
 		RTE_LOG(INFO, VHOST_CONFIG,
 			"(%d) memory regions not changed\n", dev->vid);
 
+		//没必要更新region,关闭传递过来的fd(之前已有了）
 		for (i = 0; i < memory->nregions; i++)
 			close(msg->fds[i]);
 
 		return RTE_VHOST_MSG_RESULT_OK;
 	}
 
+	//如果需要更新，则将原有的释放
 	if (dev->mem) {
 		free_mem_region(dev);
 		rte_free(dev->mem);
@@ -1019,6 +1064,7 @@ vhost_user_set_mem_table(struct virtio_net **pdev, struct VhostUserMsg *msg,
 		}
 	}
 
+	//按要求申请可包含nregions块的内存（含一个rte_vhost_memory头）
 	dev->mem = rte_zmalloc("vhost-mem-table", sizeof(struct rte_vhost_memory) +
 		sizeof(struct rte_vhost_mem_region) * memory->nregions, 0);
 	if (dev->mem == NULL) {
@@ -1029,10 +1075,13 @@ vhost_user_set_mem_table(struct virtio_net **pdev, struct VhostUserMsg *msg,
 	}
 	dev->mem->nregions = memory->nregions;
 
+	//初始化对方发送过来的地址
 	for (i = 0; i < memory->nregions; i++) {
 		fd  = msg->fds[i];
+		//上面我们申请了mem的内存，这里指向对应的region,进行初始化
 		reg = &dev->mem->regions[i];
 
+		//copy消息传递过来的信息
 		reg->guest_phys_addr = memory->regions[i].guest_phys_addr;
 		reg->guest_user_addr = memory->regions[i].userspace_addr;
 		reg->size            = memory->regions[i].memory_size;
@@ -1041,6 +1090,7 @@ vhost_user_set_mem_table(struct virtio_net **pdev, struct VhostUserMsg *msg,
 		mmap_offset = memory->regions[i].mmap_offset;
 
 		/* Check for memory_size + mmap_offset overflow */
+		//检查加之后是否会绕圈
 		if (mmap_offset >= -reg->size) {
 			RTE_LOG(ERR, VHOST_CONFIG,
 				"mmap_offset (%#"PRIx64") and memory_size "
@@ -1049,6 +1099,8 @@ vhost_user_set_mem_table(struct virtio_net **pdev, struct VhostUserMsg *msg,
 			goto err_mmap;
 		}
 
+		//由于有效内存自mmap_offset开始，而memory指定的大小为reg->size,故有mmap_size的内存
+		//需要memory map
 		mmap_size = reg->size + mmap_offset;
 
 		/* mmap() without flag of MAP_ANONYMOUS, should be called
@@ -1059,15 +1111,19 @@ vhost_user_set_mem_table(struct virtio_net **pdev, struct VhostUserMsg *msg,
 		 * to avoid failure, make sure in caller to keep length
 		 * aligned.
 		 */
+		//取此文件对应的块大小
 		alignment = get_blk_size(fd);
 		if (alignment == (uint64_t)-1) {
 			RTE_LOG(ERR, VHOST_CONFIG,
 				"couldn't get hugepage size through fstat\n");
 			goto err_mmap;
 		}
+		//将mmap_size按alignment对齐
 		mmap_size = RTE_ALIGN_CEIL(mmap_size, alignment);
 
+		//如果支持0 copy,则要求预填充
 		populate = (dev->dequeue_zero_copy) ? MAP_POPULATE : 0;
+		//map fd对应的那一段内存
 		mmap_addr = mmap(NULL, mmap_size, PROT_READ | PROT_WRITE,
 				 MAP_SHARED | populate, fd, 0);
 
@@ -1077,10 +1133,10 @@ vhost_user_set_mem_table(struct virtio_net **pdev, struct VhostUserMsg *msg,
 			goto err_mmap;
 		}
 
-		reg->mmap_addr = mmap_addr;
-		reg->mmap_size = mmap_size;
+		reg->mmap_addr = mmap_addr;//map后地址（即本端的虚拟地址，用于释放）
+		reg->mmap_size = mmap_size;//map的大小（实际mmap的内存大小）
 		reg->host_user_addr = (uint64_t)(uintptr_t)mmap_addr +
-				      mmap_offset;
+				      mmap_offset;//本进程map的开始地址
 
 		if (dev->dequeue_zero_copy)
 			if (add_guest_pages(dev, reg, alignment) < 0) {
@@ -1177,14 +1233,17 @@ vhost_user_set_mem_table(struct virtio_net **pdev, struct VhostUserMsg *msg,
 	for (i = 0; i < dev->nr_vring; i++) {
 		struct vhost_virtqueue *vq = dev->virtqueue[i];
 
+		//如果vq已设置，则先置成invalidate,再重新执行地址转换
 		if (vq->desc || vq->avail || vq->used) {
 			/*
 			 * If the memory table got updated, the ring addresses
 			 * need to be translated again as virtual addresses have
 			 * changed.
 			 */
+			//先使vq中字段无效
 			vring_invalidate(dev, vq);
 
+			//更新vq中的字段
 			dev = translate_ring_addresses(dev, i);
 			if (!dev) {
 				dev = *pdev;
@@ -1206,6 +1265,7 @@ vhost_user_set_mem_table(struct virtio_net **pdev, struct VhostUserMsg *msg,
 	return RTE_VHOST_MSG_RESULT_ERR;
 }
 
+//检查此vq是否已可以工作了
 static bool
 vq_is_ready(struct virtio_net *dev, struct vhost_virtqueue *vq)
 {
@@ -1233,6 +1293,7 @@ virtio_is_ready(struct virtio_net *dev)
 	if (dev->nr_vring == 0)
 		return 0;
 
+	//检查是否所有virtqueue都ready了
 	for (i = 0; i < dev->nr_vring; i++) {
 		vq = dev->virtqueue[i];
 
@@ -1473,6 +1534,7 @@ vhost_user_set_inflight_fd(struct virtio_net **pdev, VhostUserMsg *msg,
 	return RTE_VHOST_MSG_RESULT_OK;
 }
 
+//设置callfd
 static int
 vhost_user_set_vring_call(struct virtio_net **pdev, struct VhostUserMsg *msg,
 			int main_fd __rte_unused)
@@ -1493,7 +1555,7 @@ vhost_user_set_vring_call(struct virtio_net **pdev, struct VhostUserMsg *msg,
 	if (vq->callfd >= 0)
 		close(vq->callfd);
 
-	vq->callfd = file.fd;
+	vq->callfd = file.fd;//设置通知用的fd
 
 	return RTE_VHOST_MSG_RESULT_OK;
 }
@@ -1711,6 +1773,7 @@ vhost_user_set_vring_kick(struct virtio_net **pdev, struct VhostUserMsg *msg,
 		"vring kick idx:%d file:%d\n", file.index, file.fd);
 
 	/* Interpret ring addresses only when ring is started. */
+	//防止对应的virtqueue相指针未初始化
 	dev = translate_ring_addresses(dev, file.index);
 	if (!dev)
 		return RTE_VHOST_MSG_RESULT_ERR;
@@ -1773,6 +1836,7 @@ vhost_user_get_vring_base(struct virtio_net **pdev,
 	uint64_t val;
 
 	/* We have to stop the queue (virtio) if it is running. */
+	//必须先停止dev
 	vhost_destroy_device_notify(dev);
 
 	dev->flags &= ~VIRTIO_DEV_READY;
@@ -1857,6 +1921,7 @@ vhost_user_set_vring_enable(struct virtio_net **pdev,
 		vdpa_dev->ops->set_vring_state(dev->vid, index, enable);
 
 	if (dev->notify_ops->vring_state_changed)
+		//通知vring状态发生变化
 		dev->notify_ops->vring_state_changed(dev->vid,
 				index, enable);
 
@@ -1877,6 +1942,7 @@ vhost_user_get_protocol_features(struct virtio_net **pdev,
 	struct virtio_net *dev = *pdev;
 	uint64_t features, protocol_features;
 
+	//获取dev对应的功能及协议功能
 	rte_vhost_driver_get_features(dev->ifname, &features);
 	rte_vhost_driver_get_protocol_features(dev->ifname, &protocol_features);
 
@@ -1886,6 +1952,7 @@ vhost_user_get_protocol_features(struct virtio_net **pdev,
 	 * application, disable also REPLY_ACK feature for older buggy
 	 * Qemu versions (from v2.7.0 to v2.9.0).
 	 */
+	//无IOMMU时VHOST_USER_PROTOCOL_F_REPLY_ACK不生效
 	if (!(features & (1ULL << VIRTIO_F_IOMMU_PLATFORM)))
 		protocol_features &= ~(1ULL << VHOST_USER_PROTOCOL_F_REPLY_ACK);
 
@@ -1901,6 +1968,7 @@ vhost_user_set_protocol_features(struct virtio_net **pdev,
 			struct VhostUserMsg *msg,
 			int main_fd __rte_unused)
 {
+	//如果设置我们不支持的协议功能，直接返回
 	struct virtio_net *dev = *pdev;
 	uint64_t protocol_features = msg->payload.u64;
 	uint64_t slave_protocol_features = 0;
@@ -1922,6 +1990,7 @@ vhost_user_set_protocol_features(struct virtio_net **pdev,
 	return RTE_VHOST_MSG_RESULT_OK;
 }
 
+//申请log内存
 static int
 vhost_user_set_log_base(struct virtio_net **pdev, struct VhostUserMsg *msg,
 			int main_fd __rte_unused)
@@ -1943,8 +2012,8 @@ vhost_user_set_log_base(struct virtio_net **pdev, struct VhostUserMsg *msg,
 		return RTE_VHOST_MSG_RESULT_ERR;
 	}
 
-	size = msg->payload.log.mmap_size;
-	off  = msg->payload.log.mmap_offset;
+	size = msg->payload.log.mmap_size;//map多少字节
+	off  = msg->payload.log.mmap_offset;//map后的内存自那个偏移可以使用
 
 	/* Don't allow mmap_offset to point outside the mmap region */
 	if (off > size) {
@@ -1973,9 +2042,12 @@ vhost_user_set_log_base(struct virtio_net **pdev, struct VhostUserMsg *msg,
 	 * Free previously mapped log memory on occasionally
 	 * multiple VHOST_USER_SET_LOG_BASE.
 	 */
+	//之前已map过地址，这里释放掉
 	if (dev->log_addr) {
 		munmap((void *)(uintptr_t)dev->log_addr, dev->log_size);
 	}
+
+	//设置log_addr
 	dev->log_addr = (uint64_t)(uintptr_t)addr;
 	dev->log_base = dev->log_addr + off;
 	dev->log_size = size;
@@ -2020,7 +2092,7 @@ vhost_user_send_rarp(struct virtio_net **pdev, struct VhostUserMsg *msg,
 	RTE_LOG(DEBUG, VHOST_CONFIG,
 		":: mac: %02x:%02x:%02x:%02x:%02x:%02x\n",
 		mac[0], mac[1], mac[2], mac[3], mac[4], mac[5]);
-	memcpy(dev->mac.addr_bytes, mac, 6);
+	memcpy(dev->mac.addr_bytes, mac, 6);//设置mac地址
 
 	/*
 	 * Set the flag to inject a RARP broadcast packet at
@@ -2030,7 +2102,7 @@ vhost_user_send_rarp(struct virtio_net **pdev, struct VhostUserMsg *msg,
 	 * before the flag is set.
 	 */
 	rte_smp_wmb();
-	rte_atomic16_set(&dev->broadcast_rarp, 1);
+	rte_atomic16_set(&dev->broadcast_rarp, 1);//设置广播rarp
 	did = dev->vdpa_dev_id;
 	vdpa_dev = rte_vdpa_get_device(did);
 	if (vdpa_dev && vdpa_dev->ops->migration_done)
@@ -2039,6 +2111,7 @@ vhost_user_send_rarp(struct virtio_net **pdev, struct VhostUserMsg *msg,
 	return RTE_VHOST_MSG_RESULT_OK;
 }
 
+//设置设备的mtu
 static int
 vhost_user_net_set_mtu(struct virtio_net **pdev, struct VhostUserMsg *msg,
 			int main_fd __rte_unused)
@@ -2111,6 +2184,7 @@ is_vring_iotlb_packed(struct vhost_virtqueue *vq, struct vhost_iotlb_msg *imsg)
 	struct vhost_vring_addr *ra;
 	uint64_t start, end, len;
 
+	//需要置无效的一段内存[istart,iend]
 	start = imsg->iova;
 	end = start + imsg->size;
 
@@ -2151,6 +2225,7 @@ vhost_user_iotlb_msg(struct virtio_net **pdev, struct VhostUserMsg *msg,
 
 	switch (imsg->type) {
 	case VHOST_IOTLB_UPDATE:
+		//tlb 更新
 		len = imsg->size;
 		vva = qva_to_vva(dev, imsg->uaddr, &len);
 		if (!vva)
@@ -2159,6 +2234,7 @@ vhost_user_iotlb_msg(struct virtio_net **pdev, struct VhostUserMsg *msg,
 		for (i = 0; i < dev->nr_vring; i++) {
 			struct vhost_virtqueue *vq = dev->virtqueue[i];
 
+			//加入tlb cache
 			vhost_user_iotlb_cache_insert(vq, imsg->iova, vva,
 					len, imsg->perm);
 
@@ -2167,12 +2243,14 @@ vhost_user_iotlb_msg(struct virtio_net **pdev, struct VhostUserMsg *msg,
 		}
 		break;
 	case VHOST_IOTLB_INVALIDATE:
+		//tlb cache移除
 		for (i = 0; i < dev->nr_vring; i++) {
 			struct vhost_virtqueue *vq = dev->virtqueue[i];
 
 			vhost_user_iotlb_cache_remove(vq, imsg->iova,
 					imsg->size);
 
+			//检查此消息是否会导致vq无效，如果是，置其无效
 			if (is_vring_iotlb(dev, vq, imsg))
 				vring_invalidate(dev, vq);
 		}
@@ -2294,22 +2372,27 @@ static vhost_message_handler_t vhost_message_handlers[VHOST_USER_MAX] = {
 };
 
 /* return bytes# of read on success or negative val on failure. */
+//读取消息
 static int
 read_vhost_message(int sockfd, struct VhostUserMsg *msg)
 {
 	int ret;
 
+	//读取消息，并解析消息中包含的fd(最多容许8个）
 	ret = read_fd_message(sockfd, (char *)msg, VHOST_USER_HDR_SIZE,
 		msg->fds, VHOST_MEMORY_MAX_NREGIONS, &msg->fd_num);
 	if (ret <= 0)
 		return ret;
 
 	if (msg->size) {
+		//消息过长，超过已认识的payload
 		if (msg->size > sizeof(msg->payload)) {
 			RTE_LOG(ERR, VHOST_CONFIG,
 				"invalid msg size: %d\n", msg->size);
 			return -1;
 		}
+
+		//读取消息负载
 		ret = read(sockfd, &msg->payload, msg->size);
 		if (ret <= 0)
 			return ret;
@@ -2323,6 +2406,7 @@ read_vhost_message(int sockfd, struct VhostUserMsg *msg)
 	return ret;
 }
 
+//发送消息
 static int
 send_vhost_message(int sockfd, struct VhostUserMsg *msg)
 {
@@ -2342,7 +2426,7 @@ send_vhost_reply(int sockfd, struct VhostUserMsg *msg)
 	msg->flags &= ~VHOST_USER_VERSION_MASK;
 	msg->flags &= ~VHOST_USER_NEED_REPLY;
 	msg->flags |= VHOST_USER_VERSION;
-	msg->flags |= VHOST_USER_REPLY_MASK;
+	msg->flags |= VHOST_USER_REPLY_MASK;//标记是响应
 
 	return send_vhost_message(sockfd, msg);
 }
@@ -2365,6 +2449,7 @@ send_vhost_slave_message(struct virtio_net *dev, struct VhostUserMsg *msg)
 /*
  * Allocate a queue pair if it hasn't been allocated yet
  */
+//依据不同的消息类型，校验vring_idx是否正确，如果vring_idx指定的queue不存在，则创建
 static int
 vhost_user_check_and_alloc_queue_pair(struct virtio_net *dev,
 			struct VhostUserMsg *msg)
@@ -2389,18 +2474,22 @@ vhost_user_check_and_alloc_queue_pair(struct virtio_net *dev,
 		return 0;
 	}
 
+	//校验传入的vring_idx是否合法
 	if (vring_idx >= VHOST_MAX_VRING) {
 		RTE_LOG(ERR, VHOST_CONFIG,
 			"invalid vring index: %u\n", vring_idx);
 		return -1;
 	}
 
+	//如果vring_idx合法，且已创建，则直接返回，否则申请队列
 	if (dev->virtqueue[vring_idx])
 		return 0;
 
+	//否则创建队列
 	return alloc_vring_queue(dev, vring_idx);
 }
 
+//对所有queue进行加锁
 static void
 vhost_user_lock_all_queue_pairs(struct virtio_net *dev)
 {
@@ -2435,6 +2524,7 @@ vhost_user_unlock_all_queue_pairs(struct virtio_net *dev)
 	}
 }
 
+//控制消息处理
 int
 vhost_user_msg_handler(int vid, int fd)
 {
@@ -2447,11 +2537,13 @@ vhost_user_msg_handler(int vid, int fd)
 	bool handled;
 	int request;
 
+	//获得virtio_net
 	dev = get_device(vid);
 	if (dev == NULL)
 		return -1;
 
 	if (!dev->notify_ops) {
+		//如果未对ops赋值，则通过dev->ifname查找vsocket的通知操作集，并返回
 		dev->notify_ops = vhost_driver_callback_get(dev->ifname);
 		if (!dev->notify_ops) {
 			RTE_LOG(ERR, VHOST_CONFIG,
@@ -2461,8 +2553,10 @@ vhost_user_msg_handler(int vid, int fd)
 		}
 	}
 
+	//读取vhost消息
 	ret = read_vhost_message(fd, &msg);
 	if (ret <= 0) {
+		//读取失败或者发送的请求不认识，或者对端关闭，返回失败
 		if (ret < 0)
 			RTE_LOG(ERR, VHOST_CONFIG,
 				"vhost read message failed\n");
@@ -2487,6 +2581,7 @@ vhost_user_msg_handler(int vid, int fd)
 		RTE_LOG(DEBUG, VHOST_CONFIG, "External request %d\n", request);
 	}
 
+	//校验队列索引，如果未创建，则创建它
 	ret = vhost_user_check_and_alloc_queue_pair(dev, &msg);
 	if (ret < 0) {
 		RTE_LOG(ERR, VHOST_CONFIG,
@@ -2501,6 +2596,7 @@ vhost_user_msg_handler(int vid, int fd)
 	 * inactive, so it is safe. Otherwise taking the access_lock
 	 * would cause a dead lock.
 	 */
+	//对所有队列加锁
 	switch (request) {
 	case VHOST_USER_SET_FEATURES:
 	case VHOST_USER_SET_PROTOCOL_FEATURES:
@@ -2544,6 +2640,126 @@ vhost_user_msg_handler(int vid, int fd)
 		}
 	}
 
+/**
+	//按请求处理消息
+	switch (msg.request.master) {
+	case VHOST_USER_GET_FEATURES:
+		//获取本端支持功能，并响应对端
+		msg.payload.u64 = vhost_user_get_features(dev);
+		msg.size = sizeof(msg.payload.u64);
+		send_vhost_reply(fd, &msg);//响应本端支持的功能
+
+		break;
+	case VHOST_USER_SET_FEATURES:
+		//设置协商好的功能
+		ret = vhost_user_set_features(dev, msg.payload.u64);
+		if (ret)
+			return -1;//消息处理失败，会导致关闭连接
+		break;
+
+	case VHOST_USER_GET_PROTOCOL_FEATURES:
+		//获取本端协议功能，并响应
+		vhost_user_get_protocol_features(dev, &msg);
+		send_vhost_reply(fd, &msg);
+		break;
+	case VHOST_USER_SET_PROTOCOL_FEATURES:
+		//设置本端协议功能
+		vhost_user_set_protocol_features(dev, msg.payload.u64);
+		break;
+
+	case VHOST_USER_SET_OWNER:
+		//设置owner,实现为空
+		vhost_user_set_owner();
+		break;
+	case VHOST_USER_RESET_OWNER:
+		vhost_user_reset_owner(dev);
+		break;
+
+	case VHOST_USER_SET_MEM_TABLE:
+		//设置内存表（知道内存表，可以实现两端的地址转换）
+		ret = vhost_user_set_mem_table(&dev, &msg);
+		break;
+
+	case VHOST_USER_SET_LOG_BASE:
+		//map的log内存（未看到使用）
+		vhost_user_set_log_base(dev, &msg);
+
+		// it needs a reply 
+		msg.size = sizeof(msg.payload.u64);
+		send_vhost_reply(fd, &msg);
+		break;
+	case VHOST_USER_SET_LOG_FD:
+		//关闭fd（未使用log)
+		close(msg.fds[0]);
+		RTE_LOG(INFO, VHOST_CONFIG, "not implemented.\n");
+		break;
+
+	case VHOST_USER_SET_VRING_NUM:
+		//设置ring number
+		vhost_user_set_vring_num(dev, &msg);
+		break;
+	case VHOST_USER_SET_VRING_ADDR:
+		//设置ring地址
+		vhost_user_set_vring_addr(&dev, &msg);
+		break;
+	case VHOST_USER_SET_VRING_BASE:
+		//设置ring的读写指针位置
+		vhost_user_set_vring_base(dev, &msg);
+		break;
+
+	case VHOST_USER_GET_VRING_BASE:
+		//获取ring的读写指针位置
+		vhost_user_get_vring_base(dev, &msg);
+		msg.size = sizeof(msg.payload.state);
+		send_vhost_reply(fd, &msg);
+		break;
+
+	case VHOST_USER_SET_VRING_KICK:
+		//设置此qemu对应的kick fd
+		vhost_user_set_vring_kick(&dev, &msg);
+		break;
+	case VHOST_USER_SET_VRING_CALL:
+		//设置call fd
+		vhost_user_set_vring_call(dev, &msg);
+		break;
+
+	case VHOST_USER_SET_VRING_ERR:
+		if (!(msg.payload.u64 & VHOST_USER_VRING_NOFD_MASK))
+			close(msg.fds[0]);
+		RTE_LOG(INFO, VHOST_CONFIG, "not implemented\n");
+		break;
+
+	case VHOST_USER_GET_QUEUE_NUM:
+		//获取本端支持的最大队列数
+		msg.payload.u64 = (uint64_t)vhost_user_get_queue_num(dev);
+		msg.size = sizeof(msg.payload.u64);
+		send_vhost_reply(fd, &msg);
+		break;
+
+	case VHOST_USER_SET_VRING_ENABLE:
+		//使能指定队列
+		vhost_user_set_vring_enable(dev, &msg);
+		break;
+	case VHOST_USER_SEND_RARP:
+		//响应对端要求的发送rarp
+		vhost_user_send_rarp(dev, &msg);
+		break;
+
+	case VHOST_USER_NET_SET_MTU:
+		//设置dev的MTU
+		ret = vhost_user_net_set_mtu(dev, &msg);
+		break;
+
+	case VHOST_USER_SET_SLAVE_REQ_FD:
+		//设置slave_req_fd
+		ret = vhost_user_set_req_fd(dev, &msg);
+		break;
+
+	case VHOST_USER_IOTLB_MSG:
+		//tlb消息更新及移除
+		ret = vhost_user_iotlb_msg(&dev, &msg);
+		break;
+*/
 	if (request > VHOST_USER_NONE && request < VHOST_USER_MAX) {
 		if (!vhost_message_handlers[request])
 			goto skip_to_post_handle;
@@ -2618,6 +2834,7 @@ vhost_user_msg_handler(int vid, int fd)
 		return -1;
 	}
 
+	//如果设备已协商完成，可以工作，则触发new_device事件
 	if (!(dev->flags & VIRTIO_DEV_RUNNING) && virtio_is_ready(dev)) {
 		dev->flags |= VIRTIO_DEV_READY;
 
@@ -2627,6 +2844,7 @@ vhost_user_msg_handler(int vid, int fd)
 						"dequeue zero copy is enabled\n");
 			}
 
+			//vdev准备好了，调用添加设备，并将其状态置为running
 			if (dev->notify_ops->new_device(dev->vid) == 0)
 				dev->flags |= VIRTIO_DEV_RUNNING;
 		}
@@ -2637,6 +2855,7 @@ vhost_user_msg_handler(int vid, int fd)
 	if (vdpa_dev && virtio_is_ready(dev) &&
 			!(dev->flags & VIRTIO_DEV_VDPA_CONFIGURED) &&
 			msg.request.master == VHOST_USER_SET_VRING_CALL) {
+		//置vdap_dev　ready
 		if (vdpa_dev->ops->dev_conf)
 			vdpa_dev->ops->dev_conf(dev->vid);
 		dev->flags |= VIRTIO_DEV_VDPA_CONFIGURED;
diff --git a/lib/librte_vhost/vhost_user.h b/lib/librte_vhost/vhost_user.h
index 6563f7315..a3be006f7 100644
--- a/lib/librte_vhost/vhost_user.h
+++ b/lib/librte_vhost/vhost_user.h
@@ -14,6 +14,7 @@
 
 #define VHOST_MEMORY_MAX_NREGIONS 8
 
+//协议功能支持6种（当前）
 #define VHOST_USER_PROTOCOL_FEATURES	((1ULL << VHOST_USER_PROTOCOL_F_MQ) | \
 					 (1ULL << VHOST_USER_PROTOCOL_F_LOG_SHMFD) |\
 					 (1ULL << VHOST_USER_PROTOCOL_F_RARP) | \
@@ -67,14 +68,14 @@ typedef enum VhostUserSlaveRequest {
 } VhostUserSlaveRequest;
 
 typedef struct VhostUserMemoryRegion {
-	uint64_t guest_phys_addr;
-	uint64_t memory_size;
-	uint64_t userspace_addr;
-	uint64_t mmap_offset;
+	uint64_t guest_phys_addr;//vm的物理地址
+	uint64_t memory_size;//内存大小
+	uint64_t userspace_addr;//vm中对应的虚拟机址
+	uint64_t mmap_offset;//针对fd memory map时的偏移量
 } VhostUserMemoryRegion;
 
 typedef struct VhostUserMemory {
-	uint32_t nregions;
+	uint32_t nregions;//region数
 	uint32_t padding;
 	VhostUserMemoryRegion regions[VHOST_MEMORY_MAX_NREGIONS];
 } VhostUserMemory;
@@ -125,7 +126,7 @@ typedef struct VhostUserMsg {
 	union {
 		uint32_t master; /* a VhostUserRequest value */
 		uint32_t slave;  /* a VhostUserSlaveRequest value*/
-	} request;
+	} request;//请求类型
 
 #define VHOST_USER_VERSION_MASK     0x3
 #define VHOST_USER_REPLY_MASK       (0x1 << 2)
@@ -137,18 +138,19 @@ typedef struct VhostUserMsg {
 #define VHOST_USER_VRING_NOFD_MASK  (0x1<<8)
 		uint64_t u64;
 		struct vhost_vring_state state;
-		struct vhost_vring_addr addr;
-		VhostUserMemory memory;
+		struct vhost_vring_addr addr;//ring地址信息
+		VhostUserMemory memory;//内存表信息
 		VhostUserLog    log;
-		struct vhost_iotlb_msg iotlb;
+		struct vhost_iotlb_msg iotlb;//iotlb信息
 		VhostUserCryptoSessionParam crypto_session;
 		VhostUserVringArea area;
 		VhostUserInflight inflight;
 	} payload;
-	int fds[VHOST_MEMORY_MAX_NREGIONS];
+	int fds[VHOST_MEMORY_MAX_NREGIONS];//对端传送过来的fd
 	int fd_num;
 } __attribute((packed)) VhostUserMsg;
 
+//即VhostUserMsg的头部(payload之前占用的字节）
 #define VHOST_USER_HDR_SIZE offsetof(VhostUserMsg, payload.u64)
 
 /* The version of the protocol we support */
diff --git a/lib/librte_vhost/virtio_net.c b/lib/librte_vhost/virtio_net.c
index cde7498c7..37ddab4a0 100644
--- a/lib/librte_vhost/virtio_net.c
+++ b/lib/librte_vhost/virtio_net.c
@@ -40,6 +40,7 @@ virtio_net_is_inorder(struct virtio_net *dev)
 static bool
 is_valid_virt_queue_idx(uint32_t idx, int is_tx, uint32_t nr_vring)
 {
+	//首先队列的idx必须是小于nr_vring的，其次，如果is_tx是发队列，则idx &1 必须为1(即必须为奇数）
 	return (is_tx ^ (idx & 1)) == 0 && idx < nr_vring;
 }
 
@@ -48,6 +49,7 @@ do_flush_shadow_used_ring_split(struct virtio_net *dev,
 			struct vhost_virtqueue *vq,
 			uint16_t to, uint16_t from, uint16_t size)
 {
+	//将shadow_used_split中的信息更新到used中
 	rte_memcpy(&vq->used->ring[to],
 			&vq->shadow_used_split[from],
 			size * sizeof(struct vring_used_elem));
@@ -87,6 +89,7 @@ flush_shadow_used_ring_split(struct virtio_net *dev, struct vhost_virtqueue *vq)
 		sizeof(vq->used->idx));
 }
 
+//记录哪些描述符已被使用完成
 static __rte_always_inline void
 update_shadow_used_ring_split(struct vhost_virtqueue *vq,
 			 uint16_t desc_idx, uint32_t len)
@@ -342,6 +345,7 @@ do_data_copy_enqueue(struct virtio_net *dev, struct vhost_virtqueue *vq)
 	vq->batch_copy_nb_elems = 0;
 }
 
+//实现batch信息的copy
 static inline void
 do_data_copy_dequeue(struct vhost_virtqueue *vq)
 {
@@ -407,11 +411,14 @@ vhost_flush_dequeue_packed(struct virtio_net *dev,
 		(var) = (val);			\
 } while (0)
 
+//offload参数转换
 static __rte_always_inline void
 virtio_enqueue_offload(struct rte_mbuf *m_buf, struct virtio_net_hdr *net_hdr)
 {
+	//入队时，将mbuf上的标记，转到net_hdr中
 	uint64_t csum_l4 = m_buf->ol_flags & PKT_TX_L4_MASK;
 
+	//如果支持tso,则需要计算tcp checksum，准备checksum参数（4层checksum计算起始位置）
 	if (m_buf->ol_flags & PKT_TX_TCP_SEG)
 		csum_l4 |= PKT_TX_TCP_CKSUM;
 
@@ -419,6 +426,7 @@ virtio_enqueue_offload(struct rte_mbuf *m_buf, struct virtio_net_hdr *net_hdr)
 		net_hdr->flags = VIRTIO_NET_HDR_F_NEEDS_CSUM;
 		net_hdr->csum_start = m_buf->l2_len + m_buf->l3_len;
 
+		//记录到checksum字段的offset
 		switch (csum_l4) {
 		case PKT_TX_TCP_CKSUM:
 			net_hdr->csum_offset = (offsetof(struct rte_tcp_hdr,
@@ -434,6 +442,7 @@ virtio_enqueue_offload(struct rte_mbuf *m_buf, struct virtio_net_hdr *net_hdr)
 			break;
 		}
 	} else {
+		//硬件计处算checksum被disable,清0
 		ASSIGN_UNLESS_EQUAL(net_hdr->csum_start, 0);
 		ASSIGN_UNLESS_EQUAL(net_hdr->csum_offset, 0);
 		ASSIGN_UNLESS_EQUAL(net_hdr->flags, 0);
@@ -441,22 +450,25 @@ virtio_enqueue_offload(struct rte_mbuf *m_buf, struct virtio_net_hdr *net_hdr)
 
 	/* IP cksum verification cannot be bypassed, then calculate here */
 	if (m_buf->ol_flags & PKT_TX_IP_CKSUM) {
+		//如果mbuf中指明需要做checksum offload,则此处需要计算ip层checksum
 		struct rte_ipv4_hdr *ipv4_hdr;
 
 		ipv4_hdr = rte_pktmbuf_mtod_offset(m_buf, struct rte_ipv4_hdr *,
 						   m_buf->l2_len);
+        //计算ipv4头部checksum
 		ipv4_hdr->hdr_checksum = 0;
 		ipv4_hdr->hdr_checksum = rte_ipv4_cksum(ipv4_hdr);
 	}
 
+	//设置gso类型及其相关参数，例如gso大小，头部长度
 	if (m_buf->ol_flags & PKT_TX_TCP_SEG) {
 		if (m_buf->ol_flags & PKT_TX_IPV4)
-			net_hdr->gso_type = VIRTIO_NET_HDR_GSO_TCPV4;
+			net_hdr->gso_type = VIRTIO_NET_HDR_GSO_TCPV4;//tso
 		else
 			net_hdr->gso_type = VIRTIO_NET_HDR_GSO_TCPV6;
-		net_hdr->gso_size = m_buf->tso_segsz;
+		net_hdr->gso_size = m_buf->tso_segsz;//设置分的块大小
 		net_hdr->hdr_len = m_buf->l2_len + m_buf->l3_len
-					+ m_buf->l4_len;
+					+ m_buf->l4_len;//指定4层头
 	} else if (m_buf->ol_flags & PKT_TX_UDP_SEG) {
 		net_hdr->gso_type = VIRTIO_NET_HDR_GSO_UDP;
 		net_hdr->gso_size = m_buf->tso_segsz;
@@ -469,6 +481,8 @@ virtio_enqueue_offload(struct rte_mbuf *m_buf, struct virtio_net_hdr *net_hdr)
 	}
 }
 
+//将数据指针填充到到buf_vec中，vec_idx用于记录有多少个vector被占用
+//由于数据在物理内存上可能是不连续的，故会占用多个buf_vec
 static __rte_always_inline int
 map_one_desc(struct virtio_net *dev, struct vhost_virtqueue *vq,
 		struct buf_vector *buf_vec, uint16_t *vec_idx,
@@ -476,6 +490,7 @@ map_one_desc(struct virtio_net *dev, struct vhost_virtqueue *vq,
 {
 	uint16_t vec_id = *vec_idx;
 
+	//desc_iova指向的内存物理上可能是不连续的
 	while (desc_len) {
 		uint64_t desc_addr;
 		uint64_t desc_chunck_len = desc_len;
@@ -483,6 +498,7 @@ map_one_desc(struct virtio_net *dev, struct vhost_virtqueue *vq,
 		if (unlikely(vec_id >= BUF_VECTOR_MAX))
 			return -1;
 
+		//取描述符中buffer的虚拟地址
 		desc_addr = vhost_iova_to_vva(dev, vq,
 				desc_iova,
 				&desc_chunck_len,
@@ -494,7 +510,7 @@ map_one_desc(struct virtio_net *dev, struct vhost_virtqueue *vq,
 
 		buf_vec[vec_id].buf_iova = desc_iova;
 		buf_vec[vec_id].buf_addr = desc_addr;
-		buf_vec[vec_id].buf_len  = desc_chunck_len;
+		buf_vec[vec_id].buf_len  = desc_chunck_len;//buffer长度
 
 		desc_len -= desc_chunck_len;
 		desc_iova += desc_chunck_len;
@@ -505,12 +521,18 @@ map_one_desc(struct virtio_net *dev, struct vhost_virtqueue *vq,
 	return 0;
 }
 
+//将avail_idex描述符指出的数据地址填充到出参buf_vec中（需要考虑地址转换，需要考虑物理地址不连续）
+//出参nr_vec用于指出buf_vec中占用的数量
+//出参desc_chain_head为首个描述符索引
+//出参desc_chain_len为描述符链的buffer长度
+//perm为io地址转换时的权限位
 static __rte_always_inline int
 fill_vec_buf_split(struct virtio_net *dev, struct vhost_virtqueue *vq,
 			 uint32_t avail_idx, uint16_t *vec_idx,
 			 struct buf_vector *buf_vec, uint16_t *desc_chain_head,
 			 uint32_t *desc_chain_len, uint8_t perm)
 {
+	//取出有效索引位下记录的有效描述符索引
 	uint16_t idx = vq->avail->ring[avail_idx & (vq->size - 1)];
 	uint16_t vec_id = *vec_idx;
 	uint32_t len    = 0;
@@ -525,6 +547,8 @@ fill_vec_buf_split(struct virtio_net *dev, struct vhost_virtqueue *vq,
 
 	*desc_chain_head = idx;
 
+	//如果此描述符是the buffer contains a list of buffer descriptors
+	//则其addr中所指向的数据，仅仅是一个由dlen/sizeof(struct vring_desc)个desc组成的链表
 	if (vq->desc[idx].flags & VRING_DESC_F_INDIRECT) {
 		dlen = vq->desc[idx].len;
 		nr_descs = dlen / sizeof(struct vring_desc);
@@ -536,13 +560,15 @@ fill_vec_buf_split(struct virtio_net *dev, struct vhost_virtqueue *vq,
 						&dlen,
 						VHOST_ACCESS_RO);
 		if (unlikely(!descs))
-			return -1;
+			return -1;//描述地址为空，失败
 
 		if (unlikely(dlen < vq->desc[idx].len)) {
 			/*
 			 * The indirect desc table is not contiguous
 			 * in process VA space, we have to copy it.
 			 */
+			//dlen小于vq->desc[idx].len,说明存放描述符的内存物理上不连续
+			//申请一段连续内存把描述符 copy出来
 			idesc = vhost_alloc_copy_ind_table(dev, vq,
 					vq->desc[idx].addr, vq->desc[idx].len);
 			if (unlikely(!idesc))
@@ -562,6 +588,7 @@ fill_vec_buf_split(struct virtio_net *dev, struct vhost_virtqueue *vq,
 
 		len += descs[idx].len;
 
+		//转换descs[idx]中指向的内存到buf_vec中（转换为本端虚拟地址）
 		if (unlikely(map_one_desc(dev, vq, buf_vec, &vec_id,
 						descs[idx].addr, descs[idx].len,
 						perm))) {
@@ -569,17 +596,19 @@ fill_vec_buf_split(struct virtio_net *dev, struct vhost_virtqueue *vq,
 			return -1;
 		}
 
+		//检查此描述符是否有next
 		if ((descs[idx].flags & VRING_DESC_F_NEXT) == 0)
 			break;
 
+		//有next，取下一个描述符索引，继续转换进buf_vec中
 		idx = descs[idx].next;
 	}
 
-	*desc_chain_len = len;
-	*vec_idx = vec_id;
+	*desc_chain_len = len;//buffer的长度
+	*vec_idx = vec_id;//占用了多少个数据段
 
 	if (unlikely(!!idesc))
-		free_ind_table(idesc);
+		free_ind_table(idesc);//如果有申请idesc，则释放它
 
 	return 0;
 }
@@ -656,6 +685,7 @@ fill_vec_buf_packed_indirect(struct virtio_net *dev,
 		return -1;
 
 	if (unlikely(dlen < desc->len)) {
+		//不连续，需要copy
 		/*
 		 * The indirect desc table is not contiguous
 		 * in process VA space, we have to copy it.
@@ -668,12 +698,14 @@ fill_vec_buf_packed_indirect(struct virtio_net *dev,
 		descs = idescs;
 	}
 
+	//取出对应的一组描述符
 	nr_descs =  desc->len / sizeof(struct vring_packed_desc);
 	if (unlikely(nr_descs >= vq->size)) {
 		free_ind_table(idescs);
 		return -1;
 	}
 
+	//与split不同，这组描述符是数组形式，遍历并直接copy
 	for (i = 0; i < nr_descs; i++) {
 		if (unlikely(vec_id >= BUF_VECTOR_MAX)) {
 			free_ind_table(idescs);
@@ -694,6 +726,8 @@ fill_vec_buf_packed_indirect(struct virtio_net *dev,
 	return 0;
 }
 
+//处理avail_idx指定的描述符，将其转换为buf_vec进行保存（实现qemu地址向dpdk虚地址的转换）
+//desc_count用于表示有多少描述符被使用
 static __rte_always_inline int
 fill_vec_buf_packed(struct virtio_net *dev, struct vhost_virtqueue *vq,
 				uint16_t avail_idx, uint16_t *desc_count,
@@ -720,7 +754,7 @@ fill_vec_buf_packed(struct virtio_net *dev, struct vhost_virtqueue *vq,
 
 	while (1) {
 		if (unlikely(vec_id >= BUF_VECTOR_MAX))
-			return -1;
+			return -1;//超出vector限制，报错
 
 		if (unlikely(*desc_count >= vq->size))
 			return -1;
@@ -728,6 +762,7 @@ fill_vec_buf_packed(struct virtio_net *dev, struct vhost_virtqueue *vq,
 		*desc_count += 1;
 		*buf_id = descs[avail_idx].id;
 
+		//处理packed形式的indirect描述符形式（报文过大时占用多个描述符）
 		if (descs[avail_idx].flags & VRING_DESC_F_INDIRECT) {
 			if (unlikely(fill_vec_buf_packed_indirect(dev, vq,
 							&descs[avail_idx],
@@ -737,6 +772,7 @@ fill_vec_buf_packed(struct virtio_net *dev, struct vhost_virtqueue *vq,
 		} else {
 			*len += descs[avail_idx].len;
 
+			//填充buf_vec
 			if (unlikely(map_one_desc(dev, vq, buf_vec, &vec_id,
 							descs[avail_idx].addr,
 							descs[avail_idx].len,
@@ -744,9 +780,11 @@ fill_vec_buf_packed(struct virtio_net *dev, struct vhost_virtqueue *vq,
 				return -1;
 		}
 
+		//此描述符对应的报文结束
 		if ((descs[avail_idx].flags & VRING_DESC_F_NEXT) == 0)
 			break;
 
+		//切换到下一个idx
 		if (++avail_idx >= vq->size) {
 			avail_idx -= vq->size;
 			wrap_counter ^= 1;
@@ -805,6 +843,7 @@ copy_mbuf_to_desc(struct virtio_net *dev, struct vhost_virtqueue *vq,
 	int error = 0;
 
 	if (unlikely(m == NULL)) {
+		//mbuf为空，报错
 		error = -1;
 		goto out;
 	}
@@ -814,6 +853,7 @@ copy_mbuf_to_desc(struct virtio_net *dev, struct vhost_virtqueue *vq,
 	buf_len = buf_vec[vec_idx].buf_len;
 
 	if (unlikely(buf_len < dev->vhost_hlen && nr_vec <= 1)) {
+		//空间不足以存在dev->vhost_hlen,报错
 		error = -1;
 		goto out;
 	}
@@ -821,6 +861,7 @@ copy_mbuf_to_desc(struct virtio_net *dev, struct vhost_virtqueue *vq,
 	hdr_mbuf = m;
 	hdr_addr = buf_addr;
 	if (unlikely(buf_len < dev->vhost_hlen))
+		//首个desc空间不足以存放dev->vhost_hlen时，使hdr指向临时空间
 		hdr = &tmp_hdr;
 	else
 		hdr = (struct virtio_net_hdr_mrg_rxbuf *)(uintptr_t)hdr_addr;
@@ -829,6 +870,8 @@ copy_mbuf_to_desc(struct virtio_net *dev, struct vhost_virtqueue *vq,
 		dev->vid, num_buffers);
 
 	if (unlikely(buf_len < dev->vhost_hlen)) {
+		//描述符的长度不足vhost_len,先跳过buf_vec[vec_idx](第一个描述符）
+		//后面再处理它
 		buf_offset = dev->vhost_hlen - buf_len;
 		vec_idx++;
 		buf_addr = buf_vec[vec_idx].buf_addr;
@@ -845,10 +888,11 @@ copy_mbuf_to_desc(struct virtio_net *dev, struct vhost_virtqueue *vq,
 	while (mbuf_avail != 0 || m->next != NULL) {
 		/* done with current buf, get the next one */
 		if (buf_avail == 0) {
+			//此时当前描述符空间已使用完，切换到下一个空间
 			vec_idx++;
 			if (unlikely(vec_idx >= nr_vec)) {
 				error = -1;
-				goto out;
+				goto out;//超限，报错
 			}
 
 			buf_addr = buf_vec[vec_idx].buf_addr;
@@ -861,6 +905,7 @@ copy_mbuf_to_desc(struct virtio_net *dev, struct vhost_virtqueue *vq,
 
 		/* done with current mbuf, get the next one */
 		if (mbuf_avail == 0) {
+			//此时mbuf中的空间已拷贝完成，切换下一个mbuf
 			m = m->next;
 
 			mbuf_offset = 0;
@@ -870,10 +915,12 @@ copy_mbuf_to_desc(struct virtio_net *dev, struct vhost_virtqueue *vq,
 		if (hdr_addr) {
 			virtio_enqueue_offload(hdr_mbuf, &hdr->hdr);
 			if (rxvq_is_mergeable(dev))
+				//如果启用mergeable,则设置num_buffers
 				ASSIGN_UNLESS_EQUAL(hdr->num_buffers,
 						num_buffers);
 
 			if (unlikely(hdr == &tmp_hdr)) {
+				//首段desc空间过小，不足以存放virtio_net_hdr_mrg_rxbuf
 				copy_vnet_hdr_to_desc(dev, vq, buf_vec, hdr);
 			} else {
 				PRINT_PACKET(dev, (uintptr_t)hdr_addr,
@@ -890,6 +937,7 @@ copy_mbuf_to_desc(struct virtio_net *dev, struct vhost_virtqueue *vq,
 
 		if (likely(cpy_len > MAX_BATCH_LEN ||
 					vq->batch_copy_nb_elems >= vq->size)) {
+			//直接将mbuf内容copy到描述符指向的空间
 			rte_memcpy((void *)((uintptr_t)(buf_addr + buf_offset)),
 				rte_pktmbuf_mtod_offset(m, void *, mbuf_offset),
 				cpy_len);
@@ -899,6 +947,7 @@ copy_mbuf_to_desc(struct virtio_net *dev, struct vhost_virtqueue *vq,
 			PRINT_PACKET(dev, (uintptr_t)(buf_addr + buf_offset),
 				cpy_len, 0);
 		} else {
+			//记录此次copy到batch_copy中
 			batch_copy[vq->batch_copy_nb_elems].dst =
 				(void *)((uintptr_t)(buf_addr + buf_offset));
 			batch_copy[vq->batch_copy_nb_elems].src =
@@ -983,6 +1032,9 @@ vhost_enqueue_single_packed(struct virtio_net *dev,
 	return 0;
 }
 
+//向virtio_net收包处理（vhost设备的发包处理）
+//pkts要发送的报文
+//count要发送的报文数
 static __rte_noinline uint32_t
 virtio_dev_rx_split(struct virtio_net *dev, struct vhost_virtqueue *vq,
 	struct rte_mbuf **pkts, uint32_t count)
@@ -1002,10 +1054,14 @@ virtio_dev_rx_split(struct virtio_net *dev, struct vhost_virtqueue *vq,
 
 	rte_prefetch0(&vq->avail->ring[vq->last_avail_idx & (vq->size - 1)]);
 
+	//要发送的报文
 	for (pkt_idx = 0; pkt_idx < count; pkt_idx++) {
+		//加头后的pkt_len
 		uint32_t pkt_len = pkts[pkt_idx]->pkt_len + dev->vhost_hlen;
 		uint16_t nr_vec = 0;
 
+		//预留足够的描述符，使用的描述符来源于avail,要存放pkt_len,出参num_buffers
+		//标明我们将占用多少个avail描述符（可能会是链式的），nr_vec标明我们使用多少个描述符
 		if (unlikely(reserve_avail_buf_split(dev, vq,
 						pkt_len, buf_vec, &num_buffers,
 						avail_head, &nr_vec) < 0)) {
@@ -1020,6 +1076,7 @@ virtio_dev_rx_split(struct virtio_net *dev, struct vhost_virtqueue *vq,
 			dev->vid, vq->last_avail_idx,
 			vq->last_avail_idx + num_buffers);
 
+		//将pkts[pkt_idx] mbuf的内容copy到描述符
 		if (copy_mbuf_to_desc(dev, vq, pkts[pkt_idx],
 						buf_vec, nr_vec,
 						num_buffers) < 0) {
@@ -1030,6 +1087,7 @@ virtio_dev_rx_split(struct virtio_net *dev, struct vhost_virtqueue *vq,
 		vq->last_avail_idx += num_buffers;
 	}
 
+	//完成batch copy
 	do_data_copy_enqueue(dev, vq);
 
 	if (likely(vq->shadow_used_idx)) {
@@ -1180,6 +1238,8 @@ virtio_dev_rx_packed(struct virtio_net *dev,
 	return pkt_idx;
 }
 
+//vhost报文入队处理（发送）
+//向接口vid发送数量为count个的报文pkts，要求报文存入queue_id队列
 static __rte_always_inline uint32_t
 virtio_dev_rx(struct virtio_net *dev, uint16_t queue_id,
 	struct rte_mbuf **pkts, uint32_t count)
@@ -1188,6 +1248,7 @@ virtio_dev_rx(struct virtio_net *dev, uint16_t queue_id,
 	uint32_t nb_tx = 0;
 
 	VHOST_LOG_DEBUG(VHOST_DATA, "(%d) %s\n", dev->vid, __func__);
+	//队列合法性检查，不能是收队列，不能超过合法队列数目
 	if (unlikely(!is_valid_virt_queue_idx(queue_id, 0, dev->nr_vring))) {
 		RTE_LOG(ERR, VHOST_DATA, "(%d) %s: invalid virtqueue idx %d.\n",
 			dev->vid, __func__, queue_id);
@@ -1198,12 +1259,14 @@ virtio_dev_rx(struct virtio_net *dev, uint16_t queue_id,
 
 	rte_spinlock_lock(&vq->access_lock);
 
+	//队列未使能不处理
 	if (unlikely(vq->enabled == 0))
 		goto out_access_unlock;
 
 	if (dev->features & (1ULL << VIRTIO_F_IOMMU_PLATFORM))
 		vhost_user_iotlb_rd_lock(vq);
 
+	//队列目前还不能访问，尝试对队列进行地址转换
 	if (unlikely(vq->access_ok == 0))
 		if (unlikely(vring_translate(dev, vq) < 0))
 			goto out;
@@ -1212,6 +1275,7 @@ virtio_dev_rx(struct virtio_net *dev, uint16_t queue_id,
 	if (count == 0)
 		goto out;
 
+	//报文发送(virtio设备收包处理）
 	if (vq_is_packed(dev))
 		nb_tx = virtio_dev_rx_packed(dev, vq, pkts, count);
 	else
@@ -1227,15 +1291,18 @@ virtio_dev_rx(struct virtio_net *dev, uint16_t queue_id,
 	return nb_tx;
 }
 
+//vhost实现报文发送
 uint16_t
 rte_vhost_enqueue_burst(int vid, uint16_t queue_id,
 	struct rte_mbuf **pkts, uint16_t count)
 {
+	//获取对应的virtio设备
 	struct virtio_net *dev = get_device(vid);
 
 	if (!dev)
 		return 0;
 
+	//如果禁用了内建的virtio_net，则直接返回
 	if (unlikely(!(dev->flags & VIRTIO_DEV_BUILTIN_VIRTIO_NET))) {
 		RTE_LOG(ERR, VHOST_DATA,
 			"(%d) %s: built-in vhost net backend is disabled.\n",
@@ -1243,6 +1310,7 @@ rte_vhost_enqueue_burst(int vid, uint16_t queue_id,
 		return 0;
 	}
 
+	//向virtio_net设备的queue_id队列发送count个报文
 	return virtio_dev_rx(dev, queue_id, pkts, count);
 }
 
@@ -1307,6 +1375,7 @@ parse_ethernet(struct rte_mbuf *m, uint16_t *l4_proto, void **l4_hdr)
 	}
 }
 
+//解析报文，填充mbuf的
 static __rte_always_inline void
 vhost_dequeue_offload(struct virtio_net_hdr *hdr, struct rte_mbuf *m)
 {
@@ -1315,10 +1384,13 @@ vhost_dequeue_offload(struct virtio_net_hdr *hdr, struct rte_mbuf *m)
 	struct rte_tcp_hdr *tcp_hdr = NULL;
 
 	if (hdr->flags == 0 && hdr->gso_type == VIRTIO_NET_HDR_GSO_NONE)
+		//无offload flag,退出
 		return;
 
+	//解析4层协议，解析4层头
 	parse_ethernet(m, &l4_proto, &l4_hdr);
 	if (hdr->flags == VIRTIO_NET_HDR_F_NEEDS_CSUM) {
+		//checksum flags处理，打上相关标记位，交给？？？处理
 		if (hdr->csum_start == (m->l2_len + m->l3_len)) {
 			switch (hdr->csum_offset) {
 			case (offsetof(struct rte_tcp_hdr, cksum)):
@@ -1339,6 +1411,7 @@ vhost_dequeue_offload(struct virtio_net_hdr *hdr, struct rte_mbuf *m)
 		}
 	}
 
+	//gso功能处理
 	if (l4_hdr && hdr->gso_type != VIRTIO_NET_HDR_GSO_NONE) {
 		switch (hdr->gso_type & ~VIRTIO_NET_HDR_GSO_ECN) {
 		case VIRTIO_NET_HDR_GSO_TCPV4:
@@ -1382,6 +1455,7 @@ copy_vnet_hdr_from_desc(struct virtio_net_hdr *hdr,
 	}
 }
 
+//将描述符中指明的报文内存放置在mbuf中，目前支持两种放置方式:1。内存copy；2。zero copy
 static __rte_always_inline int
 copy_desc_to_mbuf(struct virtio_net *dev, struct vhost_virtqueue *vq,
 		  struct buf_vector *buf_vec, uint16_t nr_vec,
@@ -1403,12 +1477,14 @@ copy_desc_to_mbuf(struct virtio_net *dev, struct vhost_virtqueue *vq,
 	buf_iova = buf_vec[vec_idx].buf_iova;
 	buf_len = buf_vec[vec_idx].buf_len;
 
+	//描述长度明显小于vhost_hlen是无效报文
 	if (unlikely(buf_len < dev->vhost_hlen && nr_vec <= 1)) {
 		error = -1;
 		goto out;
 	}
 
 	if (virtio_net_with_host_offload(dev)) {
+		//需要在virtio_net这一层做offload
 		if (unlikely(buf_len < sizeof(struct virtio_net_hdr))) {
 			/*
 			 * No luck, the virtio-net header doesn't fit
@@ -1417,6 +1493,7 @@ copy_desc_to_mbuf(struct virtio_net *dev, struct vhost_virtqueue *vq,
 			copy_vnet_hdr_from_desc(&tmp_hdr, buf_vec);
 			hdr = &tmp_hdr;
 		} else {
+			//长度足够，使hdr直接指向它
 			hdr = (struct virtio_net_hdr *)((uintptr_t)buf_addr);
 		}
 	}
@@ -1426,16 +1503,20 @@ copy_desc_to_mbuf(struct virtio_net *dev, struct vhost_virtqueue *vq,
 	 * for Tx: the first for storing the header, and others
 	 * for storing the data.
 	 */
+	//如上面所言，通常virtio驱动会至少使用2个desc buffer来做传输
+	//1个存储描述信息头部，一个存储报文内容
 	if (unlikely(buf_len < dev->vhost_hlen)) {
+		//当前描述符中buffer的长度不足dev的描述信息头部长度
 		buf_offset = dev->vhost_hlen - buf_len;
-		vec_idx++;
+		vec_idx++;//切到下一个描述符
 		buf_addr = buf_vec[vec_idx].buf_addr;
 		buf_iova = buf_vec[vec_idx].buf_iova;
 		buf_len = buf_vec[vec_idx].buf_len;
-		buf_avail  = buf_len - buf_offset;
+		buf_avail  = buf_len - buf_offset;//更新有效长度（减少）
 	} else if (buf_len == dev->vhost_hlen) {
+		//当前描述符中的buffer长度恰等于dev的描述信息头部长度
 		if (unlikely(++vec_idx >= nr_vec))
-			goto out;
+			goto out;//错误情况
 		buf_addr = buf_vec[vec_idx].buf_addr;
 		buf_iova = buf_vec[vec_idx].buf_iova;
 		buf_len = buf_vec[vec_idx].buf_len;
@@ -1452,10 +1533,14 @@ copy_desc_to_mbuf(struct virtio_net *dev, struct vhost_virtqueue *vq,
 			(uint32_t)buf_avail, 0);
 
 	mbuf_offset = 0;
-	mbuf_avail  = m->buf_len - RTE_PKTMBUF_HEADROOM;
+	mbuf_avail  = m->buf_len - RTE_PKTMBUF_HEADROOM;//mbuf可存储的空间
+
+	//将描述符中指出的数据copy到mbuf中（如果支持zero copy,则直接指向数据即可）
 	while (1) {
 		uint64_t hpa;
 
+		//mbuf有一个自已的长度，对方送过来的报文有一个自已的长度
+		//如果要把对方的报文放入mbuf，那么copy的长度只能是两者中最小的一个。
 		cpy_len = RTE_MIN(buf_avail, mbuf_avail);
 
 		/*
@@ -1463,10 +1548,12 @@ copy_desc_to_mbuf(struct virtio_net *dev, struct vhost_virtqueue *vq,
 		 * not continuous. In such case (gpa_to_hpa returns 0), data
 		 * will be copied even though zero copy is enabled.
 		 */
+		//如果开启了zero copy,直接让mbuf指向这段内存就可以了
 		if (unlikely(dev->dequeue_zero_copy && (hpa = gpa_to_hpa(dev,
 					buf_iova + buf_offset, cpy_len)))) {
 			cur->data_len = cpy_len;
 			cur->data_off = 0;
+			//报文指向这一段内存
 			cur->buf_addr =
 				(void *)(uintptr_t)(buf_addr + buf_offset);
 			cur->buf_iova = hpa;
@@ -1480,6 +1567,7 @@ copy_desc_to_mbuf(struct virtio_net *dev, struct vhost_virtqueue *vq,
 			if (likely(cpy_len > MAX_BATCH_LEN ||
 				   vq->batch_copy_nb_elems >= vq->size ||
 				   (hdr && cur == m))) {
+				//直接将buf_addr指向的内存copy到mbuf中
 				rte_memcpy(rte_pktmbuf_mtod_offset(cur, void *,
 								   mbuf_offset),
 					   (void *)((uintptr_t)(buf_addr +
@@ -1500,14 +1588,17 @@ copy_desc_to_mbuf(struct virtio_net *dev, struct vhost_virtqueue *vq,
 
 		mbuf_avail  -= cpy_len;
 		mbuf_offset += cpy_len;
-		buf_avail -= cpy_len;
+		buf_avail -= cpy_len;//desc中还有多少字节没有放入到mbuf
 		buf_offset += cpy_len;
 
 		/* This buf reaches to its end, get the next one */
+		//当desc_avail为0时，说明所有字节均已放入到mbuf中了，可以处理下一个desc了
 		if (buf_avail == 0) {
 			if (++vec_idx >= nr_vec)
+				//这个desc没有指明有下一个，处理结束，退出循环
 				break;
 
+			//此时buf_vec下的buffer长度不如mbuf的长度大，故需要切换buf_vec,mbuf不用切换
 			buf_addr = buf_vec[vec_idx].buf_addr;
 			buf_iova = buf_vec[vec_idx].buf_iova;
 			buf_len = buf_vec[vec_idx].buf_len;
@@ -1523,6 +1614,7 @@ copy_desc_to_mbuf(struct virtio_net *dev, struct vhost_virtqueue *vq,
 		 * This mbuf reaches to its end, get a new one
 		 * to hold more data.
 		 */
+		//当这种情况发生时，mbuf的缓冲不够用了，为了缓存此报文需要申请mbuf
 		if (mbuf_avail == 0) {
 			cur = rte_pktmbuf_alloc(mbuf_pool);
 			if (unlikely(cur == NULL)) {
@@ -1534,9 +1626,9 @@ copy_desc_to_mbuf(struct virtio_net *dev, struct vhost_virtqueue *vq,
 			if (unlikely(dev->dequeue_zero_copy))
 				rte_mbuf_refcnt_update(cur, 1);
 
-			prev->next = cur;
+			prev->next = cur;//串在mbuf的next链上
 			prev->data_len = mbuf_offset;
-			m->nb_segs += 1;
+			m->nb_segs += 1;//段数量增加
 			m->pkt_len += mbuf_offset;
 			prev = cur;
 
@@ -1549,6 +1641,7 @@ copy_desc_to_mbuf(struct virtio_net *dev, struct vhost_virtqueue *vq,
 	m->pkt_len    += mbuf_offset;
 
 	if (hdr)
+		//处理offload
 		vhost_dequeue_offload(hdr, m);
 
 out:
@@ -1587,6 +1680,7 @@ get_zmbuf(struct vhost_virtqueue *vq)
 	return NULL;
 }
 
+
 static void
 virtio_dev_extbuf_free(void *addr __rte_unused, void *opaque)
 {
@@ -1677,6 +1771,12 @@ virtio_dev_pktmbuf_alloc(struct virtio_net *dev, struct rte_mempool *mp,
 	return NULL;
 }
 
+//自队列vq中收包，首先检查队列中是否entries，vq->avail->ring队列里存放的是描述符索引
+//通过vq->last_avail_idx指针，我们可以拿到首个描述符索引（desc_indexes)，然后通过
+//vq->desc数组取出报文描述符（vq->desc数组中每个elem表示一个报文，如果报文过大时，elem会被组织
+//成一个描述符list,并在链表头上标注VRING_DESC_F_INDIRECT）
+//再由描述符定位到实际的报文，然后copy或者zero copy构建mbuf
+//
 static __rte_noinline uint16_t
 virtio_dev_tx_split(struct virtio_net *dev, struct vhost_virtqueue *vq,
 	struct rte_mempool *mbuf_pool, struct rte_mbuf **pkts, uint16_t count)
@@ -1685,6 +1785,7 @@ virtio_dev_tx_split(struct virtio_net *dev, struct vhost_virtqueue *vq,
 	uint16_t free_entries;
 
 	if (unlikely(dev->dequeue_zero_copy)) {
+		//zero copy代码未读
 		struct zcopy_mbuf *zmbuf, *next;
 
 		for (zmbuf = TAILQ_FIRST(&vq->zmbuf_list);
@@ -1708,10 +1809,11 @@ virtio_dev_tx_split(struct virtio_net *dev, struct vhost_virtqueue *vq,
 		}
 	}
 
+	//获取有多少实体可收取
 	free_entries = *((volatile uint16_t *)&vq->avail->idx) -
 			vq->last_avail_idx;
 	if (free_entries == 0)
-		return 0;
+		return 0;//队列中无报文可收取
 
 	/*
 	 * The ordering between avail index and
@@ -1723,8 +1825,10 @@ virtio_dev_tx_split(struct virtio_net *dev, struct vhost_virtqueue *vq,
 
 	VHOST_LOG_DEBUG(VHOST_DATA, "(%d) %s\n", dev->vid, __func__);
 
+	//将count更新为实际本次可收取的数目
 	count = RTE_MIN(count, MAX_PKT_BURST);
 	count = RTE_MIN(count, free_entries);
+
 	VHOST_LOG_DEBUG(VHOST_DATA, "(%d) about to dequeue %u buffers\n",
 			dev->vid, count);
 
@@ -1735,6 +1839,10 @@ virtio_dev_tx_split(struct virtio_net *dev, struct vhost_virtqueue *vq,
 		uint16_t nr_vec = 0;
 		int err;
 
+		//将vq->last_avail_idx+i描述符指向的buffer地址映射并填充到buf_vec中
+		//注意此此填充仅对应一个报文（但可以是巨帧，此时会有多个mbuf通过next指针串连）
+		//nr_vec用于指出buf_vec中占用的数量
+		//head_idx为首个描述符索引
 		if (unlikely(fill_vec_buf_split(dev, vq,
 						vq->last_avail_idx + i,
 						&nr_vec, buf_vec,
@@ -1743,12 +1851,15 @@ virtio_dev_tx_split(struct virtio_net *dev, struct vhost_virtqueue *vq,
 			break;
 
 		if (likely(dev->dequeue_zero_copy == 0))
+			//记录head_idx已被使用（注意：还没有使用完，还需要使用其对应的buffer)
 			update_shadow_used_ring_split(vq, head_idx, 0);
 
+		//申请mbuf
 		pkts[i] = virtio_dev_pktmbuf_alloc(dev, mbuf_pool, buf_len);
 		if (unlikely(pkts[i] == NULL))
 			break;
 
+		//自mbuf_pool中申请mbuf,并将描述符中指定的数据内容copy到mbuf
 		err = copy_desc_to_mbuf(dev, vq, buf_vec, nr_vec, pkts[i],
 				mbuf_pool);
 		if (unlikely(err)) {
@@ -1779,9 +1890,12 @@ virtio_dev_tx_split(struct virtio_net *dev, struct vhost_virtqueue *vq,
 			TAILQ_INSERT_TAIL(&vq->zmbuf_list, zmbuf, next);
 		}
 	}
+
+	//切换到下一个报文的描述符
 	vq->last_avail_idx += i;
 
 	if (likely(dev->dequeue_zero_copy == 0)) {
+		//做未完成的功能（实现mbuf数据批量copy)
 		do_data_copy_dequeue(vq);
 		if (unlikely(i < count))
 			vq->shadow_used_idx = i;
@@ -2166,6 +2280,8 @@ virtio_dev_tx_packed(struct virtio_net *dev,
 	return pkt_idx;
 }
 
+//自队列queue_id中出队count个报文，将出队的报文存储在pkts中，由于出队会导致ring中无mbuf可填充，故需要
+//在出队后，自mbuf_pool中申请mbuf填充进队列
 uint16_t
 rte_vhost_dequeue_burst(int vid, uint16_t queue_id,
 	struct rte_mempool *mbuf_pool, struct rte_mbuf **pkts, uint16_t count)
@@ -2174,10 +2290,11 @@ rte_vhost_dequeue_burst(int vid, uint16_t queue_id,
 	struct rte_mbuf *rarp_mbuf = NULL;
 	struct vhost_virtqueue *vq;
 
-	dev = get_device(vid);
+	dev = get_device(vid);//取对应的virtio_net设备
 	if (!dev)
 		return 0;
 
+	//内建的virtio_net被禁用，此函数不能工作
 	if (unlikely(!(dev->flags & VIRTIO_DEV_BUILTIN_VIRTIO_NET))) {
 		RTE_LOG(ERR, VHOST_DATA,
 			"(%d) %s: built-in vhost net backend is disabled.\n",
@@ -2185,6 +2302,7 @@ rte_vhost_dequeue_burst(int vid, uint16_t queue_id,
 		return 0;
 	}
 
+	//队列id有效性检查，队列不能是tx队列
 	if (unlikely(!is_valid_virt_queue_idx(queue_id, 1, dev->nr_vring))) {
 		RTE_LOG(ERR, VHOST_DATA, "(%d) %s: invalid virtqueue idx %d.\n",
 			dev->vid, __func__, queue_id);
@@ -2193,17 +2311,21 @@ rte_vhost_dequeue_burst(int vid, uint16_t queue_id,
 
 	vq = dev->virtqueue[queue_id];
 
+	//如果拿不到访问锁，则退出
 	if (unlikely(rte_spinlock_trylock(&vq->access_lock) == 0))
 		return 0;
 
+	//对列未使能，不处理
 	if (unlikely(vq->enabled == 0)) {
 		count = 0;
 		goto out_access_unlock;
 	}
 
+	//iotlb读加锁
 	if (dev->features & (1ULL << VIRTIO_F_IOMMU_PLATFORM))
 		vhost_user_iotlb_rd_lock(vq);
 
+	//如果ring还未进行地址翻译，则尝试翻译，如果翻译失败，则退出
 	if (unlikely(vq->access_ok == 0))
 		if (unlikely(vring_translate(dev, vq) < 0)) {
 			count = 0;
@@ -2214,6 +2336,7 @@ rte_vhost_dequeue_burst(int vid, uint16_t queue_id,
 	 * Construct a RARP broadcast packet, and inject it to the "pkts"
 	 * array, to looks like that guest actually send such packet.
 	 *
+	 * 构造一个rarp广播报文，并将其注入到pkts数组中，使之看起来是guest实际发送出来的报文一样
 	 * Check user_send_rarp() for more information.
 	 *
 	 * broadcast_rarp shares a cacheline in the virtio_net structure
@@ -2225,10 +2348,12 @@ rte_vhost_dequeue_burst(int vid, uint16_t queue_id,
 	 * and only performing cmpset if the read indicates it is likely to
 	 * be set.
 	 */
+	//如果需要广播rarp,则发送rarp(保证仅一个队列发送此rarp报文）
 	if (unlikely(rte_atomic16_read(&dev->broadcast_rarp) &&
 			rte_atomic16_cmpset((volatile uint16_t *)
 				&dev->broadcast_rarp.cnt, 1, 0))) {
 
+		//构造反向arp请求(已知此dev的mac地址，请求此dev的ip地址）
 		rarp_mbuf = rte_net_make_rarp_packet(mbuf_pool, &dev->mac);
 		if (rarp_mbuf == NULL) {
 			RTE_LOG(ERR, VHOST_DATA,
@@ -2236,9 +2361,14 @@ rte_vhost_dequeue_burst(int vid, uint16_t queue_id,
 			count = 0;
 			goto out;
 		}
-		count -= 1;
+		count -= 1;//由于需要注入rarp,故count数必须减1（这里可以通过优先去掉memmove操作）
+		//但这种操作并不常见。
 	}
 
+	//这里提供供了两种tx方式，一种名为split,一种名为packet
+	//简单比对了两者的区别，发现仅仅在通知对端的方式上有差别
+	//另外packet情况下indirect方式下desc的存放方式与split也不相同
+
 	if (vq_is_packed(dev)) {
 		if (unlikely(dev->dequeue_zero_copy))
 			count = virtio_dev_tx_packed_zmbuf(dev, vq, mbuf_pool,
@@ -2262,7 +2392,7 @@ rte_vhost_dequeue_burst(int vid, uint16_t queue_id,
 		 * learning table will get updated first.
 		 */
 		memmove(&pkts[1], pkts, count * sizeof(struct rte_mbuf *));
-		pkts[0] = rarp_mbuf;
+		pkts[0] = rarp_mbuf;//注入构造的rarp_mbuf(注入到pkts 0)
 		count += 1;
 	}
 
diff --git a/mk/internal/rte.build-post.mk b/mk/internal/rte.build-post.mk
index a6b6a9a46..92220c639 100644
--- a/mk/internal/rte.build-post.mk
+++ b/mk/internal/rte.build-post.mk
@@ -11,6 +11,7 @@ _postbuild: $(_BUILD)
 
 else # slower way
 
+#提供在build时执行target钩子点
 _prebuild: $(PREBUILD)
 	@touch _prebuild
 
diff --git a/mk/internal/rte.clean-post.mk b/mk/internal/rte.clean-post.mk
index 77685edd9..f15f7eb6f 100644
--- a/mk/internal/rte.clean-post.mk
+++ b/mk/internal/rte.clean-post.mk
@@ -11,6 +11,7 @@ _postclean: $(_CLEAN)
 
 else # slower way
 
+#提供的clean钩子点
 _preclean: $(PRECLEAN)
 	@touch _preclean
 
diff --git a/mk/internal/rte.compile-pre.mk b/mk/internal/rte.compile-pre.mk
index 0cf3791b4..f58d3e982 100644
--- a/mk/internal/rte.compile-pre.mk
+++ b/mk/internal/rte.compile-pre.mk
@@ -8,6 +8,7 @@
 SRCS-all := $(SRCS-y) $(SRCS-n) $(SRCS-)
 
 # convert source to obj file
+# 将$(1)中含有的%.c替换为.o,用于指定对应的object文件
 src2obj = $(strip $(patsubst %.c,%.o,\
 	$(patsubst %.S,%_s.o,$(1))))
 
@@ -17,35 +18,42 @@ dotfile = $(strip $(foreach f,$(1),\
 
 # convert source/obj files into dot-dep filename (does not
 # include .S files)
+# 将$(1)中含有%.c的替换为%.o.d的依赖文件名称
 src2dep = $(strip $(call dotfile,$(patsubst %.c,%.o.d, \
 		$(patsubst %.S,,$(1)))))
 obj2dep = $(strip $(call dotfile,$(patsubst %.o,%.o.d,$(1))))
 
 # convert source/obj files into dot-cmd filename
+# 将$(1)中含%.c的文件替换为%.o.cmd
 src2cmd = $(strip $(call dotfile,$(patsubst %.c,%.o.cmd, \
 		$(patsubst %.S,%_s.o.cmd,$(1)))))
 obj2cmd = $(strip $(call dotfile,$(patsubst %.o,%.o.cmd,$(1))))
 
+#转换为object,依据srcs-y,srcs-n,srcs-生成object-XX
 OBJS-y := $(call src2obj,$(SRCS-y))
 OBJS-n := $(call src2obj,$(SRCS-n))
 OBJS-  := $(call src2obj,$(SRCS-))
 OBJS-all := $(filter-out $(SRCS-all),$(OBJS-y) $(OBJS-n) $(OBJS-))
 
+#转换对应的依赖文件名称
 DEPS-y := $(call src2dep,$(SRCS-y))
 DEPS-n := $(call src2dep,$(SRCS-n))
 DEPS-  := $(call src2dep,$(SRCS-))
 DEPS-all := $(DEPS-y) $(DEPS-n) $(DEPS-)
 DEPSTMP-all := $(DEPS-all:%.d=%.d.tmp)
 
+#获得对应的cmds文件名称
 CMDS-y := $(call src2cmd,$(SRCS-y))
 CMDS-n := $(call src2cmd,$(SRCS-n))
 CMDS-  := $(call src2cmd,$(SRCS-))
 CMDS-all := $(CMDS-y) $(CMDS-n) $(CMDS-)
 
+#加载并引用依赖文件产生的目标（保证正确的.o依赖）
 -include $(DEPS-y) $(CMDS-y)
 
 # command to compile a .c file to generate an object
 ifeq ($(USE_HOST),1)
+#定义 .c文件到.o文件命令行
 C_TO_O = $(HOSTCC) -Wp,-MD,$(call obj2dep,$(@)).tmp $(HOST_CPPFLAGS) $(HOST_CFLAGS) \
 	$(CFLAGS_$(@)) $(HOST_EXTRA_CPPFLAGS) $(HOST_EXTRA_CFLAGS) -o $@ -c $<
 C_TO_O_STR = $(subst ','\'',$(C_TO_O)) #'# fix syntax highlight
@@ -71,6 +79,7 @@ PMDINFO_TO_O = if grep -q 'RTE_PMD_REGISTER_.*(.*)' $<; then \
 	$(PMDINFO_LD) && \
 	mv -f $@.o $@; fi
 C_TO_O_CMD = 'cmd_$@ = $(C_TO_O_STR)'
+#将C转换为O执行的脚本（包括编译前的输出）
 C_TO_O_DO = @set -e; \
 	echo $(C_TO_O_DISP); \
 	$(C_TO_O) && \
@@ -109,6 +118,7 @@ boolean = $(if $1,1,0)
 # Note: dep_$$@ is from the .d file and DEP_$$@ can be specified by
 # user (by default it is empty)
 #
+#定义c与o之间的依赖关系
 .SECONDEXPANSION:
 %.o: %.c $$(wildcard $$(dep_$$@)) $$(DEP_$$(@)) FORCE
 	@[ -d $(dir $@) ] || mkdir -p $(dir $@)
@@ -148,6 +158,7 @@ S_TO_O_DO = @set -e; \
 # Compile .S file if needed
 # Note: DEP_$$@ can be specified by user (by default it is empty)
 #
+# 定义.S与o之间的关系
 %_s.o: %.S $$(DEP_$$@) FORCE
 	@[ ! -d $(dir $@) ] || mkdir -p $(dir $@)
 	$(if $(D),\
diff --git a/mk/internal/rte.extvars.mk b/mk/internal/rte.extvars.mk
index 98c860628..2b7aeca44 100644
--- a/mk/internal/rte.extvars.mk
+++ b/mk/internal/rte.extvars.mk
@@ -34,6 +34,7 @@ endif
 # $(RTE_SRCDIR)/build
 # Output dir can be given as command line using "O="
 #
+#定义输出目录
 ifdef O
 ifeq ("$(origin O)", "command line")
 RTE_OUTPUT := $(abspath $(O))
diff --git a/mk/internal/rte.install-post.mk b/mk/internal/rte.install-post.mk
index a1aa0ca4f..0070d9167 100644
--- a/mk/internal/rte.install-post.mk
+++ b/mk/internal/rte.install-post.mk
@@ -48,6 +48,7 @@ _postinstall: $(_INSTALL)
 
 else # slower way
 
+#提供install的target钩子点
 _preinstall: $(PREINSTALL)
 	@touch _preinstall
 
diff --git a/mk/rte.cpuflags.mk b/mk/rte.cpuflags.mk
index fa8753531..17086909f 100644
--- a/mk/rte.cpuflags.mk
+++ b/mk/rte.cpuflags.mk
@@ -5,6 +5,7 @@
 # used to set the RTE_CPUFLAG_* environment variables giving details
 # of what instruction sets the target cpu supports.
 
+#-dM 输出预定义的宏
 AUTO_CPUFLAGS := $(shell $(CC) $(MACHINE_CFLAGS) $(WERROR_FLAGS) $(EXTRA_CFLAGS) -dM -E - < /dev/null)
 
 # adding flags to CPUFLAGS
@@ -121,4 +122,5 @@ empty:=
 space:= $(empty) $(empty)
 CPUFLAGSTMP1 := $(addprefix RTE_CPUFLAG_,$(CPUFLAGS))
 CPUFLAGSTMP2 := $(subst $(space),$(comma),$(CPUFLAGSTMP1))
+#将其构造成enum可用语法
 CPUFLAGS_LIST := -DRTE_COMPILE_TIME_CPUFLAGS=$(CPUFLAGSTMP2)
diff --git a/mk/rte.extsubdir.mk b/mk/rte.extsubdir.mk
index 0f8ef94c5..62a46f493 100644
--- a/mk/rte.extsubdir.mk
+++ b/mk/rte.extsubdir.mk
@@ -10,15 +10,18 @@ O ?= $(CURDIR)
 BASE_OUTPUT ?= $(abspath $(O))
 CUR_SUBDIR ?= .
 
+#定义外部目录编译使用的all目标
 .PHONY: all
 all: $(DIRS-y)
 
 .PHONY: clean
 clean: $(DIRS-y)
 
+#编译目标
 .PHONY: $(DIRS-y)
 $(DIRS-y):
 	@echo "== $@"
+	#进入目录$(a),查找对应的makefile:$(CURDIR)/$(@)/Makefile
 	$(Q)$(MAKE) -C $(@) \
 		M=$(CURDIR)/$(@)/Makefile \
 		O=$(BASE_OUTPUT)/$(CUR_SUBDIR)/$(@)/$(RTE_TARGET) \
diff --git a/mk/rte.lib.mk b/mk/rte.lib.mk
index 4df8849a0..4cbf0131b 100644
--- a/mk/rte.lib.mk
+++ b/mk/rte.lib.mk
@@ -18,6 +18,7 @@ endif
 endif
 
 ifeq ($(CONFIG_RTE_BUILD_SHARED_LIB),y)
+#如果编译为共享库时，则更新为.so
 LIB := $(patsubst %.a,%.so.$(LIBABIVER),$(LIB))
 ifeq ($(EXTLIB_BUILD),n)
 ifeq ($(CONFIG_RTE_MAJOR_ABI),)
@@ -29,7 +30,7 @@ CPU_LDFLAGS += --version-script=$(SRCDIR)/$(EXPORT_MAP)
 endif
 endif
 
-
+#同理在rte.build-post.mk中将使_postbuild指向_BUILD变量中的目标
 _BUILD = $(LIB)
 PREINSTALL = $(SYMLINK-FILES-y)
 _INSTALL = $(INSTALL-FILES-y) $(RTE_OUTPUT)/lib/$(LIB)
@@ -37,6 +38,7 @@ _CLEAN = doclean
 
 LDLIBS += $(EXECENV_LDLIBS-y)
 
+#lib方式编译入口
 .PHONY: all
 all: install
 
@@ -64,6 +66,7 @@ else
 _CPU_LDFLAGS := $(CPU_LDFLAGS)
 endif
 
+#object到静态库命令
 O_TO_A = $(AR) crDs $(LIB) $(OBJS-y)
 O_TO_A_STR = $(subst ','\'',$(O_TO_A)) #'# fix syntax highlight
 O_TO_A_DISP = $(if $(V),"$(O_TO_A_STR)","  AR $(@)")
@@ -77,6 +80,7 @@ ifneq ($(CC_SUPPORTS_Z),false)
 NO_UNDEFINED := -z defs
 endif
 
+#obj生成共享库命令
 O_TO_S = $(LD) -L$(RTE_SDK_BIN)/lib $(_CPU_LDFLAGS) $(EXTRA_LDFLAGS) \
 	  -shared $(OBJS-y) $(NO_UNDEFINED) $(LDLIBS) -Wl,-soname,$(LIB) -o $(LIB)
 O_TO_S_STR = $(subst ','\'',$(O_TO_S)) #'# fix syntax highlight
diff --git a/mk/rte.sdkbuild.mk b/mk/rte.sdkbuild.mk
index b512de1ec..fd2340091 100644
--- a/mk/rte.sdkbuild.mk
+++ b/mk/rte.sdkbuild.mk
@@ -25,6 +25,7 @@ test: | lib buildtools drivers
 
 CLEANDIRS = $(addsuffix _clean,$(ROOTDIRS-y) $(ROOTDIRS-n) $(ROOTDIRS-))
 
+#指定build 依赖于多个目标（从GNUmakefile中可知：buildtools lib drivers app）
 .PHONY: build
 build: $(ROOTDIRS-y)
 	@echo "Build complete [$(RTE_TARGET)]"
@@ -37,15 +38,20 @@ clean: $(CLEANDIRS)
 	@[ -d $(RTE_OUTPUT)/include ] || mkdir -p $(RTE_OUTPUT)/include
 	@$(RTE_SDK)/buildtools/gen-config-h.sh $(RTE_OUTPUT)/.config \
 		> $(RTE_OUTPUT)/include/rte_config.h
+	#执行覆盖率清除
 	$(Q)$(MAKE) -f $(RTE_SDK)/GNUmakefile gcovclean
 	@echo Clean complete
 
 .SECONDEXPANSION:
 .PHONY: $(ROOTDIRS-y) $(ROOTDIRS-)
+#各目标处理(源代码编译入口）
 $(ROOTDIRS-y) $(ROOTDIRS-):
+	#创建$(BUILDDIR)/$@目录，开始构造$@
 	@[ -d $(BUILDDIR)/$@ ] || mkdir -p $(BUILDDIR)/$@
 	@echo "== Build $@"
+	#进入$(BUILDDIR)/$@ 目录，采用$(RTE_SRCDIR)/$@/Makefile文件进行处理
 	$(Q)$(MAKE) S=$@ -f $(RTE_SRCDIR)/$@/Makefile -C $(BUILDDIR)/$@ all
+	#如果当前正在构造drivers,则再执行$(RTE_SDK)/mk/rte.combinedlib.mk
 	@if [ $@ = drivers ]; then \
 		$(MAKE) -f $(RTE_SDK)/mk/rte.combinedlib.mk; \
 	fi
diff --git a/mk/rte.sdkconfig.mk b/mk/rte.sdkconfig.mk
index f538649f2..9a0d48cb6 100644
--- a/mk/rte.sdkconfig.mk
+++ b/mk/rte.sdkconfig.mk
@@ -15,10 +15,12 @@ INSTALL_CONFIGS := $(sort $(filter-out %app-icc,$(filter-out %app-clang,\
 	$(wildcard $(RTE_SRCDIR)/config/defconfig_*)))))))
 INSTALL_TARGETS := $(addsuffix _install,$(INSTALL_CONFIGS))
 
+#显示所有配置模板
 .PHONY: showconfigs
 showconfigs:
 	@$(foreach CONFIG, $(INSTALL_CONFIGS), echo $(CONFIG);)
 
+#未指定-T时报错用
 .PHONY: notemplate
 notemplate:
 	@printf "No template specified. Use 'make defconfig' or "
@@ -57,8 +59,10 @@ defconfig:
 
 .PHONY: config
 ifeq ($(RTE_CONFIG_TEMPLATE),)
+#未发现配置模板定义，报错
 config: notemplate
 else
+#配置实际上仅要求rte_config.h已生成，且Makefile已生成
 config: $(RTE_OUTPUT)/include/rte_config.h $(RTE_OUTPUT)/Makefile
 	@echo "Configuration done using" \
 		$(patsubst defconfig_%,%,$(notdir $(RTE_CONFIG_TEMPLATE)))
@@ -79,6 +83,10 @@ else
 # To ensure correct version comparison, we append ".99" to the version number
 # so that the version of a release is higher than that of its rc's.
 $(RTE_OUTPUT)/.config: $(RTE_CONFIG_TEMPLATE) FORCE | $(RTE_OUTPUT)
+	#指定了配置模板，且配置对应的文件存在，用cpp处理模板文件，解开#include指令
+	# 采用awk处理生成的内容（#号开头的行被忽略），目的防止宏定义重复，如果同一变量已定义
+	# 则后面定义的生效。
+	# 然后检查.config_tmp与.config是否相等，如果不相等，则使用新的.config
 	$(Q)if [ "$(RTE_CONFIG_TEMPLATE)" != "" -a -f "$(RTE_CONFIG_TEMPLATE)" ]; then \
 		$(CPP) -undef -P -x assembler-with-cpp \
 		`cat $(RTE_SRCDIR)/VERSION | \
@@ -109,16 +117,22 @@ SDK_RELPATH=$(shell $(RTE_SDK)/buildtools/relpath.sh $(abspath $(RTE_SRCDIR)) \
 				$(abspath $(RTE_OUTPUT)))
 OUTPUT_RELPATH=$(shell $(RTE_SDK)/buildtools/relpath.sh $(abspath $(RTE_OUTPUT)) \
 				$(abspath $(RTE_SRCDIR)))
+#生成output中的makefile文件
+# 用于在output目录下开启编译，并传入O
 $(RTE_OUTPUT)/Makefile: | $(RTE_OUTPUT)
 	$(Q)$(RTE_SDK)/buildtools/gen-build-mk.sh $(SDK_RELPATH) > $@
 
 # clean installed files, and generate a new config header file
 # if NODOTCONF variable is defined, don't try to rebuild .config
 $(RTE_OUTPUT)/include/rte_config.h: $(RTE_OUTPUT)/.config
+	#移除$(RTE_OUTPUT)中目录（防止之前编译过）
 	$(Q)rm -rf $(RTE_OUTPUT)/include $(RTE_OUTPUT)/app \
 		$(RTE_OUTPUT)/lib \
 		$(RTE_OUTPUT)/hostlib $(RTE_OUTPUT)/kmod $(RTE_OUTPUT)/build
 	$(Q)mkdir -p $(RTE_OUTPUT)/include
+	#依据.config文件生成rte_config.h，简单的将其转换为
+	#undef XX
+	#define XX=yy
 	$(Q)$(RTE_SDK)/buildtools/gen-config-h.sh $(RTE_OUTPUT)/.config \
 		> $(RTE_OUTPUT)/include/rte_config.h
 
@@ -131,10 +145,12 @@ headerconfig: $(RTE_OUTPUT)/include/rte_config.h
 # is up to date
 .PHONY: checkconfig
 checkconfig:
+	#检查$(RTE_OUTPUT)/.config文件是否存在,不存在报错
 	@if [ ! -f $(RTE_OUTPUT)/.config ]; then \
 		echo "No .config in build directory"; \
 		exit 1; \
 	fi
+	#检查配置时，不要求生成.config文件,故采用-f指定自身并传入变量
 	$(Q)$(MAKE) -f $(RTE_SDK)/mk/rte.sdkconfig.mk \
 		headerconfig NODOTCONF=1
 
diff --git a/mk/rte.sdkinstall.mk b/mk/rte.sdkinstall.mk
index 32bed5d95..29fb274f1 100644
--- a/mk/rte.sdkinstall.mk
+++ b/mk/rte.sdkinstall.mk
@@ -10,6 +10,7 @@ ifdef T # config, build and install combined
 O ?= .
 RTE_OUTPUT := $O/$T
 else # standard install
+#如果没有指定T，则直接安装在build中
 # Build directory is given with O=
 O ?= build
 RTE_OUTPUT := $O
@@ -75,6 +76,7 @@ ifdef T
 	$(Q)$(MAKE) all O=$(RTE_OUTPUT)
 endif
 
+#仅指出DESTDIR变量即可
 .PHONY: install
 install:
 ifeq ($(DESTDIR)$(if $T,,+),)
@@ -97,7 +99,7 @@ install-runtime:
 	$(Q)$(call rte_mkdir, $(DESTDIR)$(libdir))
 	$(Q)cp $(CP_FLAGS)    $O/lib/* $(DESTDIR)$(libdir)
 	$(Q)$(call rte_mkdir, $(DESTDIR)$(bindir))
-	$(Q)tar -cf -      -C $O --exclude 'app/*.map' \
+	-$(Q)tar -cf -      -C $O --exclude 'app/*.map' \
 		--exclude app/dpdk-pmdinfogen \
 		--exclude 'app/cmdline*' --exclude app/test \
 		--exclude app/testacl --exclude app/testpipeline app | \
diff --git a/mk/rte.sdkroot.mk b/mk/rte.sdkroot.mk
index 4043a9d4e..fab5d00ea 100644
--- a/mk/rte.sdkroot.mk
+++ b/mk/rte.sdkroot.mk
@@ -5,6 +5,7 @@ MAKEFLAGS += --no-print-directory
 
 # define Q to '@' or not. $(Q) is used to prefix all shell commands to
 # be executed silently.
+#如果命令行指定了V,则显示执行的命令
 Q=@
 ifeq '$V' '0'
 override V=
@@ -16,6 +17,7 @@ endif
 endif
 export Q
 
+#如果RTE_SDK未定义，则是报错
 ifeq ($(RTE_SDK),)
 $(error RTE_SDK is not defined)
 endif
@@ -30,6 +32,7 @@ export BUILDING_RTE_SDK
 # We can specify the configuration template when doing the "make
 # config". For instance: make config T=x86_64-native-linux-gcc
 #
+# 如果命令行执行T,则对应到配置模板上。
 RTE_CONFIG_TEMPLATE :=
 ifdef T
 ifeq ("$(origin T)", "command line")
@@ -42,6 +45,7 @@ export RTE_CONFIG_TEMPLATE
 # Default output is $(RTE_SRCDIR)/build
 # output files wil go in a separate directory
 #
+# 如果命令行指定了o,则指定输出，否则按默认输出到$(RTE_SRCDIR)/build
 ifdef O
 ifeq ("$(origin O)", "command line")
 RTE_OUTPUT := $(abspath $(O))
@@ -57,21 +61,30 @@ export BUILDDIR
 
 export ROOTDIRS-y ROOTDIRS- ROOTDIRS-n
 
+#定义首目标default,其依赖于all
 .PHONY: default test-build
 default test-build: all
 
+#
+# 下面的代码对目标进行分类
+#
+
+#配置版本类信息显示相关
 .PHONY: config defconfig showconfigs showversion showversionum
 config defconfig showconfigs showversion showversionum:
 	$(Q)$(MAKE) -f $(RTE_SDK)/mk/rte.sdkconfig.mk $@
 
+#代码阅读器相关
 .PHONY: cscope gtags tags etags
 cscope gtags tags etags:
 	$(Q)$(RTE_SDK)/devtools/build-tags.sh $@ $T
 
+#测试相关
 .PHONY: test test-fast test-perf coverage test-drivers test-dump
 test test-fast test-perf coverage test-drivers test-dump:
 	$(Q)$(MAKE) -f $(RTE_SDK)/mk/rte.sdktest.mk $@
 
+#安装相关
 .PHONY: install
 install:
 	$(Q)$(MAKE) -f $(RTE_SDK)/mk/rte.sdkinstall.mk pre_install
@@ -79,21 +92,27 @@ install:
 install-%:
 	$(Q)$(MAKE) -f $(RTE_SDK)/mk/rte.sdkinstall.mk $@
 
+#文档相关
 .PHONY: doc help
 doc: doc-all
 help: doc-help
 doc-%:
 	$(Q)$(MAKE) -f $(RTE_SDK)/mk/rte.sdkdoc.mk $*
 
+# 覆盖率相关
 .PHONY: gcov gcovclean
 gcov gcovclean:
 	$(Q)$(MAKE) -f $(RTE_SDK)/mk/rte.sdkgcov.mk $@
 
+#示例代码相关
 .PHONY: examples examples_clean
 examples examples_clean:
 	$(Q)$(MAKE) -f $(RTE_SDK)/mk/rte.sdkexamples.mk $@
 
 # all other build targets
+# 其它类目标,目标"all"将走此流程
 %:
+	#执行checkconfig，确保config.h生成，且已配置
 	$(Q)$(MAKE) -f $(RTE_SDK)/mk/rte.sdkconfig.mk checkconfig
+	#执行相应目标$@
 	$(Q)$(MAKE) -f $(RTE_SDK)/mk/rte.sdkbuild.mk $@
diff --git a/mk/rte.subdir.mk b/mk/rte.subdir.mk
index d6e64a246..b9b86ac45 100644
--- a/mk/rte.subdir.mk
+++ b/mk/rte.subdir.mk
@@ -14,10 +14,12 @@ ALL_DEPDIRS := $(patsubst DEPDIRS-%,%,$(filter DEPDIRS-%,$(.VARIABLES)))
 CLEANDIRS = $(addsuffix _clean,$(DIRS-y) $(DIRS-n) $(DIRS-))
 
 VPATH += $(SRCDIR)
+#由_postbuild目标在rte.build-post.mk中我们会依赖于_BUILD目标，故所有子目录将被编译
 _BUILD = $(DIRS-y)
 _INSTALL = $(INSTALL-FILES-y) $(SYMLINK-FILES-y)
 _CLEAN = $(CLEANDIRS)
 
+#定义编译入口all目标
 .PHONY: all
 all: install
 
@@ -31,8 +33,10 @@ build: _postbuild
 
 .SECONDEXPANSION:
 .PHONY: $(DIRS-y)
+#进入各子目录编译目标
 $(DIRS-y):
 	@[ -d $(CURDIR)/$@ ] || mkdir -p $(CURDIR)/$@
+	#进行指定源代码子目录进行编译
 	@echo "== Build $S/$@"
 	@$(MAKE) S=$S/$@ -f $(SRCDIR)/$@/Makefile -C $(CURDIR)/$@ all
 
diff --git a/mk/rte.vars.mk b/mk/rte.vars.mk
index 07b0db127..6a974ae38 100644
--- a/mk/rte.vars.mk
+++ b/mk/rte.vars.mk
@@ -7,6 +7,8 @@
 # config file of SDK. It also includes the config file from external
 # application if any.
 #
+#通过载入.config文件完成配置变量的定义，每个模块在编译时，均会先引入此makefile完成
+#配置变量的定义。
 
 ifeq ($(RTE_SDK),)
 $(error RTE_SDK is not defined)
@@ -30,6 +32,9 @@ export Q
 
 # if we are building SDK, only includes SDK configuration
 ifneq ($(BUILDING_RTE_SDK),)
+  #将.config载入做为设置的变量，在sangfor做开发时，有好多的变量需要又在makefile中定义，
+  # 又在gcc 编译时通过-D方式做为宏定义传入，dpdk在这里也有这个问题，dpdk的处理是，采用
+  # config方式，生成.config（makefile用）文件与config.h（gcc编译用）来解决这个问题
   include $(RTE_OUTPUT)/.config
   # remove double-quotes from config names
   RTE_ARCH := $(CONFIG_RTE_ARCH:"%"=%)
@@ -39,6 +44,7 @@ ifneq ($(BUILDING_RTE_SDK),)
   RTE_SDK_BIN := $(RTE_OUTPUT)
 endif
 
+#定义target
 RTE_TARGET ?= $(RTE_ARCH)-$(RTE_MACHINE)-$(RTE_EXEC_ENV)-$(RTE_TOOLCHAIN)
 
 ifeq ($(BUILDING_RTE_SDK),)
@@ -79,10 +85,12 @@ export RTE_TOOLCHAIN
 
 # developer build automatically enabled in a git tree
 ifneq ($(wildcard $(RTE_SDK)/.git),)
+#如果在sdk目录发现.git目录，则自动认为是devel build
 RTE_DEVEL_BUILD ?= y
 endif
 
 # SRCDIR is the current source directory
+# 用于指向当前编译的目录
 ifdef S
 SRCDIR := $(abspath $(RTE_SRCDIR)/$(S))
 else
diff --git a/usertools/dpdk-devbind.py b/usertools/dpdk-devbind.py
index 7b5cbc12c..9f9f73d7d 100755
--- a/usertools/dpdk-devbind.py
+++ b/usertools/dpdk-devbind.py
@@ -1,4 +1,5 @@
 #! /usr/bin/env python
+# encoding:utf-8
 # SPDX-License-Identifier: BSD-3-Clause
 # Copyright(c) 2010-2014 Intel Corporation
 #
@@ -48,6 +49,7 @@
 intel_ntb_skx = {'Class': '06', 'Vendor': '8086', 'Device': '201c',
               'SVendor': None, 'SDevice': None}
 
+#网络设备（pci中class为02的设备）
 network_devices = [network_class, cavium_pkx, avp_vnic, ifpga_class]
 baseband_devices = [acceleration_class]
 crypto_devices = [encryption_class, intel_processor_class]
@@ -160,11 +162,13 @@ def module_is_loaded(module):
     sysfs_path = '/sys/module/'
 
     # Get the list of directories in sysfs_path
+    # 收集sysfs_path下所有目录
     sysfs_mods = [m for m in os.listdir(sysfs_path)
                   if os.path.isdir(os.path.join(sysfs_path, m))]
 
     # special case for vfio_pci (module is named vfio-pci,
     # but its .ko is named vfio_pci)
+    # 将vfio_pci更改为vfio-pci
     sysfs_mods = [a if a != 'vfio_pci' else 'vfio-pci' for a in sysfs_mods]
 
     loaded_modules = sysfs_mods
@@ -181,14 +185,17 @@ def check_modules():
 
     # first check if module is loaded
     for mod in mods:
+        #如果sysfs_mods中包含mod["Name"]，则将其置为发现
         if module_is_loaded(mod["Name"]):
             mod["Found"] = True
 
     # check if we have at least one loaded module
     if True not in [mod["Found"] for mod in mods] and b_flag is not None:
+        #指定的driver，不被dpdk支持
         print("Warning: no supported DPDK kernel modules are loaded", file=sys.stderr)
 
     # change DPDK driver list to only contain drivers that are loaded
+    #将driver变量缩小为被发现的模块。
     dpdk_drivers = [mod["Name"] for mod in mods if mod["Found"]]
 
 
@@ -240,8 +247,38 @@ def get_device_details(devices_type):
     # request machine readable format, with numeric IDs and String
     dev = {}
     dev_lines = check_output(["lspci", "-Dvmmnnk"]).splitlines()
+    """
+    显示格式如下示：
+Slot:    0000:00:00.0
+Class:    Host bridge [0600]
+Vendor:    Intel Corporation [8086]
+Device:    440FX - 82441FX PMC [Natoma] [1237]
+SVendor:    Red Hat, Inc [1af4]
+SDevice:    Qemu virtual machine [1100]
+Rev:    02
+
+Slot:    0000:00:01.0
+Class:    ISA bridge [0601]
+Vendor:    Intel Corporation [8086]
+Device:    82371SB PIIX3 ISA [Natoma/Triton II] [7000]
+SVendor:    Red Hat, Inc [1af4]
+SDevice:    Qemu virtual machine [1100]
+
+Slot:    0000:82:00.1
+Class:    Ethernet controller [0200]
+Vendor:    Broadcom Corporation [14e4]
+Device:    Device [16d7]
+SVendor:    Broadcom Corporation [14e4]
+SDevice:    Device [1402]
+PhySlot:    42
+Rev:    01
+Driver:    bnxt_en
+    """
+
+
     for dev_line in dev_lines:
         if len(dev_line) == 0:
+            #一个块结束了
             if device_type_match(dev, devices_type):
                 # Replace "Driver" with "Driver_str" to have consistency of
                 # of dictionary key names
@@ -250,6 +287,7 @@ def get_device_details(devices_type):
                 if "Module" in dev.keys():
                     dev["Module_str"] = dev.pop("Module")
                 # use dict to make copy of dev
+                #记录设备情况（pci address)
                 devices[dev["Slot"]] = dict(dev)
             # Clear previous device's data
             dev = {}
@@ -310,13 +348,16 @@ def get_device_details(devices_type):
                 devices[d]["Module_str"] = ",".join(modules)
 
 
+#检查设备是否可与devices_type相匹配
 def device_type_match(dev, devices_type):
     for i in range(len(devices_type)):
         param_count = len(
             [x for x in devices_type[i].values() if x is not None])
         match_count = 0
         if dev["Class"][0:2] == devices_type[i]["Class"]:
+            #class中的前两个字节于devices_type中的class相等时进入，例如Class:    Ethernet controller [0200]
             match_count = match_count + 1
+            #其它key比对
             for key in devices_type[i].keys():
                 if key != 'Class' and devices_type[i][key]:
                     value_list = devices_type[i][key].split(',')
@@ -413,6 +454,7 @@ def bind_one(dev_id, driver, force):
                       % (dev_id, filename), file=sys.stderr)
                 return
             try:
+                #指定解绑定哪种driver
                 f.write("%s" % driver)
                 f.close()
             except:
@@ -449,6 +491,7 @@ def bind_one(dev_id, driver, force):
             bind_one(dev_id, saved_driver, force)
         return
     try:
+        #进行绑定
         f.write(dev_id)
         f.close()
     except:
@@ -619,6 +662,7 @@ def show_status():
     kernel driver or to no driver'''
 
     if status_dev == "net" or status_dev == "all":
+        #显示设备状态
         show_device_status(network_devices, "Network")
 
     if status_dev == "baseband" or status_dev == "all":
@@ -661,9 +705,11 @@ def parse_args():
         sys.exit(1)
 
     for opt, arg in opts:
+        #对help的处理
         if opt == "--help" or opt == "--usage":
             usage()
             sys.exit(0)
+        #检查其它的命令行参数的合法性
         if opt == "--status-dev":
             status_flag = True
             status_dev = arg
@@ -674,6 +720,7 @@ def parse_args():
             force_flag = True
         if opt == "-b" or opt == "-u" or opt == "--bind" or opt == "--unbind":
             if b_flag is not None:
+                #多次指出或者矛盾指出
                 sys.exit("Error: binding and unbinding are mutually exclusive")
             if opt == "-u" or opt == "--unbind":
                 b_flag = "none"
@@ -702,8 +749,10 @@ def do_arg_actions():
     if b_flag == "none" or b_flag == "None":
         unbind_all(args, force_flag)
     elif b_flag is not None:
+        #实现binding
         bind_all(args, b_flag, force_flag)
     if status_flag:
+        #执行./dpdk-devbind.py --status
         if b_flag is not None:
             clear_data()
             # refresh if we have changed anything
